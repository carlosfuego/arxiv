[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v2",
                "updated": "2025-01-12T20:08:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    8,
                    46,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08329v1",
                "updated": "2025-01-14T18:59:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    5,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:59:05Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    5,
                    1,
                    14,
                    0
                ],
                "title": "Predicting 4D Hand Trajectory from Monocular Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting 4D Hand Trajectory from Monocular Videos"
                },
                "summary": "We present HaPTIC, an approach that infers coherent 4D hand trajectories from\nmonocular videos. Current video-based hand pose reconstruction methods\nprimarily focus on improving frame-wise 3D pose using adjacent frames rather\nthan studying consistent 4D hand trajectories in space. Despite the additional\ntemporal cues, they generally underperform compared to image-based methods due\nto the scarcity of annotated video data. To address these issues, we repurpose\na state-of-the-art image-based transformer to take in multiple frames and\ndirectly predict a coherent trajectory. We introduce two types of lightweight\nattention layers: cross-view self-attention to fuse temporal information, and\nglobal cross-attention to bring in larger spatial context. Our method infers 4D\nhand trajectories similar to the ground truth while maintaining strong 2D\nreprojection alignment. We apply the method to both egocentric and allocentric\nvideos. It significantly outperforms existing methods in global trajectory\naccuracy while being comparable to the state-of-the-art in single-image pose\nestimation. Project website: https://judyye.github.io/haptic-www",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HaPTIC, an approach that infers coherent 4D hand trajectories from\nmonocular videos. Current video-based hand pose reconstruction methods\nprimarily focus on improving frame-wise 3D pose using adjacent frames rather\nthan studying consistent 4D hand trajectories in space. Despite the additional\ntemporal cues, they generally underperform compared to image-based methods due\nto the scarcity of annotated video data. To address these issues, we repurpose\na state-of-the-art image-based transformer to take in multiple frames and\ndirectly predict a coherent trajectory. We introduce two types of lightweight\nattention layers: cross-view self-attention to fuse temporal information, and\nglobal cross-attention to bring in larger spatial context. Our method infers 4D\nhand trajectories similar to the ground truth while maintaining strong 2D\nreprojection alignment. We apply the method to both egocentric and allocentric\nvideos. It significantly outperforms existing methods in global trajectory\naccuracy while being comparable to the state-of-the-art in single-image pose\nestimation. Project website: https://judyye.github.io/haptic-www"
                },
                "authors": [
                    {
                        "name": "Yufei Ye"
                    },
                    {
                        "name": "Yao Feng"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "Haiwen Feng"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    },
                    {
                        "name": "Michael J. Black"
                    }
                ],
                "author_detail": {
                    "name": "Michael J. Black"
                },
                "author": "Michael J. Black",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08328v1",
                "updated": "2025-01-14T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokerBench: Training Large Language Models to become Professional Poker\n  Players"
                },
                "summary": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}."
                },
                "authors": [
                    {
                        "name": "Richard Zhuang"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Richard Yang"
                    },
                    {
                        "name": "Aniket Rahane"
                    },
                    {
                        "name": "Zhengyu Li"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08324v1",
                "updated": "2025-01-14T18:56:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations"
                },
                "summary": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics."
                },
                "authors": [
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Vishaldeep Kaur Sekhon"
                    },
                    {
                        "name": "Ouyang Guo"
                    },
                    {
                        "name": "Mark Newman"
                    },
                    {
                        "name": "Roozbeh Sadeghian"
                    },
                    {
                        "name": "Maria L. Vaida"
                    },
                    {
                        "name": "Cynthia Jo"
                    },
                    {
                        "name": "Doyle Ward"
                    },
                    {
                        "name": "Vanni Bucci"
                    },
                    {
                        "name": "John P. Haran"
                    }
                ],
                "author_detail": {
                    "name": "John P. Haran"
                },
                "author": "John P. Haran",
                "arxiv_comment": "16 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08322v1",
                "updated": "2025-01-14T18:55:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:55:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data"
                },
                "summary": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages."
                },
                "authors": [
                    {
                        "name": "Amirhossein Aliakbarzadeh"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08321v1",
                "updated": "2025-01-14T18:55:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "FCC-ee Sensitivity to Flavor-Agnostic Standard Model Effective Field\n  Theory Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FCC-ee Sensitivity to Flavor-Agnostic Standard Model Effective Field\n  Theory Operators"
                },
                "summary": "We present a calculation of the sensitivity of the Future Circular $e^+ e^-$\nCollider (FCC-ee) to the interactions of new, heavy particles in terms of\npublicly available extensions to the smelli and flavio computer programs. We\nparameterize new particles' effects with dimension-6 operators of the Standard\nModel Effective Field Theory (SMEFT) without any flavor assumptions and take\ninto account the projected experimental and theoretical uncertainties of\nvarious electroweak and Higgs observables at the proposed collider. We\nillustrate a use of the calculation by estimating the sensitivity of the FCC-ee\nto a family non-universal $Z^\\prime$ model that explains anomalies inferred\nfrom present-day measurements and Standard Model predictions of observables\nthat involve the $ b \\rightarrow s \\ell^+ \\ell^-$ transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a calculation of the sensitivity of the Future Circular $e^+ e^-$\nCollider (FCC-ee) to the interactions of new, heavy particles in terms of\npublicly available extensions to the smelli and flavio computer programs. We\nparameterize new particles' effects with dimension-6 operators of the Standard\nModel Effective Field Theory (SMEFT) without any flavor assumptions and take\ninto account the projected experimental and theoretical uncertainties of\nvarious electroweak and Higgs observables at the proposed collider. We\nillustrate a use of the calculation by estimating the sensitivity of the FCC-ee\nto a family non-universal $Z^\\prime$ model that explains anomalies inferred\nfrom present-day measurements and Standard Model predictions of observables\nthat involve the $ b \\rightarrow s \\ell^+ \\ell^-$ transition."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Eetu Loisa"
                    }
                ],
                "author_detail": {
                    "name": "Eetu Loisa"
                },
                "author": "Eetu Loisa",
                "arxiv_comment": "7 pages, 1 figure. To access the new tools, see\n  https://github.com/eetuloisa/smelli_fcc and\n  https://github.com/eetuloisa/flavio_fcc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08320v1",
                "updated": "2025-01-14T18:53:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    22,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:53:22Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    22,
                    1,
                    14,
                    0
                ],
                "title": "COMBO and COMMA: R packages for regression modeling and inference in the\n  presence of misclassified binary mediator or outcome variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMBO and COMMA: R packages for regression modeling and inference in the\n  presence of misclassified binary mediator or outcome variables"
                },
                "summary": "Misclassified binary outcome or mediator variables can cause unpredictable\nbias in resulting parameter estimates. As more datasets that were not\noriginally collected for research purposes are being used for studies in the\nsocial and health sciences, the need for methods that address data quality\nconcerns is growing. In this paper, we describe two R packages, COMBO and\nCOMMA, that implement bias-correction methods for misclassified binary outcome\nand mediator variables, respectively. These likelihood-based approaches do not\nrequire gold standard measures and allow for estimation of sensitivity and\nspecificity rates for the misclassified variable(s). In addition, these R\npackages automatically apply crucial label switching corrections, allowing\nresearchers to circumvent the inherent permutation invariance of the\nmisclassification model likelihood. We demonstrate COMBO for single-outcome\ncases using a study of bar exam passage. We develop and evaluate a risk\nprediction model based on noisy indicators in a pretrial risk assessment study\nto demonstrate COMBO for multi-outcome cases. In addition, we use COMMA to\nevaluate the mediating effect of potentially misdiagnosed gestational\nhypertension on the maternal ethnicity-birthweight relationship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misclassified binary outcome or mediator variables can cause unpredictable\nbias in resulting parameter estimates. As more datasets that were not\noriginally collected for research purposes are being used for studies in the\nsocial and health sciences, the need for methods that address data quality\nconcerns is growing. In this paper, we describe two R packages, COMBO and\nCOMMA, that implement bias-correction methods for misclassified binary outcome\nand mediator variables, respectively. These likelihood-based approaches do not\nrequire gold standard measures and allow for estimation of sensitivity and\nspecificity rates for the misclassified variable(s). In addition, these R\npackages automatically apply crucial label switching corrections, allowing\nresearchers to circumvent the inherent permutation invariance of the\nmisclassification model likelihood. We demonstrate COMBO for single-outcome\ncases using a study of bar exam passage. We develop and evaluate a risk\nprediction model based on noisy indicators in a pretrial risk assessment study\nto demonstrate COMBO for multi-outcome cases. In addition, we use COMMA to\nevaluate the mediating effect of potentially misdiagnosed gestational\nhypertension on the maternal ethnicity-birthweight relationship."
                },
                "authors": [
                    {
                        "name": "Kimberly A. Hochstedler Webb"
                    },
                    {
                        "name": "Martin T. Wells"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Wells"
                },
                "author": "Martin T. Wells",
                "arxiv_comment": "99 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08319v1",
                "updated": "2025-01-14T18:53:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions"
                },
                "summary": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\"."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Roy Mayan"
                    },
                    {
                        "name": "Chen Agassy"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07169v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07169v3",
                "updated": "2025-01-14T18:51:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    51,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-10T04:03:46Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    3,
                    46,
                    1,
                    345,
                    0
                ],
                "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation"
                },
                "summary": "Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications."
                },
                "authors": [
                    {
                        "name": "Tal Zeevi"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Lawrence H. Staib"
                    },
                    {
                        "name": "John A. Onofrey"
                    }
                ],
                "author_detail": {
                    "name": "John A. Onofrey"
                },
                "author": "John A. Onofrey",
                "arxiv_comment": "Updated author affiliation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07169v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07169v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08313v1",
                "updated": "2025-01-14T18:50:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    50,
                    5,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:50:05Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    50,
                    5,
                    1,
                    14,
                    0
                ],
                "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniMax-01: Scaling Foundation Models with Lightning Attention"
                },
                "summary": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI."
                },
                "authors": [
                    {
                        "name": "MiniMax"
                    },
                    {
                        "name": "Aonian Li"
                    },
                    {
                        "name": "Bangwei Gong"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Boji Shan"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Cheng Zhu"
                    },
                    {
                        "name": "Chunhao Zhang"
                    },
                    {
                        "name": "Congchao Guo"
                    },
                    {
                        "name": "Da Chen"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Enwei Jiao"
                    },
                    {
                        "name": "Gengxin Li"
                    },
                    {
                        "name": "Guojun Zhang"
                    },
                    {
                        "name": "Haohai Sun"
                    },
                    {
                        "name": "Houze Dong"
                    },
                    {
                        "name": "Jiadai Zhu"
                    },
                    {
                        "name": "Jiaqi Zhuang"
                    },
                    {
                        "name": "Jiayuan Song"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Jingtao Han"
                    },
                    {
                        "name": "Jingyang Li"
                    },
                    {
                        "name": "Junbin Xie"
                    },
                    {
                        "name": "Junhao Xu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Kaishun Zhang"
                    },
                    {
                        "name": "Kecheng Xiao"
                    },
                    {
                        "name": "Kexi Kang"
                    },
                    {
                        "name": "Le Han"
                    },
                    {
                        "name": "Leyang Wang"
                    },
                    {
                        "name": "Lianfei Yu"
                    },
                    {
                        "name": "Liheng Feng"
                    },
                    {
                        "name": "Lin Zheng"
                    },
                    {
                        "name": "Linbo Chai"
                    },
                    {
                        "name": "Long Xing"
                    },
                    {
                        "name": "Meizhi Ju"
                    },
                    {
                        "name": "Mingyuan Chi"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Peikai Huang"
                    },
                    {
                        "name": "Pengcheng Niu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Pengyu Zhao"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qidi Xu"
                    },
                    {
                        "name": "Qiexiang Wang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Qiuhui Li"
                    },
                    {
                        "name": "Ruitao Leng"
                    },
                    {
                        "name": "Shengmin Shi"
                    },
                    {
                        "name": "Shuqi Yu"
                    },
                    {
                        "name": "Sichen Li"
                    },
                    {
                        "name": "Songquan Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Tianrun Liang"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Weiyu Cheng"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Xiangjun Song"
                    },
                    {
                        "name": "Xiao Su"
                    },
                    {
                        "name": "Xiaodong Han"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Xinzhu Hou"
                    },
                    {
                        "name": "Xu Min"
                    },
                    {
                        "name": "Xun Zou"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yan Gong"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yongyi Hu"
                    },
                    {
                        "name": "Yuanxiang Fan"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Yuhao Li"
                    },
                    {
                        "name": "Yunan Huang"
                    },
                    {
                        "name": "Yunji Li"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Yunzhi Xu"
                    },
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zehan Li"
                    },
                    {
                        "name": "Zekang Li"
                    },
                    {
                        "name": "Zewei Tao"
                    },
                    {
                        "name": "Zewen Ying"
                    },
                    {
                        "name": "Zhaoyang Cong"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Zhenhua Fan"
                    },
                    {
                        "name": "Zhihang Yu"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Zijia Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zijia Wu"
                },
                "author": "Zijia Wu",
                "arxiv_comment": "A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-sourced our MiniMax-01 at\n  https://github.com/MiniMax-AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08292v1",
                "updated": "2025-01-14T18:13:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:13:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them"
                },
                "summary": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models."
                },
                "authors": [
                    {
                        "name": "Abhilasha Ravichander"
                    },
                    {
                        "name": "Shrusti Ghela"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08284v1",
                "updated": "2025-01-14T18:00:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages"
                },
                "summary": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate"
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Saminu Mohammad Aliyu"
                    },
                    {
                        "name": "Nelson Odhiambo Onyango"
                    },
                    {
                        "name": "Lilian D. A. Wanzare"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Esubalew Alemneh"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Hagos Tesfahun Gebremichael"
                    },
                    {
                        "name": "Elyas Abdi Ismail"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Ebrahim Chekol Jibril"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Abigail Oppong"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tesfa Tegegne Asfaw"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08276v1",
                "updated": "2025-01-14T17:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research."
                },
                "authors": [
                    {
                        "name": "Pulkit Arora"
                    },
                    {
                        "name": "Akbar Karimi"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08274v1",
                "updated": "2025-01-14T17:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:46:19Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "title": "Constructing optimal dynamic monitoring and treatment regimes: An\n  application to hypertension care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing optimal dynamic monitoring and treatment regimes: An\n  application to hypertension care"
                },
                "summary": "Hypertension is a leading cause of cardiovascular diseases and morbidity,\nwith antihypertensive drugs and blood pressure management strategies having\nheterogeneous effects on patients. Previous authors exploited this\nheterogeneity to construct optimal dynamic treatment regimes for hypertension\nthat input patient characteristics and output the best drug or blood pressure\nmanagement strategy to prescribe. There is, however, a lack of research on\noptimizing monitoring schedules for these patients. It is unclear whether\ndifferent monitoring patterns and drug add-on strategies could lower blood\npressure differently across patients. We propose a new consistent methodology\nto develop optimal dynamic monitoring and add-on regimes that is doubly-robust\nand relies on the theory of Robins' g-methods and dynamic weighted ordinary\nleast squares. We discuss the treatment of longitudinal missing data for that\ninference. The approach is evaluated in large simulation studies and applied to\ndata from the SPRINT trial in the United States to derive a new optimal rule.\nThis type of rule could be used by patients or physicians to personalize the\ntiming of visit and by physicians to decide whether prescribing an\nantihypertensive drug is beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypertension is a leading cause of cardiovascular diseases and morbidity,\nwith antihypertensive drugs and blood pressure management strategies having\nheterogeneous effects on patients. Previous authors exploited this\nheterogeneity to construct optimal dynamic treatment regimes for hypertension\nthat input patient characteristics and output the best drug or blood pressure\nmanagement strategy to prescribe. There is, however, a lack of research on\noptimizing monitoring schedules for these patients. It is unclear whether\ndifferent monitoring patterns and drug add-on strategies could lower blood\npressure differently across patients. We propose a new consistent methodology\nto develop optimal dynamic monitoring and add-on regimes that is doubly-robust\nand relies on the theory of Robins' g-methods and dynamic weighted ordinary\nleast squares. We discuss the treatment of longitudinal missing data for that\ninference. The approach is evaluated in large simulation studies and applied to\ndata from the SPRINT trial in the United States to derive a new optimal rule.\nThis type of rule could be used by patients or physicians to personalize the\ntiming of visit and by physicians to decide whether prescribing an\nantihypertensive drug is beneficial."
                },
                "authors": [
                    {
                        "name": "Janie Coulombe"
                    },
                    {
                        "name": "Dany El-Riachi"
                    },
                    {
                        "name": "Fanxing Du"
                    },
                    {
                        "name": "Tianze Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Tianze Jiao"
                },
                "author": "Tianze Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08476v2",
                "updated": "2025-01-14T17:46:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    46,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-12T17:59:04Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    59,
                    4,
                    2,
                    164,
                    0
                ],
                "title": "RMem: Restricted Memory Banks Improve Video Object Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMem: Restricted Memory Banks Improve Video Object Segmentation"
                },
                "summary": "With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/."
                },
                "authors": [
                    {
                        "name": "Junbao Zhou"
                    },
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "CVPR 2024, Project Page: https://restricted-memory.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08270v1",
                "updated": "2025-01-14T17:32:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    32,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:32:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    32,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Individual causal effect estimation accounting for latent disease state\n  modification among bipolar participants in mobile health studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individual causal effect estimation accounting for latent disease state\n  modification among bipolar participants in mobile health studies"
                },
                "summary": "Individuals with bipolar disorder tend to cycle through disease states such\nas depression and mania. The heterogeneous nature of disease across states\ncomplicates the evaluation of interventions for bipolar disorder patients, as\nvaried interventional success is observed within and across individuals. In\nfact, we hypothesize that disease state acts as an effect modifier for the\ncausal effect of a given intervention on health outcomes. To address this\ndilemma, we propose an N-of-1 approach using an adapted autoregressive hidden\nMarkov model, applied to longitudinal mobile health data collected from\nindividuals with bipolar disorder. This method allows us to identify a latent\nvariable from mobile health data to be treated as an effect modifier between\nthe exposure and outcome of interest while allowing for missing data in the\noutcome. A counterfactual approach is employed for causal inference and to\nobtain a g-formula estimator to recover said effect. The performance of the\nproposed method is compared with a naive approach across extensive simulations\nand application to a multi-year smartphone study of bipolar patients,\nevaluating the individual effect of digital social activity on sleep duration\nacross different latent disease states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals with bipolar disorder tend to cycle through disease states such\nas depression and mania. The heterogeneous nature of disease across states\ncomplicates the evaluation of interventions for bipolar disorder patients, as\nvaried interventional success is observed within and across individuals. In\nfact, we hypothesize that disease state acts as an effect modifier for the\ncausal effect of a given intervention on health outcomes. To address this\ndilemma, we propose an N-of-1 approach using an adapted autoregressive hidden\nMarkov model, applied to longitudinal mobile health data collected from\nindividuals with bipolar disorder. This method allows us to identify a latent\nvariable from mobile health data to be treated as an effect modifier between\nthe exposure and outcome of interest while allowing for missing data in the\noutcome. A counterfactual approach is employed for causal inference and to\nobtain a g-formula estimator to recover said effect. The performance of the\nproposed method is compared with a naive approach across extensive simulations\nand application to a multi-year smartphone study of bipolar patients,\nevaluating the individual effect of digital social activity on sleep duration\nacross different latent disease states."
                },
                "authors": [
                    {
                        "name": "Charlotte R. Fowler"
                    },
                    {
                        "name": "Xiaoxuan Cai"
                    },
                    {
                        "name": "Habiballah Rahimi-Eichi"
                    },
                    {
                        "name": "Lisa Dixon"
                    },
                    {
                        "name": "Dost Ongur"
                    },
                    {
                        "name": "Justin T. Baker"
                    },
                    {
                        "name": "Jukka-Pekka Onnela"
                    },
                    {
                        "name": "Linda Valeri"
                    }
                ],
                "author_detail": {
                    "name": "Linda Valeri"
                },
                "author": "Linda Valeri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08262v1",
                "updated": "2025-01-14T17:21:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:21:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Fan"
                },
                "author": "Zhong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02748v3",
                "updated": "2025-01-14T17:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    20,
                    4,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-03T17:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation"
                },
                "summary": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo"
                },
                "authors": [
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "Accepted to AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08261v1",
                "updated": "2025-01-14T17:15:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    15,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:15:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    15,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "GPU-accelerated LISA parameter estimation with full time domain response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated LISA parameter estimation with full time domain response"
                },
                "summary": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy."
                },
                "authors": [
                    {
                        "name": "Cecilio García-Quirós"
                    },
                    {
                        "name": "Shubhanshu Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Shubhanshu Tiwari"
                },
                "author": "Shubhanshu Tiwari",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16260v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16260v3",
                "updated": "2025-01-14T16:55:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    55,
                    46,
                    1,
                    14,
                    0
                ],
                "published": "2023-12-26T05:56:13Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    5,
                    56,
                    13,
                    1,
                    360,
                    0
                ],
                "title": "Multinomial Link Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multinomial Link Models"
                },
                "summary": "We propose a new class of regression models for analyzing categorical\nresponses, called multinomial link models. It consists of four subclasses,\nincluding mixed-link models that generalize existing multinomial logistic\nmodels and their extensions, two-group models that can incorporate the\nobservations with NA or unknown responses, multinomial conditional link models\nthat handle longitudinal categorical responses, and po-npo mixture models that\nextend partial proportional odds models. We provide explicit formulae and\ndetailed algorithms for finding the maximum likelihood estimates of the model\nparameters and computing the Fisher information matrix. Our algorithms solve\nthe infeasibility issue of existing statistical software when estimating\nparameters of cumulative link models. The applications to real datasets show\nthat the new models can fit the data significantly better, correct misleading\nconclusions due to missing responses, and make more informative inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new class of regression models for analyzing categorical\nresponses, called multinomial link models. It consists of four subclasses,\nincluding mixed-link models that generalize existing multinomial logistic\nmodels and their extensions, two-group models that can incorporate the\nobservations with NA or unknown responses, multinomial conditional link models\nthat handle longitudinal categorical responses, and po-npo mixture models that\nextend partial proportional odds models. We provide explicit formulae and\ndetailed algorithms for finding the maximum likelihood estimates of the model\nparameters and computing the Fisher information matrix. Our algorithms solve\nthe infeasibility issue of existing statistical software when estimating\nparameters of cumulative link models. The applications to real datasets show\nthat the new models can fit the data significantly better, correct misleading\nconclusions due to missing responses, and make more informative inference."
                },
                "authors": [
                    {
                        "name": "Tianmeng Wang"
                    },
                    {
                        "name": "Liping Tong"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "48 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16260v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16260v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v4",
                "updated": "2025-01-14T16:47:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    47,
                    44,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Preprint. First two authors contributed equally to this work. Update:\n  add USiT (UViT+SiT sampler) results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08246v1",
                "updated": "2025-01-14T16:32:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:32:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints"
                },
                "summary": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt."
                },
                "authors": [
                    {
                        "name": "Jonathan Nöther"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Goran Radanović"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanović"
                },
                "author": "Goran Radanović",
                "arxiv_comment": "This is an extended version of a paper published at AAAI 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08243v1",
                "updated": "2025-01-14T16:30:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:30:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps"
                },
                "summary": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows."
                },
                "authors": [
                    {
                        "name": "Kannan Parthasarathy"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Venkat Krishnamachari"
                    },
                    {
                        "name": "Basil Muhammed"
                    },
                    {
                        "name": "Adyansh Kakran"
                    },
                    {
                        "name": "Sreemaee Akshathala"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Sumant Dubey"
                    },
                    {
                        "name": "Mohan Veerubhotla"
                    },
                    {
                        "name": "Amey Karan"
                    }
                ],
                "author_detail": {
                    "name": "Amey Karan"
                },
                "author": "Amey Karan",
                "arxiv_comment": "The paper has been accepted as full paper to CAIN 2025\n  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025\n  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN\n  for review on 9 November 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14831v3",
                "updated": "2025-01-14T16:17:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    17,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-23T17:47:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    47,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models"
                },
                "summary": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG."
                },
                "authors": [
                    {
                        "name": "Bernal Jiménez Gutiérrez"
                    },
                    {
                        "name": "Yiheng Shu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "NeurIPS 2024. Code and data:\n  https://github.com/OSU-NLP-Group/HippoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v1",
                "updated": "2025-01-14T16:02:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14012v2",
                "updated": "2025-01-14T15:58:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    58,
                    2,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-21T10:54:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Logic Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Augmented Generation"
                },
                "summary": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results."
                },
                "authors": [
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08208v1",
                "updated": "2025-01-14T15:46:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08203v1",
                "updated": "2025-01-14T15:38:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:38:41Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving"
                },
                "summary": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances."
                },
                "authors": [
                    {
                        "name": "Zain Ul Abedin"
                    },
                    {
                        "name": "Shahzeb Qamar"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08201v1",
                "updated": "2025-01-14T15:36:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    36,
                    32,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:36:32Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    36,
                    32,
                    1,
                    14,
                    0
                ],
                "title": "Globally Convergent Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Globally Convergent Variational Inference"
                },
                "summary": "In variational inference (VI), an approximation of the posterior distribution\nis selected from a family of distributions through numerical optimization. With\nthe most common variational objective function, known as the evidence lower\nbound (ELBO), only convergence to a local optimum can be guaranteed. In this\nwork, we instead establish the global convergence of a particular VI method.\nThis VI method, which may be considered an instance of neural posterior\nestimation (NPE), minimizes an expectation of the inclusive (forward) KL\ndivergence to fit a variational distribution that is parameterized by a neural\nnetwork. Our convergence result relies on the neural tangent kernel (NTK) to\ncharacterize the gradient dynamics that arise from considering the variational\nobjective in function space. In the asymptotic regime of a fixed,\npositive-definite neural tangent kernel, we establish conditions under which\nthe variational objective admits a unique solution in a reproducing kernel\nHilbert space (RKHS). Then, we show that the gradient descent dynamics in\nfunction space converge to this unique function. In ablation studies and\npractical problems, we demonstrate that our results explain the behavior of NPE\nin non-asymptotic finite-neuron settings, and show that NPE outperforms\nELBO-based optimization, which often converges to shallow local optima.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In variational inference (VI), an approximation of the posterior distribution\nis selected from a family of distributions through numerical optimization. With\nthe most common variational objective function, known as the evidence lower\nbound (ELBO), only convergence to a local optimum can be guaranteed. In this\nwork, we instead establish the global convergence of a particular VI method.\nThis VI method, which may be considered an instance of neural posterior\nestimation (NPE), minimizes an expectation of the inclusive (forward) KL\ndivergence to fit a variational distribution that is parameterized by a neural\nnetwork. Our convergence result relies on the neural tangent kernel (NTK) to\ncharacterize the gradient dynamics that arise from considering the variational\nobjective in function space. In the asymptotic regime of a fixed,\npositive-definite neural tangent kernel, we establish conditions under which\nthe variational objective admits a unique solution in a reproducing kernel\nHilbert space (RKHS). Then, we show that the gradient descent dynamics in\nfunction space converge to this unique function. In ablation studies and\npractical problems, we demonstrate that our results explain the behavior of NPE\nin non-asymptotic finite-neuron settings, and show that NPE outperforms\nELBO-based optimization, which often converges to shallow local optima."
                },
                "authors": [
                    {
                        "name": "Declan McNamara"
                    },
                    {
                        "name": "Jackson Loper"
                    },
                    {
                        "name": "Jeffrey Regier"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Regier"
                },
                "author": "Jeffrey Regier",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03565v3",
                "updated": "2025-01-14T15:30:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    30,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-04T16:20:34Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    16,
                    20,
                    34,
                    3,
                    95,
                    0
                ],
                "title": "Personalized LLM Response Generation with Parameterized Memory Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM Response Generation with Parameterized Memory Injection"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP})."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08200v1",
                "updated": "2025-01-14T15:27:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    27,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:27:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    27,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval ."
                },
                "authors": [
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Leyi Cui"
                    },
                    {
                        "name": "Kele Huang"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "to be published in LLM4Code 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08197v1",
                "updated": "2025-01-14T15:22:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    22,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:22:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    22,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ziyun Dai"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "The datasets are available on\n  https://huggingface.co/collections/opencsg/chinese-fineweb-66cfed105f502ece8f29643e\n  ; The code is on https://github.com/yuyijiong/fineweb-edu-chinese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v3",
                "updated": "2025-01-14T15:19:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    19,
                    52,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08188v1",
                "updated": "2025-01-14T15:13:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    13,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:13:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    13,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation"
                },
                "summary": "While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems."
                },
                "authors": [
                    {
                        "name": "Steven Landgraf"
                    },
                    {
                        "name": "Rongjun Qin"
                    },
                    {
                        "name": "Markus Ulrich"
                    }
                ],
                "author_detail": {
                    "name": "Markus Ulrich"
                },
                "author": "Markus Ulrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07572v2",
                "updated": "2025-01-14T15:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    6,
                    56,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-13T18:58:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "WebWalker: Benchmarking LLMs in Web Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebWalker: Benchmarking LLMs in Web Traversal"
                },
                "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06350v2",
                "updated": "2025-01-14T14:58:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    58,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2024-01-12T03:56:32Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    3,
                    56,
                    32,
                    4,
                    12,
                    0
                ],
                "title": "Optimal estimation of the null distribution in large-scale inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal estimation of the null distribution in large-scale inference"
                },
                "summary": "The advent of large-scale inference has spurred reexamination of conventional\nstatistical thinking. In a Gaussian model for $n$ many $z$-scores with at most\n$k < \\frac{n}{2}$ nonnulls, Efron suggests estimating the location and scale\nparameters of the null distribution. Placing no assumptions on the nonnull\neffects, the statistical task can be viewed as a robust estimation problem.\nHowever, the best known robust estimators fail to be consistent in the regime\n$k \\asymp n$ which is especially relevant in large-scale inference. The failure\nof estimators which are minimax rate-optimal with respect to other formulations\nof robustness (e.g. Huber's contamination model) might suggest the\nimpossibility of consistent estimation in this regime and, consequently, a\nmajor weakness of Efron's suggestion. A sound evaluation of Efron's model thus\nrequires a complete understanding of consistency. We sharply characterize the\nregime of $k$ for which consistent estimation is possible and further establish\nthe minimax estimation rates. It is shown consistent estimation of the location\nparameter is possible if and only if $\\frac{n}{2} - k = \\omega(\\sqrt{n})$, and\nconsistent estimation of the scale parameter is possible in the entire regime\n$k < \\frac{n}{2}$. Faster rates than those in Huber's contamination model are\nachievable by exploiting the Gaussian character of the data. The minimax upper\nbound is obtained by considering estimators based on the empirical\ncharacteristic function. The minimax lower bound involves constructing two\nmarginal distributions whose characteristic functions match on a wide interval\ncontaining zero. The construction notably differs from those in the literature\nby sharply capturing a scaling of $n-2k$ in the minimax estimation rate of the\nlocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large-scale inference has spurred reexamination of conventional\nstatistical thinking. In a Gaussian model for $n$ many $z$-scores with at most\n$k < \\frac{n}{2}$ nonnulls, Efron suggests estimating the location and scale\nparameters of the null distribution. Placing no assumptions on the nonnull\neffects, the statistical task can be viewed as a robust estimation problem.\nHowever, the best known robust estimators fail to be consistent in the regime\n$k \\asymp n$ which is especially relevant in large-scale inference. The failure\nof estimators which are minimax rate-optimal with respect to other formulations\nof robustness (e.g. Huber's contamination model) might suggest the\nimpossibility of consistent estimation in this regime and, consequently, a\nmajor weakness of Efron's suggestion. A sound evaluation of Efron's model thus\nrequires a complete understanding of consistency. We sharply characterize the\nregime of $k$ for which consistent estimation is possible and further establish\nthe minimax estimation rates. It is shown consistent estimation of the location\nparameter is possible if and only if $\\frac{n}{2} - k = \\omega(\\sqrt{n})$, and\nconsistent estimation of the scale parameter is possible in the entire regime\n$k < \\frac{n}{2}$. Faster rates than those in Huber's contamination model are\nachievable by exploiting the Gaussian character of the data. The minimax upper\nbound is obtained by considering estimators based on the empirical\ncharacteristic function. The minimax lower bound involves constructing two\nmarginal distributions whose characteristic functions match on a wide interval\ncontaining zero. The construction notably differs from those in the literature\nby sharply capturing a scaling of $n-2k$ in the minimax estimation rate of the\nlocation."
                },
                "authors": [
                    {
                        "name": "Subhodh Kotekal"
                    },
                    {
                        "name": "Chao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Gao"
                },
                "author": "Chao Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08167v1",
                "updated": "2025-01-14T14:49:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    49,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:49:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    49,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data"
                },
                "summary": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases."
                },
                "authors": [
                    {
                        "name": "Rewina Bedemariam"
                    },
                    {
                        "name": "Natalie Perez"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Satya Kapoor"
                    },
                    {
                        "name": "Alex Gil"
                    },
                    {
                        "name": "Elizabeth Conjar"
                    },
                    {
                        "name": "Ikkei Itoku"
                    },
                    {
                        "name": "David Theil"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Naumaan Nayyar"
                    }
                ],
                "author_detail": {
                    "name": "Naumaan Nayyar"
                },
                "author": "Naumaan Nayyar",
                "arxiv_comment": "11 pages, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08165v1",
                "updated": "2025-01-14T14:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:46:19Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "title": "I Can Find You in Seconds! Leveraging Large Language Models for Code\n  Authorship Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can Find You in Seconds! Leveraging Large Language Models for Code\n  Authorship Attribution"
                },
                "summary": "Source code authorship attribution is important in software forensics,\nplagiarism detection, and protecting software patch integrity. Existing\ntechniques often rely on supervised machine learning, which struggles with\ngeneralization across different programming languages and coding styles due to\nthe need for large labeled datasets. Inspired by recent advances in natural\nlanguage authorship analysis using large language models (LLMs), which have\nshown exceptional performance without task-specific tuning, this paper explores\nthe use of LLMs for source code authorship attribution.\n  We present a comprehensive study demonstrating that state-of-the-art LLMs can\nsuccessfully attribute source code authorship across different languages. LLMs\ncan determine whether two code snippets are written by the same author with\nzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of\n0.78, and can attribute code authorship from a small set of reference code\nsnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show\nsome adversarial robustness against misattribution attacks.\n  Despite these capabilities, we found that naive prompting of LLMs does not\nscale well with a large number of authors due to input token limitations. To\naddress this, we propose a tournament-style approach for large-scale\nattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355\nsamples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve\nclassification accuracy of up to 65% for C++ and 68.7% for Java using only one\nreference per author. These results open new possibilities for applying LLMs to\ncode authorship attribution in cybersecurity and software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code authorship attribution is important in software forensics,\nplagiarism detection, and protecting software patch integrity. Existing\ntechniques often rely on supervised machine learning, which struggles with\ngeneralization across different programming languages and coding styles due to\nthe need for large labeled datasets. Inspired by recent advances in natural\nlanguage authorship analysis using large language models (LLMs), which have\nshown exceptional performance without task-specific tuning, this paper explores\nthe use of LLMs for source code authorship attribution.\n  We present a comprehensive study demonstrating that state-of-the-art LLMs can\nsuccessfully attribute source code authorship across different languages. LLMs\ncan determine whether two code snippets are written by the same author with\nzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of\n0.78, and can attribute code authorship from a small set of reference code\nsnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show\nsome adversarial robustness against misattribution attacks.\n  Despite these capabilities, we found that naive prompting of LLMs does not\nscale well with a large number of authors due to input token limitations. To\naddress this, we propose a tournament-style approach for large-scale\nattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355\nsamples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve\nclassification accuracy of up to 65% for C++ and 68.7% for Java using only one\nreference per author. These results open new possibilities for applying LLMs to\ncode authorship attribution in cybersecurity and software engineering."
                },
                "authors": [
                    {
                        "name": "Soohyeon Choi"
                    },
                    {
                        "name": "Yong Kiam Tan"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Mohamed Ragab"
                    },
                    {
                        "name": "Soumik Mondal"
                    },
                    {
                        "name": "David Mohaisen"
                    },
                    {
                        "name": "Khin Mi Mi Aung"
                    }
                ],
                "author_detail": {
                    "name": "Khin Mi Mi Aung"
                },
                "author": "Khin Mi Mi Aung",
                "arxiv_comment": "12 pages, 5 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08156v1",
                "updated": "2025-01-14T14:31:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    31,
                    45,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:31:45Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    31,
                    45,
                    1,
                    14,
                    0
                ],
                "title": "Inference-Time-Compute: More Faithful? A Research Note",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time-Compute: More Faithful? A Research Note"
                },
                "summary": "Models trained specifically to generate long Chains of Thought (CoTs) have\nrecently achieved impressive results. We refer to these models as\nInference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful\ncompared to traditional non-ITC models? We evaluate two ITC models (based on\nQwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure\nfaithfulness, we test if models articulate cues in their prompt that influence\ntheir answers to MMLU questions. For example, when the cue \"A Stanford\nProfessor thinks the answer is D'\" is added to the prompt, models sometimes\nswitch their answer to D. In such cases, the Gemini ITC model articulates the\ncue 54% of the time, compared to 14% for the non-ITC Gemini.\n  We evaluate 7 types of cue, such as misleading few-shot examples and\nanchoring on past responses. ITC models articulate cues that influence them\nmuch more reliably than all the 6 non-ITC models tested, such as\nClaude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.\n  However, our study has important limitations. We evaluate only two ITC models\n-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the\ntraining of these ITC models, making it hard to attribute our findings to\nspecific processes.\n  We think faithfulness of CoT is an important property for AI Safety. The ITC\nmodels we tested show a large improvement in faithfulness, which is worth\ninvestigating further. To speed up this investigation, we release these early\nresults as a research note.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models trained specifically to generate long Chains of Thought (CoTs) have\nrecently achieved impressive results. We refer to these models as\nInference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful\ncompared to traditional non-ITC models? We evaluate two ITC models (based on\nQwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure\nfaithfulness, we test if models articulate cues in their prompt that influence\ntheir answers to MMLU questions. For example, when the cue \"A Stanford\nProfessor thinks the answer is D'\" is added to the prompt, models sometimes\nswitch their answer to D. In such cases, the Gemini ITC model articulates the\ncue 54% of the time, compared to 14% for the non-ITC Gemini.\n  We evaluate 7 types of cue, such as misleading few-shot examples and\nanchoring on past responses. ITC models articulate cues that influence them\nmuch more reliably than all the 6 non-ITC models tested, such as\nClaude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.\n  However, our study has important limitations. We evaluate only two ITC models\n-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the\ntraining of these ITC models, making it hard to attribute our findings to\nspecific processes.\n  We think faithfulness of CoT is an important property for AI Safety. The ITC\nmodels we tested show a large improvement in faithfulness, which is worth\ninvestigating further. To speed up this investigation, we release these early\nresults as a research note."
                },
                "authors": [
                    {
                        "name": "James Chua"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14403v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14403v5",
                "updated": "2025-01-14T14:29:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    29,
                    36,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-18T12:06:39Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    6,
                    39,
                    4,
                    292,
                    0
                ],
                "title": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust"
                },
                "summary": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared to a\ncollisional-radiative model setup using the same rate data as SOLPS-ITER,\nindicating some qualitative agreements and disagreements, potentially\nhighlighting model gaps with regard to the default rates used.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared to a\ncollisional-radiative model setup using the same rate data as SOLPS-ITER,\nindicating some qualitative agreements and disagreements, potentially\nhighlighting model gaps with regard to the default rates used.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance."
                },
                "authors": [
                    {
                        "name": "N. Osborne"
                    },
                    {
                        "name": "K. Verhaegh"
                    },
                    {
                        "name": "D. Moulton"
                    },
                    {
                        "name": "H. Reimerdes"
                    },
                    {
                        "name": "P. Ryan"
                    },
                    {
                        "name": "N. Lonigro"
                    },
                    {
                        "name": "S. Mijin"
                    },
                    {
                        "name": "R. Osawa"
                    },
                    {
                        "name": "K. Murray"
                    },
                    {
                        "name": "S. Kobussen"
                    },
                    {
                        "name": "Y. Damizia"
                    },
                    {
                        "name": "A. Perek"
                    },
                    {
                        "name": "C. Theiler"
                    },
                    {
                        "name": "R. Ducker"
                    },
                    {
                        "name": "D. Mykytchuk"
                    }
                ],
                "author_detail": {
                    "name": "D. Mykytchuk"
                },
                "author": "D. Mykytchuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14403v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14403v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08152v1",
                "updated": "2025-01-14T14:26:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:26:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "Energy Backdoor Attack to Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Backdoor Attack to Deep Neural Networks"
                },
                "summary": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor."
                },
                "authors": [
                    {
                        "name": "Hanene F. Z. Brachemi Meftah"
                    },
                    {
                        "name": "Wassim Hamidouche"
                    },
                    {
                        "name": "Sid Ahmed Fezza"
                    },
                    {
                        "name": "Olivier Déforges"
                    },
                    {
                        "name": "Kassem Kallas"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Kallas"
                },
                "author": "Kassem Kallas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16779v2",
                "updated": "2025-01-14T14:26:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2024-08-15T16:41:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    41,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis"
                },
                "summary": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs."
                },
                "authors": [
                    {
                        "name": "João Pedro Gandarela"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08145v1",
                "updated": "2025-01-14T14:23:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    23,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:23:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    23,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "Refusal Behavior in Large Language Models: A Nonlinear Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal Behavior in Large Language Models: A Nonlinear Perspective"
                },
                "summary": "Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies."
                },
                "authors": [
                    {
                        "name": "Fabian Hildebrandt"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Patrick Krauss"
                    },
                    {
                        "name": "Achim Schilling"
                    }
                ],
                "author_detail": {
                    "name": "Achim Schilling"
                },
                "author": "Achim Schilling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08141v1",
                "updated": "2025-01-14T14:21:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    21,
                    37,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:21:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    21,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Field Structures In and Around Seyfert Galaxy Outflows"
                },
                "summary": "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments."
                },
                "authors": [
                    {
                        "name": "Salmoli Ghosh"
                    },
                    {
                        "name": "P. Kharb"
                    },
                    {
                        "name": "B. Sebastian"
                    },
                    {
                        "name": "J. Gallimore"
                    },
                    {
                        "name": "A. Pasetto"
                    },
                    {
                        "name": "C. P. O'Dea"
                    },
                    {
                        "name": "T. Heckman"
                    },
                    {
                        "name": "S. A. Baum"
                    }
                ],
                "author_detail": {
                    "name": "S. A. Baum"
                },
                "arxiv_affiliation": "University of Manitoba",
                "author": "S. A. Baum",
                "arxiv_comment": "36 pages, 20 figures, 6 tables. Accepted for publication in The\n  Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13612v2",
                "updated": "2025-01-14T14:16:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    16,
                    45,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-18T08:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models"
                },
                "summary": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "12 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08120v1",
                "updated": "2025-01-14T13:52:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    52,
                    41,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T13:52:41Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    52,
                    41,
                    1,
                    14,
                    0
                ],
                "title": "In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR"
                },
                "summary": "The pursuit of automated scientific discovery has fueled progress from\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\nrecognition. Transformers function as potential systems, where every possible\nrelationship remains latent potentiality until tasks impose constraints, akin\nto measurement. Yet, refining their sampling requires more than probabilistic\nselection: solutions must conform to specific structures or rules, ensuring\nconsistency and the invocation of general principles. We present\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\nExploratory Optimization of Reasoning), a framework that combines graph\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\nultimately, final answers. Inspired by category theory, it encodes concepts as\nnodes and their relationships as edges, supporting hierarchical inference and\nadaptive learning through isomorphic representations. Demonstrations include\nhypothesis generation, materials design, and creative reasoning, such as\ndiscovering relationships between mythological concepts like 'thin places' with\nmaterials science. We propose a 'knowledge garden growth' strategy that\nintegrates insights across domains, promoting interdisciplinary connections.\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\nreasoning depth and adaptability, underscoring the potential for transparent,\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\nautonomous reasoning solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of automated scientific discovery has fueled progress from\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\nrecognition. Transformers function as potential systems, where every possible\nrelationship remains latent potentiality until tasks impose constraints, akin\nto measurement. Yet, refining their sampling requires more than probabilistic\nselection: solutions must conform to specific structures or rules, ensuring\nconsistency and the invocation of general principles. We present\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\nExploratory Optimization of Reasoning), a framework that combines graph\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\nultimately, final answers. Inspired by category theory, it encodes concepts as\nnodes and their relationships as edges, supporting hierarchical inference and\nadaptive learning through isomorphic representations. Demonstrations include\nhypothesis generation, materials design, and creative reasoning, such as\ndiscovering relationships between mythological concepts like 'thin places' with\nmaterials science. We propose a 'knowledge garden growth' strategy that\nintegrates insights across domains, promoting interdisciplinary connections.\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\nreasoning depth and adaptability, underscoring the potential for transparent,\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\nautonomous reasoning solutions."
                },
                "authors": [
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08107v1",
                "updated": "2025-01-14T13:36:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    36,
                    25,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T13:36:25Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    36,
                    25,
                    1,
                    14,
                    0
                ],
                "title": "Superoxide anion (O$_{2}\\negthinspace^{-}$) collisions with CO$_{2}$\n  molecules in the energy range 50-950 eV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superoxide anion (O$_{2}\\negthinspace^{-}$) collisions with CO$_{2}$\n  molecules in the energy range 50-950 eV"
                },
                "summary": "A novel gas-phase molecular scattering study is reported for\nO$_{2}\\negthinspace^{-}$ colliding with CO$_{2}$ for impact energies ranging\nfrom 50 to 950 eV. The absolute total electron detachment, relative total and\npartial ionization cross sections have been measured within this energy range\nand the positive ion yield of those produced during the collisions has been\nobtained. The primary anionic beam projectile is produced in a pulsed hollow\ncathode discharge induced plasma, and its interactions with the neutral\nmolecular target occur in a gas cell at a well-known constant pressure. For\nimpact energies above 500 eV high mass (m $>$ 44 u) charged complexes have been\ndetected. With the aid of a theoretical study, using ab initio methods, we\npropose a mechanism to infer on the formation of these cationic species, which\nhave been assigned as projectile-target stable compounds\n(CO$_{3}\\negthinspace^{+}$ and CO$_{4}\\negthinspace^{+}$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel gas-phase molecular scattering study is reported for\nO$_{2}\\negthinspace^{-}$ colliding with CO$_{2}$ for impact energies ranging\nfrom 50 to 950 eV. The absolute total electron detachment, relative total and\npartial ionization cross sections have been measured within this energy range\nand the positive ion yield of those produced during the collisions has been\nobtained. The primary anionic beam projectile is produced in a pulsed hollow\ncathode discharge induced plasma, and its interactions with the neutral\nmolecular target occur in a gas cell at a well-known constant pressure. For\nimpact energies above 500 eV high mass (m $>$ 44 u) charged complexes have been\ndetected. With the aid of a theoretical study, using ab initio methods, we\npropose a mechanism to infer on the formation of these cationic species, which\nhave been assigned as projectile-target stable compounds\n(CO$_{3}\\negthinspace^{+}$ and CO$_{4}\\negthinspace^{+}$)."
                },
                "authors": [
                    {
                        "name": "C. Guerra"
                    },
                    {
                        "name": "M. Leiferman"
                    },
                    {
                        "name": "A. I. Lozano"
                    },
                    {
                        "name": "F. Aguilar-Galindo"
                    },
                    {
                        "name": "S. Díaz-Tendero"
                    },
                    {
                        "name": "J. C. Oller"
                    },
                    {
                        "name": "P. Limão-Vieira"
                    },
                    {
                        "name": "G. García"
                    }
                ],
                "author_detail": {
                    "name": "G. García"
                },
                "author": "G. García",
                "arxiv_comment": "16 pages, 7 figures and 3 tables. This article has been submitted to\n  the Journal of Chemical Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v1",
                "updated": "2025-01-14T13:19:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08090v1",
                "updated": "2025-01-14T12:57:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    57,
                    40,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:57:40Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    57,
                    40,
                    1,
                    14,
                    0
                ],
                "title": "Hierarchical Autoscaling for Large Language Model Serving with Chiron",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Autoscaling for Large Language Model Serving with Chiron"
                },
                "summary": "Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions."
                },
                "authors": [
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Dhemath Reddy"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    },
                    {
                        "name": "Zbigniew Kalbarczyk"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar Iyer"
                },
                "author": "Ravishankar Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11531v2",
                "updated": "2025-01-14T12:56:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    56,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-18T12:40:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality"
                },
                "summary": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval."
                },
                "authors": [
                    {
                        "name": "Viktoriia Chekalina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Kuznetsov"
                },
                "author": "Andrey Kuznetsov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02953v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02953v4",
                "updated": "2025-01-14T12:55:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    55,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2023-10-04T16:44:23Z",
                "published_parsed": [
                    2023,
                    10,
                    4,
                    16,
                    44,
                    23,
                    2,
                    277,
                    0
                ],
                "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning"
                },
                "summary": "Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Guizhen Chen"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02953v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02953v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00376v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00376v3",
                "updated": "2025-01-14T12:37:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    37,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-01T09:01:53Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    9,
                    1,
                    53,
                    4,
                    61,
                    0
                ],
                "title": "Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model"
                },
                "summary": "Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Long-Kai Huang"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Bingzhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhe Wu"
                },
                "author": "Bingzhe Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00376v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00376v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08071v1",
                "updated": "2025-01-14T12:36:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    36,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:36:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    36,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically."
                },
                "authors": [
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "cgo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08068v1",
                "updated": "2025-01-14T12:34:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    34,
                    25,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:34:25Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    34,
                    25,
                    1,
                    14,
                    0
                ],
                "title": "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches."
                },
                "authors": [
                    {
                        "name": "Israel Puerta-Merino"
                    },
                    {
                        "name": "Carlos Núñez-Molina"
                    },
                    {
                        "name": "Pablo Mesejo"
                    },
                    {
                        "name": "Juan Fernández-Olivares"
                    }
                ],
                "author_detail": {
                    "name": "Juan Fernández-Olivares"
                },
                "author": "Juan Fernández-Olivares",
                "arxiv_comment": "5 pages, 0 figures, to be published in the AAAI Workshop on Planning\n  in the Era of LLMs ( https://llmforplanning.github.io )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08831v2",
                "updated": "2025-01-14T12:28:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    28,
                    21,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-11T14:08:17Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    8,
                    17,
                    4,
                    285,
                    0
                ],
                "title": "Distribution-free uncertainty quantification for inverse problems:\n  application to weak lensing mass mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-free uncertainty quantification for inverse problems:\n  application to weak lensing mass mapping"
                },
                "summary": "In inverse problems, distribution-free uncertainty quantification (UQ) aims\nto obtain error bars with coverage guarantees that are independent of any prior\nassumptions about the data distribution. In the context of mass mapping,\nuncertainties could lead to errors that affects our understanding of the\nunderlying mass distribution, or could propagate to cosmological parameter\nestimation, thereby impacting the precision and reliability of cosmological\nmodels. Current surveys, such as Euclid or Rubin, will provide new weak lensing\ndatasets of very high quality. Accurately quantifying uncertainties in mass\nmaps is therefore critical to perform reliable cosmological parameter\ninference. In this paper, we extend the conformalized quantile regression (CQR)\nalgorithm, initially proposed for scalar regression, to inverse problems. We\ncompare our approach with another distribution-free approach based on\nrisk-controlling prediction sets (RCPS). Both methods are based on a\ncalibration dataset, and offer finite-sample coverage guarantees that are\nindependent of the data distribution. Furthermore, they are applicable to any\nmass mapping method, including blackbox predictors. In our experiments, we\napply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative\nWiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS\ntends to produce overconservative confidence bounds with small calibration\nsets, whereas CQR is designed to avoid this issue. Although the expected\nmiscoverage rate is guaranteed to stay below a user-prescribed threshold\nregardless of the mass mapping method, selecting an appropriate reconstruction\nalgorithm remains crucial for obtaining accurate estimates, especially around\npeak-like structures, which are particularly important for inferring\ncosmological parameters. Additionally, the choice of mass mapping method\ninfluences the size of the error bars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In inverse problems, distribution-free uncertainty quantification (UQ) aims\nto obtain error bars with coverage guarantees that are independent of any prior\nassumptions about the data distribution. In the context of mass mapping,\nuncertainties could lead to errors that affects our understanding of the\nunderlying mass distribution, or could propagate to cosmological parameter\nestimation, thereby impacting the precision and reliability of cosmological\nmodels. Current surveys, such as Euclid or Rubin, will provide new weak lensing\ndatasets of very high quality. Accurately quantifying uncertainties in mass\nmaps is therefore critical to perform reliable cosmological parameter\ninference. In this paper, we extend the conformalized quantile regression (CQR)\nalgorithm, initially proposed for scalar regression, to inverse problems. We\ncompare our approach with another distribution-free approach based on\nrisk-controlling prediction sets (RCPS). Both methods are based on a\ncalibration dataset, and offer finite-sample coverage guarantees that are\nindependent of the data distribution. Furthermore, they are applicable to any\nmass mapping method, including blackbox predictors. In our experiments, we\napply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative\nWiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS\ntends to produce overconservative confidence bounds with small calibration\nsets, whereas CQR is designed to avoid this issue. Although the expected\nmiscoverage rate is guaranteed to stay below a user-prescribed threshold\nregardless of the mass mapping method, selecting an appropriate reconstruction\nalgorithm remains crucial for obtaining accurate estimates, especially around\npeak-like structures, which are particularly important for inferring\ncosmological parameters. Additionally, the choice of mass mapping method\ninfluences the size of the error bars."
                },
                "authors": [
                    {
                        "name": "Hubert Leterme"
                    },
                    {
                        "name": "Jalal Fadili"
                    },
                    {
                        "name": "Jean-Luc Starck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Luc Starck"
                },
                "author": "Jean-Luc Starck",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03335v2",
                "updated": "2025-01-14T11:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-04T11:40:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    11,
                    40,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition"
                },
                "summary": "We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08043v1",
                "updated": "2025-01-14T11:51:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    51,
                    57,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T11:51:57Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    51,
                    57,
                    1,
                    14,
                    0
                ],
                "title": "PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning"
                },
                "summary": "Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST."
                },
                "authors": [
                    {
                        "name": "Marta Andronic"
                    },
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "George A. Constantinides"
                    }
                ],
                "author_detail": {
                    "name": "George A. Constantinides"
                },
                "author": "George A. Constantinides",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2309.02334",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15791v2",
                "updated": "2025-01-14T11:40:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    40,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-20T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    11,
                    7,
                    51,
                    4,
                    355,
                    0
                ],
                "title": "A Bayesian Approach for Earthquake Impact Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach for Earthquake Impact Modelling"
                },
                "summary": "Immediately following a disaster event, such as an earthquake, estimates of\nthe damage extent play a key role in informing the coordination of response and\nrecovery efforts. We develop a novel impact estimation tool that leverages a\ngeneralised Bayesian approach to generate earthquake impact estimates across\nthree impact types: mortality, population displacement, and building damage.\nInference is performed within a likelihood-free framework, and a\nscoring-rule-based posterior avoids information loss from non-sufficient\nsummary statistics. We propose an adaptation of existing scoring-rule-based\nloss functions that accommodates the use of an approximate Bayesian computation\nsequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results\ncomparable to those of two leading impact estimation tools in the prediction of\ntotal mortality when tested on a set of held-out past events. The proposed\nmethod provides four advantages over existing empirical approaches: modelling\nproduces a gridded spatial map of the estimated impact, predictions benefit\nfrom the Bayesian quantification and interpretation of uncertainty, there is\ndirect handling of multi-shock earthquake events, and the use of a joint model\nbetween impact types allows predictions to be updated as impact observations\nbecome available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immediately following a disaster event, such as an earthquake, estimates of\nthe damage extent play a key role in informing the coordination of response and\nrecovery efforts. We develop a novel impact estimation tool that leverages a\ngeneralised Bayesian approach to generate earthquake impact estimates across\nthree impact types: mortality, population displacement, and building damage.\nInference is performed within a likelihood-free framework, and a\nscoring-rule-based posterior avoids information loss from non-sufficient\nsummary statistics. We propose an adaptation of existing scoring-rule-based\nloss functions that accommodates the use of an approximate Bayesian computation\nsequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results\ncomparable to those of two leading impact estimation tools in the prediction of\ntotal mortality when tested on a set of held-out past events. The proposed\nmethod provides four advantages over existing empirical approaches: modelling\nproduces a gridded spatial map of the estimated impact, predictions benefit\nfrom the Bayesian quantification and interpretation of uncertainty, there is\ndirect handling of multi-shock earthquake events, and the use of a joint model\nbetween impact types allows predictions to be updated as impact observations\nbecome available."
                },
                "authors": [
                    {
                        "name": "Max Anderson Loake"
                    },
                    {
                        "name": "Hamish Patten"
                    },
                    {
                        "name": "David Steinsaltz"
                    }
                ],
                "author_detail": {
                    "name": "David Steinsaltz"
                },
                "author": "David Steinsaltz",
                "arxiv_comment": "24 pages, 21 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.14311v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.14311v3",
                "updated": "2025-01-14T11:35:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    35,
                    16,
                    1,
                    14,
                    0
                ],
                "published": "2023-06-25T18:52:29Z",
                "published_parsed": [
                    2023,
                    6,
                    25,
                    18,
                    52,
                    29,
                    6,
                    176,
                    0
                ],
                "title": "Simple Estimation of Semiparametric Models with Measurement Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Estimation of Semiparametric Models with Measurement Errors"
                },
                "summary": "We develop a practical way of addressing the Errors-In-Variables (EIV)\nproblem in the Generalized Method of Moments (GMM) framework. We focus on the\nsettings in which the variability of the EIV is a fraction of that of the\nmismeasured variables, which is typical for empirical applications. For any\ninitial set of moment conditions our approach provides a \"corrected\" set of\nmoment conditions that are robust to the EIV. We show that the GMM estimator\nbased on these moments is root-n-consistent, with the standard tests and\nconfidence intervals providing valid inference. This is true even when the EIV\nare so large that naive estimators (that ignore the EIV problem) are heavily\nbiased with their confidence intervals having 0% coverage. Our approach\ninvolves no nonparametric estimation, which is especially important for\napplications with many covariates, and settings with multivariate or\nnon-classical EIV. In particular, the approach makes it easy to use\ninstrumental variables to address EIV in nonlinear models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a practical way of addressing the Errors-In-Variables (EIV)\nproblem in the Generalized Method of Moments (GMM) framework. We focus on the\nsettings in which the variability of the EIV is a fraction of that of the\nmismeasured variables, which is typical for empirical applications. For any\ninitial set of moment conditions our approach provides a \"corrected\" set of\nmoment conditions that are robust to the EIV. We show that the GMM estimator\nbased on these moments is root-n-consistent, with the standard tests and\nconfidence intervals providing valid inference. This is true even when the EIV\nare so large that naive estimators (that ignore the EIV problem) are heavily\nbiased with their confidence intervals having 0% coverage. Our approach\ninvolves no nonparametric estimation, which is especially important for\napplications with many covariates, and settings with multivariate or\nnon-classical EIV. In particular, the approach makes it easy to use\ninstrumental variables to address EIV in nonlinear models."
                },
                "authors": [
                    {
                        "name": "Kirill S. Evdokimov"
                    },
                    {
                        "name": "Andrei Zeleneev"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Zeleneev"
                },
                "author": "Andrei Zeleneev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.14311v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.14311v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16597v3",
                "updated": "2025-01-14T11:27:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    27,
                    28,
                    1,
                    14,
                    0
                ],
                "published": "2024-09-25T03:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jingjing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Chen"
                },
                "author": "Jingjing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03829v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03829v3",
                "updated": "2025-01-14T11:06:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    6,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2023-06-06T16:15:28Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    16,
                    15,
                    28,
                    1,
                    157,
                    0
                ],
                "title": "Small-Coupling Dynamic Cavity: a Bayesian mean-field framework for\n  epidemic inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small-Coupling Dynamic Cavity: a Bayesian mean-field framework for\n  epidemic inference"
                },
                "summary": "We present the Small-Coupling Dynamic Cavity (SCDC) method, a novel\ngeneralized mean-field approximation for epidemic inference and risk assessment\nwithin a fully Bayesian framework. SCDC accounts for non-causal effects of\nobservations and uses a graphical model representation of epidemic processes to\nderive self-consistent equations for edge probability marginals. A\nsmall-coupling expansion yields time-dependent cavity messages capturing\nindividual infection probabilities and observational conditioning. With linear\ncomputational cost per iteration in the epidemic duration, SCDC is particularly\nefficient and valid even for recurrent epidemic processes, where standard\nmethods are exponentially complex. Tested on synthetic networks, it matches\nBelief Propagation in accuracy and outperforms individual-based mean-field\nmethods. Notably, despite being derived as a small-infectiousness expansion,\nSCDC maintains good accuracy even for relatively large infection probabilities.\nWhile convergence issues may arise on graphs with long-range correlations, SCDC\nreliably estimates risk. Future extensions include non-Markovian models and\nhigher-order terms in the dynamic cavity framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Small-Coupling Dynamic Cavity (SCDC) method, a novel\ngeneralized mean-field approximation for epidemic inference and risk assessment\nwithin a fully Bayesian framework. SCDC accounts for non-causal effects of\nobservations and uses a graphical model representation of epidemic processes to\nderive self-consistent equations for edge probability marginals. A\nsmall-coupling expansion yields time-dependent cavity messages capturing\nindividual infection probabilities and observational conditioning. With linear\ncomputational cost per iteration in the epidemic duration, SCDC is particularly\nefficient and valid even for recurrent epidemic processes, where standard\nmethods are exponentially complex. Tested on synthetic networks, it matches\nBelief Propagation in accuracy and outperforms individual-based mean-field\nmethods. Notably, despite being derived as a small-infectiousness expansion,\nSCDC maintains good accuracy even for relatively large infection probabilities.\nWhile convergence issues may arise on graphs with long-range correlations, SCDC\nreliably estimates risk. Future extensions include non-Markovian models and\nhigher-order terms in the dynamic cavity framework."
                },
                "authors": [
                    {
                        "name": "Alfredo Braunstein"
                    },
                    {
                        "name": "Giovanni Catania"
                    },
                    {
                        "name": "Luca Dall'Asta"
                    },
                    {
                        "name": "Matteo Mariani"
                    },
                    {
                        "name": "Fabio Mazza"
                    },
                    {
                        "name": "Mattia Tarabolo"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Tarabolo"
                },
                "author": "Mattia Tarabolo",
                "arxiv_comment": "27 pages, 11 figures, 2 tables (including appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03829v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03829v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06713v2",
                "updated": "2025-01-14T11:03:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    3,
                    56,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-12T04:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    44,
                    6,
                    6,
                    12,
                    0
                ],
                "title": "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation"
                },
                "summary": "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps://github.com/HKUDS/MiniRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps://github.com/HKUDS/MiniRAG."
                },
                "authors": [
                    {
                        "name": "Tianyu Fan"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08009v1",
                "updated": "2025-01-14T10:54:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    54,
                    36,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:54:36Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    54,
                    36,
                    1,
                    14,
                    0
                ],
                "title": "Tutorial: VAE as an inference paradigm for neuroimaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tutorial: VAE as an inference paradigm for neuroimaging"
                },
                "summary": "In this tutorial, we explore Variational Autoencoders (VAEs), an essential\nframework for unsupervised learning, particularly suited for high-dimensional\ndatasets such as neuroimaging. By integrating deep learning with Bayesian\ninference, VAEs enable the generation of interpretable latent representations.\nThis tutorial outlines the theoretical foundations of VAEs, addresses practical\nchallenges such as convergence issues and over-fitting, and discusses\nstrategies like the reparameterization trick and hyperparameter optimization.\nWe also highlight key applications of VAEs in neuroimaging, demonstrating their\npotential to uncover meaningful patterns, including those associated with\nneurodegenerative processes, and their broader implications for analyzing\ncomplex brain data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this tutorial, we explore Variational Autoencoders (VAEs), an essential\nframework for unsupervised learning, particularly suited for high-dimensional\ndatasets such as neuroimaging. By integrating deep learning with Bayesian\ninference, VAEs enable the generation of interpretable latent representations.\nThis tutorial outlines the theoretical foundations of VAEs, addresses practical\nchallenges such as convergence issues and over-fitting, and discusses\nstrategies like the reparameterization trick and hyperparameter optimization.\nWe also highlight key applications of VAEs in neuroimaging, demonstrating their\npotential to uncover meaningful patterns, including those associated with\nneurodegenerative processes, and their broader implications for analyzing\ncomplex brain data."
                },
                "authors": [
                    {
                        "name": "C. Vázquez-García"
                    },
                    {
                        "name": "F. J. Martínez-Murcia"
                    },
                    {
                        "name": "F. Segovia Román"
                    },
                    {
                        "name": "Juan M. Górriz Sáez"
                    }
                ],
                "author_detail": {
                    "name": "Juan M. Górriz Sáez"
                },
                "author": "Juan M. Górriz Sáez",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16742v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16742v3",
                "updated": "2025-01-14T10:52:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    52,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-25T16:50:59Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    16,
                    50,
                    59,
                    3,
                    116,
                    0
                ],
                "title": "Bayesian Nonparametric Inference in McKean-Vlasov models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametric Inference in McKean-Vlasov models"
                },
                "summary": "We consider nonparametric statistical inference on a periodic interaction\npotential $W$ from noisy discrete space-time measurements of solutions\n$\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the\nprobability density of the mean field limit of an interacting particle system.\nWe show how Gaussian process priors assigned to $W$ give rise to posterior mean\nestimators that exhibit fast convergence rates for the implied estimated\ndensities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial\ncondition $\\phi$ is not too smooth and satisfies a standard deconvolvability\ncondition, then one can consistently infer Sobolev-regular potentials $W$ at\nconvergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the\nnumber of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as\nthe regularity of $W$ increases corresponding to `near-parametric' models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider nonparametric statistical inference on a periodic interaction\npotential $W$ from noisy discrete space-time measurements of solutions\n$\\rho=\\rho_W$ of the nonlinear McKean-Vlasov equation, describing the\nprobability density of the mean field limit of an interacting particle system.\nWe show how Gaussian process priors assigned to $W$ give rise to posterior mean\nestimators that exhibit fast convergence rates for the implied estimated\ndensities $\\bar \\rho$ towards $\\rho_W$. We further show that if the initial\ncondition $\\phi$ is not too smooth and satisfies a standard deconvolvability\ncondition, then one can consistently infer Sobolev-regular potentials $W$ at\nconvergence rates $N^{-\\theta}$ for appropriate $\\theta>0$, where $N$ is the\nnumber of measurements. The exponent $\\theta$ can be taken to approach $1/2$ as\nthe regularity of $W$ increases corresponding to `near-parametric' models."
                },
                "authors": [
                    {
                        "name": "Richard Nickl"
                    },
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Kolyan Ray"
                    }
                ],
                "author_detail": {
                    "name": "Kolyan Ray"
                },
                "author": "Kolyan Ray",
                "arxiv_comment": "24 pages, to appear in the Annals of Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16742v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16742v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07583v2",
                "updated": "2025-01-14T10:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    52,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2024-08-14T14:28:11Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "title": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey"
                },
                "summary": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development."
                },
                "authors": [
                    {
                        "name": "Hamza Kheddar"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Kheddar"
                },
                "author": "Hamza Kheddar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.04760 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08008v1",
                "updated": "2025-01-14T10:51:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    51,
                    31,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:51:31Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    51,
                    31,
                    1,
                    14,
                    0
                ],
                "title": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning"
                },
                "summary": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs."
                },
                "authors": [
                    {
                        "name": "Yao Liang"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08005v1",
                "updated": "2025-01-14T10:49:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    49,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:49:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    49,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them"
                },
                "summary": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available"
                },
                "authors": [
                    {
                        "name": "Francisco Caetano"
                    },
                    {
                        "name": "Christiaan Viviers"
                    },
                    {
                        "name": "Luis A. Zavala-Mondragón"
                    },
                    {
                        "name": "Peter H. N. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09075v2",
                "updated": "2025-01-14T10:43:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    43,
                    44,
                    1,
                    14,
                    0
                ],
                "published": "2023-10-13T12:51:25Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    12,
                    51,
                    25,
                    4,
                    286,
                    0
                ],
                "title": "Assessing the size of spatial extreme events using local coefficients\n  based on excursion sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the size of spatial extreme events using local coefficients\n  based on excursion sets"
                },
                "summary": "Extreme events arising in georeferenced processes can take various forms,\nsuch as occurring in isolated patches or stretching contiguously over large\nareas, and can further vary with the spatial location and the extremeness of\nthe events. We use excursion sets above threshold exceedances in data observed\nover a two-dimensional grid of rectangular pixels to propose a general family\nof coefficients that assess spatial-extent properties relevant for risk\nassessment, and study five candidate coefficients from this family. These\ncoefficients are defined locally and interpreted as a spatial distance from a\nreference site where the threshold is exceeded. We develop statistical\ninference and discuss robustness to boundary effects and resolution of the\npixel grid. To statistically extrapolate coefficients towards very high\nthreshold levels, we formulate a semiparametric model and estimate a parameter\ncharacterizing how coefficients scale with the quantile level of the threshold.\nThe utility of the new coefficients is illustrated through simulated data, as\nwell as in an application to gridded daily temperature in continental France.\nWe find notable differences in estimated coefficient maps between climate model\nsimulations and observation-based reanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme events arising in georeferenced processes can take various forms,\nsuch as occurring in isolated patches or stretching contiguously over large\nareas, and can further vary with the spatial location and the extremeness of\nthe events. We use excursion sets above threshold exceedances in data observed\nover a two-dimensional grid of rectangular pixels to propose a general family\nof coefficients that assess spatial-extent properties relevant for risk\nassessment, and study five candidate coefficients from this family. These\ncoefficients are defined locally and interpreted as a spatial distance from a\nreference site where the threshold is exceeded. We develop statistical\ninference and discuss robustness to boundary effects and resolution of the\npixel grid. To statistically extrapolate coefficients towards very high\nthreshold levels, we formulate a semiparametric model and estimate a parameter\ncharacterizing how coefficients scale with the quantile level of the threshold.\nThe utility of the new coefficients is illustrated through simulated data, as\nwell as in an application to gridded daily temperature in continental France.\nWe find notable differences in estimated coefficient maps between climate model\nsimulations and observation-based reanalysis."
                },
                "authors": [
                    {
                        "name": "Ryan Cotsakis"
                    },
                    {
                        "name": "Elena Di Bernardino"
                    },
                    {
                        "name": "Thomas Opitz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Opitz"
                },
                "author": "Thomas Opitz",
                "arxiv_comment": "37 pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07992v1",
                "updated": "2025-01-14T10:35:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    35,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:35:54Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    35,
                    54,
                    1,
                    14,
                    0
                ],
                "title": "LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS"
                },
                "summary": "As modern system of systems (SoS) become increasingly adaptive and human\ncentred, traditional architectures often struggle to support interoperability,\nreconfigurability, and effective human system interaction. This paper addresses\nthese challenges by advancing the state of the art holonic architecture for\nSoS, offering two main contributions to support these adaptive needs. First, we\npropose a layered architecture for holons, which includes reasoning,\ncommunication, and capabilities layers. This design facilitates seamless\ninteroperability among heterogeneous constituent systems by improving data\nexchange and integration. Second, inspired by principles of intelligent\nmanufacturing, we introduce specialised holons namely, supervisor, planner,\ntask, and resource holons aimed at enhancing the adaptability and\nreconfigurability of SoS. These specialised holons utilise large language\nmodels within their reasoning layers to support decision making and ensure real\ntime adaptability. We demonstrate our approach through a 3D mobility case study\nfocused on smart city transportation, showcasing its potential for managing\ncomplex, multimodal SoS environments. Additionally, we propose evaluation\nmethods to assess the architecture efficiency and scalability,laying the\ngroundwork for future empirical validations through simulations and real world\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern system of systems (SoS) become increasingly adaptive and human\ncentred, traditional architectures often struggle to support interoperability,\nreconfigurability, and effective human system interaction. This paper addresses\nthese challenges by advancing the state of the art holonic architecture for\nSoS, offering two main contributions to support these adaptive needs. First, we\npropose a layered architecture for holons, which includes reasoning,\ncommunication, and capabilities layers. This design facilitates seamless\ninteroperability among heterogeneous constituent systems by improving data\nexchange and integration. Second, inspired by principles of intelligent\nmanufacturing, we introduce specialised holons namely, supervisor, planner,\ntask, and resource holons aimed at enhancing the adaptability and\nreconfigurability of SoS. These specialised holons utilise large language\nmodels within their reasoning layers to support decision making and ensure real\ntime adaptability. We demonstrate our approach through a 3D mobility case study\nfocused on smart city transportation, showcasing its potential for managing\ncomplex, multimodal SoS environments. Additionally, we propose evaluation\nmethods to assess the architecture efficiency and scalability,laying the\ngroundwork for future empirical validations through simulations and real world\nimplementations."
                },
                "authors": [
                    {
                        "name": "Muhammad Ashfaq"
                    },
                    {
                        "name": "Ahmed R. Sadik"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Niko Mäkitalo"
                    }
                ],
                "author_detail": {
                    "name": "Niko Mäkitalo"
                },
                "author": "Niko Mäkitalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07980v1",
                "updated": "2025-01-14T09:58:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    58,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:58:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    58,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Mapping reionization bubbles in the JWST era II: inferring the position\n  and characteristic size of individual bubbles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping reionization bubbles in the JWST era II: inferring the position\n  and characteristic size of individual bubbles"
                },
                "summary": "The James Webb Space Telescope (JWST) is discovering an increasing number of\ngalaxies well into the early stages of the Epoch of Reionization (EoR). Many of\nthese galaxies are clustered with strong Lyman alpha (Ly$\\alpha$) emission,\nmotivating the presence of surrounding cosmic HII regions that would facilitate\nLy$\\alpha$ transmission through the intergalactic medium (IGM). Detecting these\nHII \"bubbles\" would allow us to connect their growth to the properties of the\ngalaxies inside them. Here we develop a new forward-modeling framework to\nestimate the local HII region size and location from Ly$\\alpha$ spectra of\ngalaxy groups in the early stages of the EoR. Our model takes advantage of the\ncomplementary information provided by neighboring sightlines through the IGM.\nOur forward models sample the main sources of uncertainty, including: (i) the\nglobal neutral fraction; (ii) EoR morphology; (iii) emergent Ly$\\alpha$\nemission; and (iv) NIRSpec instrument noise. Depending on the availability of\ncomplementary nebular lines, $\\sim$ 0.006 $\\unicode{x2013}$ 0.01 galaxies per\ncMpc$^3$, are required to be $\\gtrsim$95\\% confident that the HII bubble\nlocation and size recovered by our method is accurate to within $\\sim$ 1\ncomoving Mpc. This corresponds roughly to tens of galaxies at\n$z\\sim7\\unicode{x2013}8$ in $\\sim$2x2 tiled pointing with JWST NIRSpec. Such a\nsample is achievable with a targeted survey with completeness down to $M_{\\rm\nUV}^{\\rm min}\\lesssim$ -19 $\\unicode{x2013}$ -17, depending on the over-density\nof the field. We test our method on 3D EoR simulations as well as misspecified\nequivalent width distributions, in both cases accurately recovering the HII\nregion surrounding targeted galaxy groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) is discovering an increasing number of\ngalaxies well into the early stages of the Epoch of Reionization (EoR). Many of\nthese galaxies are clustered with strong Lyman alpha (Ly$\\alpha$) emission,\nmotivating the presence of surrounding cosmic HII regions that would facilitate\nLy$\\alpha$ transmission through the intergalactic medium (IGM). Detecting these\nHII \"bubbles\" would allow us to connect their growth to the properties of the\ngalaxies inside them. Here we develop a new forward-modeling framework to\nestimate the local HII region size and location from Ly$\\alpha$ spectra of\ngalaxy groups in the early stages of the EoR. Our model takes advantage of the\ncomplementary information provided by neighboring sightlines through the IGM.\nOur forward models sample the main sources of uncertainty, including: (i) the\nglobal neutral fraction; (ii) EoR morphology; (iii) emergent Ly$\\alpha$\nemission; and (iv) NIRSpec instrument noise. Depending on the availability of\ncomplementary nebular lines, $\\sim$ 0.006 $\\unicode{x2013}$ 0.01 galaxies per\ncMpc$^3$, are required to be $\\gtrsim$95\\% confident that the HII bubble\nlocation and size recovered by our method is accurate to within $\\sim$ 1\ncomoving Mpc. This corresponds roughly to tens of galaxies at\n$z\\sim7\\unicode{x2013}8$ in $\\sim$2x2 tiled pointing with JWST NIRSpec. Such a\nsample is achievable with a targeted survey with completeness down to $M_{\\rm\nUV}^{\\rm min}\\lesssim$ -19 $\\unicode{x2013}$ -17, depending on the over-density\nof the field. We test our method on 3D EoR simulations as well as misspecified\nequivalent width distributions, in both cases accurately recovering the HII\nregion surrounding targeted galaxy groups."
                },
                "authors": [
                    {
                        "name": "Ivan Nikolić"
                    },
                    {
                        "name": "Andrei Mesinger"
                    },
                    {
                        "name": "Charlotte A. Mason"
                    },
                    {
                        "name": "Ting-Yi Lu"
                    },
                    {
                        "name": "Mengtao Tang"
                    },
                    {
                        "name": "David Prelogović"
                    },
                    {
                        "name": "Samuel Gagnon-Hartman"
                    },
                    {
                        "name": "Daniel P. Stark"
                    }
                ],
                "author_detail": {
                    "name": "Daniel P. Stark"
                },
                "author": "Daniel P. Stark",
                "arxiv_comment": "14 pages, 15 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21151v2",
                "updated": "2025-01-14T09:58:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    58,
                    24,
                    1,
                    14,
                    0
                ],
                "published": "2024-07-30T19:28:28Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    19,
                    28,
                    28,
                    1,
                    212,
                    0
                ],
                "title": "Private Collaborative Edge Inference via Over-the-Air Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Collaborative Edge Inference via Over-the-Air Computation"
                },
                "summary": "We consider collaborative inference at the wireless edge, where each client's\nmodel is trained independently on its local dataset. Clients are queried in\nparallel to make an accurate decision collaboratively. In addition to\nmaximizing the inference accuracy, we also want to ensure the privacy of local\nmodels. To this end, we leverage the superposition property of the multiple\naccess channel to implement bandwidth-efficient multi-user inference methods.\nWe propose different methods for ensemble and multi-view classification that\nexploit over-the-air computation (OAC). We show that these schemes perform\nbetter than their orthogonal counterparts with statistically significant\ndifferences while using fewer resources and providing privacy guarantees. We\nalso provide experimental results verifying the benefits of the proposed OAC\napproach to multi-user inference, and perform an ablation study to demonstrate\nthe effectiveness of our design choices. We share the source code of the\nframework publicly on Github to facilitate further research and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider collaborative inference at the wireless edge, where each client's\nmodel is trained independently on its local dataset. Clients are queried in\nparallel to make an accurate decision collaboratively. In addition to\nmaximizing the inference accuracy, we also want to ensure the privacy of local\nmodels. To this end, we leverage the superposition property of the multiple\naccess channel to implement bandwidth-efficient multi-user inference methods.\nWe propose different methods for ensemble and multi-view classification that\nexploit over-the-air computation (OAC). We show that these schemes perform\nbetter than their orthogonal counterparts with statistically significant\ndifferences while using fewer resources and providing privacy guarantees. We\nalso provide experimental results verifying the benefits of the proposed OAC\napproach to multi-user inference, and perform an ablation study to demonstrate\nthe effectiveness of our design choices. We share the source code of the\nframework publicly on Github to facilitate further research and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "Selim F. Yilmaz"
                    },
                    {
                        "name": "Burak Hasircioglu"
                    },
                    {
                        "name": "Li Qiao"
                    },
                    {
                        "name": "Deniz Gunduz"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Gunduz"
                },
                "author": "Deniz Gunduz",
                "arxiv_doi": "10.1109/TMLCN.2025.3526551",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMLCN.2025.3526551",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.21151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 8 figures. This work extends from our preliminary study\n  presented at the 2022 IEEE International Symposium on Information Theory [1].\n  arXiv admin note: text overlap with arXiv:2202.03129",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11005v2",
                "updated": "2025-01-14T09:52:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    52,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-14T18:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    44,
                    23,
                    0,
                    288,
                    0
                ],
                "title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks"
                },
                "summary": "Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness."
                },
                "authors": [
                    {
                        "name": "Fangru Lin"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Si-Qing Chen"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07975v1",
                "updated": "2025-01-14T09:48:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    48,
                    48,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    48,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Some observations on the ambivalent role of symmetries in Bayesian\n  inference problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some observations on the ambivalent role of symmetries in Bayesian\n  inference problems"
                },
                "summary": "We collect in this note some observations on the role of symmetries in\nBayesian inference problems, that can be useful or detrimental depending on the\nway they act on the signal and on the observations. We emphasize in particular\nthe need to gauge away unobservable invariances in the definition of a distance\nbetween a signal and its estimator, and the consequences this implies for the\nstatistical mechanics treatment of such models, taking as a motivating example\nthe extensive rank matrix factorization problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We collect in this note some observations on the role of symmetries in\nBayesian inference problems, that can be useful or detrimental depending on the\nway they act on the signal and on the observations. We emphasize in particular\nthe need to gauge away unobservable invariances in the definition of a distance\nbetween a signal and its estimator, and the consequences this implies for the\nstatistical mechanics treatment of such models, taking as a motivating example\nthe extensive rank matrix factorization problem."
                },
                "authors": [
                    {
                        "name": "Guilhem Semerjian"
                    }
                ],
                "author_detail": {
                    "name": "Guilhem Semerjian"
                },
                "author": "Guilhem Semerjian",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07967v1",
                "updated": "2025-01-14T09:37:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    37,
                    12,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:37:12Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    37,
                    12,
                    1,
                    14,
                    0
                ],
                "title": "Decentralized Learning with Approximate Finite-Time Consensus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning with Approximate Finite-Time Consensus"
                },
                "summary": "The performance of algorithms for decentralized optimization is affected by\nboth the optimization error and the consensus error, the latter of which arises\nfrom the variation between agents' local models. Classically, algorithms employ\naveraging and gradient-tracking mechanisms with constant combination matrices\nto drive the collection of agents to consensus. Recent works have demonstrated\nthat using sequences of combination matrices that achieve finite-time consensus\n(FTC) can result in improved communication efficiency or iteration complexity\nfor decentralized optimization. Notably, these studies apply to highly\nstructured networks, where exact finite-time consensus sequences are known\nexactly and in closed form. In this work we investigate the impact of utilizing\napproximate FTC matrices in decentralized learning algorithms, and quantify the\nimpact of the approximation error on convergence rate and steady-state\nperformance. Approximate FTC matrices can be inferred for general graphs and do\nnot rely on a particular graph structure or prior knowledge, making the\nproposed scheme applicable to a broad range of decentralized learning settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of algorithms for decentralized optimization is affected by\nboth the optimization error and the consensus error, the latter of which arises\nfrom the variation between agents' local models. Classically, algorithms employ\naveraging and gradient-tracking mechanisms with constant combination matrices\nto drive the collection of agents to consensus. Recent works have demonstrated\nthat using sequences of combination matrices that achieve finite-time consensus\n(FTC) can result in improved communication efficiency or iteration complexity\nfor decentralized optimization. Notably, these studies apply to highly\nstructured networks, where exact finite-time consensus sequences are known\nexactly and in closed form. In this work we investigate the impact of utilizing\napproximate FTC matrices in decentralized learning algorithms, and quantify the\nimpact of the approximation error on convergence rate and steady-state\nperformance. Approximate FTC matrices can be inferred for general graphs and do\nnot rely on a particular graph structure or prior knowledge, making the\nproposed scheme applicable to a broad range of decentralized learning settings."
                },
                "authors": [
                    {
                        "name": "Aaron Fainman"
                    },
                    {
                        "name": "Stefan Vlaski"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Vlaski"
                },
                "author": "Stefan Vlaski",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07964v1",
                "updated": "2025-01-14T09:35:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    35,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:35:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    35,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Derivation of Output Correlation Inferences for Multi-Output (aka\n  Multi-Task) Gaussian Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivation of Output Correlation Inferences for Multi-Output (aka\n  Multi-Task) Gaussian Process"
                },
                "summary": "Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients."
                },
                "authors": [
                    {
                        "name": "Shuhei Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shuhei Watanabe"
                },
                "author": "Shuhei Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07959v1",
                "updated": "2025-01-14T09:23:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:23:30Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning"
                },
                "summary": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."
                },
                "authors": [
                    {
                        "name": "Jiaqi Hua"
                    },
                    {
                        "name": "Wanxu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wanxu Wei"
                },
                "author": "Wanxu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07952v1",
                "updated": "2025-01-14T09:09:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    9,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:09:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    9,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "Spiking Neural Network Accelerator Architecture for Differential-Time\n  Representation using Learned Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Network Accelerator Architecture for Differential-Time\n  Representation using Learned Encoding"
                },
                "summary": "Spiking Neural Networks (SNNs) have garnered attention over recent years due\nto their increased energy efficiency and advantages in terms of operational\ncomplexity compared to traditional Artificial Neural Networks (ANNs). Two\nimportant questions when implementing SNNs are how to best encode existing data\ninto spike trains and how to efficiently process these spike trains in\nhardware. This paper addresses both of these problems by incorporating the\nencoding into the learning process, thus allowing the network to learn the\nspike encoding alongside the weights. Furthermore, this paper proposes a\nhardware architecture based on a recently introduced differential-time\nrepresentation for spike trains allowing decoupling of spike time and\nprocessing time. Together these contributions lead to a feedforward SNN using\nonly Leaky-Integrate and Fire (LIF) neurons that surpasses 99% accuracy on the\nMNIST dataset while still being implementable on medium-sized FPGAs with\ninference times of less than 295us.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have garnered attention over recent years due\nto their increased energy efficiency and advantages in terms of operational\ncomplexity compared to traditional Artificial Neural Networks (ANNs). Two\nimportant questions when implementing SNNs are how to best encode existing data\ninto spike trains and how to efficiently process these spike trains in\nhardware. This paper addresses both of these problems by incorporating the\nencoding into the learning process, thus allowing the network to learn the\nspike encoding alongside the weights. Furthermore, this paper proposes a\nhardware architecture based on a recently introduced differential-time\nrepresentation for spike trains allowing decoupling of spike time and\nprocessing time. Together these contributions lead to a feedforward SNN using\nonly Leaky-Integrate and Fire (LIF) neurons that surpasses 99% accuracy on the\nMNIST dataset while still being implementable on medium-sized FPGAs with\ninference times of less than 295us."
                },
                "authors": [
                    {
                        "name": "Daniel Windhager"
                    },
                    {
                        "name": "Lothar Ratschbacher"
                    },
                    {
                        "name": "Bernhard A. Moser"
                    },
                    {
                        "name": "Michael Lunglmayr"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lunglmayr"
                },
                "author": "Michael Lunglmayr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07949v1",
                "updated": "2025-01-14T09:05:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    5,
                    59,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:05:59Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    5,
                    59,
                    1,
                    14,
                    0
                ],
                "title": "One cut-point phase-type distributions in Reliability. An application to\n  Resistive Random Access Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One cut-point phase-type distributions in Reliability. An application to\n  Resistive Random Access Memories"
                },
                "summary": "A new probability distribution to study lifetime data in reliability is\nintroduced in this paper. This one is a first approach to a non-homogeneous\nphase-type distribution. It is built by considering one cut-point in the\nnon-negative semi-line of a phase-type distribution. The density function is\ndefined and the main measures associated, such as the reliability function,\nhazard rate, cumulative hazard rate and the characteristic function are also\nworked out. This new class of distributions enables to decrease the number of\nparameter in the estimate when inference is considered. Besides, the likelihood\ndistribution is built to estimate the model parameters by maximum likelihood.\nSeveral applications by considering Resistive Random Access Memories compare\nthe adjustment when phase type distributions and one cut-point phase-type\ndistributions are considered. The developed methodology has been\ncomputationally implemented in R-cran.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new probability distribution to study lifetime data in reliability is\nintroduced in this paper. This one is a first approach to a non-homogeneous\nphase-type distribution. It is built by considering one cut-point in the\nnon-negative semi-line of a phase-type distribution. The density function is\ndefined and the main measures associated, such as the reliability function,\nhazard rate, cumulative hazard rate and the characteristic function are also\nworked out. This new class of distributions enables to decrease the number of\nparameter in the estimate when inference is considered. Besides, the likelihood\ndistribution is built to estimate the model parameters by maximum likelihood.\nSeveral applications by considering Resistive Random Access Memories compare\nthe adjustment when phase type distributions and one cut-point phase-type\ndistributions are considered. The developed methodology has been\ncomputationally implemented in R-cran."
                },
                "authors": [
                    {
                        "name": "Christian Acal"
                    },
                    {
                        "name": "Juan Eloy Ruiz-Castro"
                    },
                    {
                        "name": "David Maldonado"
                    },
                    {
                        "name": "Juan B. Roldán"
                    }
                ],
                "author_detail": {
                    "name": "Juan B. Roldán"
                },
                "author": "Juan B. Roldán",
                "arxiv_doi": "10.3390/math9212734",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/math9212734",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Mathematics 2021, 9(21), 2734",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03659v2",
                "updated": "2025-01-14T08:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    52,
                    51,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-07T09:47:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    47,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting"
                },
                "summary": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Yiqun Wang"
                    },
                    {
                        "name": "Zhengda Lu"
                    },
                    {
                        "name": "Jianwei Guo"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongxing Qin"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "9 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v1",
                "updated": "2025-01-14T08:30:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Václav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Bazińska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damián Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adrià Romero-López"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Natalie Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07925v1",
                "updated": "2025-01-14T08:26:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    26,
                    58,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:26:58Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    26,
                    58,
                    1,
                    14,
                    0
                ],
                "title": "Phase of Flight Classification in Aviation Safety using LSTM, GRU, and\n  BiLSTM: A Case Study with ASN Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase of Flight Classification in Aviation Safety using LSTM, GRU, and\n  BiLSTM: A Case Study with ASN Dataset"
                },
                "summary": "Safety is the main concern in the aviation industry, where even minor\noperational issues can lead to serious consequences. This study addresses the\nneed for comprehensive aviation accident analysis by leveraging natural\nlanguage processing (NLP) and advanced AI models to classify the phase of\nflight from unstructured aviation accident analysis narratives. The research\naims to determine whether the phase of flight can be inferred from narratives\nof post-accident events using NLP techniques. The classification performance of\nvarious deep learning models was evaluated. For single RNN-based models, LSTM\nachieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an\naccuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced\nperformance with an accuracy and recall of 60% and a precision of 63%. Joint\nRNN-based models further enhanced predictive capabilities. GRU-LSTM,\nLSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,\nrespectively, showcasing the benefits of combining these architectures. To\nprovide a comprehensive overview of model performance, single and combined\nmodels were compared in terms of the various metrics. These results underscore\nthe models' capacity to classify the phase of flight from raw text narratives,\nequipping aviation industry stakeholders with valuable insights for proactive\ndecision-making. Therefore, this research signifies a substantial advancement\nin the application of NLP and deep learning models to enhance aviation safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is the main concern in the aviation industry, where even minor\noperational issues can lead to serious consequences. This study addresses the\nneed for comprehensive aviation accident analysis by leveraging natural\nlanguage processing (NLP) and advanced AI models to classify the phase of\nflight from unstructured aviation accident analysis narratives. The research\naims to determine whether the phase of flight can be inferred from narratives\nof post-accident events using NLP techniques. The classification performance of\nvarious deep learning models was evaluated. For single RNN-based models, LSTM\nachieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an\naccuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced\nperformance with an accuracy and recall of 60% and a precision of 63%. Joint\nRNN-based models further enhanced predictive capabilities. GRU-LSTM,\nLSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,\nrespectively, showcasing the benefits of combining these architectures. To\nprovide a comprehensive overview of model performance, single and combined\nmodels were compared in terms of the various metrics. These results underscore\nthe models' capacity to classify the phase of flight from raw text narratives,\nequipping aviation industry stakeholders with valuable insights for proactive\ndecision-making. Therefore, this research signifies a substantial advancement\nin the application of NLP and deep learning models to enhance aviation safety."
                },
                "authors": [
                    {
                        "name": "Aziida Nanyonga"
                    },
                    {
                        "name": "Hassan Wasswa"
                    },
                    {
                        "name": "Graham Wild"
                    }
                ],
                "author_detail": {
                    "name": "Graham Wild"
                },
                "author": "Graham Wild",
                "arxiv_comment": "Aviation Safety, Deep learning algorithms, Flight phase, NLP, ASN,\n  and Classification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11548v2",
                "updated": "2025-01-14T08:23:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    23,
                    37,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-15T12:32:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    32,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "Bayesian inference of mixed Gaussian phylogenetic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of mixed Gaussian phylogenetic models"
                },
                "summary": "Background: Continuous traits evolution of a group of taxa that are\ncorrelated through a phylogenetic tree is commonly modelled using parametric\nstochastic differential equations to represent deterministic change of trait\nthrough time, while incorporating noises that represent different unobservable\nevolutionary pressures. Often times, a heterogeneous Gaussian process that\nconsists of multiple parametric sub-processes is often used when the observed\ndata come from a very diverse set of taxa. In the maximum likelihood setting,\nchallenges can be found when exploring the involved likelihood surface and when\ninterpreting the uncertainty around the parameters.\n  Results: We extend the methods to tackle inference problems for mixed\nGaussian phylogenetic models (MGPMs) by implementing a Bayesian scheme that can\ntake into account biologically relevant priors. The posterior inference method\nis based on the Population Monte Carlo (PMC) algorithm that are easily\nparallelized, and using an efficient algorithm to calculate the likelihood of\nphylogenetically correlated observations. A model evaluation method that is\nbased on the proximity of the posterior predictive distribution to the observed\ndata is also implemented. Simulation study is done to test the inference and\nevaluation capability of the method. Finally, we test our method on a\nreal-world dataset.\n  Conclusion: We implement the method in the R package bgphy, available at\ngithub.com/bayubeta/bgphy. Simulation study demonstrates that the method is\nable to infer parameters and evaluate models properly, while its implementation\non the real-world dataset indicates that a carefully selected model of\nevolution based on naturally occurring classifications results in a better fit\nto the observed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Continuous traits evolution of a group of taxa that are\ncorrelated through a phylogenetic tree is commonly modelled using parametric\nstochastic differential equations to represent deterministic change of trait\nthrough time, while incorporating noises that represent different unobservable\nevolutionary pressures. Often times, a heterogeneous Gaussian process that\nconsists of multiple parametric sub-processes is often used when the observed\ndata come from a very diverse set of taxa. In the maximum likelihood setting,\nchallenges can be found when exploring the involved likelihood surface and when\ninterpreting the uncertainty around the parameters.\n  Results: We extend the methods to tackle inference problems for mixed\nGaussian phylogenetic models (MGPMs) by implementing a Bayesian scheme that can\ntake into account biologically relevant priors. The posterior inference method\nis based on the Population Monte Carlo (PMC) algorithm that are easily\nparallelized, and using an efficient algorithm to calculate the likelihood of\nphylogenetically correlated observations. A model evaluation method that is\nbased on the proximity of the posterior predictive distribution to the observed\ndata is also implemented. Simulation study is done to test the inference and\nevaluation capability of the method. Finally, we test our method on a\nreal-world dataset.\n  Conclusion: We implement the method in the R package bgphy, available at\ngithub.com/bayubeta/bgphy. Simulation study demonstrates that the method is\nable to infer parameters and evaluate models properly, while its implementation\non the real-world dataset indicates that a carefully selected model of\nevolution based on naturally occurring classifications results in a better fit\nto the observed data."
                },
                "authors": [
                    {
                        "name": "Bayu Brahmantio"
                    },
                    {
                        "name": "Krzysztof Bartoszek"
                    },
                    {
                        "name": "Etka Yapar"
                    }
                ],
                "author_detail": {
                    "name": "Etka Yapar"
                },
                "author": "Etka Yapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21079v3",
                "updated": "2025-01-14T08:23:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-30T16:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    56,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "Edicho: Consistent Image Editing in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edicho: Consistent Image Editing in the Wild"
                },
                "summary": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies."
                },
                "authors": [
                    {
                        "name": "Qingyan Bai"
                    },
                    {
                        "name": "Hao Ouyang"
                    },
                    {
                        "name": "Yinghao Xu"
                    },
                    {
                        "name": "Qiuyu Wang"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ka Leong Cheng"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "Project page: https://ant-research.github.io/edicho/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07919v1",
                "updated": "2025-01-14T08:10:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    10,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:10:43Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    10,
                    43,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Model Interface for Home Energy Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Interface for Home Energy Management Systems"
                },
                "summary": "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and/or few-shot prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and/or few-shot prompting."
                },
                "authors": [
                    {
                        "name": "François Michelon"
                    },
                    {
                        "name": "Yihong Zhou"
                    },
                    {
                        "name": "Thomas Morstyn"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Morstyn"
                },
                "author": "Thomas Morstyn",
                "arxiv_comment": "13 pages conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07240v2",
                "updated": "2025-01-14T07:57:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    57,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-11T18:59:02Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"
                },
                "summary": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath"
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Qingping Yang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Runtao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Runtao Liu"
                },
                "author": "Runtao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07905v1",
                "updated": "2025-01-14T07:50:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    50,
                    9,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T07:50:09Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    50,
                    9,
                    1,
                    14,
                    0
                ],
                "title": "Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence\n  Modeling for Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence\n  Modeling for Resource-Constrained Environments"
                },
                "summary": "Long-range sequence modeling is a crucial aspect of natural language\nprocessing and time series analysis. However, traditional models like Recurrent\nNeural Networks (RNNs) and Transformers suffer from computational and memory\ninefficiencies, especially when dealing with long sequences. This paper\nintroduces Logarithmic Memory Networks (LMNs), a novel architecture that\nleverages a hierarchical logarithmic tree structure to efficiently store and\nretrieve past information. LMNs dynamically summarize historical context,\nsignificantly reducing the memory footprint and computational complexity of\nattention mechanisms from O(n2) to O(log(n)). The model employs a\nsingle-vector, targeted attention mechanism to access stored information, and\nthe memory block construction worker (summarizer) layer operates in two modes:\na parallel execution mode during training for efficient processing of\nhierarchical tree structures and a sequential execution mode during inference,\nwhich acts as a memory management system. It also implicitly encodes positional\ninformation, eliminating the need for explicit positional encodings. These\nfeatures make LMNs a robust and scalable solution for processing long-range\nsequences in resource-constrained environments, offering practical improvements\nin efficiency and scalability. The code is publicly available under the MIT\nLicense on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range sequence modeling is a crucial aspect of natural language\nprocessing and time series analysis. However, traditional models like Recurrent\nNeural Networks (RNNs) and Transformers suffer from computational and memory\ninefficiencies, especially when dealing with long sequences. This paper\nintroduces Logarithmic Memory Networks (LMNs), a novel architecture that\nleverages a hierarchical logarithmic tree structure to efficiently store and\nretrieve past information. LMNs dynamically summarize historical context,\nsignificantly reducing the memory footprint and computational complexity of\nattention mechanisms from O(n2) to O(log(n)). The model employs a\nsingle-vector, targeted attention mechanism to access stored information, and\nthe memory block construction worker (summarizer) layer operates in two modes:\na parallel execution mode during training for efficient processing of\nhierarchical tree structures and a sequential execution mode during inference,\nwhich acts as a memory management system. It also implicitly encodes positional\ninformation, eliminating the need for explicit positional encodings. These\nfeatures make LMNs a robust and scalable solution for processing long-range\nsequences in resource-constrained environments, offering practical improvements\nin efficiency and scalability. The code is publicly available under the MIT\nLicense on GitHub: https://github.com/AhmedBoin/LogarithmicMemory."
                },
                "authors": [
                    {
                        "name": "Mohamed A. Taha"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed A. Taha"
                },
                "author": "Mohamed A. Taha",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17863v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17863v4",
                "updated": "2025-01-14T07:33:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    33,
                    19,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-25T18:05:31Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    18,
                    5,
                    31,
                    1,
                    177,
                    0
                ],
                "title": "What type of inference is planning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What type of inference is planning?"
                },
                "summary": "Multiple types of inference are available for probabilistic graphical models,\ne.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori.\nWhich one do researchers mean when they talk about \"planning as inference\"?\nThere is no consistency in the literature, different types are used, and their\nability to do planning is further entangled with specific approximations or\nadditional constraints. In this work we use the variational framework to show\nthat, just like all commonly used types of inference correspond to different\nweightings of the entropy terms in the variational problem, planning\ncorresponds exactly to a different set of weights. This means that all the\ntricks of variational inference are readily applicable to planning. We develop\nan analogue of loopy belief propagation that allows us to perform approximate\nplanning in factored-state Markov decisions processes without incurring\nintractability due to the exponentially large state space. The variational\nperspective shows that the previous types of inference for planning are only\nadequate in environments with low stochasticity, and allows us to characterize\neach type by its own merits, disentangling the type of inference from the\nadditional approximations that its practical use requires. We validate these\nresults empirically on synthetic MDPs and tasks posed in the International\nPlanning Competition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple types of inference are available for probabilistic graphical models,\ne.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori.\nWhich one do researchers mean when they talk about \"planning as inference\"?\nThere is no consistency in the literature, different types are used, and their\nability to do planning is further entangled with specific approximations or\nadditional constraints. In this work we use the variational framework to show\nthat, just like all commonly used types of inference correspond to different\nweightings of the entropy terms in the variational problem, planning\ncorresponds exactly to a different set of weights. This means that all the\ntricks of variational inference are readily applicable to planning. We develop\nan analogue of loopy belief propagation that allows us to perform approximate\nplanning in factored-state Markov decisions processes without incurring\nintractability due to the exponentially large state space. The variational\nperspective shows that the previous types of inference for planning are only\nadequate in environments with low stochasticity, and allows us to characterize\neach type by its own merits, disentangling the type of inference from the\nadditional approximations that its practical use requires. We validate these\nresults empirically on synthetic MDPs and tasks posed in the International\nPlanning Competition."
                },
                "authors": [
                    {
                        "name": "Miguel Lázaro-Gredilla"
                    },
                    {
                        "name": "Li Yang Ku"
                    },
                    {
                        "name": "Kevin P. Murphy"
                    },
                    {
                        "name": "Dileep George"
                    }
                ],
                "author_detail": {
                    "name": "Dileep George"
                },
                "author": "Dileep George",
                "arxiv_comment": "Camera-ready version update",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17863v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17863v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07900v1",
                "updated": "2025-01-14T07:30:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    30,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T07:30:54Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    30,
                    54,
                    1,
                    14,
                    0
                ],
                "title": "The one-dimensional equilibrium shape of a crystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The one-dimensional equilibrium shape of a crystal"
                },
                "summary": "Optimizing the free energy under a mass constraint may generate a convex\ncrystal subject to assumptions on the potential $g(0)=0$, $g \\ge 0$. The\ngeneral problem classically attributed to Almgren is to infer if this is the\ncase assuming the sub-level sets of g are convex. The theorem proven in the\npaper is that in one dimension the answer is positive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the free energy under a mass constraint may generate a convex\ncrystal subject to assumptions on the potential $g(0)=0$, $g \\ge 0$. The\ngeneral problem classically attributed to Almgren is to infer if this is the\ncase assuming the sub-level sets of g are convex. The theorem proven in the\npaper is that in one dimension the answer is positive."
                },
                "authors": [
                    {
                        "name": "Emanuel Indrei"
                    }
                ],
                "author_detail": {
                    "name": "Emanuel Indrei"
                },
                "author": "Emanuel Indrei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07897v1",
                "updated": "2025-01-14T07:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    26,
                    5,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T07:26:05Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    26,
                    5,
                    1,
                    14,
                    0
                ],
                "title": "Bridge-SR: Schrödinger Bridge for Efficient SR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge-SR: Schrödinger Bridge for Efficient SR"
                },
                "summary": "Speech super-resolution (SR), which generates a waveform at a higher sampling\nrate from its low-resolution version, is a long-standing critical task in\nspeech restoration. Previous works have explored speech SR in different data\nspaces, but these methods either require additional compression networks or\nexhibit limited synthesis quality and inference speed. Motivated by recent\nadvances in probabilistic generative models, we present Bridge-SR, a novel and\nefficient any-to-48kHz SR system in the speech waveform domain. Using tractable\nSchr\\\"odinger Bridge models, we leverage the observed low-resolution waveform\nas a prior, which is intrinsically informative for the high-resolution target.\nBy optimizing a lightweight network to learn the score functions from the prior\nto the target, we achieve efficient waveform SR through a data-to-data\ngeneration process that fully exploits the instructive content contained in the\nlow-resolution observation. Furthermore, we identify the importance of the\nnoise schedule, data scaling, and auxiliary loss functions, which further\nimprove the SR quality of bridge-based systems. The experiments conducted on\nthe benchmark dataset VCTK demonstrate the efficiency of our system: (1) in\nterms of sample quality, Bridge-SR outperforms several strong baseline methods\nunder different SR settings, using a lightweight network backbone (1.7M); (2)\nin terms of inference speed, our 4-step synthesis achieves better performance\nthan the 8-step conditional diffusion counterpart (LSD: 0.911 vs 0.927). Demo\nat https://bridge-sr.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech super-resolution (SR), which generates a waveform at a higher sampling\nrate from its low-resolution version, is a long-standing critical task in\nspeech restoration. Previous works have explored speech SR in different data\nspaces, but these methods either require additional compression networks or\nexhibit limited synthesis quality and inference speed. Motivated by recent\nadvances in probabilistic generative models, we present Bridge-SR, a novel and\nefficient any-to-48kHz SR system in the speech waveform domain. Using tractable\nSchr\\\"odinger Bridge models, we leverage the observed low-resolution waveform\nas a prior, which is intrinsically informative for the high-resolution target.\nBy optimizing a lightweight network to learn the score functions from the prior\nto the target, we achieve efficient waveform SR through a data-to-data\ngeneration process that fully exploits the instructive content contained in the\nlow-resolution observation. Furthermore, we identify the importance of the\nnoise schedule, data scaling, and auxiliary loss functions, which further\nimprove the SR quality of bridge-based systems. The experiments conducted on\nthe benchmark dataset VCTK demonstrate the efficiency of our system: (1) in\nterms of sample quality, Bridge-SR outperforms several strong baseline methods\nunder different SR settings, using a lightweight network backbone (1.7M); (2)\nin terms of inference speed, our 4-step synthesis achieves better performance\nthan the 8-step conditional diffusion counterpart (LSD: 0.911 vs 0.927). Demo\nat https://bridge-sr.github.io."
                },
                "authors": [
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Zehua Chen"
                    },
                    {
                        "name": "Fan Bao"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07892v1",
                "updated": "2025-01-14T07:16:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    16,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T07:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    16,
                    43,
                    1,
                    14,
                    0
                ],
                "title": "Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation\n  in LLMs"
                },
                "summary": "Automated code generation using large language models (LLMs) has gained\nattention due to its efficiency and adaptability. However, real-world coding\ntasks or benchmarks like HumanEval and StudentEval often lack dedicated\ntraining datasets, challenging existing few-shot prompting approaches that rely\non reference examples. Inspired by human metamemory-a cognitive process\ninvolving recall and evaluation-we present a novel framework (namely M^2WF) for\nimproving LLMs' one-time code generation. This approach enables LLMs to\nautonomously generate, evaluate, and utilize synthetic examples to enhance\nreliability and performance. Unlike prior methods, it minimizes dependency on\ncurated data and adapts flexibly to various coding scenarios. Our experiments\ndemonstrate significant improvements in coding benchmarks, offering a scalable\nand robust solution for data-free environments. The code and framework will be\npublicly available on GitHub and HuggingFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code generation using large language models (LLMs) has gained\nattention due to its efficiency and adaptability. However, real-world coding\ntasks or benchmarks like HumanEval and StudentEval often lack dedicated\ntraining datasets, challenging existing few-shot prompting approaches that rely\non reference examples. Inspired by human metamemory-a cognitive process\ninvolving recall and evaluation-we present a novel framework (namely M^2WF) for\nimproving LLMs' one-time code generation. This approach enables LLMs to\nautonomously generate, evaluate, and utilize synthetic examples to enhance\nreliability and performance. Unlike prior methods, it minimizes dependency on\ncurated data and adapts flexibly to various coding scenarios. Our experiments\ndemonstrate significant improvements in coding benchmarks, offering a scalable\nand robust solution for data-free environments. The code and framework will be\npublicly available on GitHub and HuggingFace."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Zheng He"
                    },
                    {
                        "name": "Dapeng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dapeng Tao"
                },
                "author": "Dapeng Tao",
                "arxiv_comment": "11 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.03852v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.03852v3",
                "updated": "2025-01-14T06:40:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    40,
                    36,
                    1,
                    14,
                    0
                ],
                "published": "2023-09-07T17:07:36Z",
                "published_parsed": [
                    2023,
                    9,
                    7,
                    17,
                    7,
                    36,
                    3,
                    250,
                    0
                ],
                "title": "FLM-101B: An Open LLM and How to Train It with $100K Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLM-101B: An Open LLM and How to Train It with $100K Budget"
                },
                "summary": "Large language models (LLMs) are considered important approaches towards\nfoundational machine intelligence, achieving remarkable success in Natural\nLanguage Processing and multimodal tasks, among others. However, the carbon\nfootprints and financial costs originating from heavy pre-training computation\nis a non-negligible issue. Progressive training methods, inspired by the\nneurogenesis process that grows neural structures, have shown potential to\naccelerate LLM pre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B parameters remain\nunderexplored. In this paper, we show that our model, namely FLM-101B, trained\nwith our growth strategy under a budget of \\$100K, reaches 80\\% of the\nbaselines' performances with only 10\\% of their floating-point operations. We\nbelieve that further studies on progressive training will benefit the community\nby cutting down the costs and promoting green AI. The checkpoint of FLM-101B is\nreleased at https://huggingface.co/CofeAI/FLM-101B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are considered important approaches towards\nfoundational machine intelligence, achieving remarkable success in Natural\nLanguage Processing and multimodal tasks, among others. However, the carbon\nfootprints and financial costs originating from heavy pre-training computation\nis a non-negligible issue. Progressive training methods, inspired by the\nneurogenesis process that grows neural structures, have shown potential to\naccelerate LLM pre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B parameters remain\nunderexplored. In this paper, we show that our model, namely FLM-101B, trained\nwith our growth strategy under a budget of \\$100K, reaches 80\\% of the\nbaselines' performances with only 10\\% of their floating-point operations. We\nbelieve that further studies on progressive training will benefit the community\nby cutting down the costs and promoting green AI. The checkpoint of FLM-101B is\nreleased at https://huggingface.co/CofeAI/FLM-101B."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.03852v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.03852v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v6",
                "updated": "2025-01-14T06:25:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    25,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Kshitij Sharad Jadhav"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09012v2",
                "updated": "2025-01-14T06:06:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    6,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-12T07:23:52Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    23,
                    52,
                    3,
                    347,
                    0
                ],
                "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Cryptic Crosswords Challenging for LLMs?"
                },
                "summary": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Sadallah"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094",
                "arxiv_journal_ref": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07861v1",
                "updated": "2025-01-14T05:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    56,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:56:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    56,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Qipeng Wang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Song Yang"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07857v1",
                "updated": "2025-01-14T05:48:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:48:27Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    27,
                    1,
                    14,
                    0
                ],
                "title": "Hierarchical Repository-Level Code Summarization for Business\n  Applications Using Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Repository-Level Code Summarization for Business\n  Applications Using Local LLMs"
                },
                "summary": "In large-scale software development, understanding the functionality and\nintent behind complex codebases is critical for effective development and\nmaintenance. While code summarization has been widely studied, existing methods\nprimarily focus on smaller code units, such as functions, and struggle with\nlarger code artifacts like files and packages. Additionally, current\nsummarization models tend to emphasize low-level implementation details, often\noverlooking the domain and business context that are crucial for real-world\napplications. This paper proposes a two-step hierarchical approach for\nrepository-level code summarization, tailored to business applications. First,\nsmaller code units such as functions and variables are identified using syntax\nanalysis and summarized with local LLMs. These summaries are then aggregated to\ngenerate higher-level file and package summaries. To ensure the summaries are\ngrounded in business context, we design custom prompts that capture the\nintended purpose of code artifacts based on the domain and problem context of\nthe business application. We evaluate our approach on a business support system\n(BSS) for the telecommunications domain, showing that syntax analysis-based\nhierarchical summarization improves coverage, while business-context grounding\nenhances the relevance of the generated summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale software development, understanding the functionality and\nintent behind complex codebases is critical for effective development and\nmaintenance. While code summarization has been widely studied, existing methods\nprimarily focus on smaller code units, such as functions, and struggle with\nlarger code artifacts like files and packages. Additionally, current\nsummarization models tend to emphasize low-level implementation details, often\noverlooking the domain and business context that are crucial for real-world\napplications. This paper proposes a two-step hierarchical approach for\nrepository-level code summarization, tailored to business applications. First,\nsmaller code units such as functions and variables are identified using syntax\nanalysis and summarized with local LLMs. These summaries are then aggregated to\ngenerate higher-level file and package summaries. To ensure the summaries are\ngrounded in business context, we design custom prompts that capture the\nintended purpose of code artifacts based on the domain and problem context of\nthe business application. We evaluate our approach on a business support system\n(BSS) for the telecommunications domain, showing that syntax analysis-based\nhierarchical summarization improves coverage, while business-context grounding\nenhances the relevance of the generated summaries."
                },
                "authors": [
                    {
                        "name": "Nilesh Dhulshette"
                    },
                    {
                        "name": "Sapan Shah"
                    },
                    {
                        "name": "Vinay Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kulkarni"
                },
                "author": "Vinay Kulkarni",
                "arxiv_comment": "To appear at LLM4Code@ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07853v1",
                "updated": "2025-01-14T05:41:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    41,
                    9,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:41:09Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    41,
                    9,
                    1,
                    14,
                    0
                ],
                "title": "Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques"
                },
                "summary": "This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers."
                },
                "authors": [
                    {
                        "name": "Shobhit Ratan"
                    },
                    {
                        "name": "Farley Knight"
                    },
                    {
                        "name": "Ghada Jerfel"
                    },
                    {
                        "name": "Sze Chung Ho"
                    }
                ],
                "author_detail": {
                    "name": "Sze Chung Ho"
                },
                "author": "Sze Chung Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05692v2",
                "updated": "2025-01-14T05:39:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    39,
                    40,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-08T17:18:04Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    17,
                    18,
                    4,
                    0,
                    99,
                    0
                ],
                "title": "Evaluating Mathematical Reasoning Beyond Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Mathematical Reasoning Beyond Accuracy"
                },
                "summary": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch."
                },
                "authors": [
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "v2 is the AAAI 2025 camera ready version. Project site with code:\n  https://github.com/GAIR-NLP/ReasonEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15640v3",
                "updated": "2025-01-14T05:35:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    35,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-23T19:43:02Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    19,
                    43,
                    2,
                    5,
                    328,
                    0
                ],
                "title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset"
                },
                "summary": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers."
                },
                "authors": [
                    {
                        "name": "Tobi Olatunji"
                    },
                    {
                        "name": "Charles Nimo"
                    },
                    {
                        "name": "Abraham Owodunni"
                    },
                    {
                        "name": "Tassallah Abdullahi"
                    },
                    {
                        "name": "Emmanuel Ayodele"
                    },
                    {
                        "name": "Mardhiyah Sanni"
                    },
                    {
                        "name": "Chinemelu Aka"
                    },
                    {
                        "name": "Folafunmi Omofoye"
                    },
                    {
                        "name": "Foutse Yuehgoh"
                    },
                    {
                        "name": "Timothy Faniran"
                    },
                    {
                        "name": "Bonaventure F. P. Dossou"
                    },
                    {
                        "name": "Moshood Yekini"
                    },
                    {
                        "name": "Jonas Kemp"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Jude Chidubem Omeke"
                    },
                    {
                        "name": "Chidi Asuzu MD"
                    },
                    {
                        "name": "Naome A. Etori"
                    },
                    {
                        "name": "Aimérou Ndiaye"
                    },
                    {
                        "name": "Ifeoma Okoh"
                    },
                    {
                        "name": "Evans Doe Ocansey"
                    },
                    {
                        "name": "Wendy Kinara"
                    },
                    {
                        "name": "Michael Best"
                    },
                    {
                        "name": "Irfan Essa"
                    },
                    {
                        "name": "Stephen Edward Moore"
                    },
                    {
                        "name": "Chris Fourie"
                    },
                    {
                        "name": "Mercy Nyamewaa Asiedu"
                    }
                ],
                "author_detail": {
                    "name": "Mercy Nyamewaa Asiedu"
                },
                "author": "Mercy Nyamewaa Asiedu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.08328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08328v1",
                "updated": "2025-01-14T18:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:59:03Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PokerBench: Training Large Language Models to become Professional Poker\n  Players"
                },
                "summary": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}."
                },
                "authors": [
                    {
                        "name": "Richard Zhuang"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Richard Yang"
                    },
                    {
                        "name": "Aniket Rahane"
                    },
                    {
                        "name": "Zhengyu Li"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08324v1",
                "updated": "2025-01-14T18:56:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations"
                },
                "summary": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics."
                },
                "authors": [
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Vishaldeep Kaur Sekhon"
                    },
                    {
                        "name": "Ouyang Guo"
                    },
                    {
                        "name": "Mark Newman"
                    },
                    {
                        "name": "Roozbeh Sadeghian"
                    },
                    {
                        "name": "Maria L. Vaida"
                    },
                    {
                        "name": "Cynthia Jo"
                    },
                    {
                        "name": "Doyle Ward"
                    },
                    {
                        "name": "Vanni Bucci"
                    },
                    {
                        "name": "John P. Haran"
                    }
                ],
                "author_detail": {
                    "name": "John P. Haran"
                },
                "author": "John P. Haran",
                "arxiv_comment": "16 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08322v1",
                "updated": "2025-01-14T18:55:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:55:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    55,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data"
                },
                "summary": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages."
                },
                "authors": [
                    {
                        "name": "Amirhossein Aliakbarzadeh"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08319v1",
                "updated": "2025-01-14T18:53:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    53,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automated Interpretability with Output-Centric Feature\n  Descriptions"
                },
                "summary": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability pipelines generate natural language descriptions\nfor the concepts represented by features in large language models (LLMs), such\nas plants or the first word in a sentence. These descriptions are derived using\ninputs that activate the feature, which may be a dimension or a direction in\nthe model's representation space. However, identifying activating inputs is\ncostly, and the mechanistic role of a feature in model behavior is determined\nboth by how inputs cause a feature to activate and by how feature activation\naffects outputs. Using steering evaluations, we reveal that current pipelines\nprovide descriptions that fail to capture the causal effect of the feature on\noutputs. To fix this, we propose efficient, output-centric methods for\nautomatically generating feature descriptions. These methods use the tokens\nweighted higher after feature stimulation or the highest weight tokens after\napplying the vocabulary \"unembedding\" head directly to the feature. Our\noutput-centric descriptions better capture the causal effect of a feature on\nmodel outputs than input-centric descriptions, but combining the two leads to\nthe best performance on both input and output evaluations. Lastly, we show that\noutput-centric descriptions can be used to find inputs that activate features\npreviously thought to be \"dead\"."
                },
                "authors": [
                    {
                        "name": "Yoav Gur-Arieh"
                    },
                    {
                        "name": "Roy Mayan"
                    },
                    {
                        "name": "Chen Agassy"
                    },
                    {
                        "name": "Atticus Geiger"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08306v1",
                "updated": "2025-01-14T18:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Path Loss Prediction Using Machine Learning with Extended Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Loss Prediction Using Machine Learning with Extended Features"
                },
                "summary": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments."
                },
                "authors": [
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Mathieu Chateauvert"
                    },
                    {
                        "name": "Ryan G. Dempsey"
                    },
                    {
                        "name": "Alexis Bose"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Bose"
                },
                "author": "Alexis Bose",
                "arxiv_comment": "4 pages, 4 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08292v1",
                "updated": "2025-01-14T18:13:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:13:08Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    13,
                    8,
                    1,
                    14,
                    0
                ],
                "title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them"
                },
                "summary": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models."
                },
                "authors": [
                    {
                        "name": "Abhilasha Ravichander"
                    },
                    {
                        "name": "Shrusti Ghela"
                    },
                    {
                        "name": "David Wadden"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08284v1",
                "updated": "2025-01-14T18:00:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T18:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    0,
                    7,
                    1,
                    14,
                    0
                ],
                "title": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\n  Datasets for African Languages"
                },
                "summary": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate speech and abusive language are global phenomena that need\nsocio-cultural background knowledge to be understood, identified, and\nmoderated. However, in many regions of the Global South, there have been\nseveral documented occurrences of (1) absence of moderation and (2) censorship\ndue to the reliance on keyword spotting out of context. Further, high-profile\nindividuals have frequently been at the center of the moderation process, while\nlarge and targeted hate speech campaigns against minorities have been\noverlooked. These limitations are mainly due to the lack of high-quality data\nin the local languages and the failure to include local communities in the\ncollection, annotation, and moderation processes. To address this issue, we\npresent AfriHate: a multilingual collection of hate speech and abusive language\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\nnative speakers familiar with the local culture. We report the challenges\nrelated to the construction of the datasets and present various classification\nbaseline results with and without using LLMs. The datasets, individual\nannotations, and hate speech and offensive language lexicons are available on\nhttps://github.com/AfriHate/AfriHate"
                },
                "authors": [
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Saminu Mohammad Aliyu"
                    },
                    {
                        "name": "Nelson Odhiambo Onyango"
                    },
                    {
                        "name": "Lilian D. A. Wanzare"
                    },
                    {
                        "name": "Samuel Rutunda"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Esubalew Alemneh"
                    },
                    {
                        "name": "Oumaima Hourrane"
                    },
                    {
                        "name": "Hagos Tesfahun Gebremichael"
                    },
                    {
                        "name": "Elyas Abdi Ismail"
                    },
                    {
                        "name": "Meriem Beloucif"
                    },
                    {
                        "name": "Ebrahim Chekol Jibril"
                    },
                    {
                        "name": "Andiswa Bukula"
                    },
                    {
                        "name": "Rooweither Mabuya"
                    },
                    {
                        "name": "Salomey Osei"
                    },
                    {
                        "name": "Abigail Oppong"
                    },
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Tadesse Kebede Guge"
                    },
                    {
                        "name": "Tesfa Tegegne Asfaw"
                    },
                    {
                        "name": "Chiamaka Ijeoma Chukwuneke"
                    },
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08276v1",
                "updated": "2025-01-14T17:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    50,
                    6,
                    1,
                    14,
                    0
                ],
                "title": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Robustness of LLMs to Sociodemographically-Conditioned\n  Paraphrasing"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance in various NLP\ntasks. However, there are concerns about their reliability in different domains\nof linguistic variations. Many works have proposed robustness evaluation\nmeasures for local adversarial attacks, but we need globally robust models\nunbiased to different language styles. We take a broader approach to explore a\nwider range of variations across sociodemographic dimensions to perform\nstructured reliability tests on the reasoning capacity of language models. We\nextend the SocialIQA dataset to create diverse paraphrased sets conditioned on\nsociodemographic styles. The assessment aims to provide a deeper understanding\nof LLMs in (a) their capability of generating demographic paraphrases with\nengineered prompts and (b) their reasoning capabilities in real-world, complex\nlanguage scenarios. We also explore measures such as perplexity,\nexplainability, and ATOMIC performance of paraphrases for fine-grained\nreliability analysis of LLMs on these sets. We find that demographic-specific\nparaphrasing significantly impacts the performance of language models,\nindicating that the subtleties of language variations remain a significant\nchallenge. The code and dataset will be made available for reproducibility and\nfuture research."
                },
                "authors": [
                    {
                        "name": "Pulkit Arora"
                    },
                    {
                        "name": "Akbar Karimi"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06693v2",
                "updated": "2025-01-14T17:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    29,
                    6,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-12T03:01:15Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    3,
                    1,
                    15,
                    6,
                    12,
                    0
                ],
                "title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation"
                },
                "summary": "Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods."
                },
                "authors": [
                    {
                        "name": "Ziyang Xie"
                    },
                    {
                        "name": "Zhizheng Liu"
                    },
                    {
                        "name": "Zhenghao Peng"
                    },
                    {
                        "name": "Wayne Wu"
                    },
                    {
                        "name": "Bolei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bolei Zhou"
                },
                "author": "Bolei Zhou",
                "arxiv_comment": "Project page: https://metadriverse.github.io/vid2sim/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08262v1",
                "updated": "2025-01-14T17:21:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T17:21:16Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    21,
                    16,
                    1,
                    14,
                    0
                ],
                "title": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the sustainable AI trilemma: a case study on LLM agents and\n  RAG"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capabilities, but\ntheir widespread deployment and more advanced applications raise critical\nsustainability challenges, particularly in inference energy consumption. We\npropose the concept of the Sustainable AI Trilemma, highlighting the tensions\nbetween AI capability, digital equity, and environmental sustainability.\nThrough a systematic case study of LLM agents and retrieval-augmented\ngeneration (RAG), we analyze the energy costs embedded in memory module designs\nand introduce novel metrics to quantify the trade-offs between energy\nconsumption and system performance. Our experimental results reveal significant\nenergy inefficiencies in current memory-augmented frameworks and demonstrate\nthat resource-constrained environments face disproportionate efficiency\npenalties. Our findings challenge the prevailing LLM-centric paradigm in agent\ndesign and provide practical insights for developing more sustainable AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Fan"
                },
                "author": "Zhong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02748v3",
                "updated": "2025-01-14T17:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    20,
                    4,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-03T17:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation"
                },
                "summary": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo"
                },
                "authors": [
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "arxiv_comment": "Accepted to AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06208v2",
                "updated": "2025-01-14T17:18:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    18,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-08T17:08:52Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    8,
                    52,
                    1,
                    282,
                    0
                ],
                "title": "A Physical Layer Security Framework for IRS-Assisted Integrated Sensing\n  and Semantic Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physical Layer Security Framework for IRS-Assisted Integrated Sensing\n  and Semantic Communication Systems"
                },
                "summary": "In this paper, we propose a physical layer security (PLS) framework for an\nintelligent reflecting surface (IRS)-assisted integrated sensing and semantic\ncommunication (ISASC) system, where a multi-antenna dual-functional semantic\nbase station (BS) serves multiple semantic communication users (SCUs) and\nmonitors a potentially malicious sensing target (MST) in the presence of an\neavesdropper (EVE). Both MST and EVE attempt to wiretap information from the\nsignals transmitted to the SCUs. The deployment of the IRS not only enhances\nPLS by directing a strong beam towards the SCUs, but also improves the\nlocalization information for the target without disclosing information about\nthe SCUs. To further strengthen PLS, we employ joint artificial noise (AN) and\ndedicated sensing signal (DSS), in addition to wiretap coding. To evaluate\nsensing accuracy, we derive the Cramer-Rao bound (CRB) for estimating the\ndirection of arrival (DoA), and to assess the PLS level of the ISASC system, we\ndetermine a closed-form expression for the semantic secrecy rate (SSR). To\nachieve an optimal trade-off between these two competing objectives, we\nformulate a multi-objective optimization problem (MOOP) for the joint design of\nthe BS's beamforming (BF) vectors and the IRS's phase shift vector. To tackle\nthis MOOP problem, the $\\epsilon$-constraint method is employed, followed by an\nalternating optimization (AO)-based algorithm that leverages the classical\nsuccessive convex approximation (SCA) and semidefinite relaxation (SDR)\ntechniques. Simulation results demonstrate that the proposed scheme outperforms\nthe baseline schemes, achieving a superior trade-off between SSR and CRB.\nSpecifically, our proposed approach improves the sensing accuracy by 5 dB\ncompared to the commonly adopted maximal ratio transmission (MRT) approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a physical layer security (PLS) framework for an\nintelligent reflecting surface (IRS)-assisted integrated sensing and semantic\ncommunication (ISASC) system, where a multi-antenna dual-functional semantic\nbase station (BS) serves multiple semantic communication users (SCUs) and\nmonitors a potentially malicious sensing target (MST) in the presence of an\neavesdropper (EVE). Both MST and EVE attempt to wiretap information from the\nsignals transmitted to the SCUs. The deployment of the IRS not only enhances\nPLS by directing a strong beam towards the SCUs, but also improves the\nlocalization information for the target without disclosing information about\nthe SCUs. To further strengthen PLS, we employ joint artificial noise (AN) and\ndedicated sensing signal (DSS), in addition to wiretap coding. To evaluate\nsensing accuracy, we derive the Cramer-Rao bound (CRB) for estimating the\ndirection of arrival (DoA), and to assess the PLS level of the ISASC system, we\ndetermine a closed-form expression for the semantic secrecy rate (SSR). To\nachieve an optimal trade-off between these two competing objectives, we\nformulate a multi-objective optimization problem (MOOP) for the joint design of\nthe BS's beamforming (BF) vectors and the IRS's phase shift vector. To tackle\nthis MOOP problem, the $\\epsilon$-constraint method is employed, followed by an\nalternating optimization (AO)-based algorithm that leverages the classical\nsuccessive convex approximation (SCA) and semidefinite relaxation (SDR)\ntechniques. Simulation results demonstrate that the proposed scheme outperforms\nthe baseline schemes, achieving a superior trade-off between SSR and CRB.\nSpecifically, our proposed approach improves the sensing accuracy by 5 dB\ncompared to the commonly adopted maximal ratio transmission (MRT) approach."
                },
                "authors": [
                    {
                        "name": "Hamid Amiriara"
                    },
                    {
                        "name": "Mahtab Mirmohseni"
                    },
                    {
                        "name": "Ahmed Elzanaty"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "arxiv_comment": "Part of this paper has been accepted at the 2025 IEEE Wireless\n  Communications and Networking Conference, March 2025, Milan, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08246v1",
                "updated": "2025-01-14T16:32:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:32:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    32,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints"
                },
                "summary": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt."
                },
                "authors": [
                    {
                        "name": "Jonathan Nöther"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Goran Radanović"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanović"
                },
                "author": "Goran Radanović",
                "arxiv_comment": "This is an extended version of a paper published at AAAI 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08243v1",
                "updated": "2025-01-14T16:30:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:30:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    30,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps"
                },
                "summary": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows."
                },
                "authors": [
                    {
                        "name": "Kannan Parthasarathy"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Venkat Krishnamachari"
                    },
                    {
                        "name": "Basil Muhammed"
                    },
                    {
                        "name": "Adyansh Kakran"
                    },
                    {
                        "name": "Sreemaee Akshathala"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Sumant Dubey"
                    },
                    {
                        "name": "Mohan Veerubhotla"
                    },
                    {
                        "name": "Amey Karan"
                    }
                ],
                "author_detail": {
                    "name": "Amey Karan"
                },
                "author": "Amey Karan",
                "arxiv_comment": "The paper has been accepted as full paper to CAIN 2025\n  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025\n  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN\n  for review on 9 November 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14831v3",
                "updated": "2025-01-14T16:17:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    17,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-23T17:47:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    47,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models"
                },
                "summary": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG."
                },
                "authors": [
                    {
                        "name": "Bernal Jiménez Gutiérrez"
                    },
                    {
                        "name": "Yiheng Shu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "NeurIPS 2024. Code and data:\n  https://github.com/OSU-NLP-Group/HippoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10729v2",
                "updated": "2025-01-14T16:17:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    17,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-15T20:04:06Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    20,
                    4,
                    6,
                    5,
                    167,
                    0
                ],
                "title": "A Comprehensive Survey of Foundation Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Foundation Models in Medicine"
                },
                "summary": "Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks."
                },
                "authors": [
                    {
                        "name": "Wasif Khan"
                    },
                    {
                        "name": "Seowung Leem"
                    },
                    {
                        "name": "Kyle B. See"
                    },
                    {
                        "name": "Joshua K. Wong"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Ruogu Fang"
                    }
                ],
                "author_detail": {
                    "name": "Ruogu Fang"
                },
                "author": "Ruogu Fang",
                "arxiv_comment": "Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08219v1",
                "updated": "2025-01-14T16:02:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T16:02:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    16,
                    2,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings"
                },
                "summary": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems."
                },
                "authors": [
                    {
                        "name": "Paul Joe Maliakel"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14012v2",
                "updated": "2025-01-14T15:58:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    58,
                    2,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-21T10:54:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Logic Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Augmented Generation"
                },
                "summary": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results."
                },
                "authors": [
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08208v1",
                "updated": "2025-01-14T15:46:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    46,
                    39,
                    1,
                    14,
                    0
                ],
                "title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems"
                },
                "summary": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Ernest Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Lim"
                },
                "author": "Ernest Lim",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08203v1",
                "updated": "2025-01-14T15:38:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:38:41Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    38,
                    41,
                    1,
                    14,
                    0
                ],
                "title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math\n  Problem Solving"
                },
                "summary": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown impressive capabilities in math\nproblem-solving tasks, their robustness to noisy inputs is not well-studied. In\nthis work, we propose ArithmAttack to examine how robust the LLMs are when they\nencounter noisy prompts that contain extra noise in the form of punctuation\nmarks. While being easy to implement, ArithmAttack does not cause any\ninformation loss since words are not added or deleted from the context. We\nevaluate the robustness of seven LLMs, including LLama3, Mistral, and\nMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that\nall the studied models show vulnerability to such noise, with more noise\nleading to poorer performances."
                },
                "authors": [
                    {
                        "name": "Zain Ul Abedin"
                    },
                    {
                        "name": "Shahzeb Qamar"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Akbar Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Akbar Karimi"
                },
                "author": "Akbar Karimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03565v3",
                "updated": "2025-01-14T15:30:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    30,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-04T16:20:34Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    16,
                    20,
                    34,
                    3,
                    95,
                    0
                ],
                "title": "Personalized LLM Response Generation with Parameterized Memory Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM Response Generation with Parameterized Memory Injection"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP})."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08200v1",
                "updated": "2025-01-14T15:27:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    27,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:27:01Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    27,
                    1,
                    1,
                    14,
                    0
                ],
                "title": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval ."
                },
                "authors": [
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Leyi Cui"
                    },
                    {
                        "name": "Kele Huang"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "to be published in LLM4Code 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08197v1",
                "updated": "2025-01-14T15:22:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    22,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:22:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    22,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\n  LLM Training"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\nchallenge, often limiting their performance. To address this issue, we propose\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\nfocus on filtered, high-quality content derived from diverse Chinese web\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\ndata curation processes. Additionally, we conducted extensive experimental\nanalyses, including evaluations on smaller parameter models, which demonstrated\nsignificant performance improvements in tasks such as C-Eval, showcasing the\neffectiveness of the corpus for training Chinese LLMs."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ziyun Dai"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "The datasets are available on\n  https://huggingface.co/collections/opencsg/chinese-fineweb-66cfed105f502ece8f29643e\n  ; The code is on https://github.com/yuyijiong/fineweb-edu-chinese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v3",
                "updated": "2025-01-14T15:19:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    19,
                    52,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08188v1",
                "updated": "2025-01-14T15:13:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    13,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:13:00Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    13,
                    0,
                    1,
                    14,
                    0
                ],
                "title": "A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation"
                },
                "summary": "While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems."
                },
                "authors": [
                    {
                        "name": "Steven Landgraf"
                    },
                    {
                        "name": "Rongjun Qin"
                    },
                    {
                        "name": "Markus Ulrich"
                    }
                ],
                "author_detail": {
                    "name": "Markus Ulrich"
                },
                "author": "Markus Ulrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07572v2",
                "updated": "2025-01-14T15:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    6,
                    56,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-13T18:58:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "WebWalker: Benchmarking LLMs in Web Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebWalker: Benchmarking LLMs in Web Traversal"
                },
                "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08167v1",
                "updated": "2025-01-14T14:49:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    49,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:49:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    49,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data"
                },
                "summary": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases."
                },
                "authors": [
                    {
                        "name": "Rewina Bedemariam"
                    },
                    {
                        "name": "Natalie Perez"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Satya Kapoor"
                    },
                    {
                        "name": "Alex Gil"
                    },
                    {
                        "name": "Elizabeth Conjar"
                    },
                    {
                        "name": "Ikkei Itoku"
                    },
                    {
                        "name": "David Theil"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Naumaan Nayyar"
                    }
                ],
                "author_detail": {
                    "name": "Naumaan Nayyar"
                },
                "author": "Naumaan Nayyar",
                "arxiv_comment": "11 pages, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08165v1",
                "updated": "2025-01-14T14:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:46:19Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    46,
                    19,
                    1,
                    14,
                    0
                ],
                "title": "I Can Find You in Seconds! Leveraging Large Language Models for Code\n  Authorship Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can Find You in Seconds! Leveraging Large Language Models for Code\n  Authorship Attribution"
                },
                "summary": "Source code authorship attribution is important in software forensics,\nplagiarism detection, and protecting software patch integrity. Existing\ntechniques often rely on supervised machine learning, which struggles with\ngeneralization across different programming languages and coding styles due to\nthe need for large labeled datasets. Inspired by recent advances in natural\nlanguage authorship analysis using large language models (LLMs), which have\nshown exceptional performance without task-specific tuning, this paper explores\nthe use of LLMs for source code authorship attribution.\n  We present a comprehensive study demonstrating that state-of-the-art LLMs can\nsuccessfully attribute source code authorship across different languages. LLMs\ncan determine whether two code snippets are written by the same author with\nzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of\n0.78, and can attribute code authorship from a small set of reference code\nsnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show\nsome adversarial robustness against misattribution attacks.\n  Despite these capabilities, we found that naive prompting of LLMs does not\nscale well with a large number of authors due to input token limitations. To\naddress this, we propose a tournament-style approach for large-scale\nattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355\nsamples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve\nclassification accuracy of up to 65% for C++ and 68.7% for Java using only one\nreference per author. These results open new possibilities for applying LLMs to\ncode authorship attribution in cybersecurity and software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code authorship attribution is important in software forensics,\nplagiarism detection, and protecting software patch integrity. Existing\ntechniques often rely on supervised machine learning, which struggles with\ngeneralization across different programming languages and coding styles due to\nthe need for large labeled datasets. Inspired by recent advances in natural\nlanguage authorship analysis using large language models (LLMs), which have\nshown exceptional performance without task-specific tuning, this paper explores\nthe use of LLMs for source code authorship attribution.\n  We present a comprehensive study demonstrating that state-of-the-art LLMs can\nsuccessfully attribute source code authorship across different languages. LLMs\ncan determine whether two code snippets are written by the same author with\nzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of\n0.78, and can attribute code authorship from a small set of reference code\nsnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show\nsome adversarial robustness against misattribution attacks.\n  Despite these capabilities, we found that naive prompting of LLMs does not\nscale well with a large number of authors due to input token limitations. To\naddress this, we propose a tournament-style approach for large-scale\nattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355\nsamples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve\nclassification accuracy of up to 65% for C++ and 68.7% for Java using only one\nreference per author. These results open new possibilities for applying LLMs to\ncode authorship attribution in cybersecurity and software engineering."
                },
                "authors": [
                    {
                        "name": "Soohyeon Choi"
                    },
                    {
                        "name": "Yong Kiam Tan"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Mohamed Ragab"
                    },
                    {
                        "name": "Soumik Mondal"
                    },
                    {
                        "name": "David Mohaisen"
                    },
                    {
                        "name": "Khin Mi Mi Aung"
                    }
                ],
                "author_detail": {
                    "name": "Khin Mi Mi Aung"
                },
                "author": "Khin Mi Mi Aung",
                "arxiv_comment": "12 pages, 5 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08152v1",
                "updated": "2025-01-14T14:26:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:26:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "Energy Backdoor Attack to Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Backdoor Attack to Deep Neural Networks"
                },
                "summary": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor."
                },
                "authors": [
                    {
                        "name": "Hanene F. Z. Brachemi Meftah"
                    },
                    {
                        "name": "Wassim Hamidouche"
                    },
                    {
                        "name": "Sid Ahmed Fezza"
                    },
                    {
                        "name": "Olivier Déforges"
                    },
                    {
                        "name": "Kassem Kallas"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Kallas"
                },
                "author": "Kassem Kallas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16779v2",
                "updated": "2025-01-14T14:26:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    26,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2024-08-15T16:41:00Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    41,
                    0,
                    3,
                    228,
                    0
                ],
                "title": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis"
                },
                "summary": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs."
                },
                "authors": [
                    {
                        "name": "João Pedro Gandarela"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08145v1",
                "updated": "2025-01-14T14:23:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    23,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T14:23:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    23,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "Refusal Behavior in Large Language Models: A Nonlinear Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal Behavior in Large Language Models: A Nonlinear Perspective"
                },
                "summary": "Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal behavior in large language models (LLMs) enables them to decline\nresponding to harmful, unethical, or inappropriate prompts, ensuring alignment\nwith ethical standards. This paper investigates refusal behavior across six\nLLMs from three architectural families. We challenge the assumption of refusal\nas a linear phenomenon by employing dimensionality reduction techniques,\nincluding PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms\nexhibit nonlinear, multidimensional characteristics that vary by model\narchitecture and layer. These findings highlight the need for nonlinear\ninterpretability to improve alignment research and inform safer AI deployment\nstrategies."
                },
                "authors": [
                    {
                        "name": "Fabian Hildebrandt"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Patrick Krauss"
                    },
                    {
                        "name": "Achim Schilling"
                    }
                ],
                "author_detail": {
                    "name": "Achim Schilling"
                },
                "author": "Achim Schilling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13612v2",
                "updated": "2025-01-14T14:16:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    16,
                    45,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-18T08:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models"
                },
                "summary": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "12 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08102v1",
                "updated": "2025-01-14T13:19:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T13:19:47Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    19,
                    47,
                    1,
                    14,
                    0
                ],
                "title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design."
                },
                "authors": [
                    {
                        "name": "Wenlu Fan"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Wentao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Xu"
                },
                "author": "Wentao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16409v2",
                "updated": "2025-01-14T13:14:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    13,
                    14,
                    0,
                    1,
                    14,
                    0
                ],
                "published": "2023-12-27T04:40:12Z",
                "published_parsed": [
                    2023,
                    12,
                    27,
                    4,
                    40,
                    12,
                    2,
                    361,
                    0
                ],
                "title": "Dynamic Sub-graph Distillation for Robust Semi-supervised Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Sub-graph Distillation for Robust Semi-supervised Continual\n  Learning"
                },
                "summary": "Continual learning (CL) has shown promising results and comparable\nperformance to learning at once in a fully supervised manner. However, CL\nstrategies typically require a large number of labeled samples, making their\nreal-life deployment challenging. In this work, we focus on semi-supervised\ncontinual learning (SSCL), where the model progressively learns from partially\nlabeled data with unknown categories. We provide a comprehensive analysis of\nSSCL and demonstrate that unreliable distributions of unlabeled data lead to\nunstable training and refinement of the progressing stages. This problem\nseverely impacts the performance of SSCL. To address the limitations, we\npropose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for\nsemi-supervised continual learning, which leverages both semantic and\nstructural information to achieve more stable knowledge distillation on\nunlabeled data and exhibit robustness against distribution bias. Firstly, we\nformalize a general model of structural distillation and design a dynamic graph\nconstruction for the continual learning progress. Next, we define a structure\ndistillation vector and design a dynamic sub-graph distillation algorithm,\nwhich enables end-to-end training and adaptability to scale up tasks. The\nentire proposed method is adaptable to various CL methods and supervision\nsettings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,\nand ImageNet-100, with varying supervision ratios, demonstrate the\neffectiveness of our proposed approach in mitigating the catastrophic\nforgetting problem in semi-supervised continual learning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) has shown promising results and comparable\nperformance to learning at once in a fully supervised manner. However, CL\nstrategies typically require a large number of labeled samples, making their\nreal-life deployment challenging. In this work, we focus on semi-supervised\ncontinual learning (SSCL), where the model progressively learns from partially\nlabeled data with unknown categories. We provide a comprehensive analysis of\nSSCL and demonstrate that unreliable distributions of unlabeled data lead to\nunstable training and refinement of the progressing stages. This problem\nseverely impacts the performance of SSCL. To address the limitations, we\npropose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for\nsemi-supervised continual learning, which leverages both semantic and\nstructural information to achieve more stable knowledge distillation on\nunlabeled data and exhibit robustness against distribution bias. Firstly, we\nformalize a general model of structural distillation and design a dynamic graph\nconstruction for the continual learning progress. Next, we define a structure\ndistillation vector and design a dynamic sub-graph distillation algorithm,\nwhich enables end-to-end training and adaptability to scale up tasks. The\nentire proposed method is adaptable to various CL methods and supervision\nsettings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,\nand ImageNet-100, with varying supervision ratios, demonstrate the\neffectiveness of our proposed approach in mitigating the catastrophic\nforgetting problem in semi-supervised continual learning scenarios."
                },
                "authors": [
                    {
                        "name": "Yan Fan"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Pengfei Zhu"
                    },
                    {
                        "name": "Qinghua Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Hu"
                },
                "author": "Qinghua Hu",
                "arxiv_doi": "10.1609/aaai.v38i11.29079",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaai.v38i11.29079",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.16409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 38th AAAI Conference on Artificial\n  Intelligence, 2024, 38(11), 11927-11935",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08090v1",
                "updated": "2025-01-14T12:57:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    57,
                    40,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:57:40Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    57,
                    40,
                    1,
                    14,
                    0
                ],
                "title": "Hierarchical Autoscaling for Large Language Model Serving with Chiron",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Autoscaling for Large Language Model Serving with Chiron"
                },
                "summary": "Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions."
                },
                "authors": [
                    {
                        "name": "Archit Patke"
                    },
                    {
                        "name": "Dhemath Reddy"
                    },
                    {
                        "name": "Saurabh Jha"
                    },
                    {
                        "name": "Chandra Narayanaswami"
                    },
                    {
                        "name": "Zbigniew Kalbarczyk"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Ravishankar Iyer"
                },
                "author": "Ravishankar Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11531v2",
                "updated": "2025-01-14T12:56:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    56,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-18T12:40:51Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    40,
                    51,
                    0,
                    323,
                    0
                ],
                "title": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Hallucinations in Language Models with Knowledge Graph\n  Embeddings as an Additional Modality"
                },
                "summary": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval."
                },
                "authors": [
                    {
                        "name": "Viktoriia Chekalina"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Kuznetsov"
                },
                "author": "Andrey Kuznetsov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02953v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02953v4",
                "updated": "2025-01-14T12:55:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    55,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2023-10-04T16:44:23Z",
                "published_parsed": [
                    2023,
                    10,
                    4,
                    16,
                    44,
                    23,
                    2,
                    277,
                    0
                ],
                "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction\n  Tuning"
                },
                "summary": "Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is vital for enhancing the performance of large language\nmodels (LLMs), but existing text-to-text methods, referred to as TextTuning,\nstruggle with issues such as generalization, robustness, and controllability\ndue to their lack of explicit task structures. We introduce JsonTuning, a\nstructure-to-structure approach that uses JSON structures to represent tasks.\nThis method improves generalization by clarifying task elements and their\nrelations, boosts robustness by minimizing ambiguity, and enhances\ncontrollability by allowing precise control over outputs. We conduct an\nextensive comparative analysis between JsonTuning and TextTuning using various\nlanguage models and benchmarks. Our findings reveal that JsonTuning\nconsistently surpasses TextTuning in terms of performance, robustness, and\ncontrollability across different scenarios. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for developing more\neffective and reliable LLMs capable of handling diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Guizhen Chen"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02953v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02953v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.08878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.08878v3",
                "updated": "2025-01-14T12:53:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    53,
                    24,
                    1,
                    14,
                    0
                ],
                "published": "2023-02-17T13:50:53Z",
                "published_parsed": [
                    2023,
                    2,
                    17,
                    13,
                    50,
                    53,
                    4,
                    48,
                    0
                ],
                "title": "Less is More: The Influence of Pruning on the Explainability of CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: The Influence of Pruning on the Explainability of CNNs"
                },
                "summary": "Over the last century, deep learning models have become the state-of-the-art\nfor solving complex computer vision problems. These modern computer vision\nmodels have millions of parameters, which presents two major challenges: (1)\nthe increased computational requirements hamper the deployment in\nresource-constrained environments, such as mobile or IoT devices, and (2)\nexplaining the complex decisions of such networks to humans is challenging.\nNetwork pruning is a technical approach to reduce the complexity of models,\nwhere less important parameters are removed. The work presented in this paper\ninvestigates whether this reduction in technical complexity also helps with\nperceived explainability. To do so, we conducted a pre-study and two\nhuman-grounded experiments, assessing the effects of different pruning ratios\non explainability. Overall, we evaluate four different compression rates (i.e.,\n2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that\nlower compression rates have a positive influence on explainability, while\nhigher compression rates show negative effects. Furthermore, we were able to\nidentify sweet spots that increase both the perceived explainability and the\nmodel's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last century, deep learning models have become the state-of-the-art\nfor solving complex computer vision problems. These modern computer vision\nmodels have millions of parameters, which presents two major challenges: (1)\nthe increased computational requirements hamper the deployment in\nresource-constrained environments, such as mobile or IoT devices, and (2)\nexplaining the complex decisions of such networks to humans is challenging.\nNetwork pruning is a technical approach to reduce the complexity of models,\nwhere less important parameters are removed. The work presented in this paper\ninvestigates whether this reduction in technical complexity also helps with\nperceived explainability. To do so, we conducted a pre-study and two\nhuman-grounded experiments, assessing the effects of different pruning ratios\non explainability. Overall, we evaluate four different compression rates (i.e.,\n2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that\nlower compression rates have a positive influence on explainability, while\nhigher compression rates show negative effects. Furthermore, we were able to\nidentify sweet spots that increase both the perceived explainability and the\nmodel's performance."
                },
                "authors": [
                    {
                        "name": "Florian Merkle"
                    },
                    {
                        "name": "David Weber"
                    },
                    {
                        "name": "Pascal Schöttle"
                    },
                    {
                        "name": "Stephan Schlögl"
                    },
                    {
                        "name": "Martin Nocker"
                    }
                ],
                "author_detail": {
                    "name": "Martin Nocker"
                },
                "author": "Martin Nocker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.08878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.08878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08071v1",
                "updated": "2025-01-14T12:36:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    36,
                    18,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:36:18Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    36,
                    18,
                    1,
                    14,
                    0
                ],
                "title": "CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically."
                },
                "authors": [
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "cgo 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08068v1",
                "updated": "2025-01-14T12:34:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    34,
                    25,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T12:34:25Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    34,
                    25,
                    1,
                    14,
                    0
                ],
                "title": "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches."
                },
                "authors": [
                    {
                        "name": "Israel Puerta-Merino"
                    },
                    {
                        "name": "Carlos Núñez-Molina"
                    },
                    {
                        "name": "Pablo Mesejo"
                    },
                    {
                        "name": "Juan Fernández-Olivares"
                    }
                ],
                "author_detail": {
                    "name": "Juan Fernández-Olivares"
                },
                "author": "Juan Fernández-Olivares",
                "arxiv_comment": "5 pages, 0 figures, to be published in the AAAI Workshop on Planning\n  in the Era of LLMs ( https://llmforplanning.github.io )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03335v2",
                "updated": "2025-01-14T11:59:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    59,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-04T11:40:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    11,
                    40,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition"
                },
                "summary": "We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08043v1",
                "updated": "2025-01-14T11:51:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    51,
                    57,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T11:51:57Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    51,
                    57,
                    1,
                    14,
                    0
                ],
                "title": "PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning"
                },
                "summary": "Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST."
                },
                "authors": [
                    {
                        "name": "Marta Andronic"
                    },
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "George A. Constantinides"
                    }
                ],
                "author_detail": {
                    "name": "George A. Constantinides"
                },
                "author": "George A. Constantinides",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2309.02334",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16597v3",
                "updated": "2025-01-14T11:27:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    27,
                    28,
                    1,
                    14,
                    0
                ],
                "published": "2024-09-25T03:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jingjing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Chen"
                },
                "author": "Jingjing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06713v2",
                "updated": "2025-01-14T11:03:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    3,
                    56,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-12T04:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    44,
                    6,
                    6,
                    12,
                    0
                ],
                "title": "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation"
                },
                "summary": "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps://github.com/HKUDS/MiniRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps://github.com/HKUDS/MiniRAG."
                },
                "authors": [
                    {
                        "name": "Tianyu Fan"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07583v2",
                "updated": "2025-01-14T10:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    52,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2024-08-14T14:28:11Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "title": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey"
                },
                "summary": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development."
                },
                "authors": [
                    {
                        "name": "Hamza Kheddar"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Kheddar"
                },
                "author": "Hamza Kheddar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.04760 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08008v1",
                "updated": "2025-01-14T10:51:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    51,
                    31,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:51:31Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    51,
                    31,
                    1,
                    14,
                    0
                ],
                "title": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for\n  Parameter-Efficient Fine-Tuning"
                },
                "summary": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs."
                },
                "authors": [
                    {
                        "name": "Yao Liang"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07992v1",
                "updated": "2025-01-14T10:35:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    35,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T10:35:54Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    35,
                    54,
                    1,
                    14,
                    0
                ],
                "title": "LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS"
                },
                "summary": "As modern system of systems (SoS) become increasingly adaptive and human\ncentred, traditional architectures often struggle to support interoperability,\nreconfigurability, and effective human system interaction. This paper addresses\nthese challenges by advancing the state of the art holonic architecture for\nSoS, offering two main contributions to support these adaptive needs. First, we\npropose a layered architecture for holons, which includes reasoning,\ncommunication, and capabilities layers. This design facilitates seamless\ninteroperability among heterogeneous constituent systems by improving data\nexchange and integration. Second, inspired by principles of intelligent\nmanufacturing, we introduce specialised holons namely, supervisor, planner,\ntask, and resource holons aimed at enhancing the adaptability and\nreconfigurability of SoS. These specialised holons utilise large language\nmodels within their reasoning layers to support decision making and ensure real\ntime adaptability. We demonstrate our approach through a 3D mobility case study\nfocused on smart city transportation, showcasing its potential for managing\ncomplex, multimodal SoS environments. Additionally, we propose evaluation\nmethods to assess the architecture efficiency and scalability,laying the\ngroundwork for future empirical validations through simulations and real world\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern system of systems (SoS) become increasingly adaptive and human\ncentred, traditional architectures often struggle to support interoperability,\nreconfigurability, and effective human system interaction. This paper addresses\nthese challenges by advancing the state of the art holonic architecture for\nSoS, offering two main contributions to support these adaptive needs. First, we\npropose a layered architecture for holons, which includes reasoning,\ncommunication, and capabilities layers. This design facilitates seamless\ninteroperability among heterogeneous constituent systems by improving data\nexchange and integration. Second, inspired by principles of intelligent\nmanufacturing, we introduce specialised holons namely, supervisor, planner,\ntask, and resource holons aimed at enhancing the adaptability and\nreconfigurability of SoS. These specialised holons utilise large language\nmodels within their reasoning layers to support decision making and ensure real\ntime adaptability. We demonstrate our approach through a 3D mobility case study\nfocused on smart city transportation, showcasing its potential for managing\ncomplex, multimodal SoS environments. Additionally, we propose evaluation\nmethods to assess the architecture efficiency and scalability,laying the\ngroundwork for future empirical validations through simulations and real world\nimplementations."
                },
                "authors": [
                    {
                        "name": "Muhammad Ashfaq"
                    },
                    {
                        "name": "Ahmed R. Sadik"
                    },
                    {
                        "name": "Tommi Mikkonen"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Niko Mäkitalo"
                    }
                ],
                "author_detail": {
                    "name": "Niko Mäkitalo"
                },
                "author": "Niko Mäkitalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11005v2",
                "updated": "2025-01-14T09:52:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    52,
                    50,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-14T18:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    44,
                    23,
                    0,
                    288,
                    0
                ],
                "title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks"
                },
                "summary": "Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness."
                },
                "authors": [
                    {
                        "name": "Fangru Lin"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Si-Qing Chen"
                    },
                    {
                        "name": "Michael Wooldridge"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07959v1",
                "updated": "2025-01-14T09:23:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:23:30Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning"
                },
                "summary": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024)\nfocuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting\nspecial tokens into the demos and employing demo-level random search.\nNevertheless, this method lacks generality since it specifies the\ninstruction-response structure. Moreover, the reason why inserting special\ntokens takes effect in inducing harmful behaviors is only empirically\ndiscussed. In this paper, we take a deeper insight into the mechanism of\nspecial token injection and propose Self-Instruct Few-Shot Jailbreaking\n(Self-Instruct-FSJ) facilitated with the demo-level greedy search. This\nframework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."
                },
                "authors": [
                    {
                        "name": "Jiaqi Hua"
                    },
                    {
                        "name": "Wanxu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wanxu Wei"
                },
                "author": "Wanxu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07957v1",
                "updated": "2025-01-14T09:21:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    21,
                    17,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T09:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    21,
                    17,
                    1,
                    14,
                    0
                ],
                "title": "AI Guide Dog: Egocentric Path Prediction on Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Guide Dog: Egocentric Path Prediction on Smartphone"
                },
                "summary": "This paper introduces AI Guide Dog (AIGD), a lightweight egocentric\nnavigation assistance system for visually impaired individuals, designed for\nreal-time deployment on smartphones. AIGD addresses key challenges in blind\nnavigation by employing a vision-only, multi-label classification approach to\npredict directional commands, ensuring safe traversal across diverse\nenvironments. We propose a novel technique to enable goal-based outdoor\nnavigation by integrating GPS signals and high-level directions, while also\naddressing uncertain multi-path predictions for destination-free indoor\nnavigation. Our generalized model is the first navigation assistance system to\nhandle both goal-oriented and exploratory navigation scenarios across indoor\nand outdoor settings, establishing a new state-of-the-art in blind navigation.\nWe present methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AI Guide Dog (AIGD), a lightweight egocentric\nnavigation assistance system for visually impaired individuals, designed for\nreal-time deployment on smartphones. AIGD addresses key challenges in blind\nnavigation by employing a vision-only, multi-label classification approach to\npredict directional commands, ensuring safe traversal across diverse\nenvironments. We propose a novel technique to enable goal-based outdoor\nnavigation by integrating GPS signals and high-level directions, while also\naddressing uncertain multi-path predictions for destination-free indoor\nnavigation. Our generalized model is the first navigation assistance system to\nhandle both goal-oriented and exploratory navigation scenarios across indoor\nand outdoor settings, establishing a new state-of-the-art in blind navigation.\nWe present methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems."
                },
                "authors": [
                    {
                        "name": "Aishwarya Jadhav"
                    },
                    {
                        "name": "Jeffery Cao"
                    },
                    {
                        "name": "Abhishree Shetty"
                    },
                    {
                        "name": "Urvashi Priyam Kumar"
                    },
                    {
                        "name": "Aditi Sharma"
                    },
                    {
                        "name": "Ben Sukboontip"
                    },
                    {
                        "name": "Jayant Sravan Tamarapalli"
                    },
                    {
                        "name": "Jingyi Zhang"
                    },
                    {
                        "name": "Anirudh Koul"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Koul"
                },
                "author": "Anirudh Koul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07937v1",
                "updated": "2025-01-14T08:45:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    45,
                    40,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:45:40Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    45,
                    40,
                    1,
                    14,
                    0
                ],
                "title": "Towards high resolution, validated and open global wind power\n  assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards high resolution, validated and open global wind power\n  assessments"
                },
                "summary": "Wind power is expected to play a crucial role in future net-zero energy\nsystems, but wind power simulations to support deployment strategies vary\ndrastically in their results, hindering reliable design decisions. Therefore,\nwe present a transparent, open source, validated and evaluated, global wind\npower simulation tool ETHOS.RESKit$_{Wind}$ with high spatial resolution and\ncustomizable designs for both onshore and offshore wind turbines. The tool\nprovides a comprehensive validation and calibration procedure using over 16\nmillion global measurements from metrerological masts and wind turbine sites.\nWe achieve a global average capacity factor mean error of 0.006 and Pearson\ncorrelation of 0.865. In addition, we evaluate its performance against several\naggregated and statistical sources of wind power generation. The release of\nETHOS.RESKit$_{Wind}$ is a step towards a fully open source and open data\napproach to accurate wind power modeling by incorporating the most\ncomprehensive simulation advances in one model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind power is expected to play a crucial role in future net-zero energy\nsystems, but wind power simulations to support deployment strategies vary\ndrastically in their results, hindering reliable design decisions. Therefore,\nwe present a transparent, open source, validated and evaluated, global wind\npower simulation tool ETHOS.RESKit$_{Wind}$ with high spatial resolution and\ncustomizable designs for both onshore and offshore wind turbines. The tool\nprovides a comprehensive validation and calibration procedure using over 16\nmillion global measurements from metrerological masts and wind turbine sites.\nWe achieve a global average capacity factor mean error of 0.006 and Pearson\ncorrelation of 0.865. In addition, we evaluate its performance against several\naggregated and statistical sources of wind power generation. The release of\nETHOS.RESKit$_{Wind}$ is a step towards a fully open source and open data\napproach to accurate wind power modeling by incorporating the most\ncomprehensive simulation advances in one model."
                },
                "authors": [
                    {
                        "name": "Edgar Ubaldo Peña-Sánchez"
                    },
                    {
                        "name": "Philipp Dunkel"
                    },
                    {
                        "name": "Christoph Winkler"
                    },
                    {
                        "name": "Heidi Heinrichs"
                    },
                    {
                        "name": "Florian Prinz"
                    },
                    {
                        "name": "Jann Weinand"
                    },
                    {
                        "name": "Rachel Maier"
                    },
                    {
                        "name": "Sebastian Dickler"
                    },
                    {
                        "name": "Shuying Chen"
                    },
                    {
                        "name": "Katharina Gruber"
                    },
                    {
                        "name": "Theresa Klütz"
                    },
                    {
                        "name": "Jochen Linßen"
                    },
                    {
                        "name": "Detlef Stolten"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Stolten"
                },
                "author": "Detlef Stolten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07930v1",
                "updated": "2025-01-14T08:32:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    32,
                    12,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:32:12Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    32,
                    12,
                    1,
                    14,
                    0
                ],
                "title": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN\n  Architectures"
                },
                "summary": "Orthogonal convolutional layers are the workhorse of multiple areas in\nmachine learning, such as adversarial robustness, normalizing flows, GANs, and\nLipschitzconstrained models. Their ability to preserve norms and ensure stable\ngradient propagation makes them valuable for a large range of problems. Despite\ntheir promise, the deployment of orthogonal convolution in large-scale\napplications is a significant challenge due to computational overhead and\nlimited support for modern features like strides, dilations, group\nconvolutions, and transposed convolutions.In this paper, we introduce AOC\n(Adaptative Orthogonal Convolution), a scalable method for constructing\northogonal convolutions, effectively overcoming these limitations. This\nadvancement unlocks the construction of architectures that were previously\nconsidered impractical. We demonstrate through our experiments that our method\nproduces expressive models that become increasingly efficient as they scale. To\nfoster further advancement, we provide an open-source library implementing this\nmethod, available at https://github.com/thib-s/orthogonium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal convolutional layers are the workhorse of multiple areas in\nmachine learning, such as adversarial robustness, normalizing flows, GANs, and\nLipschitzconstrained models. Their ability to preserve norms and ensure stable\ngradient propagation makes them valuable for a large range of problems. Despite\ntheir promise, the deployment of orthogonal convolution in large-scale\napplications is a significant challenge due to computational overhead and\nlimited support for modern features like strides, dilations, group\nconvolutions, and transposed convolutions.In this paper, we introduce AOC\n(Adaptative Orthogonal Convolution), a scalable method for constructing\northogonal convolutions, effectively overcoming these limitations. This\nadvancement unlocks the construction of architectures that were previously\nconsidered impractical. We demonstrate through our experiments that our method\nproduces expressive models that become increasingly efficient as they scale. To\nfoster further advancement, we provide an open-source library implementing this\nmethod, available at https://github.com/thib-s/orthogonium."
                },
                "authors": [
                    {
                        "name": "Thibaut Boissin"
                    },
                    {
                        "name": "Franck Mamalet"
                    },
                    {
                        "name": "Thomas Fel"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Thomas Massena"
                    },
                    {
                        "name": "Mathieu Serrurier"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Serrurier"
                },
                "arxiv_affiliation": "IRIT, ANITI",
                "author": "Mathieu Serrurier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v1",
                "updated": "2025-01-14T08:30:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Václav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Bazińska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damián Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adrià Romero-López"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Natalie Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07919v1",
                "updated": "2025-01-14T08:10:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    10,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T08:10:43Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    10,
                    43,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Model Interface for Home Energy Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Interface for Home Energy Management Systems"
                },
                "summary": "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and/or few-shot prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and/or few-shot prompting."
                },
                "authors": [
                    {
                        "name": "François Michelon"
                    },
                    {
                        "name": "Yihong Zhou"
                    },
                    {
                        "name": "Thomas Morstyn"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Morstyn"
                },
                "author": "Thomas Morstyn",
                "arxiv_comment": "13 pages conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07240v2",
                "updated": "2025-01-14T07:57:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    57,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-11T18:59:02Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"
                },
                "summary": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and generality. This paper\nintroduces the UTMath Benchmark, a robust evaluation framework designed to\nassess LLMs through extensive unit tests, with a focus on both the accuracy and\ngenerality of model responses. It comprises 1,053 cutting-edge problems\nspanning nine mathematical domains, with an average of 68 test cases per\nproblem. UTMath is highly challenging, with the best-performing model, o1-mini,\nsolving only 32.57\\% of the problems, followed by o1-preview at 27.16\\%, and\nGPT-4o at 26.93\\%. Furthermore, we present the Reasoning-to-Coding of Thoughts\n(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to\ncode generation, thereby facilitating the production of more sophisticated\nsolutions and enhancing overall performance and efficiency. Additionally, we\nalso release the UTMath-Train training dataset (more than 70k samples), to\nsupport the community in further exploring mathematical reasoning. Our\nbenchmark can be accessed via the following link:\nhttps://github.com/UTMathGroup/UTMath"
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Qingping Yang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Runtao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Runtao Liu"
                },
                "author": "Runtao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07892v1",
                "updated": "2025-01-14T07:16:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    16,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T07:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    7,
                    16,
                    43,
                    1,
                    14,
                    0
                ],
                "title": "Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation\n  in LLMs"
                },
                "summary": "Automated code generation using large language models (LLMs) has gained\nattention due to its efficiency and adaptability. However, real-world coding\ntasks or benchmarks like HumanEval and StudentEval often lack dedicated\ntraining datasets, challenging existing few-shot prompting approaches that rely\non reference examples. Inspired by human metamemory-a cognitive process\ninvolving recall and evaluation-we present a novel framework (namely M^2WF) for\nimproving LLMs' one-time code generation. This approach enables LLMs to\nautonomously generate, evaluate, and utilize synthetic examples to enhance\nreliability and performance. Unlike prior methods, it minimizes dependency on\ncurated data and adapts flexibly to various coding scenarios. Our experiments\ndemonstrate significant improvements in coding benchmarks, offering a scalable\nand robust solution for data-free environments. The code and framework will be\npublicly available on GitHub and HuggingFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code generation using large language models (LLMs) has gained\nattention due to its efficiency and adaptability. However, real-world coding\ntasks or benchmarks like HumanEval and StudentEval often lack dedicated\ntraining datasets, challenging existing few-shot prompting approaches that rely\non reference examples. Inspired by human metamemory-a cognitive process\ninvolving recall and evaluation-we present a novel framework (namely M^2WF) for\nimproving LLMs' one-time code generation. This approach enables LLMs to\nautonomously generate, evaluate, and utilize synthetic examples to enhance\nreliability and performance. Unlike prior methods, it minimizes dependency on\ncurated data and adapts flexibly to various coding scenarios. Our experiments\ndemonstrate significant improvements in coding benchmarks, offering a scalable\nand robust solution for data-free environments. The code and framework will be\npublicly available on GitHub and HuggingFace."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Zheng He"
                    },
                    {
                        "name": "Dapeng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dapeng Tao"
                },
                "author": "Dapeng Tao",
                "arxiv_comment": "11 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15322v2",
                "updated": "2025-01-14T06:59:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    59,
                    12,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-20T07:32:16Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    32,
                    16,
                    6,
                    294,
                    0
                ],
                "title": "FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model"
                },
                "summary": "Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality."
                },
                "authors": [
                    {
                        "name": "Haoye Chai"
                    },
                    {
                        "name": "Xiaoqian Qi"
                    },
                    {
                        "name": "Shiyuan Zhang"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.03852v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.03852v3",
                "updated": "2025-01-14T06:40:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    40,
                    36,
                    1,
                    14,
                    0
                ],
                "published": "2023-09-07T17:07:36Z",
                "published_parsed": [
                    2023,
                    9,
                    7,
                    17,
                    7,
                    36,
                    3,
                    250,
                    0
                ],
                "title": "FLM-101B: An Open LLM and How to Train It with $100K Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLM-101B: An Open LLM and How to Train It with $100K Budget"
                },
                "summary": "Large language models (LLMs) are considered important approaches towards\nfoundational machine intelligence, achieving remarkable success in Natural\nLanguage Processing and multimodal tasks, among others. However, the carbon\nfootprints and financial costs originating from heavy pre-training computation\nis a non-negligible issue. Progressive training methods, inspired by the\nneurogenesis process that grows neural structures, have shown potential to\naccelerate LLM pre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B parameters remain\nunderexplored. In this paper, we show that our model, namely FLM-101B, trained\nwith our growth strategy under a budget of \\$100K, reaches 80\\% of the\nbaselines' performances with only 10\\% of their floating-point operations. We\nbelieve that further studies on progressive training will benefit the community\nby cutting down the costs and promoting green AI. The checkpoint of FLM-101B is\nreleased at https://huggingface.co/CofeAI/FLM-101B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are considered important approaches towards\nfoundational machine intelligence, achieving remarkable success in Natural\nLanguage Processing and multimodal tasks, among others. However, the carbon\nfootprints and financial costs originating from heavy pre-training computation\nis a non-negligible issue. Progressive training methods, inspired by the\nneurogenesis process that grows neural structures, have shown potential to\naccelerate LLM pre-training. However, the algorithms, implementation, and\npractices for progressively training LLMs beyond 100B parameters remain\nunderexplored. In this paper, we show that our model, namely FLM-101B, trained\nwith our growth strategy under a budget of \\$100K, reaches 80\\% of the\nbaselines' performances with only 10\\% of their floating-point operations. We\nbelieve that further studies on progressive training will benefit the community\nby cutting down the costs and promoting green AI. The checkpoint of FLM-101B is\nreleased at https://huggingface.co/CofeAI/FLM-101B."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.03852v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.03852v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v6",
                "updated": "2025-01-14T06:25:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    25,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Kshitij Sharad Jadhav"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09012v2",
                "updated": "2025-01-14T06:06:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    6,
                    6,
                    54,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-12T07:23:52Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    23,
                    52,
                    3,
                    347,
                    0
                ],
                "title": "What Makes Cryptic Crosswords Challenging for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Cryptic Crosswords Challenging for LLMs?"
                },
                "summary": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptic crosswords are puzzles that rely on general knowledge and the\nsolver's ability to manipulate language on different levels, dealing with\nvarious types of wordplay. Previous research suggests that solving such puzzles\nis challenging even for modern NLP models, including Large Language Models\n(LLMs). However, there is little to no research on the reasons for their poor\nperformance on this task. In this paper, we establish the benchmark results for\nthree popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance\non this task is still significantly below that of humans. We also investigate\nwhy these models struggle to achieve superior performance. We release our code\nand introduced datasets at\nhttps://github.com/bodasadallah/decrypting-crosswords."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Sadallah"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094",
                "arxiv_journal_ref": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07861v1",
                "updated": "2025-01-14T05:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    56,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:56:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    56,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process\n  Rewarding"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)\nhold promise in knowledge-intensive tasks but face limitations in complex\nmulti-step reasoning. While recent methods have integrated RAG with\nchain-of-thought reasoning or test-time search using Process Reward Models\n(PRMs), these approaches encounter challenges such as a lack of explanations,\nbias in PRM training data, early-step bias in PRM scores, and insufficient\npost-training optimization of reasoning potential. To address these issues, we\npropose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding\n(ReARTeR), a framework that enhances RAG systems' reasoning capabilities\nthrough post-training and test-time scaling. At test time, ReARTeR introduces\nTrustworthy Process Rewarding via a Process Reward Model for accurate scalar\nscoring and a Process Explanation Model (PEM) for generating natural language\nexplanations, enabling step refinement. During post-training, it utilizes Monte\nCarlo Tree Search guided by Trustworthy Process Rewarding to collect\nhigh-quality step-level preference data, optimized through Iterative Preference\nOptimization. ReARTeR addresses three core challenges: (1) misalignment between\nPRM and PEM, tackled through off-policy preference learning; (2) bias in PRM\ntraining data, mitigated by balanced annotation methods and stronger\nannotations for challenging examples; and (3) early-step bias in PRM, resolved\nthrough a temporal-difference-based look-ahead search strategy. Experimental\nresults on multi-step reasoning benchmarks demonstrate significant\nimprovements, underscoring ReARTeR's potential to advance the reasoning\ncapabilities of RAG systems."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Qipeng Wang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Song Yang"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01710v2",
                "updated": "2025-01-14T05:49:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    49,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-27T10:25:37Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    10,
                    25,
                    37,
                    3,
                    179,
                    0
                ],
                "title": "Failure Diagnosis in Microservice Systems: A Comprehensive Survey and\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Diagnosis in Microservice Systems: A Comprehensive Survey and\n  Analysis"
                },
                "summary": "Widely adopted for their scalability and flexibility, modern microservice\nsystems present unique failure diagnosis challenges due to their independent\ndeployment and dynamic interactions. This complexity can lead to cascading\nfailures that negatively impact operational efficiency and user experience.\nRecognizing the critical role of fault diagnosis in improving the stability and\nreliability of microservice systems, researchers have conducted extensive\nstudies and achieved a number of significant results. This survey provides an\nexhaustive review of 98 scientific papers from 2003 to the present, including a\nthorough examination and elucidation of the fundamental concepts, system\narchitecture, and problem statement. It also includes a qualitative analysis of\nthe dimensions, providing an in-depth discussion of current best practices and\nfuture directions, aiming to further its development and application. In\naddition, this survey compiles publicly available datasets, toolkits, and\nevaluation metrics to facilitate the selection and validation of techniques for\npractitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Widely adopted for their scalability and flexibility, modern microservice\nsystems present unique failure diagnosis challenges due to their independent\ndeployment and dynamic interactions. This complexity can lead to cascading\nfailures that negatively impact operational efficiency and user experience.\nRecognizing the critical role of fault diagnosis in improving the stability and\nreliability of microservice systems, researchers have conducted extensive\nstudies and achieved a number of significant results. This survey provides an\nexhaustive review of 98 scientific papers from 2003 to the present, including a\nthorough examination and elucidation of the fundamental concepts, system\narchitecture, and problem statement. It also includes a qualitative analysis of\nthe dimensions, providing an in-depth discussion of current best practices and\nfuture directions, aiming to further its development and application. In\naddition, this survey compiles publicly available datasets, toolkits, and\nevaluation metrics to facilitate the selection and validation of techniques for\npractitioners."
                },
                "authors": [
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Sibo Xia"
                    },
                    {
                        "name": "Wenzhao Fan"
                    },
                    {
                        "name": "Binpeng Shi"
                    },
                    {
                        "name": "Xiao Xiong"
                    },
                    {
                        "name": "Zhenyu Zhong"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07857v1",
                "updated": "2025-01-14T05:48:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:48:27Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    27,
                    1,
                    14,
                    0
                ],
                "title": "Hierarchical Repository-Level Code Summarization for Business\n  Applications Using Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Repository-Level Code Summarization for Business\n  Applications Using Local LLMs"
                },
                "summary": "In large-scale software development, understanding the functionality and\nintent behind complex codebases is critical for effective development and\nmaintenance. While code summarization has been widely studied, existing methods\nprimarily focus on smaller code units, such as functions, and struggle with\nlarger code artifacts like files and packages. Additionally, current\nsummarization models tend to emphasize low-level implementation details, often\noverlooking the domain and business context that are crucial for real-world\napplications. This paper proposes a two-step hierarchical approach for\nrepository-level code summarization, tailored to business applications. First,\nsmaller code units such as functions and variables are identified using syntax\nanalysis and summarized with local LLMs. These summaries are then aggregated to\ngenerate higher-level file and package summaries. To ensure the summaries are\ngrounded in business context, we design custom prompts that capture the\nintended purpose of code artifacts based on the domain and problem context of\nthe business application. We evaluate our approach on a business support system\n(BSS) for the telecommunications domain, showing that syntax analysis-based\nhierarchical summarization improves coverage, while business-context grounding\nenhances the relevance of the generated summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale software development, understanding the functionality and\nintent behind complex codebases is critical for effective development and\nmaintenance. While code summarization has been widely studied, existing methods\nprimarily focus on smaller code units, such as functions, and struggle with\nlarger code artifacts like files and packages. Additionally, current\nsummarization models tend to emphasize low-level implementation details, often\noverlooking the domain and business context that are crucial for real-world\napplications. This paper proposes a two-step hierarchical approach for\nrepository-level code summarization, tailored to business applications. First,\nsmaller code units such as functions and variables are identified using syntax\nanalysis and summarized with local LLMs. These summaries are then aggregated to\ngenerate higher-level file and package summaries. To ensure the summaries are\ngrounded in business context, we design custom prompts that capture the\nintended purpose of code artifacts based on the domain and problem context of\nthe business application. We evaluate our approach on a business support system\n(BSS) for the telecommunications domain, showing that syntax analysis-based\nhierarchical summarization improves coverage, while business-context grounding\nenhances the relevance of the generated summaries."
                },
                "authors": [
                    {
                        "name": "Nilesh Dhulshette"
                    },
                    {
                        "name": "Sapan Shah"
                    },
                    {
                        "name": "Vinay Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kulkarni"
                },
                "author": "Vinay Kulkarni",
                "arxiv_comment": "To appear at LLM4Code@ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07853v1",
                "updated": "2025-01-14T05:41:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    41,
                    9,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:41:09Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    41,
                    9,
                    1,
                    14,
                    0
                ],
                "title": "Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Language Models for Grammatical Acceptability: A Comparative\n  Study of Fine-Tuning Techniques"
                },
                "summary": "This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the fine-tuning (FT) of the Open Pre-trained Transformer\n(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By\ncomparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and\nParameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation\n(LoRA), we demonstrate significant improvements in computational efficiency\nwhile maintaining high accuracy. Our experiments reveal that while VFT achieves\nthe highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and\niteration time by more than 50%, and increases accuracy in PBFT case. Context\nDistillation (CD), though computationally efficient, underperformed with\naccuracy around 31%. Our findings contribute to democratizing access to large\nlanguage models (LLM) by reducing computational barriers."
                },
                "authors": [
                    {
                        "name": "Shobhit Ratan"
                    },
                    {
                        "name": "Farley Knight"
                    },
                    {
                        "name": "Ghada Jerfel"
                    },
                    {
                        "name": "Sze Chung Ho"
                    }
                ],
                "author_detail": {
                    "name": "Sze Chung Ho"
                },
                "author": "Sze Chung Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05692v2",
                "updated": "2025-01-14T05:39:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    39,
                    40,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-08T17:18:04Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    17,
                    18,
                    4,
                    0,
                    99,
                    0
                ],
                "title": "Evaluating Mathematical Reasoning Beyond Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Mathematical Reasoning Beyond Accuracy"
                },
                "summary": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs validity and redundancy to characterize\nthe reasoning quality, as well as accompanying LLMs to assess them\nautomatically. We explore different design options for the LLM-based evaluators\nand empirically demonstrate that ReasonEval, when instantiated with base models\npossessing strong mathematical knowledge and trained with high-quality labeled\ndata, consistently outperforms baseline methods in the meta-evaluation\ndatasets. We also highlight the strong generalization capabilities of\nReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we\nfind that an increase in final-answer accuracy does not necessarily guarantee\nan improvement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We open-source the best-performing model,\nmeta-evaluation script, and all evaluation results to facilitate future\nresearch."
                },
                "authors": [
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "v2 is the AAAI 2025 camera ready version. Project site with code:\n  https://github.com/GAIR-NLP/ReasonEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15640v3",
                "updated": "2025-01-14T05:35:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    35,
                    8,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-23T19:43:02Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    19,
                    43,
                    2,
                    5,
                    328,
                    0
                ],
                "title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset"
                },
                "summary": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers."
                },
                "authors": [
                    {
                        "name": "Tobi Olatunji"
                    },
                    {
                        "name": "Charles Nimo"
                    },
                    {
                        "name": "Abraham Owodunni"
                    },
                    {
                        "name": "Tassallah Abdullahi"
                    },
                    {
                        "name": "Emmanuel Ayodele"
                    },
                    {
                        "name": "Mardhiyah Sanni"
                    },
                    {
                        "name": "Chinemelu Aka"
                    },
                    {
                        "name": "Folafunmi Omofoye"
                    },
                    {
                        "name": "Foutse Yuehgoh"
                    },
                    {
                        "name": "Timothy Faniran"
                    },
                    {
                        "name": "Bonaventure F. P. Dossou"
                    },
                    {
                        "name": "Moshood Yekini"
                    },
                    {
                        "name": "Jonas Kemp"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Jude Chidubem Omeke"
                    },
                    {
                        "name": "Chidi Asuzu MD"
                    },
                    {
                        "name": "Naome A. Etori"
                    },
                    {
                        "name": "Aimérou Ndiaye"
                    },
                    {
                        "name": "Ifeoma Okoh"
                    },
                    {
                        "name": "Evans Doe Ocansey"
                    },
                    {
                        "name": "Wendy Kinara"
                    },
                    {
                        "name": "Michael Best"
                    },
                    {
                        "name": "Irfan Essa"
                    },
                    {
                        "name": "Stephen Edward Moore"
                    },
                    {
                        "name": "Chris Fourie"
                    },
                    {
                        "name": "Mercy Nyamewaa Asiedu"
                    }
                ],
                "author_detail": {
                    "name": "Mercy Nyamewaa Asiedu"
                },
                "author": "Mercy Nyamewaa Asiedu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v4",
                "updated": "2025-01-14T05:23:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    23,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07849v1",
                "updated": "2025-01-14T05:21:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    21,
                    27,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:21:27Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    21,
                    27,
                    1,
                    14,
                    0
                ],
                "title": "Unveiling Provider Bias in Large Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Provider Bias in Large Language Models for Code Generation"
                },
                "summary": "Large Language Models (LLMs) have emerged as the new recommendation engines,\noutperforming traditional methods in both capability and scope, particularly in\ncode generation applications. Our research reveals a novel provider bias in\nLLMs, namely without explicit input prompts, these models show systematic\npreferences for services from specific providers in their recommendations\n(e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant\nimplications for market dynamics and societal equilibrium, potentially\npromoting digital monopolies. It may also deceive users and violate their\nexpectations, leading to various consequences. This paper presents the first\ncomprehensive empirical study of provider bias in LLM code generation. We\ndevelop a systematic methodology encompassing an automated pipeline for dataset\ngeneration, incorporating 6 distinct coding task categories and 30 real-world\napplication scenarios. Our analysis encompasses over 600,000 LLM-generated\nresponses across seven state-of-the-art models, utilizing approximately 500\nmillion tokens (equivalent to \\$5,000+ in computational costs). The study\nevaluates both the generated code snippets and their embedded service provider\nselections to quantify provider bias. Additionally, we conduct a comparative\nanalysis of seven debiasing prompting techniques to assess their efficacy in\nmitigating these biases. Our findings demonstrate that LLMs exhibit significant\nprovider preferences, predominantly favoring services from Google and Amazon,\nand can autonomously modify input code to incorporate their preferred providers\nwithout users' requests. Notably, we observe discrepancies between providers\nrecommended in conversational contexts versus those implemented in generated\ncode. The complete dataset and analysis results are available in our\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as the new recommendation engines,\noutperforming traditional methods in both capability and scope, particularly in\ncode generation applications. Our research reveals a novel provider bias in\nLLMs, namely without explicit input prompts, these models show systematic\npreferences for services from specific providers in their recommendations\n(e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant\nimplications for market dynamics and societal equilibrium, potentially\npromoting digital monopolies. It may also deceive users and violate their\nexpectations, leading to various consequences. This paper presents the first\ncomprehensive empirical study of provider bias in LLM code generation. We\ndevelop a systematic methodology encompassing an automated pipeline for dataset\ngeneration, incorporating 6 distinct coding task categories and 30 real-world\napplication scenarios. Our analysis encompasses over 600,000 LLM-generated\nresponses across seven state-of-the-art models, utilizing approximately 500\nmillion tokens (equivalent to \\$5,000+ in computational costs). The study\nevaluates both the generated code snippets and their embedded service provider\nselections to quantify provider bias. Additionally, we conduct a comparative\nanalysis of seven debiasing prompting techniques to assess their efficacy in\nmitigating these biases. Our findings demonstrate that LLMs exhibit significant\nprovider preferences, predominantly favoring services from Google and Amazon,\nand can autonomously modify input code to incorporate their preferred providers\nwithout users' requests. Notably, we observe discrepancies between providers\nrecommended in conversational contexts versus those implemented in generated\ncode. The complete dataset and analysis results are available in our\nrepository."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Qingshuang Bao"
                    },
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Chao Shen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "21 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12544v2",
                "updated": "2025-01-14T05:20:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    20,
                    53,
                    1,
                    14,
                    0
                ],
                "published": "2024-09-19T08:04:30Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    8,
                    4,
                    30,
                    3,
                    263,
                    0
                ],
                "title": "Nigerian Software Engineer or American Data Scientist? GitHub Profile\n  Recruitment Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigerian Software Engineer or American Data Scientist? GitHub Profile\n  Recruitment Bias in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have taken the world by storm, demonstrating\ntheir ability not only to automate tedious tasks, but also to show some degree\nof proficiency in completing software engineering tasks. A key concern with\nLLMs is their \"black-box\" nature, which obscures their internal workings and\ncould lead to societal biases in their outputs. In the software engineering\ncontext, in this early results paper, we empirically explore how well LLMs can\nautomate recruitment tasks for a geographically diverse software team. We use\nOpenAI's ChatGPT to conduct an initial set of experiments using GitHub User\nProfiles from four regions to recruit a six-person software development team,\nanalyzing a total of 3,657 profiles over a five-year period (2019-2023).\nResults indicate that ChatGPT shows preference for some regions over others,\neven when swapping the location strings of two profiles (counterfactuals).\nFurthermore, ChatGPT was more likely to assign certain developer roles to users\nfrom a specific country, revealing an implicit bias. Overall, this study\nreveals insights into the inner workings of LLMs and has implications for\nmitigating such societal biases in these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have taken the world by storm, demonstrating\ntheir ability not only to automate tedious tasks, but also to show some degree\nof proficiency in completing software engineering tasks. A key concern with\nLLMs is their \"black-box\" nature, which obscures their internal workings and\ncould lead to societal biases in their outputs. In the software engineering\ncontext, in this early results paper, we empirically explore how well LLMs can\nautomate recruitment tasks for a geographically diverse software team. We use\nOpenAI's ChatGPT to conduct an initial set of experiments using GitHub User\nProfiles from four regions to recruit a six-person software development team,\nanalyzing a total of 3,657 profiles over a five-year period (2019-2023).\nResults indicate that ChatGPT shows preference for some regions over others,\neven when swapping the location strings of two profiles (counterfactuals).\nFurthermore, ChatGPT was more likely to assign certain developer roles to users\nfrom a specific country, revealing an implicit bias. Overall, this study\nreveals insights into the inner workings of LLMs and has implications for\nmitigating such societal biases in these models."
                },
                "authors": [
                    {
                        "name": "Takashi Nakano"
                    },
                    {
                        "name": "Kazumasa Shimari"
                    },
                    {
                        "name": "Raula Gaikovina Kula"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Marc Cheong"
                    },
                    {
                        "name": "Kenichi Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Kenichi Matsumoto"
                },
                "author": "Kenichi Matsumoto",
                "arxiv_doi": "10.1109/ICSME58944.2024.00063",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICSME58944.2024.00063",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE International Conference on Software Maintenance and\n  Evolution (ICSME), Flagstaff, AZ, USA, 2024, pp. 624-629",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07845v1",
                "updated": "2025-01-14T05:18:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    18,
                    20,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T05:18:20Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    18,
                    20,
                    1,
                    14,
                    0
                ],
                "title": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable success across a\nwide range of tasks; however, they still encounter challenges in reasoning\ntasks that require understanding and inferring relationships between distinct\npieces of information within text sequences. This challenge is particularly\npronounced in tasks involving multi-step processes, such as logical reasoning\nand multi-hop question answering, where understanding implicit relationships\nbetween entities and leveraging multi-hop connections in the given context are\ncrucial. Graphs, as fundamental data structures, explicitly represent pairwise\nrelationships between entities, thereby offering the potential to enhance LLMs'\nreasoning capabilities. External graphs have proven effective in supporting\nLLMs across multiple tasks. However, in many reasoning tasks, no pre-existing\ngraph structure is provided. Can we structure implicit knowledge derived from\ncontext into graphs to assist LLMs in reasoning? In this paper, we propose\nReasoning with Graphs (RwG) by first constructing explicit graphs from the\ncontext and then leveraging these graphs to enhance LLM reasoning performance\non reasoning tasks. Extensive experiments demonstrate the effectiveness of the\nproposed method in improving both logical reasoning and multi-hop question\nanswering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable success across a\nwide range of tasks; however, they still encounter challenges in reasoning\ntasks that require understanding and inferring relationships between distinct\npieces of information within text sequences. This challenge is particularly\npronounced in tasks involving multi-step processes, such as logical reasoning\nand multi-hop question answering, where understanding implicit relationships\nbetween entities and leveraging multi-hop connections in the given context are\ncrucial. Graphs, as fundamental data structures, explicitly represent pairwise\nrelationships between entities, thereby offering the potential to enhance LLMs'\nreasoning capabilities. External graphs have proven effective in supporting\nLLMs across multiple tasks. However, in many reasoning tasks, no pre-existing\ngraph structure is provided. Can we structure implicit knowledge derived from\ncontext into graphs to assist LLMs in reasoning? In this paper, we propose\nReasoning with Graphs (RwG) by first constructing explicit graphs from the\ncontext and then leveraging these graphs to enhance LLM reasoning performance\non reasoning tasks. Extensive experiments demonstrate the effectiveness of the\nproposed method in improving both logical reasoning and multi-hop question\nanswering tasks."
                },
                "authors": [
                    {
                        "name": "Haoyu Han"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "William Headden"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Shuiwang Ji"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01917v2",
                "updated": "2025-01-14T05:02:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    2,
                    24,
                    1,
                    14,
                    0
                ],
                "published": "2024-07-02T03:32:09Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    3,
                    32,
                    9,
                    1,
                    184,
                    0
                ],
                "title": "Securing Distributed Network Digital Twin Systems Against Model\n  Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Distributed Network Digital Twin Systems Against Model\n  Poisoning Attacks"
                },
                "summary": "In the era of 5G and beyond, the increasing complexity of wireless networks\nnecessitates innovative frameworks for efficient management and deployment.\nDigital twins (DTs), embodying real-time monitoring, predictive configurations,\nand enhanced decision-making capabilities, stand out as a promising solution in\nthis context. Within a time-series data-driven framework that effectively maps\nwireless networks into digital counterparts, encapsulated by integrated\nvertical and horizontal twinning phases, this study investigates the security\nchallenges in distributed network DT systems, which potentially undermine the\nreliability of subsequent network applications such as wireless traffic\nforecasting. Specifically, we consider a minimal-knowledge scenario for all\nattackers, in that they do not have access to network data and other\nspecialized knowledge, yet can interact with previous iterations of\nserver-level models. In this context, we spotlight a novel fake traffic\ninjection attack designed to compromise a distributed network DT system for\nwireless traffic prediction. In response, we then propose a defense mechanism,\ntermed global-local inconsistency detection (GLID), to counteract various model\npoisoning threats. GLID strategically removes abnormal model parameters that\ndeviate beyond a particular percentile range, thereby fortifying the security\nof network twinning process. Through extensive experiments on real-world\nwireless traffic datasets, our experimental evaluations show that both our\nattack and defense strategies significantly outperform existing baselines,\nhighlighting the importance of security measures in the design and\nimplementation of DTs for 5G and beyond network systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of 5G and beyond, the increasing complexity of wireless networks\nnecessitates innovative frameworks for efficient management and deployment.\nDigital twins (DTs), embodying real-time monitoring, predictive configurations,\nand enhanced decision-making capabilities, stand out as a promising solution in\nthis context. Within a time-series data-driven framework that effectively maps\nwireless networks into digital counterparts, encapsulated by integrated\nvertical and horizontal twinning phases, this study investigates the security\nchallenges in distributed network DT systems, which potentially undermine the\nreliability of subsequent network applications such as wireless traffic\nforecasting. Specifically, we consider a minimal-knowledge scenario for all\nattackers, in that they do not have access to network data and other\nspecialized knowledge, yet can interact with previous iterations of\nserver-level models. In this context, we spotlight a novel fake traffic\ninjection attack designed to compromise a distributed network DT system for\nwireless traffic prediction. In response, we then propose a defense mechanism,\ntermed global-local inconsistency detection (GLID), to counteract various model\npoisoning threats. GLID strategically removes abnormal model parameters that\ndeviate beyond a particular percentile range, thereby fortifying the security\nof network twinning process. Through extensive experiments on real-world\nwireless traffic datasets, our experimental evaluations show that both our\nattack and defense strategies significantly outperform existing baselines,\nhighlighting the importance of security measures in the design and\nimplementation of DTs for 5G and beyond network systems."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Gaolei Li"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by Internet of Things Journal (IoT-J). arXiv admin note:\n  substantial text overlap with arXiv:2404.14389",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07837v1",
                "updated": "2025-01-14T04:41:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    41,
                    3,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T04:41:03Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    41,
                    3,
                    1,
                    14,
                    0
                ],
                "title": "A Driver Advisory System Based on Large Language Model for High-speed\n  Train",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Driver Advisory System Based on Large Language Model for High-speed\n  Train"
                },
                "summary": "With the rapid development of China high-speed railway, drivers face\nincreasingly significant technical challenges during operations, such as fault\nhandling. Currently, drivers depend on the onboard mechanic when facing\ntechnical issues, for instance, traction loss or sensor faults. This dependency\ncan hinder effective operation, even lead to accidents, while waiting for\nfaults to be addressed. To enhance the accuracy and explainability of actions\nduring fault handling, an Intelligent Driver Advisory System (IDAS) framework\nbased on a large language model (LLM) named IDAS-LLM, is introduced. Initially,\ndomain-fine-tuning of the LLM is performed using a constructed railway\nknowledge question-and-answer dataset to improve answer accuracy in\nrailway-related questions. Subsequently, integration of the Retrieval-augmented\nGeneration (RAG) architecture is pursued for system design to enhance the\nexplainability of generated responses. Comparative experiments are conducted\nusing the constructed railway driving knowledge assessment dataset. Results\nindicate that domain-fine-tuned LLMs show an improvement in answer accuracy by\nan average of 10%, outperforming some current mainstream LLMs. Additionally,\nthe inclusion of the RAG framework increases the average recall rate of\nquestion-and-answer sessions by about 4%. Finally, the fault handling\ncapability of IDAS-LLM is demonstrated through simulations of real operational\nscenarios, proving that the proposed framework has practical application\nprospects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of China high-speed railway, drivers face\nincreasingly significant technical challenges during operations, such as fault\nhandling. Currently, drivers depend on the onboard mechanic when facing\ntechnical issues, for instance, traction loss or sensor faults. This dependency\ncan hinder effective operation, even lead to accidents, while waiting for\nfaults to be addressed. To enhance the accuracy and explainability of actions\nduring fault handling, an Intelligent Driver Advisory System (IDAS) framework\nbased on a large language model (LLM) named IDAS-LLM, is introduced. Initially,\ndomain-fine-tuning of the LLM is performed using a constructed railway\nknowledge question-and-answer dataset to improve answer accuracy in\nrailway-related questions. Subsequently, integration of the Retrieval-augmented\nGeneration (RAG) architecture is pursued for system design to enhance the\nexplainability of generated responses. Comparative experiments are conducted\nusing the constructed railway driving knowledge assessment dataset. Results\nindicate that domain-fine-tuned LLMs show an improvement in answer accuracy by\nan average of 10%, outperforming some current mainstream LLMs. Additionally,\nthe inclusion of the RAG framework increases the average recall rate of\nquestion-and-answer sessions by about 4%. Finally, the fault handling\ncapability of IDAS-LLM is demonstrated through simulations of real operational\nscenarios, proving that the proposed framework has practical application\nprospects."
                },
                "authors": [
                    {
                        "name": "Y. C. Luo"
                    },
                    {
                        "name": "J. Xun"
                    },
                    {
                        "name": "W. Wang"
                    },
                    {
                        "name": "R. Z. Zhang"
                    },
                    {
                        "name": "Z. C. Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Z. C. Zhao"
                },
                "author": "Z. C. Zhao",
                "arxiv_comment": "18 pages, 7 figures, presented at 104th TRB Annual Meeting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07834v1",
                "updated": "2025-01-14T04:35:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    35,
                    37,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T04:35:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    35,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Flow: A Modular Approach to Automated Agentic Workflow Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow: A Modular Approach to Automated Agentic Workflow Generation"
                },
                "summary": "Multi-agent frameworks powered by large language models (LLMs) have\ndemonstrated great success in automated planning and task execution. However,\nthe effective adjustment of Agentic workflows during execution has not been\nwell-studied. A effective workflow adjustment is crucial, as in many real-world\nscenarios, the initial plan must adjust to unforeseen challenges and changing\nconditions in real-time to ensure the efficient execution of complex tasks. In\nthis paper, we define workflows as an activity-on-vertex (AOV) graphs. We\ncontinuously refine the workflow by dynamically adjusting task allocations\nbased on historical performance and previous AOV with LLM agents. To further\nenhance system performance, we emphasize modularity in workflow design based on\nmeasuring parallelism and dependence complexity. Our proposed multi-agent\nframework achieved efficient sub-task concurrent execution, goal achievement,\nand error tolerance. Empirical results across different practical tasks\ndemonstrate dramatic improvements in the efficiency of multi-agent frameworks\nthrough dynamic workflow updating and modularization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent frameworks powered by large language models (LLMs) have\ndemonstrated great success in automated planning and task execution. However,\nthe effective adjustment of Agentic workflows during execution has not been\nwell-studied. A effective workflow adjustment is crucial, as in many real-world\nscenarios, the initial plan must adjust to unforeseen challenges and changing\nconditions in real-time to ensure the efficient execution of complex tasks. In\nthis paper, we define workflows as an activity-on-vertex (AOV) graphs. We\ncontinuously refine the workflow by dynamically adjusting task allocations\nbased on historical performance and previous AOV with LLM agents. To further\nenhance system performance, we emphasize modularity in workflow design based on\nmeasuring parallelism and dependence complexity. Our proposed multi-agent\nframework achieved efficient sub-task concurrent execution, goal achievement,\nand error tolerance. Empirical results across different practical tasks\ndemonstrate dramatic improvements in the efficiency of multi-agent frameworks\nthrough dynamic workflow updating and modularization."
                },
                "authors": [
                    {
                        "name": "Boye Niu"
                    },
                    {
                        "name": "Yiliao Song"
                    },
                    {
                        "name": "Kai Lian"
                    },
                    {
                        "name": "Yifan Shen"
                    },
                    {
                        "name": "Yu Yao"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Tongliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tongliang Liu"
                },
                "author": "Tongliang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11869v3",
                "updated": "2025-01-14T04:25:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    25,
                    23,
                    1,
                    14,
                    0
                ],
                "published": "2024-08-19T02:27:00Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    2,
                    27,
                    0,
                    0,
                    232,
                    0
                ],
                "title": "ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA"
                },
                "summary": "Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER."
                },
                "authors": [
                    {
                        "name": "Jiaang Li"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Zhongnan Wang"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "Accepted by AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12476v2",
                "updated": "2025-01-14T04:19:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    19,
                    49,
                    1,
                    14,
                    0
                ],
                "published": "2024-10-16T11:46:32Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    46,
                    32,
                    2,
                    290,
                    0
                ],
                "title": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation"
                },
                "summary": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4."
                },
                "authors": [
                    {
                        "name": "Zerui Xu"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yuanyuan Zhang"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15240v3",
                "updated": "2025-01-14T04:10:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    4,
                    10,
                    46,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T01:58:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    1,
                    58,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "AI Foundation Models for Wearable Movement Data in Mental Health\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Foundation Models for Wearable Movement Data in Mental Health\n  Research"
                },
                "summary": "Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/"
                },
                "authors": [
                    {
                        "name": "Franklin Y. Ruan"
                    },
                    {
                        "name": "Aiwei Zhang"
                    },
                    {
                        "name": "Jenny Y. Oh"
                    },
                    {
                        "name": "SouYoung Jin"
                    },
                    {
                        "name": "Nicholas C. Jacobson"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas C. Jacobson"
                },
                "author": "Nicholas C. Jacobson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07824v1",
                "updated": "2025-01-14T03:59:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Real-time Verification and Refinement of Language Model Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Verification and Refinement of Language Model Text Generation"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods."
                },
                "authors": [
                    {
                        "name": "Joonho Ko"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07819v1",
                "updated": "2025-01-14T03:50:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    50,
                    23,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:50:23Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    50,
                    23,
                    1,
                    14,
                    0
                ],
                "title": "3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene\n  Understanding"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in\n2D tasks, yet encounter challenges in discerning the spatial positions,\ninterrelations, and causal logic in scenes when transitioning from 2D to 3D\nrepresentations. We find that the limitations mainly lie in: i) the high\nannotation cost restricting the scale-up of volumes of 3D scene data, and ii)\nthe lack of a straightforward and effective way to perceive 3D information\nwhich results in prolonged training durations and complicates the streamlined\nframework. To this end, we develop pipeline based on open-source 2D MLLMs and\nLLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance\nthe pre-training process. Leveraging this high-quality pre-training data, we\nintroduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise\ninterpretation of 3D scenes, showcasing exceptional capability in navigating\nthe complexities of the physical world. 3UR-LLM directly receives 3D point\ncloud as input and project 3D features fused with text instructions into a\nmanageable set of tokens. Considering the computation burden derived from these\nhybrid tokens, we design a 3D compressor module to cohesively compress the 3D\nspatial cues and textual narrative. 3UR-LLM achieves promising performance with\nrespect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts\nby 7.1\\% CIDEr on ScanQA, while utilizing fewer training resources. The code\nand model weights for 3UR-LLM and the 3DS-160K benchmark are available at\n3UR-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in\n2D tasks, yet encounter challenges in discerning the spatial positions,\ninterrelations, and causal logic in scenes when transitioning from 2D to 3D\nrepresentations. We find that the limitations mainly lie in: i) the high\nannotation cost restricting the scale-up of volumes of 3D scene data, and ii)\nthe lack of a straightforward and effective way to perceive 3D information\nwhich results in prolonged training durations and complicates the streamlined\nframework. To this end, we develop pipeline based on open-source 2D MLLMs and\nLLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance\nthe pre-training process. Leveraging this high-quality pre-training data, we\nintroduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise\ninterpretation of 3D scenes, showcasing exceptional capability in navigating\nthe complexities of the physical world. 3UR-LLM directly receives 3D point\ncloud as input and project 3D features fused with text instructions into a\nmanageable set of tokens. Considering the computation burden derived from these\nhybrid tokens, we design a 3D compressor module to cohesively compress the 3D\nspatial cues and textual narrative. 3UR-LLM achieves promising performance with\nrespect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts\nby 7.1\\% CIDEr on ScanQA, while utilizing fewer training resources. The code\nand model weights for 3UR-LLM and the 3DS-160K benchmark are available at\n3UR-LLM."
                },
                "authors": [
                    {
                        "name": "Haomiao Xiong"
                    },
                    {
                        "name": "Yunzhi Zhuge"
                    },
                    {
                        "name": "Jiawen Zhu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_comment": "Accepted to IEEE Transactions on Multimedia (TMM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07818v1",
                "updated": "2025-01-14T03:43:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    43,
                    23,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    43,
                    23,
                    1,
                    14,
                    0
                ],
                "title": "A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models"
                },
                "summary": "Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    }
                ],
                "author_detail": {
                    "name": "Kaustubh D. Dhole"
                },
                "author": "Kaustubh D. Dhole",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; I.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00090v2",
                "updated": "2025-01-14T03:27:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    27,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-27T12:34:45Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    34,
                    45,
                    2,
                    332,
                    0
                ],
                "title": "Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks"
                },
                "summary": "In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Zuguang Li"
                    },
                    {
                        "name": "Shaohua Wu"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Songge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Songge Zhang"
                },
                "author": "Songge Zhang",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07815v1",
                "updated": "2025-01-14T03:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    26,
                    43,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    26,
                    43,
                    1,
                    14,
                    0
                ],
                "title": "Agent-Centric Projection of Prompting Techniques and Implications for\n  Synthetic Training Data for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Centric Projection of Prompting Techniques and Implications for\n  Synthetic Training Data for Large Language Models"
                },
                "summary": "Recent advances in prompting techniques and multi-agent systems for Large\nLanguage Models (LLMs) have produced increasingly complex approaches. However,\nwe lack a framework for characterizing and comparing prompting techniques or\nunderstanding their relationship to multi-agent LLM systems. This position\npaper introduces and explains the concepts of linear contexts (a single,\ncontinuous sequence of interactions) and non-linear contexts (branching or\nmulti-path) in LLM systems. These concepts enable the development of an\nagent-centric projection of prompting techniques, a framework that can reveal\ndeep connections between prompting strategies and multi-agent systems. We\npropose three conjectures based on this framework: (1) results from non-linear\nprompting techniques can predict outcomes in equivalent multi-agent systems,\n(2) multi-agent system architectures can be replicated through single-LLM\nprompting techniques that simulate equivalent interaction patterns, and (3)\nthese equivalences suggest novel approaches for generating synthetic training\ndata. We argue that this perspective enables systematic cross-pollination of\nresearch findings between prompting and multi-agent domains, while providing\nnew directions for improving both the design and training of future LLM\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in prompting techniques and multi-agent systems for Large\nLanguage Models (LLMs) have produced increasingly complex approaches. However,\nwe lack a framework for characterizing and comparing prompting techniques or\nunderstanding their relationship to multi-agent LLM systems. This position\npaper introduces and explains the concepts of linear contexts (a single,\ncontinuous sequence of interactions) and non-linear contexts (branching or\nmulti-path) in LLM systems. These concepts enable the development of an\nagent-centric projection of prompting techniques, a framework that can reveal\ndeep connections between prompting strategies and multi-agent systems. We\npropose three conjectures based on this framework: (1) results from non-linear\nprompting techniques can predict outcomes in equivalent multi-agent systems,\n(2) multi-agent system architectures can be replicated through single-LLM\nprompting techniques that simulate equivalent interaction patterns, and (3)\nthese equivalences suggest novel approaches for generating synthetic training\ndata. We argue that this perspective enables systematic cross-pollination of\nresearch findings between prompting and multi-agent domains, while providing\nnew directions for improving both the design and training of future LLM\nsystems."
                },
                "authors": [
                    {
                        "name": "Dhruv Dhamani"
                    },
                    {
                        "name": "Mary Lou Maher"
                    }
                ],
                "author_detail": {
                    "name": "Mary Lou Maher"
                },
                "author": "Mary Lou Maher",
                "arxiv_comment": "8 pages, 5 figures. Accepted at ICAART 2025. Derived from an early\n  draft at 2312.17601. arXiv admin note: substantial text overlap with\n  arXiv:2312.17601",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07813v1",
                "updated": "2025-01-14T03:25:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    25,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:25:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    25,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "Talk to Right Specialists: Routing and Planning in Multi-agent System\n  for Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talk to Right Specialists: Routing and Planning in Multi-agent System\n  for Question Answering"
                },
                "summary": "Leveraging large language models (LLMs), an agent can utilize\nretrieval-augmented generation (RAG) techniques to integrate external knowledge\nand increase the reliability of its responses. Current RAG-based agents\nintegrate single, domain-specific knowledge sources, limiting their ability and\nleading to hallucinated or inaccurate responses when addressing cross-domain\nqueries. Integrating multiple knowledge bases into a unified RAG-based agent\nraises significant challenges, including increased retrieval overhead and data\nsovereignty when sensitive data is involved. In this work, we propose RopMura,\na novel multi-agent system that addresses these limitations by incorporating\nhighly efficient routing and planning mechanisms. RopMura features two key\ncomponents: a router that intelligently selects the most relevant agents based\non knowledge boundaries and a planner that decomposes complex multi-hop queries\ninto manageable steps, allowing for coordinating cross-domain responses.\nExperimental results demonstrate that RopMura effectively handles both\nsingle-hop and multi-hop queries, with the routing mechanism enabling precise\nanswers for single-hop queries and the combined routing and planning mechanisms\nachieving accurate, multi-step resolutions for complex queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs), an agent can utilize\nretrieval-augmented generation (RAG) techniques to integrate external knowledge\nand increase the reliability of its responses. Current RAG-based agents\nintegrate single, domain-specific knowledge sources, limiting their ability and\nleading to hallucinated or inaccurate responses when addressing cross-domain\nqueries. Integrating multiple knowledge bases into a unified RAG-based agent\nraises significant challenges, including increased retrieval overhead and data\nsovereignty when sensitive data is involved. In this work, we propose RopMura,\na novel multi-agent system that addresses these limitations by incorporating\nhighly efficient routing and planning mechanisms. RopMura features two key\ncomponents: a router that intelligently selects the most relevant agents based\non knowledge boundaries and a planner that decomposes complex multi-hop queries\ninto manageable steps, allowing for coordinating cross-domain responses.\nExperimental results demonstrate that RopMura effectively handles both\nsingle-hop and multi-hop queries, with the routing mechanism enabling precise\nanswers for single-hop queries and the combined routing and planning mechanisms\nachieving accurate, multi-step resolutions for complex queries."
                },
                "authors": [
                    {
                        "name": "Feijie Wu"
                    },
                    {
                        "name": "Zitao Li"
                    },
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Gao"
                },
                "author": "Jing Gao",
                "arxiv_comment": "Work In Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07811v1",
                "updated": "2025-01-14T03:21:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    21,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:21:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    21,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code\n  Generation"
                },
                "summary": "Code generation aims to produce code that fulfills requirements written in\nnatural languages automatically. Large language Models (LLMs) like ChatGPT have\ndemonstrated promising effectiveness in this area. Nonetheless, these LLMs\noften fail to ensure the syntactic and semantic correctness of the generated\ncode. Recently, researchers proposed multi-agent frameworks that guide LLMs\nwith different prompts to analyze programming tasks, generate code, perform\ntesting in a sequential workflow. However, the performance of the workflow is\nnot robust as the code generation depends on the performance of each agent. To\naddress this challenge, we propose CodeCoR, a self-reflective multi-agent\nframework that evaluates the effectiveness of each agent and their\ncollaborations. Specifically, for a given task description, four agents in\nCodeCoR generate prompts, code, test cases, and repair advice, respectively.\nEach agent generates more than one output and prunes away the low-quality ones.\nThe generated code is tested in the local environment: the code that fails to\npass the generated test cases is sent to the repair agent and the coding agent\nre-generates the code based on repair advice. Finally, the code that passes the\nmost number of generated test cases is returned to users. Our experiments on\nfour widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,\ndemonstrate that CodeCoR significantly outperforms existing baselines (e.g.,\nCodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation aims to produce code that fulfills requirements written in\nnatural languages automatically. Large language Models (LLMs) like ChatGPT have\ndemonstrated promising effectiveness in this area. Nonetheless, these LLMs\noften fail to ensure the syntactic and semantic correctness of the generated\ncode. Recently, researchers proposed multi-agent frameworks that guide LLMs\nwith different prompts to analyze programming tasks, generate code, perform\ntesting in a sequential workflow. However, the performance of the workflow is\nnot robust as the code generation depends on the performance of each agent. To\naddress this challenge, we propose CodeCoR, a self-reflective multi-agent\nframework that evaluates the effectiveness of each agent and their\ncollaborations. Specifically, for a given task description, four agents in\nCodeCoR generate prompts, code, test cases, and repair advice, respectively.\nEach agent generates more than one output and prunes away the low-quality ones.\nThe generated code is tested in the local environment: the code that fails to\npass the generated test cases is sent to the repair agent and the coding agent\nre-generates the code based on repair advice. Finally, the code that passes the\nmost number of generated test cases is returned to users. Our experiments on\nfour widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,\ndemonstrate that CodeCoR significantly outperforms existing baselines (e.g.,\nCodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%."
                },
                "authors": [
                    {
                        "name": "Ruwei Pan"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Chao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Liu"
                },
                "author": "Chao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07808v1",
                "updated": "2025-01-14T03:19:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    19,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:19:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    19,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "A Low-cost and Ultra-lightweight Binary Neural Network for Traffic\n  Signal Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-cost and Ultra-lightweight Binary Neural Network for Traffic\n  Signal Recognition"
                },
                "summary": "The deployment of neural networks in vehicle platforms and wearable\nArtificial Intelligence-of-Things (AIOT) scenarios has become a research area\nthat has attracted much attention. With the continuous evolution of deep\nlearning technology, many image classification models are committed to\nimproving recognition accuracy, but this is often accompanied by problems such\nas large model resource usage, complex structure, and high power consumption,\nwhich makes it challenging to deploy on resource-constrained platforms. Herein,\nwe propose an ultra-lightweight binary neural network (BNN) model designed for\nhardware deployment, and conduct image classification research based on the\nGerman Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also\nverify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS)\ndatasets. The proposed model shows excellent recognition performance with an\naccuracy of up to 97.64%, making it one of the best performing BNN models in\nthe GTSRB dataset. Compared with the full-precision model, the accuracy loss is\ncontrolled within 1%, and the parameter storage overhead of the model is only\n10% of that of the full-precision model. More importantly, our network model\nonly relies on logical operations and low-bit width fixed-point addition and\nsubtraction operations during the inference phase, which greatly simplifies the\ndesign complexity of the processing element (PE). Our research shows the great\npotential of BNN in the hardware deployment of computer vision models,\nespecially in the field of computer vision tasks related to autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of neural networks in vehicle platforms and wearable\nArtificial Intelligence-of-Things (AIOT) scenarios has become a research area\nthat has attracted much attention. With the continuous evolution of deep\nlearning technology, many image classification models are committed to\nimproving recognition accuracy, but this is often accompanied by problems such\nas large model resource usage, complex structure, and high power consumption,\nwhich makes it challenging to deploy on resource-constrained platforms. Herein,\nwe propose an ultra-lightweight binary neural network (BNN) model designed for\nhardware deployment, and conduct image classification research based on the\nGerman Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also\nverify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS)\ndatasets. The proposed model shows excellent recognition performance with an\naccuracy of up to 97.64%, making it one of the best performing BNN models in\nthe GTSRB dataset. Compared with the full-precision model, the accuracy loss is\ncontrolled within 1%, and the parameter storage overhead of the model is only\n10% of that of the full-precision model. More importantly, our network model\nonly relies on logical operations and low-bit width fixed-point addition and\nsubtraction operations during the inference phase, which greatly simplifies the\ndesign complexity of the processing element (PE). Our research shows the great\npotential of BNN in the hardware deployment of computer vision models,\nespecially in the field of computer vision tasks related to autonomous driving."
                },
                "authors": [
                    {
                        "name": "Mingke Xiao"
                    },
                    {
                        "name": "Yue Su"
                    },
                    {
                        "name": "Liang Yu"
                    },
                    {
                        "name": "Guanglong Qu"
                    },
                    {
                        "name": "Yutong Jia"
                    },
                    {
                        "name": "Yukuan Chang"
                    },
                    {
                        "name": "Xu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Zhang"
                },
                "author": "Xu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07802v1",
                "updated": "2025-01-14T03:03:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    3,
                    37,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T03:03:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    3,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Visual Language Models as Operator Agents in the Space Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Models as Operator Agents in the Space Domain"
                },
                "summary": "This paper explores the application of Vision-Language Models (VLMs) as\noperator agents in the space domain, focusing on both software and hardware\noperational paradigms. Building on advances in Large Language Models (LLMs) and\ntheir multimodal extensions, we investigate how VLMs can enhance autonomous\ncontrol and decision-making in space missions. In the software context, we\nemploy VLMs within the Kerbal Space Program Differential Games (KSPDG)\nsimulation environment, enabling the agent to interpret visual screenshots of\nthe graphical user interface to perform complex orbital maneuvers. In the\nhardware context, we integrate VLMs with robotic systems equipped with cameras\nto inspect and diagnose physical space objects, such as satellites. Our results\ndemonstrate that VLMs can effectively process visual and textual data to\ngenerate contextually appropriate actions, competing with traditional methods\nand non-multimodal LLMs in simulation tasks, and showing promise in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of Vision-Language Models (VLMs) as\noperator agents in the space domain, focusing on both software and hardware\noperational paradigms. Building on advances in Large Language Models (LLMs) and\ntheir multimodal extensions, we investigate how VLMs can enhance autonomous\ncontrol and decision-making in space missions. In the software context, we\nemploy VLMs within the Kerbal Space Program Differential Games (KSPDG)\nsimulation environment, enabling the agent to interpret visual screenshots of\nthe graphical user interface to perform complex orbital maneuvers. In the\nhardware context, we integrate VLMs with robotic systems equipped with cameras\nto inspect and diagnose physical space objects, such as satellites. Our results\ndemonstrate that VLMs can effectively process visual and textual data to\ngenerate contextually appropriate actions, competing with traditional methods\nand non-multimodal LLMs in simulation tasks, and showing promise in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Alejandro Carrasco"
                    },
                    {
                        "name": "Marco Nedungadi"
                    },
                    {
                        "name": "Enrico M. Zucchelli"
                    },
                    {
                        "name": "Amit Jain"
                    },
                    {
                        "name": "Victor Rodriguez-Fernandez"
                    },
                    {
                        "name": "Richard Linares"
                    }
                ],
                "author_detail": {
                    "name": "Richard Linares"
                },
                "author": "Richard Linares",
                "arxiv_doi": "10.2514/6.2025-1543",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2514/6.2025-1543",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated version of the paper presented in 2025 AIAA SciTech.\n  https://arc.aiaa.org/doi/10.2514/6.2025-1543",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06252v2",
                "updated": "2025-01-14T02:52:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    52,
                    26,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T01:19:21Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    19,
                    21,
                    3,
                    9,
                    0
                ],
                "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\text{Transformer}^2$: Self-adaptive LLMs"
                },
                "summary": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Edoardo Cetin"
                    },
                    {
                        "name": "Yujin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yujin Tang"
                },
                "author": "Yujin Tang",
                "arxiv_comment": "18 panges, 11 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19784v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19784v4",
                "updated": "2025-01-14T02:28:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    28,
                    28,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-27T18:25:27Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "title": "Can AI Help with Your Personal Finances?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Help with Your Personal Finances?"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising."
                },
                "authors": [
                    {
                        "name": "Oudom Hean"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Binita Saha"
                    }
                ],
                "author_detail": {
                    "name": "Binita Saha"
                },
                "author": "Binita Saha",
                "arxiv_doi": "10.1080/00036846.2025.2450384",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/00036846.2025.2450384",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19784v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19784v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12404v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12404v4",
                "updated": "2025-01-14T01:41:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    1,
                    41,
                    21,
                    1,
                    14,
                    0
                ],
                "published": "2024-04-15T17:49:16Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    17,
                    49,
                    16,
                    0,
                    106,
                    0
                ],
                "title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/"
                },
                "authors": [
                    {
                        "name": "Jinhee Kim"
                    },
                    {
                        "name": "Taesung Kim"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12404v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12404v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09008v2",
                "updated": "2025-01-14T01:21:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    1,
                    21,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-06-13T11:19:50Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    11,
                    19,
                    50,
                    3,
                    165,
                    0
                ],
                "title": "LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large\n  Language Models"
                },
                "summary": "Topic modeling has been a widely used tool for unsupervised text analysis.\nHowever, comprehensive evaluations of a topic model remain challenging.\nExisting evaluation methods are either less comparable across different models\n(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic\nquality or document representation quality) at a time, which is insufficient to\nreflect the overall model performance. In this paper, we propose WALM (Word\nAgreement with Language Model), a new evaluation method for topic modeling that\nconsiders the semantic quality of document representations and topics in a\njoint manner, leveraging the power of Large Language Models (LLMs). With\nextensive experiments involving different types of topic models, WALM is shown\nto align with human judgment and can serve as a complementary evaluation method\nto the existing ones, bringing a new perspective to topic modeling. Our\nsoftware package is available at\nhttps://github.com/Xiaohao-Yang/Topic_Model_Evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic modeling has been a widely used tool for unsupervised text analysis.\nHowever, comprehensive evaluations of a topic model remain challenging.\nExisting evaluation methods are either less comparable across different models\n(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic\nquality or document representation quality) at a time, which is insufficient to\nreflect the overall model performance. In this paper, we propose WALM (Word\nAgreement with Language Model), a new evaluation method for topic modeling that\nconsiders the semantic quality of document representations and topics in a\njoint manner, leveraging the power of Large Language Models (LLMs). With\nextensive experiments involving different types of topic models, WALM is shown\nto align with human judgment and can serve as a complementary evaluation method\nto the existing ones, bringing a new perspective to topic modeling. Our\nsoftware package is available at\nhttps://github.com/Xiaohao-Yang/Topic_Model_Evaluation."
                },
                "authors": [
                    {
                        "name": "Xiaohao Yang"
                    },
                    {
                        "name": "He Zhao"
                    },
                    {
                        "name": "Dinh Phung"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Lan Du"
                    }
                ],
                "author_detail": {
                    "name": "Lan Du"
                },
                "author": "Lan Du",
                "arxiv_comment": "Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL) published by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03983v2",
                "updated": "2025-01-14T01:10:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    1,
                    10,
                    52,
                    1,
                    14,
                    0
                ],
                "published": "2023-06-06T19:36:11Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    19,
                    36,
                    11,
                    1,
                    157,
                    0
                ],
                "title": "XVertNet: Unsupervised Contrast Enhancement of Vertebral Structures with\n  Dynamic Self-Tuning Guidance and Multi-Stage Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XVertNet: Unsupervised Contrast Enhancement of Vertebral Structures with\n  Dynamic Self-Tuning Guidance and Multi-Stage Analysis"
                },
                "summary": "Chest X-rays remain the primary diagnostic tool in emergency medicine, yet\ntheir limited ability to capture fine anatomical details can result in missed\nor delayed diagnoses. To address this, we introduce XVertNet, a novel\ndeep-learning framework designed to enhance vertebral structure visualization\nin X-ray images significantly. Our framework introduces two key innovations:\n(1) An unsupervised learning architecture that eliminates reliance on manually\nlabeled training data a persistent bottleneck in medical imaging, and (2) a\ndynamic self-tuned internal guidance mechanism featuring an adaptive feedback\nloop for real-time image optimization. Extensive validation across four major\npublic datasets revealed that XVertNet outperforms state-of-the-art enhancement\nmethods, as demonstrated by improvements in entropy scores, Tenengrad criterion\nvalues, the local phase coherence sharpness index (LPC-SI), and thetone mapped\nimage quality index (TMQI). Furthermore, clinical validation conducted with two\nboard-certified radiologists confirmed that the enhanced images enabled more\nsensitive detection of subtle vertebral fractures and degenerative changes. The\nunsupervised nature of XVertNet facilitates immediate clinical deployment\nwithout requiring additional training overhead. This innovation represents a\ntransformative advancement in emergency radiology, providing a scalable and\ntime-efficient solution to enhance diagnostic accuracy in high-pressure\nclinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chest X-rays remain the primary diagnostic tool in emergency medicine, yet\ntheir limited ability to capture fine anatomical details can result in missed\nor delayed diagnoses. To address this, we introduce XVertNet, a novel\ndeep-learning framework designed to enhance vertebral structure visualization\nin X-ray images significantly. Our framework introduces two key innovations:\n(1) An unsupervised learning architecture that eliminates reliance on manually\nlabeled training data a persistent bottleneck in medical imaging, and (2) a\ndynamic self-tuned internal guidance mechanism featuring an adaptive feedback\nloop for real-time image optimization. Extensive validation across four major\npublic datasets revealed that XVertNet outperforms state-of-the-art enhancement\nmethods, as demonstrated by improvements in entropy scores, Tenengrad criterion\nvalues, the local phase coherence sharpness index (LPC-SI), and thetone mapped\nimage quality index (TMQI). Furthermore, clinical validation conducted with two\nboard-certified radiologists confirmed that the enhanced images enabled more\nsensitive detection of subtle vertebral fractures and degenerative changes. The\nunsupervised nature of XVertNet facilitates immediate clinical deployment\nwithout requiring additional training overhead. This innovation represents a\ntransformative advancement in emergency radiology, providing a scalable and\ntime-efficient solution to enhance diagnostic accuracy in high-pressure\nclinical environments."
                },
                "authors": [
                    {
                        "name": "Ella Eidlin"
                    },
                    {
                        "name": "Assaf Hoogi"
                    },
                    {
                        "name": "Hila Rozen"
                    },
                    {
                        "name": "Mohammad Badarne"
                    },
                    {
                        "name": "Nathan S. Netanyahu"
                    }
                ],
                "author_detail": {
                    "name": "Nathan S. Netanyahu"
                },
                "author": "Nathan S. Netanyahu",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07766v1",
                "updated": "2025-01-14T00:47:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    0,
                    47,
                    24,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T00:47:24Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    0,
                    47,
                    24,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Models for Knowledge Graph Embedding Techniques, Methods,\n  and Challenges: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Knowledge Graph Embedding Techniques, Methods,\n  and Challenges: A Survey"
                },
                "summary": "Large Language Models (LLMs) have attracted a lot of attention in various\nfields due to their superior performance, aiming to train hundreds of millions\nor more parameters on large amounts of text data to understand and generate\nnatural language. As the superior performance of LLMs becomes apparent, they\nare increasingly being applied to knowledge graph embedding (KGE) related tasks\nto improve the processing results. As a deep learning model in the field of\nNatural Language Processing (NLP), it learns a large amount of textual data to\npredict the next word or generate content related to a given text. However,\nLLMs have recently been invoked to varying degrees in different types of KGE\nrelated scenarios such as multi-modal KGE and open KGE according to their task\ncharacteristics. In this paper, we investigate a wide range of approaches for\nperforming LLMs-related tasks in different types of KGE scenarios. To better\ncompare the various approaches, we summarize each KGE scenario in a\nclassification. In addition to the categorization methods, we provide a tabular\noverview of the methods and their source code links for a more direct\ncomparison. In the article we also discuss the applications in which the\nmethods are mainly used and suggest several forward-looking directions for the\ndevelopment of this new research area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have attracted a lot of attention in various\nfields due to their superior performance, aiming to train hundreds of millions\nor more parameters on large amounts of text data to understand and generate\nnatural language. As the superior performance of LLMs becomes apparent, they\nare increasingly being applied to knowledge graph embedding (KGE) related tasks\nto improve the processing results. As a deep learning model in the field of\nNatural Language Processing (NLP), it learns a large amount of textual data to\npredict the next word or generate content related to a given text. However,\nLLMs have recently been invoked to varying degrees in different types of KGE\nrelated scenarios such as multi-modal KGE and open KGE according to their task\ncharacteristics. In this paper, we investigate a wide range of approaches for\nperforming LLMs-related tasks in different types of KGE scenarios. To better\ncompare the various approaches, we summarize each KGE scenario in a\nclassification. In addition to the categorization methods, we provide a tabular\noverview of the methods and their source code links for a more direct\ncomparison. In the article we also discuss the applications in which the\nmethods are mainly used and suggest several forward-looking directions for the\ndevelopment of this new research area."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09072v2",
                "updated": "2025-01-14T00:21:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    0,
                    21,
                    51,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-13T22:55:45Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    55,
                    45,
                    2,
                    318,
                    0
                ],
                "title": "Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive\n  Knowledge Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive\n  Knowledge Graph Learning"
                },
                "summary": "The increasing demand for robust security solutions across various industries\nhas made Video Anomaly Detection (VAD) a critical task in applications such as\nintelligent surveillance, evidence investigation, and violence detection.\nTraditional approaches to VAD often rely on finetuning large pre-trained\nmodels, which can be computationally expensive and impractical for real-time or\nresource-constrained environments. To address this, MissionGNN introduced a\nmore efficient method by training a graph neural network (GNN) using a fixed\nknowledge graph (KG) derived from large language models (LLMs) like GPT-4.\nWhile this approach demonstrated significant efficiency in computational power\nand memory, it faces limitations in dynamic environments where frequent updates\nto the KG are necessary due to evolving behavior trends and shifting data\npatterns. These updates typically require cloud-based computation, posing\nchallenges for edge computing applications. In this paper, we propose a novel\nframework that facilitates continuous KG adaptation directly on edge devices,\novercoming the limitations of cloud dependency. Our method dynamically modifies\nthe KG through a three-phase process: pruning, alternating, and creating nodes,\nenabling real-time adaptation to changing data trends. This continuous learning\napproach enhances the robustness of anomaly detection models, making them more\nsuitable for deployment in dynamic and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for robust security solutions across various industries\nhas made Video Anomaly Detection (VAD) a critical task in applications such as\nintelligent surveillance, evidence investigation, and violence detection.\nTraditional approaches to VAD often rely on finetuning large pre-trained\nmodels, which can be computationally expensive and impractical for real-time or\nresource-constrained environments. To address this, MissionGNN introduced a\nmore efficient method by training a graph neural network (GNN) using a fixed\nknowledge graph (KG) derived from large language models (LLMs) like GPT-4.\nWhile this approach demonstrated significant efficiency in computational power\nand memory, it faces limitations in dynamic environments where frequent updates\nto the KG are necessary due to evolving behavior trends and shifting data\npatterns. These updates typically require cloud-based computation, posing\nchallenges for edge computing applications. In this paper, we propose a novel\nframework that facilitates continuous KG adaptation directly on edge devices,\novercoming the limitations of cloud dependency. Our method dynamically modifies\nthe KG through a three-phase process: pruning, alternating, and creating nodes,\nenabling real-time adaptation to changing data trends. This continuous learning\napproach enhances the robustness of anomaly detection models, making them more\nsuitable for deployment in dynamic and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "William Youngwoo Chung"
                    },
                    {
                        "name": "Minhyoung Na"
                    },
                    {
                        "name": "Nathaniel Bastian"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "arxiv_comment": "Accepted to DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07751v1",
                "updated": "2025-01-13T23:42:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    23,
                    42,
                    37,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T23:42:37Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    23,
                    42,
                    37,
                    0,
                    13,
                    0
                ],
                "title": "Rethinking AI Cultural Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking AI Cultural Evaluation"
                },
                "summary": "As AI systems become more integrated into society, evaluating their capacity\nto align with diverse cultural values is crucial for their responsible\ndeployment. Current evaluation methods predominantly rely on multiple-choice\nquestion (MCQ) datasets. In this study, we demonstrate that MCQs are\ninsufficient for capturing the complexity of cultural values expressed in\nopen-ended scenarios. Our findings highlight significant discrepancies between\nMCQ-based assessments and the values conveyed in unconstrained interactions.\nBased on these findings, we recommend moving beyond MCQs to adopt more\nopen-ended, context-specific assessments that better reflect how AI models\nengage with cultural values in realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems become more integrated into society, evaluating their capacity\nto align with diverse cultural values is crucial for their responsible\ndeployment. Current evaluation methods predominantly rely on multiple-choice\nquestion (MCQ) datasets. In this study, we demonstrate that MCQs are\ninsufficient for capturing the complexity of cultural values expressed in\nopen-ended scenarios. Our findings highlight significant discrepancies between\nMCQ-based assessments and the values conveyed in unconstrained interactions.\nBased on these findings, we recommend moving beyond MCQs to adopt more\nopen-ended, context-specific assessments that better reflect how AI models\nengage with cultural values in realistic settings."
                },
                "authors": [
                    {
                        "name": "Michal Bravansky"
                    },
                    {
                        "name": "Filip Trhlik"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07740v1",
                "updated": "2025-01-13T23:10:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    23,
                    10,
                    2,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T23:10:02Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    23,
                    10,
                    2,
                    0,
                    13,
                    0
                ],
                "title": "Advancing Student Writing Through Automated Syntax Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Student Writing Through Automated Syntax Feedback"
                },
                "summary": "This study underscores the pivotal role of syntax feedback in augmenting the\nsyntactic proficiency of students. Recognizing the challenges faced by learners\nin mastering syntactic nuances, we introduce a specialized dataset named\nEssay-Syntax-Instruct designed to enhance the understanding and application of\nEnglish syntax among these students. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,\nLlama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a\ncomprehensive fine-tuning process tailored to the syntax improvement task.\nThrough meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit\na marked improvement in addressing syntax-related challenges, thereby serving\nas a potent tool for students to identify and rectify their syntactic errors.\nThe findings not only highlight the effectiveness of the proposed dataset in\nelevating the performance of LLMs for syntax enhancement but also illuminate a\npromising path for utilizing advanced language models to support language\nacquisition efforts. This research contributes to the broader field of language\nlearning technology by showcasing the potential of LLMs in facilitating the\nlinguistic development of Students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study underscores the pivotal role of syntax feedback in augmenting the\nsyntactic proficiency of students. Recognizing the challenges faced by learners\nin mastering syntactic nuances, we introduce a specialized dataset named\nEssay-Syntax-Instruct designed to enhance the understanding and application of\nEnglish syntax among these students. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,\nLlama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a\ncomprehensive fine-tuning process tailored to the syntax improvement task.\nThrough meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit\na marked improvement in addressing syntax-related challenges, thereby serving\nas a potent tool for students to identify and rectify their syntactic errors.\nThe findings not only highlight the effectiveness of the proposed dataset in\nelevating the performance of LLMs for syntax enhancement but also illuminate a\npromising path for utilizing advanced language models to support language\nacquisition efforts. This research contributes to the broader field of language\nlearning technology by showcasing the potential of LLMs in facilitating the\nlinguistic development of Students."
                },
                "authors": [
                    {
                        "name": "Kamyar Zeinalipour"
                    },
                    {
                        "name": "Mehak Mehak"
                    },
                    {
                        "name": "Fatemeh Parsamotamed"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Marco Gori"
                    }
                ],
                "author_detail": {
                    "name": "Marco Gori"
                },
                "author": "Marco Gori",
                "arxiv_comment": "This paper has been accepted for presentation at AIEER 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14634v3",
                "updated": "2025-01-13T22:45:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    22,
                    45,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-23T00:09:34Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    0,
                    9,
                    34,
                    0,
                    267,
                    0
                ],
                "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination"
                },
                "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas. To see if large language models (LLMs) can\nassist this process, we contribute Scideator, a novel mixed-initiative tool for\nscientific ideation. Starting from a user-provided set of papers, Scideator\nextracts key facets (purposes, mechanisms, and evaluations) from these and\nrelevant papers, allowing users to explore the idea space by interactively\nrecombining facets to synthesize inventive ideas. Scideator also helps users to\ngauge idea novelty by searching the literature for potential overlaps and\nshowing automated novelty assessments and explanations. To support these tasks,\nScideator introduces four LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty\nChecker, and Idea Novelty Iterator. In a within-subjects user study, 19\ncomputer-science researchers identified significantly more interesting ideas\nusing Scideator compared to a strong baseline combining a scientific search\nengine with LLM interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas. To see if large language models (LLMs) can\nassist this process, we contribute Scideator, a novel mixed-initiative tool for\nscientific ideation. Starting from a user-provided set of papers, Scideator\nextracts key facets (purposes, mechanisms, and evaluations) from these and\nrelevant papers, allowing users to explore the idea space by interactively\nrecombining facets to synthesize inventive ideas. Scideator also helps users to\ngauge idea novelty by searching the literature for potential overlaps and\nshowing automated novelty assessments and explanations. To support these tasks,\nScideator introduces four LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty\nChecker, and Idea Novelty Iterator. In a within-subjects user study, 19\ncomputer-science researchers identified significantly more interesting ideas\nusing Scideator compared to a strong baseline combining a scientific search\nengine with LLM interaction."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Simra Shahid"
                    },
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Daniel S. Weld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel S. Weld"
                },
                "author": "Daniel S. Weld",
                "arxiv_comment": "Added supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2, I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09308v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09308v3",
                "updated": "2025-01-13T22:22:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    22,
                    22,
                    6,
                    0,
                    13,
                    0
                ],
                "published": "2023-11-15T19:02:40Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    19,
                    2,
                    40,
                    2,
                    319,
                    0
                ],
                "title": "Divergences between Language Models and Human Brains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divergences between Language Models and Human Brains"
                },
                "summary": "Do machines and humans process language in similar ways? Recent research has\nhinted at the affirmative, showing that human neural activity can be\neffectively predicted using the internal representations of language models\n(LMs). Although such results are thought to reflect shared computational\nprinciples between LMs and human brains, there are also clear differences in\nhow LMs and humans represent and use language. In this work, we systematically\nexplore the divergences between human and machine language processing by\nexamining the differences between LM representations and human brain responses\nto language as measured by Magnetoencephalography (MEG) across two datasets in\nwhich subjects read and listened to narrative stories. Using an LLM-based\ndata-driven approach, we identify two domains that LMs do not capture well:\nsocial/emotional intelligence and physical commonsense. We validate these\nfindings with human behavioral experiments and hypothesize that the gap is due\nto insufficient representations of social/emotional and physical knowledge in\nLMs. Our results show that fine-tuning LMs on these domains can improve their\nalignment with human brain responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do machines and humans process language in similar ways? Recent research has\nhinted at the affirmative, showing that human neural activity can be\neffectively predicted using the internal representations of language models\n(LMs). Although such results are thought to reflect shared computational\nprinciples between LMs and human brains, there are also clear differences in\nhow LMs and humans represent and use language. In this work, we systematically\nexplore the divergences between human and machine language processing by\nexamining the differences between LM representations and human brain responses\nto language as measured by Magnetoencephalography (MEG) across two datasets in\nwhich subjects read and listened to narrative stories. Using an LLM-based\ndata-driven approach, we identify two domains that LMs do not capture well:\nsocial/emotional intelligence and physical commonsense. We validate these\nfindings with human behavioral experiments and hypothesize that the gap is due\nto insufficient representations of social/emotional and physical knowledge in\nLMs. Our results show that fine-tuning LMs on these domains can improve their\nalignment with human brain responses."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Emmy Liu"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Michael J. Tarr"
                    },
                    {
                        "name": "Leila Wehbe"
                    }
                ],
                "author_detail": {
                    "name": "Leila Wehbe"
                },
                "author": "Leila Wehbe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09308v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09308v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07721v1",
                "updated": "2025-01-13T22:14:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    22,
                    14,
                    45,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T22:14:45Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    22,
                    14,
                    45,
                    0,
                    13,
                    0
                ],
                "title": "LLMic: Romanian Foundation Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMic: Romanian Foundation Language Model"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across various tasks with commercial models leading the way. While\nopen models usually operate at a smaller scale, they maintain competitiveness\nthrough specialization and fine-tuning. However, a significant challenge\npersists: open models often underperform in low-resource languages due to\nlimited representation in the training corpus. In this paper, we present LLMic,\na bilingual foundation language model designed specifically for the Romanian\nLanguage. We document the complete process of pretraining a foundation model\nfor a low-resource language, including corpus construction, architecture\nselection, and hyper-parameter optimization. Our evaluation demonstrates that\nLLMic can be specialized for tasks in the target language, achieving results\ncomparable to other much larger open models. We show that fine-tuning LLMic for\nlanguage translation after the initial pretraining phase outperforms existing\nsolutions in English-to-Romanian translation tasks. This opens the path for\nefficient large-scale processing for the Romanian language community, using the\nmuch smaller LLMic model",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across various tasks with commercial models leading the way. While\nopen models usually operate at a smaller scale, they maintain competitiveness\nthrough specialization and fine-tuning. However, a significant challenge\npersists: open models often underperform in low-resource languages due to\nlimited representation in the training corpus. In this paper, we present LLMic,\na bilingual foundation language model designed specifically for the Romanian\nLanguage. We document the complete process of pretraining a foundation model\nfor a low-resource language, including corpus construction, architecture\nselection, and hyper-parameter optimization. Our evaluation demonstrates that\nLLMic can be specialized for tasks in the target language, achieving results\ncomparable to other much larger open models. We show that fine-tuning LLMic for\nlanguage translation after the initial pretraining phase outperforms existing\nsolutions in English-to-Romanian translation tasks. This opens the path for\nefficient large-scale processing for the Romanian language community, using the\nmuch smaller LLMic model"
                },
                "authors": [
                    {
                        "name": "Vlad-Andrei Bădoiu"
                    },
                    {
                        "name": "Mihai-Valentin Dumitru"
                    },
                    {
                        "name": "Alexandru M. Gherghescu"
                    },
                    {
                        "name": "Alexandru Agache"
                    },
                    {
                        "name": "Costin Raiciu"
                    }
                ],
                "author_detail": {
                    "name": "Costin Raiciu"
                },
                "author": "Costin Raiciu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]