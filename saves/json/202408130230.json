[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v1",
                "updated": "2024-08-09T05:20:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v2",
                "updated": "2024-08-11T18:58:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    58,
                    35,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x--5x$ during\nSMT solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x--5x$ during\nSMT solving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v1",
                "updated": "2024-07-31T21:33:56Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v2",
                "updated": "2024-07-28T14:42:12Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    14,
                    42,
                    12,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v1",
                "updated": "2024-07-27T16:20:21Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v1",
                "updated": "2024-07-22T15:42:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Enhanced Achievable DoF Bounds for Cache-Aided MIMO Communication\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Achievable DoF Bounds for Cache-Aided MIMO Communication\n  Systems"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications may significantly enhance the achievable degrees of freedom\n(DoF) of the wireless networks. In this paper, we consider a cache-aided MIMO\nconfiguration with a CC gain $t$, where a server with $L$ Tx antennas\ncommunicates with $K$ users, each with $G$ Rx antennas. In the proposed\ncontent-aware MIMO strategy, we carefully adjust the number of users $\\Omega$\nand the number of parallel streams decoded by each user $\\beta$ served in each\ntransmission to maximize the DoF. As a result, we achieve a DoF of\n${\\max_{\\beta, \\Omega }}{\\Omega \\beta}$, where ${\\beta \\le\n\\mathrm{min}\\big(G,\\frac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega -\nt-1)\\binom{\\Omega-1}{t}}\\big)}$. To prove the achievability of the proposed DoF\nbound, we provide a novel transmission strategy based on the simultaneous\nunicasting of multiple data streams. In this strategy, the missing data packets\nare scheduled such that the number of parallel streams per transmission is\nmaximized while the decodability of all useful terms by each target user is\nguaranteed. Numerical simulations validate the findings, confirming the\nenhanced DoF and improved performance of the proposed design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications may significantly enhance the achievable degrees of freedom\n(DoF) of the wireless networks. In this paper, we consider a cache-aided MIMO\nconfiguration with a CC gain $t$, where a server with $L$ Tx antennas\ncommunicates with $K$ users, each with $G$ Rx antennas. In the proposed\ncontent-aware MIMO strategy, we carefully adjust the number of users $\\Omega$\nand the number of parallel streams decoded by each user $\\beta$ served in each\ntransmission to maximize the DoF. As a result, we achieve a DoF of\n${\\max_{\\beta, \\Omega }}{\\Omega \\beta}$, where ${\\beta \\le\n\\mathrm{min}\\big(G,\\frac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega -\nt-1)\\binom{\\Omega-1}{t}}\\big)}$. To prove the achievability of the proposed DoF\nbound, we provide a novel transmission strategy based on the simultaneous\nunicasting of multiple data streams. In this strategy, the missing data packets\nare scheduled such that the number of parallel streams per transmission is\nmaximized while the decodability of all useful terms by each target user is\nguaranteed. Numerical simulations validate the findings, confirming the\nenhanced DoF and improved performance of the proposed design."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2304.13827",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v1",
                "updated": "2024-07-22T07:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo Ruiz"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v2",
                "updated": "2024-07-21T14:08:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    8,
                    42,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v1",
                "updated": "2024-07-15T17:25:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v1",
                "updated": "2024-07-15T14:09:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02350v3",
                "updated": "2024-07-15T14:00:24Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    0,
                    24,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-02T15:16:06Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    16,
                    6,
                    1,
                    184,
                    0
                ],
                "title": "Conceptual Codebook Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual Codebook Learning for Vision-Language Models"
                },
                "summary": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ke Yu"
                    },
                    {
                        "name": "Siqi Wu"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10354v1",
                "updated": "2024-07-14T23:15:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-14T23:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "title": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs"
                },
                "summary": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Kenneth Stephenson"
                    },
                    {
                        "name": "Md Abdullah-Al Mamun"
                    },
                    {
                        "name": "Abdullah Al Mamun Mazumder"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Asif Khan"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04876v2",
                "updated": "2024-07-14T16:12:48Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    16,
                    12,
                    48,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-05T22:07:36Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    22,
                    7,
                    36,
                    4,
                    187,
                    0
                ],
                "title": "Swimming Cylinder Wake Control with Plasma Actuator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swimming Cylinder Wake Control with Plasma Actuator"
                },
                "summary": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%."
                },
                "authors": [
                    {
                        "name": "Javad Omidi"
                    }
                ],
                "author_detail": {
                    "name": "Javad Omidi"
                },
                "author": "Javad Omidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12866v1",
                "updated": "2024-07-13T07:23:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    13,
                    7,
                    23,
                    7,
                    5,
                    195,
                    0
                ],
                "published": "2024-07-13T07:23:07Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    7,
                    23,
                    7,
                    5,
                    195,
                    0
                ],
                "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond KV Caching: Shared Attention for Efficient LLMs"
                },
                "summary": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Bingli Liao"
                    },
                    {
                        "name": "Danilo Vasconcellos Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Vasconcellos Vargas"
                },
                "author": "Danilo Vasconcellos Vargas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v2",
                "updated": "2024-07-12T23:44:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    12,
                    23,
                    44,
                    41,
                    4,
                    194,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 ID Selection Correctness,\n  Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 ID Selection Correctness,\n  Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "32 pages, 10 figures, 1 table, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v2",
                "updated": "2024-07-12T10:33:31Z",
                "updated_parsed": [
                    2024,
                    7,
                    12,
                    10,
                    33,
                    31,
                    4,
                    194,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "Efficient management of GPU memory is essential for high throughput LLM\ninference. Prior systems used to reserve KV-cache memory ahead-of-time that\nresulted in wasted capacity due to internal fragmentation. Inspired by demand\npaging, vLLM proposed PagedAttention to enable dynamic memory allocation for\nKV-cache. This approach eliminates fragmentation and improves serving\nthroughout. However, to be able to allocate physical memory dynamically,\nPagedAttention changes the layout of KV-cache from contiguous virtual memory to\nnon-contiguous virtual memory. As a consequence, one needs to rewrite the\nattention kernels to support paging, and implement a memory manager in the\nserving framework. This results in both performance and programming overheads,\nas well as portability challenges in adopting state-of-the-art attention\nkernels.\n  In this paper, we propose vAttention, a new approach for dynamic KV-cache\nmemory management. In contrast to PagedAttention, vAttention stores KV-cache in\ncontiguous virtual memory and leverages OS support for on-demand allocation of\nphysical memory. vAttention thus enables one to use state-of-the art attention\nkernels out-of-the-box by adding support for dynamic allocation of physical\nmemory without having to re-write their code. We implement vAttention in the\nvLLM serving stack to show that it also helps improve decode throughput by up\nto 1.99x over vLLM, and the end-to-end serving throughput by up to 1.22x and\n1.29x, compared to using the state-of-the-art PagedAttention based kernels of\nFlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient management of GPU memory is essential for high throughput LLM\ninference. Prior systems used to reserve KV-cache memory ahead-of-time that\nresulted in wasted capacity due to internal fragmentation. Inspired by demand\npaging, vLLM proposed PagedAttention to enable dynamic memory allocation for\nKV-cache. This approach eliminates fragmentation and improves serving\nthroughout. However, to be able to allocate physical memory dynamically,\nPagedAttention changes the layout of KV-cache from contiguous virtual memory to\nnon-contiguous virtual memory. As a consequence, one needs to rewrite the\nattention kernels to support paging, and implement a memory manager in the\nserving framework. This results in both performance and programming overheads,\nas well as portability challenges in adopting state-of-the-art attention\nkernels.\n  In this paper, we propose vAttention, a new approach for dynamic KV-cache\nmemory management. In contrast to PagedAttention, vAttention stores KV-cache in\ncontiguous virtual memory and leverages OS support for on-demand allocation of\nphysical memory. vAttention thus enables one to use state-of-the art attention\nkernels out-of-the-box by adding support for dynamic allocation of physical\nmemory without having to re-write their code. We implement vAttention in the\nvLLM serving stack to show that it also helps improve decode throughput by up\nto 1.99x over vLLM, and the end-to-end serving throughput by up to 1.22x and\n1.29x, compared to using the state-of-the-art PagedAttention based kernels of\nFlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "14 pages, 13 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08845v2",
                "updated": "2024-07-11T20:07:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    11,
                    20,
                    7,
                    30,
                    3,
                    193,
                    0
                ],
                "published": "2024-03-13T16:30:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    16,
                    30,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "Bifurcated Attention: Accelerating Massively Parallel Decoding with\n  Shared Prefixes in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bifurcated Attention: Accelerating Massively Parallel Decoding with\n  Shared Prefixes in LLMs"
                },
                "summary": "This study introduces bifurcated attention, a method designed to enhance\nlanguage model inference in shared-context batch decoding scenarios. Our\napproach addresses the challenge of redundant memory IO costs, a critical\nfactor contributing to latency in high batch sizes and extended context\nlengths. Bifurcated attention achieves this by strategically dividing the\nattention mechanism during incremental decoding into two separate GEMM\noperations: one focusing on the KV cache from prefill, and another on the\ndecoding process itself. While maintaining the computational load (FLOPs) of\nstandard attention mechanisms, bifurcated attention ensures precise computation\nwith significantly reduced memory IO. Our empirical results show over\n2.1$\\times$ speedup when sampling 16 output sequences and more than 6.2$\\times$\nspeedup when sampling 32 sequences at context lengths exceeding 8k tokens on a\n7B model that uses multi-head attention. The efficiency gains from bifurcated\nattention translate into lower latency, making it particularly suitable for\nreal-time applications. For instance, it enables massively parallel answer\ngeneration without substantially increasing latency, thus enhancing performance\nwhen integrated with post-processing techniques such as re-ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces bifurcated attention, a method designed to enhance\nlanguage model inference in shared-context batch decoding scenarios. Our\napproach addresses the challenge of redundant memory IO costs, a critical\nfactor contributing to latency in high batch sizes and extended context\nlengths. Bifurcated attention achieves this by strategically dividing the\nattention mechanism during incremental decoding into two separate GEMM\noperations: one focusing on the KV cache from prefill, and another on the\ndecoding process itself. While maintaining the computational load (FLOPs) of\nstandard attention mechanisms, bifurcated attention ensures precise computation\nwith significantly reduced memory IO. Our empirical results show over\n2.1$\\times$ speedup when sampling 16 output sequences and more than 6.2$\\times$\nspeedup when sampling 32 sequences at context lengths exceeding 8k tokens on a\n7B model that uses multi-head attention. The efficiency gains from bifurcated\nattention translate into lower latency, making it particularly suitable for\nreal-time applications. For instance, it enables massively parallel answer\ngeneration without substantially increasing latency, thus enhancing performance\nwhen integrated with post-processing techniques such as re-ranking."
                },
                "authors": [
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Sujan Kumar Gonugondla"
                    },
                    {
                        "name": "Sanjay Krishna Gouda"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Hantian Ding"
                    },
                    {
                        "name": "Qing Sun"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Liangfu Chen"
                    },
                    {
                        "name": "Parminder Bhatia"
                    },
                    {
                        "name": "Ramesh Nallapati"
                    },
                    {
                        "name": "Sudipta Sengupta"
                    },
                    {
                        "name": "Bing Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xiang"
                },
                "author": "Bing Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08701v1",
                "updated": "2024-07-11T17:34:51Z",
                "updated_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    34,
                    51,
                    3,
                    193,
                    0
                ],
                "published": "2024-07-11T17:34:51Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    34,
                    51,
                    3,
                    193,
                    0
                ],
                "title": "Live2Diff: Live Stream Translation via Uni-directional Attention in\n  Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live2Diff: Live Stream Translation via Uni-directional Attention in\n  Video Diffusion Models"
                },
                "summary": "Large Language Models have shown remarkable efficacy in generating streaming\ndata such as text and audio, thanks to their temporally uni-directional\nattention mechanism, which models correlations between the current token and\nprevious tokens. However, video streaming remains much less explored, despite a\ngrowing need for live video processing. State-of-the-art video diffusion models\nleverage bi-directional temporal attention to model the correlations between\nthe current frame and all the surrounding (i.e. including future) frames, which\nhinders them from processing streaming videos. To address this problem, we\npresent Live2Diff, the first attempt at designing a video diffusion model with\nuni-directional temporal attention, specifically targeting live streaming video\ntranslation. Compared to previous works, our approach ensures temporal\nconsistency and smoothness by correlating the current frame with its\npredecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring a KV-cache\nmechanism and pipelining, to facilitate streaming video translation at\ninteractive framerates. Extensive experiments demonstrate the effectiveness of\nthe proposed attention mechanism and pipeline, outperforming previous methods\nin terms of temporal smoothness and/or efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have shown remarkable efficacy in generating streaming\ndata such as text and audio, thanks to their temporally uni-directional\nattention mechanism, which models correlations between the current token and\nprevious tokens. However, video streaming remains much less explored, despite a\ngrowing need for live video processing. State-of-the-art video diffusion models\nleverage bi-directional temporal attention to model the correlations between\nthe current frame and all the surrounding (i.e. including future) frames, which\nhinders them from processing streaming videos. To address this problem, we\npresent Live2Diff, the first attempt at designing a video diffusion model with\nuni-directional temporal attention, specifically targeting live streaming video\ntranslation. Compared to previous works, our approach ensures temporal\nconsistency and smoothness by correlating the current frame with its\npredecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring a KV-cache\nmechanism and pipelining, to facilitate streaming video translation at\ninteractive framerates. Extensive experiments demonstrate the effectiveness of\nthe proposed attention mechanism and pipeline, outperforming previous methods\nin terms of temporal smoothness and/or efficiency."
                },
                "authors": [
                    {
                        "name": "Zhening Xing"
                    },
                    {
                        "name": "Gereon Fox"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Xingang Pan"
                    },
                    {
                        "name": "Mohamed Elgharib"
                    },
                    {
                        "name": "Christian Theobalt"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "https://live2diff.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07850v1",
                "updated": "2024-07-10T17:12:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    10,
                    17,
                    12,
                    57,
                    2,
                    192,
                    0
                ],
                "published": "2024-07-10T17:12:57Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    17,
                    12,
                    57,
                    2,
                    192,
                    0
                ],
                "title": "Harnessing Integrated CPU-GPU System Memory for HPC: a first look into\n  Grace Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Integrated CPU-GPU System Memory for HPC: a first look into\n  Grace Hopper"
                },
                "summary": "Memory management across discrete CPU and GPU physical memory is\ntraditionally achieved through explicit GPU allocations and data copy or\nunified virtual memory. The Grace Hopper Superchip, for the first time,\nsupports an integrated CPU-GPU system page table, hardware-level addressing of\nsystem allocated memory, and cache-coherent NVLink-C2C interconnect, bringing\nan alternative solution for enabling a Unified Memory system. In this work, we\nprovide the first in-depth study of the system memory management on the Grace\nHopper Superchip, in both in-memory and memory oversubscription scenarios. We\nprovide a suite of six representative applications, including the Qiskit\nquantum computing simulator, using system memory and managed memory. Using our\nmemory utilization profiler and hardware counters, we quantify and characterize\nthe impact of the integrated CPU-GPU system page table on GPU applications. Our\nstudy focuses on first-touch policy, page table entry initialization, page\nsizes, and page migration. We identify practical optimization strategies for\ndifferent access patterns. Our results show that as a new solution for unified\nmemory, the system-allocated memory can benefit most use cases with minimal\nporting efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory management across discrete CPU and GPU physical memory is\ntraditionally achieved through explicit GPU allocations and data copy or\nunified virtual memory. The Grace Hopper Superchip, for the first time,\nsupports an integrated CPU-GPU system page table, hardware-level addressing of\nsystem allocated memory, and cache-coherent NVLink-C2C interconnect, bringing\nan alternative solution for enabling a Unified Memory system. In this work, we\nprovide the first in-depth study of the system memory management on the Grace\nHopper Superchip, in both in-memory and memory oversubscription scenarios. We\nprovide a suite of six representative applications, including the Qiskit\nquantum computing simulator, using system memory and managed memory. Using our\nmemory utilization profiler and hardware counters, we quantify and characterize\nthe impact of the integrated CPU-GPU system page table on GPU applications. Our\nstudy focuses on first-touch policy, page table entry initialization, page\nsizes, and page migration. We identify practical optimization strategies for\ndifferent access patterns. Our results show that as a new solution for unified\nmemory, the system-allocated memory can benefit most use cases with minimal\nporting efforts."
                },
                "authors": [
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Jennifer Faj"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "Accepted to ICPP '24 (The 53rd International Conference on Parallel\n  Processing)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07718v1",
                "updated": "2024-07-10T14:50:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    50,
                    0,
                    2,
                    192,
                    0
                ],
                "published": "2024-07-10T14:50:00Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    50,
                    0,
                    2,
                    192,
                    0
                ],
                "title": "High-Performance Sorting-Based k-mer Counting in Distributed Memory with\n  Flexible Hybrid Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Sorting-Based k-mer Counting in Distributed Memory with\n  Flexible Hybrid Parallelism"
                },
                "summary": "In generating large quantities of DNA data, high-throughput sequencing\ntechnologies require advanced bioinformatics infrastructures for efficient data\nanalysis. k-mer counting, the process of quantifying the frequency of\nfixed-length k DNA subsequences, is a fundamental step in various\nbioinformatics pipelines, including genome assembly and protein prediction. Due\nto the growing volume of data, the scaling of the counting process is critical.\nIn the literature, distributed memory software uses hash tables, which exhibit\npoor cache friendliness and consume excessive memory. They often also lack\nsupport for flexible parallelism, which makes integration into existing\nbioinformatics pipelines difficult. In this work, we propose HySortK, a highly\nefficient sorting-based distributed memory k-mer counter. HySortK reduces the\ncommunication volume through a carefully designed communication scheme and\ndomain-specific optimization strategies. Furthermore, we introduce an abstract\ntask layer for flexible hybrid parallelism to address load imbalances in\ndifferent scenarios. HySortK achieves a 2-10x speedup compared to the GPU\nbaseline on 4 and 8 nodes. Compared to state-of-the-art CPU software, HySortK\nachieves up to 2x speedup while reducing peak memory usage by 30% on 16 nodes.\nFinally, we integrated HySortK into an existing genome assembly pipeline and\nachieved up to 1.8x speedup, proving its flexibility and practicality in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In generating large quantities of DNA data, high-throughput sequencing\ntechnologies require advanced bioinformatics infrastructures for efficient data\nanalysis. k-mer counting, the process of quantifying the frequency of\nfixed-length k DNA subsequences, is a fundamental step in various\nbioinformatics pipelines, including genome assembly and protein prediction. Due\nto the growing volume of data, the scaling of the counting process is critical.\nIn the literature, distributed memory software uses hash tables, which exhibit\npoor cache friendliness and consume excessive memory. They often also lack\nsupport for flexible parallelism, which makes integration into existing\nbioinformatics pipelines difficult. In this work, we propose HySortK, a highly\nefficient sorting-based distributed memory k-mer counter. HySortK reduces the\ncommunication volume through a carefully designed communication scheme and\ndomain-specific optimization strategies. Furthermore, we introduce an abstract\ntask layer for flexible hybrid parallelism to address load imbalances in\ndifferent scenarios. HySortK achieves a 2-10x speedup compared to the GPU\nbaseline on 4 and 8 nodes. Compared to state-of-the-art CPU software, HySortK\nachieves up to 2x speedup while reducing peak memory usage by 30% on 16 nodes.\nFinally, we integrated HySortK into an existing genome assembly pipeline and\nachieved up to 1.8x speedup, proving its flexibility and practicality in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Giulia Guidi"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Guidi"
                },
                "author": "Giulia Guidi",
                "arxiv_doi": "10.1145/3673038.3673072",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3673038.3673072",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "In The 53rd International Conference on Parallel Processing (ICPP\n  24), August 12-15, 2024, Gotland, Sweden",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18936v1",
                "updated": "2024-07-10T13:09:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    10,
                    13,
                    9,
                    58,
                    2,
                    192,
                    0
                ],
                "published": "2024-07-10T13:09:58Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    13,
                    9,
                    58,
                    2,
                    192,
                    0
                ],
                "title": "Svalbard Marginal Ice Zone 2024 Campaign -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Svalbard Marginal Ice Zone 2024 Campaign -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting system and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2024\nCampaign was to observe and better understand the complex interplay between\natmosphere, waves, and sea-ice in the winter Marginal Ice Zone (MIZ) in order\nto advance the predictive skill of coupled Arctic forecasting systems. The main\nobjective has been to set up a network of observations with a spatial\ndistribution that allows for a representative comparison between in situ\nobservations and gridded model data. The observed variables include air and\nsurface temperature, sea-ice drift, and wave energy spectra. With the support\nof the Norwegian Coast Guard, we participated in the research cruise with KV\nSvalbard from 4. April - 21.April 2024. In total 34 buoys were deployed in the\nMarginal Ice Zone north of the Svalbard Archipelago. The first part of the\nreport describes the instruments and their calibration (Section 2), and the\nsecond part briefly describes the weather, sea ice, and wave conditions during\nthe campaign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting system and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2024\nCampaign was to observe and better understand the complex interplay between\natmosphere, waves, and sea-ice in the winter Marginal Ice Zone (MIZ) in order\nto advance the predictive skill of coupled Arctic forecasting systems. The main\nobjective has been to set up a network of observations with a spatial\ndistribution that allows for a representative comparison between in situ\nobservations and gridded model data. The observed variables include air and\nsurface temperature, sea-ice drift, and wave energy spectra. With the support\nof the Norwegian Coast Guard, we participated in the research cruise with KV\nSvalbard from 4. April - 21.April 2024. In total 34 buoys were deployed in the\nMarginal Ice Zone north of the Svalbard Archipelago. The first part of the\nreport describes the instruments and their calibration (Section 2), and the\nsecond part briefly describes the weather, sea ice, and wave conditions during\nthe campaign."
                },
                "authors": [
                    {
                        "name": "Malte Müller"
                    },
                    {
                        "name": "Jean Rabault"
                    },
                    {
                        "name": "Cyril Palerme"
                    }
                ],
                "author_detail": {
                    "name": "Cyril Palerme"
                },
                "author": "Cyril Palerme",
                "arxiv_comment": "13 pages, 11 figures, Cruise report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.05212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05212v1",
                "updated": "2024-08-10T05:41:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    5,
                    41,
                    19,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T05:41:19Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    5,
                    41,
                    19,
                    5,
                    223,
                    0
                ],
                "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions"
                },
                "summary": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks."
                },
                "authors": [
                    {
                        "name": "Michele Miranda"
                    },
                    {
                        "name": "Elena Sofia Ruzzetti"
                    },
                    {
                        "name": "Andrea Santilli"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    },
                    {
                        "name": "Sébastien Bratières"
                    },
                    {
                        "name": "Emanuele Rodolà"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Rodolà"
                },
                "author": "Emanuele Rodolà",
                "arxiv_comment": "GitHub repository:\n  https://github.com/michele17284/Awesome-Privacy-Preserving-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05211v1",
                "updated": "2024-08-09T17:59:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    59,
                    49,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T17:59:49Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    59,
                    49,
                    4,
                    222,
                    0
                ],
                "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
                },
                "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Project Page: https://vita-home.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09805v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09805v3",
                "updated": "2024-08-09T17:57:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    57,
                    26,
                    4,
                    222,
                    0
                ],
                "published": "2023-11-16T11:30:53Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    30,
                    53,
                    3,
                    320,
                    0
                ],
                "title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in\n  Understanding Long and Specialized Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in\n  Understanding Long and Specialized Documents"
                },
                "summary": "Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning capabilities of LLMs in the context of understanding and analyzing\nspecialized documents containing both text and tables. We conduct an extensive\nevaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting\nmethods, aiming to comprehensively assess the capabilities and limitations of\nexisting LLMs in DocMath-Eval. We found that even the current best-performing\nsystem (i.e., GPT-4o) still significantly lags behind human experts in solving\ncomplex numerical reasoning problems grounded in long contexts. We believe that\nDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'\ncapabilities in solving challenging numerical reasoning problems within expert\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning capabilities of LLMs in the context of understanding and analyzing\nspecialized documents containing both text and tables. We conduct an extensive\nevaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting\nmethods, aiming to comprehensively assess the capabilities and limitations of\nexisting LLMs in DocMath-Eval. We found that even the current best-performing\nsystem (i.e., GPT-4o) still significantly lags behind human experts in solving\ncomplex numerical reasoning problems grounded in long contexts. We believe that\nDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'\ncapabilities in solving challenging numerical reasoning problems within expert\ndomains."
                },
                "authors": [
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Yitao Long"
                    },
                    {
                        "name": "Hongjun Liu"
                    },
                    {
                        "name": "Ryo Kamoi"
                    },
                    {
                        "name": "Linyong Nan"
                    },
                    {
                        "name": "Lyuhao Chen"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "ACL 2024 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09805v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18783v3",
                "updated": "2024-08-09T17:57:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    57,
                    0,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-26T23:04:52Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    23,
                    4,
                    52,
                    2,
                    178,
                    0
                ],
                "title": "Psychological Profiling in Cybersecurity: A Look at LLMs and\n  Psycholinguistic Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological Profiling in Cybersecurity: A Look at LLMs and\n  Psycholinguistic Features"
                },
                "summary": "The increasing sophistication of cyber threats necessitates innovative\napproaches to cybersecurity. In this paper, we explore the potential of\npsychological profiling techniques, particularly focusing on the utilization of\nLarge Language Models (LLMs) and psycholinguistic features. We investigate the\nintersection of psychology and cybersecurity, discussing how LLMs can be\nemployed to analyze textual data for identifying psychological traits of threat\nactors. We explore the incorporation of psycholinguistic features, such as\nlinguistic patterns and emotional cues, into cybersecurity frameworks. Our\nresearch underscores the importance of integrating psychological perspectives\ninto cybersecurity practices to bolster defense mechanisms against evolving\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing sophistication of cyber threats necessitates innovative\napproaches to cybersecurity. In this paper, we explore the potential of\npsychological profiling techniques, particularly focusing on the utilization of\nLarge Language Models (LLMs) and psycholinguistic features. We investigate the\nintersection of psychology and cybersecurity, discussing how LLMs can be\nemployed to analyze textual data for identifying psychological traits of threat\nactors. We explore the incorporation of psycholinguistic features, such as\nlinguistic patterns and emotional cues, into cybersecurity frameworks. Our\nresearch underscores the importance of integrating psychological perspectives\ninto cybersecurity practices to bolster defense mechanisms against evolving\nthreats."
                },
                "authors": [
                    {
                        "name": "Jean Marie Tshimula"
                    },
                    {
                        "name": "D'Jeff K. Nkashama"
                    },
                    {
                        "name": "Jean Tshibangu Muabila"
                    },
                    {
                        "name": "René Manassé Galekwa"
                    },
                    {
                        "name": "Hugues Kanda"
                    },
                    {
                        "name": "Maximilien V. Dialufuma"
                    },
                    {
                        "name": "Mbuyi Mukendi Didier"
                    },
                    {
                        "name": "Kalonji Kalala"
                    },
                    {
                        "name": "Serge Mundele"
                    },
                    {
                        "name": "Patience Kinshie Lenye"
                    },
                    {
                        "name": "Tighana Wenge Basele"
                    },
                    {
                        "name": "Aristarque Ilunga"
                    },
                    {
                        "name": "Christian N. Mayemba"
                    },
                    {
                        "name": "Nathanaël M. Kasoro"
                    },
                    {
                        "name": "Selain K. Kasereka"
                    },
                    {
                        "name": "Hardy Mikese"
                    },
                    {
                        "name": "Pierre-Martin Tardif"
                    },
                    {
                        "name": "Marc Frappier"
                    },
                    {
                        "name": "Froduald Kabanza"
                    },
                    {
                        "name": "Belkacem Chikhaoui"
                    },
                    {
                        "name": "Shengrui Wang"
                    },
                    {
                        "name": "Ali Mulenda Sumbu"
                    },
                    {
                        "name": "Xavier Ndona"
                    },
                    {
                        "name": "Raoul Kienge-Kienge Intudi"
                    }
                ],
                "author_detail": {
                    "name": "Raoul Kienge-Kienge Intudi"
                },
                "author": "Raoul Kienge-Kienge Intudi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05204v1",
                "updated": "2024-08-09T17:53:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    53,
                    35,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T17:53:35Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    53,
                    35,
                    4,
                    222,
                    0
                ],
                "title": "Evaluating the capability of large language models to personalize\n  science texts for diverse middle-school-age learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the capability of large language models to personalize\n  science texts for diverse middle-school-age learners"
                },
                "summary": "Large language models (LLMs), including OpenAI's GPT-series, have made\nsignificant advancements in recent years. Known for their expertise across\ndiverse subject areas and quick adaptability to user-provided prompts, LLMs\nhold unique potential as Personalized Learning (PL) tools. Despite this\npotential, their application in K-12 education remains largely unexplored. This\npaper presents one of the first randomized controlled trials (n = 23) to\nevaluate the effectiveness of GPT-4 in personalizing educational science texts\nfor middle school students. In this study, GPT-4 was used to profile student\nlearning preferences based on choices made during a training session. For the\nexperimental group, GPT-4 was used to rewrite science texts to align with the\nstudent's predicted profile while, for students in the control group, texts\nwere rewritten to contradict their learning preferences. The results of a\nMann-Whitney U test showed that students significantly preferred (at the .10\nlevel) the rewritten texts when they were aligned with their profile (p =\n.059). These findings suggest that GPT-4 can effectively interpret and tailor\neducational content to diverse learner preferences, marking a significant\nadvancement in PL technology. The limitations of this study and ethical\nconsiderations for using artificial intelligence in education are also\ndiscussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), including OpenAI's GPT-series, have made\nsignificant advancements in recent years. Known for their expertise across\ndiverse subject areas and quick adaptability to user-provided prompts, LLMs\nhold unique potential as Personalized Learning (PL) tools. Despite this\npotential, their application in K-12 education remains largely unexplored. This\npaper presents one of the first randomized controlled trials (n = 23) to\nevaluate the effectiveness of GPT-4 in personalizing educational science texts\nfor middle school students. In this study, GPT-4 was used to profile student\nlearning preferences based on choices made during a training session. For the\nexperimental group, GPT-4 was used to rewrite science texts to align with the\nstudent's predicted profile while, for students in the control group, texts\nwere rewritten to contradict their learning preferences. The results of a\nMann-Whitney U test showed that students significantly preferred (at the .10\nlevel) the rewritten texts when they were aligned with their profile (p =\n.059). These findings suggest that GPT-4 can effectively interpret and tailor\neducational content to diverse learner preferences, marking a significant\nadvancement in PL technology. The limitations of this study and ethical\nconsiderations for using artificial intelligence in education are also\ndiscussed."
                },
                "authors": [
                    {
                        "name": "Michael Vaccaro Jr"
                    },
                    {
                        "name": "Mikayla Friday"
                    },
                    {
                        "name": "Arash Zaghi"
                    }
                ],
                "author_detail": {
                    "name": "Arash Zaghi"
                },
                "author": "Arash Zaghi",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05200v1",
                "updated": "2024-08-09T17:44:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T17:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    44,
                    45,
                    4,
                    222,
                    0
                ],
                "title": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaSL: Task Skill Localization and Consolidation for Language Model\n  Continual Learning"
                },
                "summary": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings."
                },
                "authors": [
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Extension of ACL 2024 paper titled: Continual Dialog State Tracking\n  via Task Skill Localization and Consolidation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00588v2",
                "updated": "2024-08-09T17:34:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    34,
                    4,
                    4,
                    222,
                    0
                ],
                "published": "2023-11-30T18:55:23Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    55,
                    23,
                    3,
                    334,
                    0
                ],
                "title": "LucidDreaming: Controllable Object-Centric 3D Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LucidDreaming: Controllable Object-Centric 3D Generation"
                },
                "summary": "With the recent development of generative models, Text-to-3D generations have\nalso seen significant growth, opening a door for creating video-game 3D assets\nfrom a more general public. Nonetheless, people without any professional 3D\nediting experience would find it hard to achieve precise control over the 3D\ngeneration, especially if there are multiple objects in the prompt, as using\ntext to control often leads to missing objects and imprecise locations. In this\npaper, we present LucidDreaming as an effective pipeline capable of spatial and\nnumerical control over 3D generation from only textual prompt commands or 3D\nbounding boxes. Specifically, our research demonstrates that Large Language\nModels (LLMs) possess 3D spatial awareness and can effectively translate\ntextual 3D information into precise 3D bounding boxes. We leverage LLMs to get\nindividual object information and their 3D bounding boxes as the initial step\nof our process. Then with the bounding boxes, We further propose clipped ray\nsampling and object-centric density blob bias to generate 3D objects aligning\nwith the bounding boxes. We show that our method exhibits remarkable\nadaptability across a spectrum of mainstream Score Distillation Sampling-based\n3D generation frameworks and our pipeline can even used to insert objects into\nan existing NeRF scene. Moreover, we also provide a dataset of prompts with 3D\nbounding boxes, benchmarking 3D spatial controllability. With extensive\nqualitative and quantitative experiments, we demonstrate that LucidDreaming\nachieves superior results in object placement precision and generation fidelity\ncompared to current approaches, while maintaining flexibility and ease of use\nfor non-expert users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent development of generative models, Text-to-3D generations have\nalso seen significant growth, opening a door for creating video-game 3D assets\nfrom a more general public. Nonetheless, people without any professional 3D\nediting experience would find it hard to achieve precise control over the 3D\ngeneration, especially if there are multiple objects in the prompt, as using\ntext to control often leads to missing objects and imprecise locations. In this\npaper, we present LucidDreaming as an effective pipeline capable of spatial and\nnumerical control over 3D generation from only textual prompt commands or 3D\nbounding boxes. Specifically, our research demonstrates that Large Language\nModels (LLMs) possess 3D spatial awareness and can effectively translate\ntextual 3D information into precise 3D bounding boxes. We leverage LLMs to get\nindividual object information and their 3D bounding boxes as the initial step\nof our process. Then with the bounding boxes, We further propose clipped ray\nsampling and object-centric density blob bias to generate 3D objects aligning\nwith the bounding boxes. We show that our method exhibits remarkable\nadaptability across a spectrum of mainstream Score Distillation Sampling-based\n3D generation frameworks and our pipeline can even used to insert objects into\nan existing NeRF scene. Moreover, we also provide a dataset of prompts with 3D\nbounding boxes, benchmarking 3D spatial controllability. With extensive\nqualitative and quantitative experiments, we demonstrate that LucidDreaming\nachieves superior results in object placement precision and generation fidelity\ncompared to current approaches, while maintaining flexibility and ease of use\nfor non-expert users."
                },
                "authors": [
                    {
                        "name": "Zhaoning Wang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00211v2",
                "updated": "2024-08-09T17:28:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    28,
                    30,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-30T01:26:05Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    1,
                    26,
                    5,
                    5,
                    90,
                    0
                ],
                "title": "Multi-Conditional Ranking with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Conditional Ranking with Large Language Models"
                },
                "summary": "Utilizing large language models (LLMs) to rank a set of items has become a\ncommon approach in recommendation and retrieval systems. Typically, these\nsystems focus on ordering a substantial number of documents in a monotonic\norder based on a given query. However, real-world scenarios often present a\ndifferent challenge: ranking a comparatively smaller set of items, but\naccording to a variety of diverse and occasionally conflicting conditions. In\nthis paper, we define and explore the task of multi-conditional ranking by\nintroducing MCRank, a benchmark tailored for assessing multi-conditional\nranking across various item types and conditions. Our analysis of LLMs using\nMCRank indicates a significant decrease in performance as the number and\ncomplexity of items and conditions grow. To overcome this limitation, we\npropose a novel decomposed reasoning method, consisting of EXtracting and\nSorting the conditions, and then Iteratively Ranking the items (EXSIR). Our\nextensive experiments show that this decomposed reasoning method enhances LLMs'\nperformance significantly, achieving up to a 12% improvement over existing\nLLMs. We also provide a detailed analysis of LLMs performance across various\ncondition categories, and examine the effectiveness of decomposition step.\nFurthermore, we compare our method with existing approaches such as\nChain-of-Thought and existing ranking models, demonstrating the superiority of\nour approach and complexity of MCR task. We released our dataset and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing large language models (LLMs) to rank a set of items has become a\ncommon approach in recommendation and retrieval systems. Typically, these\nsystems focus on ordering a substantial number of documents in a monotonic\norder based on a given query. However, real-world scenarios often present a\ndifferent challenge: ranking a comparatively smaller set of items, but\naccording to a variety of diverse and occasionally conflicting conditions. In\nthis paper, we define and explore the task of multi-conditional ranking by\nintroducing MCRank, a benchmark tailored for assessing multi-conditional\nranking across various item types and conditions. Our analysis of LLMs using\nMCRank indicates a significant decrease in performance as the number and\ncomplexity of items and conditions grow. To overcome this limitation, we\npropose a novel decomposed reasoning method, consisting of EXtracting and\nSorting the conditions, and then Iteratively Ranking the items (EXSIR). Our\nextensive experiments show that this decomposed reasoning method enhances LLMs'\nperformance significantly, achieving up to a 12% improvement over existing\nLLMs. We also provide a detailed analysis of LLMs performance across various\ncondition categories, and examine the effectiveness of decomposition step.\nFurthermore, we compare our method with existing approaches such as\nChain-of-Thought and existing ranking models, demonstrating the superiority of\nour approach and complexity of MCR task. We released our dataset and code."
                },
                "authors": [
                    {
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2108.04852v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2108.04852v6",
                "updated": "2024-08-09T17:27:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    27,
                    52,
                    4,
                    222,
                    0
                ],
                "published": "2021-08-10T18:13:22Z",
                "published_parsed": [
                    2021,
                    8,
                    10,
                    18,
                    13,
                    22,
                    1,
                    222,
                    0
                ],
                "title": "Multiway empirical likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiway empirical likelihood"
                },
                "summary": "This paper develops a general methodology to conduct statistical inference\nfor observations indexed by multiple sets of entities. We propose a novel\nmultiway empirical likelihood statistic that converges to a chi-square\ndistribution under the non-degenerate case, where corresponding Hoeffding type\ndecomposition is dominated by linear terms. Our methodology is related to the\nnotion of jackknife empirical likelihood but the leave-out pseudo values are\nconstructed by leaving columns or rows. We further develop a modified version\nof our multiway empirical likelihood statistic, which converges to a chi-square\ndistribution regardless of the degeneracy, and discover its desirable\nhigher-order property compared to the t-ratio by the conventional Eicker-White\ntype variance estimator. The proposed methodology is illustrated by several\nimportant statistical problems, such as bipartite network, generalized\nestimating equations, and three-way observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a general methodology to conduct statistical inference\nfor observations indexed by multiple sets of entities. We propose a novel\nmultiway empirical likelihood statistic that converges to a chi-square\ndistribution under the non-degenerate case, where corresponding Hoeffding type\ndecomposition is dominated by linear terms. Our methodology is related to the\nnotion of jackknife empirical likelihood but the leave-out pseudo values are\nconstructed by leaving columns or rows. We further develop a modified version\nof our multiway empirical likelihood statistic, which converges to a chi-square\ndistribution regardless of the degeneracy, and discover its desirable\nhigher-order property compared to the t-ratio by the conventional Eicker-White\ntype variance estimator. The proposed methodology is illustrated by several\nimportant statistical problems, such as bipartite network, generalized\nestimating equations, and three-way observations."
                },
                "authors": [
                    {
                        "name": "Harold D Chiang"
                    },
                    {
                        "name": "Yukitoshi Matsushita"
                    },
                    {
                        "name": "Taisuke Otsu"
                    }
                ],
                "author_detail": {
                    "name": "Taisuke Otsu"
                },
                "author": "Taisuke Otsu",
                "arxiv_comment": "29 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2108.04852v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2108.04852v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.03708v2",
                "updated": "2024-08-09T17:17:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    17,
                    53,
                    4,
                    222,
                    0
                ],
                "published": "2023-04-07T15:45:15Z",
                "published_parsed": [
                    2023,
                    4,
                    7,
                    15,
                    45,
                    15,
                    4,
                    97,
                    0
                ],
                "title": "Efficient automatic segmentation for multi-level pulmonary arteries: The\n  PARSE challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient automatic segmentation for multi-level pulmonary arteries: The\n  PARSE challenge"
                },
                "summary": "Efficient automatic segmentation of multi-level (i.e. main and branch)\npulmonary arteries (PA) in CTPA images plays a significant role in clinical\napplications. However, most existing methods concentrate only on main PA or\nbranch PA segmentation separately and ignore segmentation efficiency. Besides,\nthere is no public large-scale dataset focused on PA segmentation, which makes\nit highly challenging to compare the different methods. To benchmark\nmulti-level PA segmentation algorithms, we organized the first\n\\textbf{P}ulmonary \\textbf{AR}tery \\textbf{SE}gmentation (PARSE) challenge. On\nthe one hand, we focus on both the main PA and the branch PA segmentation. On\nthe other hand, for better clinical application, we assign the same score\nweight to segmentation efficiency (mainly running time and GPU memory\nconsumption during inference) while ensuring PA segmentation accuracy. We\npresent a summary of the top algorithms and offer some suggestions for\nefficient and accurate multi-level PA automatic segmentation. We provide the\nPARSE challenge as open-access for the community to benchmark future algorithm\ndevelopments at \\url{https://parse2022.grand-challenge.org/Parse2022/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient automatic segmentation of multi-level (i.e. main and branch)\npulmonary arteries (PA) in CTPA images plays a significant role in clinical\napplications. However, most existing methods concentrate only on main PA or\nbranch PA segmentation separately and ignore segmentation efficiency. Besides,\nthere is no public large-scale dataset focused on PA segmentation, which makes\nit highly challenging to compare the different methods. To benchmark\nmulti-level PA segmentation algorithms, we organized the first\n\\textbf{P}ulmonary \\textbf{AR}tery \\textbf{SE}gmentation (PARSE) challenge. On\nthe one hand, we focus on both the main PA and the branch PA segmentation. On\nthe other hand, for better clinical application, we assign the same score\nweight to segmentation efficiency (mainly running time and GPU memory\nconsumption during inference) while ensuring PA segmentation accuracy. We\npresent a summary of the top algorithms and offer some suggestions for\nefficient and accurate multi-level PA automatic segmentation. We provide the\nPARSE challenge as open-access for the community to benchmark future algorithm\ndevelopments at \\url{https://parse2022.grand-challenge.org/Parse2022/}."
                },
                "authors": [
                    {
                        "name": "Gongning Luo"
                    },
                    {
                        "name": "Kuanquan Wang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Xinjie Liang"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Shaowei Gan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Suyu Dong"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Pengxin Yu"
                    },
                    {
                        "name": "Enyou Liu"
                    },
                    {
                        "name": "Hongrong Wei"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Ziwei Zhao"
                    },
                    {
                        "name": "Na Gao"
                    },
                    {
                        "name": "Nan An"
                    },
                    {
                        "name": "Ashkan Pakzad"
                    },
                    {
                        "name": "Bojidar Rangelov"
                    },
                    {
                        "name": "Jiaqi Dou"
                    },
                    {
                        "name": "Song Tian"
                    },
                    {
                        "name": "Zeyu Liu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ampatishan Sivalingam"
                    },
                    {
                        "name": "Kumaradevan Punithakumar"
                    },
                    {
                        "name": "Zhaowen Qiu"
                    },
                    {
                        "name": "Xin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Gao"
                },
                "author": "Xin Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.04420v3",
                "updated": "2024-08-09T17:14:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    17,
                    14,
                    7,
                    4,
                    222,
                    0
                ],
                "published": "2023-08-08T17:36:43Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    17,
                    36,
                    43,
                    1,
                    220,
                    0
                ],
                "title": "Contour Location for Reliability in Airfoil Simulation Experiments using\n  Deep Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contour Location for Reliability in Airfoil Simulation Experiments using\n  Deep Gaussian Processes"
                },
                "summary": "Bayesian deep Gaussian processes (DGPs) outperform ordinary GPs as surrogate\nmodels of complex computer experiments when response surface dynamics are\nnon-stationary, which is especially prevalent in aerospace simulations. Yet DGP\nsurrogates have not been deployed for the canonical downstream task in that\nsetting: reliability analysis through contour location (CL). In that context,\nwe are motivated by a simulation of an RAE-2822 transonic airfoil which\ndemarcates efficient and inefficient flight conditions. Level sets separating\npassable versus failable operating conditions are best learned through\nstrategic sequential designs. There are two limitations to modern CL\nmethodology which hinder DGP integration in this setting. First,\nderivative-based optimization underlying acquisition functions is thwarted by\nsampling-based Bayesian (i.e., MCMC) inference, which is essential for DGP\nposterior integration. Second, canonical acquisition criteria, such as entropy,\nare famously myopic to the extent that optimization may even be undesirable.\nHere we tackle both of these limitations at once, proposing a hybrid criterion\nthat explores along the Pareto front of entropy and (predictive) uncertainty,\nrequiring evaluation only at strategically located \"triangulation\" candidates.\nWe showcase DGP CL performance in several synthetic benchmark exercises and on\nthe RAE-2822 airfoil.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian deep Gaussian processes (DGPs) outperform ordinary GPs as surrogate\nmodels of complex computer experiments when response surface dynamics are\nnon-stationary, which is especially prevalent in aerospace simulations. Yet DGP\nsurrogates have not been deployed for the canonical downstream task in that\nsetting: reliability analysis through contour location (CL). In that context,\nwe are motivated by a simulation of an RAE-2822 transonic airfoil which\ndemarcates efficient and inefficient flight conditions. Level sets separating\npassable versus failable operating conditions are best learned through\nstrategic sequential designs. There are two limitations to modern CL\nmethodology which hinder DGP integration in this setting. First,\nderivative-based optimization underlying acquisition functions is thwarted by\nsampling-based Bayesian (i.e., MCMC) inference, which is essential for DGP\nposterior integration. Second, canonical acquisition criteria, such as entropy,\nare famously myopic to the extent that optimization may even be undesirable.\nHere we tackle both of these limitations at once, proposing a hybrid criterion\nthat explores along the Pareto front of entropy and (predictive) uncertainty,\nrequiring evaluation only at strategically located \"triangulation\" candidates.\nWe showcase DGP CL performance in several synthetic benchmark exercises and on\nthe RAE-2822 airfoil."
                },
                "authors": [
                    {
                        "name": "Annie S. Booth"
                    },
                    {
                        "name": "S. Ashwin Renganathan"
                    },
                    {
                        "name": "Robert B. Gramacy"
                    }
                ],
                "author_detail": {
                    "name": "Robert B. Gramacy"
                },
                "author": "Robert B. Gramacy",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21436v2",
                "updated": "2024-08-09T16:59:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    59,
                    35,
                    4,
                    222,
                    0
                ],
                "published": "2024-07-31T08:38:55Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    8,
                    38,
                    55,
                    2,
                    213,
                    0
                ],
                "title": "Enriching thermal point clouds of buildings using semantic 3D building\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriching thermal point clouds of buildings using semantic 3D building\n  models"
                },
                "summary": "Thermal point clouds integrate thermal radiation and laser point clouds\neffectively. However, the semantic information for the interpretation of\nbuilding thermal point clouds can hardly be precisely inferred. Transferring\nthe semantics encapsulated in 3D building models at LoD3 has a potential to\nfill this gap. In this work, we propose a workflow enriching thermal point\nclouds with the geo-position and semantics of LoD3 building models, which\nutilizes features of both modalities: The proposed method can automatically\nco-register the point clouds from different sources and enrich the thermal\npoint cloud in facade-detailed semantics. The enriched thermal point cloud\nsupports thermal analysis and can facilitate the development of currently\nscarce deep learning models operating directly on thermal point clouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal point clouds integrate thermal radiation and laser point clouds\neffectively. However, the semantic information for the interpretation of\nbuilding thermal point clouds can hardly be precisely inferred. Transferring\nthe semantics encapsulated in 3D building models at LoD3 has a potential to\nfill this gap. In this work, we propose a workflow enriching thermal point\nclouds with the geo-position and semantics of LoD3 building models, which\nutilizes features of both modalities: The proposed method can automatically\nco-register the point clouds from different sources and enrich the thermal\npoint cloud in facade-detailed semantics. The enriched thermal point cloud\nsupports thermal analysis and can facilitate the development of currently\nscarce deep learning models operating directly on thermal point clouds."
                },
                "authors": [
                    {
                        "name": "Jingwei Zhu"
                    },
                    {
                        "name": "Olaf Wysocki"
                    },
                    {
                        "name": "Christoph Holst"
                    },
                    {
                        "name": "Thomas H. Kolbe"
                    }
                ],
                "author_detail": {
                    "name": "Thomas H. Kolbe"
                },
                "author": "Thomas H. Kolbe",
                "arxiv_doi": "10.5194/isprs-annals-X-4-W5-2024-341-2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5194/isprs-annals-X-4-W5-2024-341-2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.21436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the 3D GeoInfo 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15112v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15112v4",
                "updated": "2024-08-09T16:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    57,
                    9,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-22T11:08:48Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    11,
                    8,
                    48,
                    4,
                    82,
                    0
                ],
                "title": "Text Clustering with LLM Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering with LLM Embeddings"
                },
                "summary": "Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis."
                },
                "authors": [
                    {
                        "name": "Alina Petukhova"
                    },
                    {
                        "name": "João P. Matos-Carvalho"
                    },
                    {
                        "name": "Nuno Fachada"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Fachada"
                },
                "author": "Nuno Fachada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15112v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15112v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.7.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05173v1",
                "updated": "2024-08-09T16:52:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    52,
                    44,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:52:44Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    52,
                    44,
                    4,
                    222,
                    0
                ],
                "title": "Revisiting physical parameters of the benchmark brown dwarf LHS 6343 C\n  through a HST/WFC3 secondary eclipse observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting physical parameters of the benchmark brown dwarf LHS 6343 C\n  through a HST/WFC3 secondary eclipse observation"
                },
                "summary": "The LHS 6343 system consists of a resolved M-dwarf binary with an evolved,\nnegligibly irradiated brown dwarf, LHS 6343 C, orbiting the primary star. Such\nbrown dwarf eclipsing binaries present rare and unique opportunities to\ncalibrate sub-stellar evolutionary and atmosphere models since mass, radius,\ntemperature and luminosity can be directly measured. We update this brown\ndwarf's mass (62.6+/-2.2 MJup) and radius (0.788+/-0.043 RJup) using empirical\nstellar relations and a Gaia DR3 distance. We use Hubble Space Telescope/WFC3\nobservations of an LHS 6343 C secondary eclipse to obtain a NIR emission\nspectrum, which matches to a spectral type of T1.5+/-1. We combine this\nspectrum with existing Kepler and Spitzer/IRAC secondary eclipse photometry to\nperform atmospheric characterization using the ATMO-2020, Sonora-Bobcat and\nBT-Settl model grids. ATMO-2020 models with strong non-equilibrium chemistry\nyield the best fit to observations across all modelled bandpasses while\npredicting physical parameters consistent with Gaia-dependant analogs. BT-Settl\npredicts values slightly more consistent with such analogs but offers a\nsignificantly poorer fit to the WFC3 spectrum. Finally, we obtain a\nsemi-empirical measurement of LHS 6343 C's apparent luminosity by integrating\nits observed and modelled spectral energy distribution. Applying knowledge of\nthe system's distance yields a bolometric luminosity of log(Lbol/Lsun) =\n-4.77+/-0.03 and, applying the Stefan-Boltzmann law for the known radius, an\neffective temperature of 1303+/-29 K. We also use the ATMO-2020 and\nSonora-Bobcat evolutionary model grids to infer an age for LHS 6343 C of 2.86\n+0.40-0.33 Gyr and 3.11 +0.50-0.38 Gyr respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LHS 6343 system consists of a resolved M-dwarf binary with an evolved,\nnegligibly irradiated brown dwarf, LHS 6343 C, orbiting the primary star. Such\nbrown dwarf eclipsing binaries present rare and unique opportunities to\ncalibrate sub-stellar evolutionary and atmosphere models since mass, radius,\ntemperature and luminosity can be directly measured. We update this brown\ndwarf's mass (62.6+/-2.2 MJup) and radius (0.788+/-0.043 RJup) using empirical\nstellar relations and a Gaia DR3 distance. We use Hubble Space Telescope/WFC3\nobservations of an LHS 6343 C secondary eclipse to obtain a NIR emission\nspectrum, which matches to a spectral type of T1.5+/-1. We combine this\nspectrum with existing Kepler and Spitzer/IRAC secondary eclipse photometry to\nperform atmospheric characterization using the ATMO-2020, Sonora-Bobcat and\nBT-Settl model grids. ATMO-2020 models with strong non-equilibrium chemistry\nyield the best fit to observations across all modelled bandpasses while\npredicting physical parameters consistent with Gaia-dependant analogs. BT-Settl\npredicts values slightly more consistent with such analogs but offers a\nsignificantly poorer fit to the WFC3 spectrum. Finally, we obtain a\nsemi-empirical measurement of LHS 6343 C's apparent luminosity by integrating\nits observed and modelled spectral energy distribution. Applying knowledge of\nthe system's distance yields a bolometric luminosity of log(Lbol/Lsun) =\n-4.77+/-0.03 and, applying the Stefan-Boltzmann law for the known radius, an\neffective temperature of 1303+/-29 K. We also use the ATMO-2020 and\nSonora-Bobcat evolutionary model grids to infer an age for LHS 6343 C of 2.86\n+0.40-0.33 Gyr and 3.11 +0.50-0.38 Gyr respectively."
                },
                "authors": [
                    {
                        "name": "William Frost"
                    },
                    {
                        "name": "Loïc Albert"
                    },
                    {
                        "name": "René Doyon"
                    },
                    {
                        "name": "Jonathan Gagné"
                    },
                    {
                        "name": "Benjamin T. Montet"
                    },
                    {
                        "name": "Clémence Fontanive"
                    },
                    {
                        "name": "Étienne Artigau"
                    },
                    {
                        "name": "John Asher Johnson"
                    },
                    {
                        "name": "Billy Edwards"
                    },
                    {
                        "name": "Björn Benneke"
                    }
                ],
                "author_detail": {
                    "name": "Björn Benneke"
                },
                "arxiv_affiliation": "Trottier Institute for Research on Exoplanets, Université de Montréal",
                "author": "Björn Benneke",
                "arxiv_comment": "Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05159v1",
                "updated": "2024-08-09T16:31:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    31,
                    2,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:31:02Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    31,
                    2,
                    4,
                    222,
                    0
                ],
                "title": "EasyInv: Toward Fast and Better DDIM Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyInv: Toward Fast and Better DDIM Inversion"
                },
                "summary": "This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EasyInv, an easy yet novel approach that significantly\nadvances the field of DDIM Inversion by addressing the inherent inefficiencies\nand performance limitations of traditional iterative optimization methods. At\nthe core of our EasyInv is a refined strategy for approximating inversion\nnoise, which is pivotal for enhancing the accuracy and reliability of the\ninversion process. By prioritizing the initial latent state, which encapsulates\nrich information about the original images, EasyInv steers clear of the\niterative refinement of noise items. Instead, we introduce a methodical\naggregation of the latent state from the preceding time step with the current\nstate, effectively increasing the influence of the initial latent state and\nmitigating the impact of noise. We illustrate that EasyInv is capable of\ndelivering results that are either on par with or exceed those of the\nconventional DDIM Inversion approach, especially under conditions where the\nmodel's precision is limited or computational resources are scarce.\nConcurrently, our EasyInv offers an approximate threefold enhancement regarding\ninference efficiency over off-the-shelf iterative optimization techniques."
                },
                "authors": [
                    {
                        "name": "Ziyue Zhang"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12196v2",
                "updated": "2024-08-09T16:29:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    29,
                    44,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-18T19:10:12Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    19,
                    10,
                    12,
                    0,
                    78,
                    0
                ],
                "title": "Shifting the Lens: Detecting Malicious npm Packages using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting the Lens: Detecting Malicious npm Packages using Large Language\n  Models"
                },
                "summary": "Existing malicious code detection techniques can aid the manual review\nprocess by predicting which packages are likely to be malicious. However, these\ntechniques often suffer from high misclassification rates. Therefore, malicious\ncode detection techniques could be enhanced by adopting advanced, more\nautomated approaches to achieve high accuracy and a low misclassification rate.\nThe goal of this study is to assist security analysts in detecting malicious\npackages through the empirical study of using Large Language Models (LLMs) to\ndetect malicious code in the npm ecosystem. We present SecurityAI, a malicious\ncode review workflow to detect malicious code using ChatGPT. We leverage a\nbenchmark dataset of 5,115 npm packages, of which 2,180 packages have malicious\ncode. We conducted a baseline comparison of GPT-3 and GPT- 4 models with the\nstate-of-the-art CodeQL static analysis tool, using 39 custom CodeQL rules\ndeveloped in prior research to detect malicious Javascript code. We compare the\neffectiveness of static analysis as a pre-screener with SecurityAI workflow,\nmeasuring the number of files that need to be analyzed and the associated\ncosts. Additionally, we performed a qualitative study to understand the types\nof malicious packages detected or missed by our workflow. Our baseline\ncomparison demonstrates a 16% and 9% improvement over static analysis in\nprecision and F1 scores, respectively. We attained precision and F1 scores of\n91% and 94% for GPT-3, and 99% & 97% for GPT-4, respectively, with GPT-3\noffering a cost-effective balance. Pre-screening files with a static analyzer\nreduces the number of files requiring LLM analysis by 77.9% and decreases costs\nby 60.9% for GPT-3 and 76.1% for GPT-4. Our qualitative analysis identified\ndata theft, hidden backdoors, and suspicious domain connection categories as\nthe top detected malicious packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing malicious code detection techniques can aid the manual review\nprocess by predicting which packages are likely to be malicious. However, these\ntechniques often suffer from high misclassification rates. Therefore, malicious\ncode detection techniques could be enhanced by adopting advanced, more\nautomated approaches to achieve high accuracy and a low misclassification rate.\nThe goal of this study is to assist security analysts in detecting malicious\npackages through the empirical study of using Large Language Models (LLMs) to\ndetect malicious code in the npm ecosystem. We present SecurityAI, a malicious\ncode review workflow to detect malicious code using ChatGPT. We leverage a\nbenchmark dataset of 5,115 npm packages, of which 2,180 packages have malicious\ncode. We conducted a baseline comparison of GPT-3 and GPT- 4 models with the\nstate-of-the-art CodeQL static analysis tool, using 39 custom CodeQL rules\ndeveloped in prior research to detect malicious Javascript code. We compare the\neffectiveness of static analysis as a pre-screener with SecurityAI workflow,\nmeasuring the number of files that need to be analyzed and the associated\ncosts. Additionally, we performed a qualitative study to understand the types\nof malicious packages detected or missed by our workflow. Our baseline\ncomparison demonstrates a 16% and 9% improvement over static analysis in\nprecision and F1 scores, respectively. We attained precision and F1 scores of\n91% and 94% for GPT-3, and 99% & 97% for GPT-4, respectively, with GPT-3\noffering a cost-effective balance. Pre-screening files with a static analyzer\nreduces the number of files requiring LLM analysis by 77.9% and decreases costs\nby 60.9% for GPT-3 and 76.1% for GPT-4. Our qualitative analysis identified\ndata theft, hidden backdoors, and suspicious domain connection categories as\nthe top detected malicious packages."
                },
                "authors": [
                    {
                        "name": "Nusrat Zahan"
                    },
                    {
                        "name": "Philipp Burckhardt"
                    },
                    {
                        "name": "Mikola Lysenko"
                    },
                    {
                        "name": "Feross Aboukhadijeh"
                    },
                    {
                        "name": "Laurie Williams"
                    }
                ],
                "author_detail": {
                    "name": "Laurie Williams"
                },
                "author": "Laurie Williams",
                "arxiv_comment": "13 pages, 2 Figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05152v1",
                "updated": "2024-08-09T16:16:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    16,
                    53,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:16:53Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    16,
                    53,
                    4,
                    222,
                    0
                ],
                "title": "Sparsity-Preserving Encodings for Straggler-Optimal Distributed Matrix\n  Computations at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsity-Preserving Encodings for Straggler-Optimal Distributed Matrix\n  Computations at the Edge"
                },
                "summary": "Matrix computations are a fundamental building-block of edge computing\nsystems, with a major recent uptick in demand due to their use in AI/ML\ntraining and inference procedures. Existing approaches for distributing matrix\ncomputations involve allocating coded combinations of submatrices to worker\nnodes, to build resilience to slower nodes, called stragglers. In the edge\nlearning context, however, these approaches will compromise sparsity properties\nthat are often present in the original matrices found at the edge server. In\nthis study, we consider the challenge of augmenting such approaches to preserve\ninput sparsity when distributing the task across edge devices, thereby\nretaining the associated computational efficiency enhancements. First, we find\na lower bound on the weight of coding, i.e., the number of submatrices to be\ncombined to obtain coded submatrices, to provide the resilience to the maximum\npossible number of straggler devices (for given number of devices and their\nstorage constraints). Next we propose distributed matrix computation schemes\nwhich meet the exact lower bound on the weight of the coding. Numerical\nexperiments conducted in Amazon Web Services (AWS) validate our assertions\nregarding straggler mitigation and computation speed for sparse matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix computations are a fundamental building-block of edge computing\nsystems, with a major recent uptick in demand due to their use in AI/ML\ntraining and inference procedures. Existing approaches for distributing matrix\ncomputations involve allocating coded combinations of submatrices to worker\nnodes, to build resilience to slower nodes, called stragglers. In the edge\nlearning context, however, these approaches will compromise sparsity properties\nthat are often present in the original matrices found at the edge server. In\nthis study, we consider the challenge of augmenting such approaches to preserve\ninput sparsity when distributing the task across edge devices, thereby\nretaining the associated computational efficiency enhancements. First, we find\na lower bound on the weight of coding, i.e., the number of submatrices to be\ncombined to obtain coded submatrices, to provide the resilience to the maximum\npossible number of straggler devices (for given number of devices and their\nstorage constraints). Next we propose distributed matrix computation schemes\nwhich meet the exact lower bound on the weight of the coding. Numerical\nexperiments conducted in Amazon Web Services (AWS) validate our assertions\nregarding straggler mitigation and computation speed for sparse matrices."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Aditya Ramamoorthy"
                    },
                    {
                        "name": "David J. Love"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2308.04331",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09158v2",
                "updated": "2024-08-09T16:14:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    14,
                    44,
                    4,
                    222,
                    0
                ],
                "published": "2023-10-13T14:53:06Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    14,
                    53,
                    6,
                    4,
                    286,
                    0
                ],
                "title": "Improving Large Language Models in Event Relation Logical Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Models in Event Relation Logical Prediction"
                },
                "summary": "Event relations are crucial for narrative understanding and reasoning.\nGoverned by nuanced logic, event relation extraction (ERE) is a challenging\ntask that demands thorough semantic understanding and rigorous logical\nreasoning. In this paper, we conduct an in-depth investigation to\nsystematically explore the capability of LLMs in understanding and applying\nevent relation logic. More in detail, we first investigate the deficiencies of\nLLMs in logical reasoning across different tasks. Our study reveals that LLMs\nare not logically consistent reasoners, which results in their suboptimal\nperformance on tasks that need rigorous reasoning. To address this, we explore\nthree different approaches to endow LLMs with event relation logic, and thus\nenable them to generate more coherent answers across various scenarios. Based\non our approach, we also contribute a synthesized dataset (LLM-ERL) involving\nhigh-order reasoning for evaluation and fine-tuning. Extensive quantitative and\nqualitative analyses on different tasks also validate the effectiveness of our\napproaches and provide insights for solving practical tasks with LLMs in future\nwork. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event relations are crucial for narrative understanding and reasoning.\nGoverned by nuanced logic, event relation extraction (ERE) is a challenging\ntask that demands thorough semantic understanding and rigorous logical\nreasoning. In this paper, we conduct an in-depth investigation to\nsystematically explore the capability of LLMs in understanding and applying\nevent relation logic. More in detail, we first investigate the deficiencies of\nLLMs in logical reasoning across different tasks. Our study reveals that LLMs\nare not logically consistent reasoners, which results in their suboptimal\nperformance on tasks that need rigorous reasoning. To address this, we explore\nthree different approaches to endow LLMs with event relation logic, and thus\nenable them to generate more coherent answers across various scenarios. Based\non our approach, we also contribute a synthesized dataset (LLM-ERL) involving\nhigh-order reasoning for evaluation and fine-tuning. Extensive quantitative and\nqualitative analyses on different tasks also validate the effectiveness of our\napproaches and provide insights for solving practical tasks with LLMs in future\nwork. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR."
                },
                "authors": [
                    {
                        "name": "Meiqi Chen"
                    },
                    {
                        "name": "Yubo Ma"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05149v1",
                "updated": "2024-08-09T16:10:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    10,
                    35,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:10:35Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    10,
                    35,
                    4,
                    222,
                    0
                ],
                "title": "AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity\n  Recognition Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity\n  Recognition Dataset"
                },
                "summary": "Cyber-attack attribution is an important process that allows experts to put\nin place attacker-oriented countermeasures and legal actions. The analysts\nmainly perform attribution manually, given the complex nature of this task. AI\nand, more specifically, Natural Language Processing (NLP) techniques can be\nleveraged to support cybersecurity analysts during the attribution process.\nHowever powerful these techniques are, they need to deal with the lack of\ndatasets in the attack attribution domain. In this work, we will fill this gap\nand will provide, to the best of our knowledge, the first dataset on\ncyber-attack attribution. We designed our dataset with the primary goal of\nextracting attack attribution information from cybersecurity texts, utilizing\nnamed entity recognition (NER) methodologies from the field of NLP. Unlike\nother cybersecurity NER datasets, ours offers a rich set of annotations with\ncontextual details, including some that span phrases and sentences. We\nconducted extensive experiments and applied NLP techniques to demonstrate the\ndataset's effectiveness for attack attribution. These experiments highlight the\npotential of Large Language Models (LLMs) capabilities to improve the NER tasks\nin cybersecurity datasets for cyber-attack attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-attack attribution is an important process that allows experts to put\nin place attacker-oriented countermeasures and legal actions. The analysts\nmainly perform attribution manually, given the complex nature of this task. AI\nand, more specifically, Natural Language Processing (NLP) techniques can be\nleveraged to support cybersecurity analysts during the attribution process.\nHowever powerful these techniques are, they need to deal with the lack of\ndatasets in the attack attribution domain. In this work, we will fill this gap\nand will provide, to the best of our knowledge, the first dataset on\ncyber-attack attribution. We designed our dataset with the primary goal of\nextracting attack attribution information from cybersecurity texts, utilizing\nnamed entity recognition (NER) methodologies from the field of NLP. Unlike\nother cybersecurity NER datasets, ours offers a rich set of annotations with\ncontextual details, including some that span phrases and sentences. We\nconducted extensive experiments and applied NLP techniques to demonstrate the\ndataset's effectiveness for attack attribution. These experiments highlight the\npotential of Large Language Models (LLMs) capabilities to improve the NER tasks\nin cybersecurity datasets for cyber-attack attribution."
                },
                "authors": [
                    {
                        "name": "Pritam Deka"
                    },
                    {
                        "name": "Sampath Rajapaksha"
                    },
                    {
                        "name": "Ruby Rani"
                    },
                    {
                        "name": "Amirah Almutairi"
                    },
                    {
                        "name": "Erisa Karafili"
                    }
                ],
                "author_detail": {
                    "name": "Erisa Karafili"
                },
                "author": "Erisa Karafili",
                "arxiv_comment": "Submitted to WISE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05148v1",
                "updated": "2024-08-09T16:07:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:07:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    7,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of floating-point non-associativity on reproducibility for HPC\n  and deep learning applications"
                },
                "summary": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run-by-run variability in parallel programs caused by floating-point\nnon-associativity (FPNA) has been known to significantly affect reproducibility\nin iterative algorithms, due to accumulating errors. Non-reproducibility\nnegatively affects efficiency and effectiveness of correctness testing for\nstochastic programs. Recently, the sensitivity of deep learning (DL) training\nand inference pipelines to FPNA have been found to be extreme, and can prevent\ncertification for commercial applications, accurate assessment of robustness\nand sensitivity, and bug detection. New approaches in scientific computing\napplications have coupled DL models with high-performance computing (HPC)\nsimulations, leading to an aggravation of debugging and testing challenges.\nHere we perform an investigation of the statistical properties of FPNA within\nmodern parallel programming models, analyze performance and productivity\nimpacts of replacing atomic operations with deterministic alternatives on GPUs,\nand examine the recently-added deterministic options within the PyTorch\nframework within the context of GPU deployment, uncovering and quantifying the\nimpacts of input parameters triggering run-by-run variability and reporting on\nthe reliability and completeness of the documentation. Finally, we evaluate the\nstrategy of exploiting automatic determinism provided by deterministic\nhardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the\nDL pipeline. We demonstrate the benefits that this strategy can provide within\nreproducibility and correctness efforts."
                },
                "authors": [
                    {
                        "name": "Sanjif Shanmugavelu"
                    },
                    {
                        "name": "Mathieu Taillefumier"
                    },
                    {
                        "name": "Christopher Culver"
                    },
                    {
                        "name": "Oscar Hernandez"
                    },
                    {
                        "name": "Mark Coletti"
                    },
                    {
                        "name": "Ada Sedova"
                    }
                ],
                "author_detail": {
                    "name": "Ada Sedova"
                },
                "author": "Ada Sedova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06323v2",
                "updated": "2024-08-09T15:57:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    57,
                    41,
                    4,
                    222,
                    0
                ],
                "published": "2024-04-09T14:02:34Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    14,
                    2,
                    34,
                    1,
                    100,
                    0
                ],
                "title": "From chiral EFT to perturbative QCD: a Bayesian model mixing approach to\n  symmetric nuclear matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From chiral EFT to perturbative QCD: a Bayesian model mixing approach to\n  symmetric nuclear matter"
                },
                "summary": "Constraining the equation of state (EOS) of strongly interacting, dense\nmatter is the focus of intense experimental, observational, and theoretical\neffort. Chiral effective field theory ($\\chi$EFT) can describe the EOS between\nthe typical densities of nuclei and those in the outer cores of neutron stars\nwhile perturbative QCD (pQCD) can be applied to properties of deconfined quark\nmatter, both with quantified theoretical uncertainties. However, describing the\ncomplete range of densities between nuclear saturation and an almost-free quark\ngas with a single EOS that has well-quantified uncertainties is a challenging\nproblem. In this work, we argue that Bayesian multi-model inference from\n$\\chi$EFT and pQCD can help bridge the gap between the two theories. We develop\na correlated Bayesian model mixing approach that uses a Gaussian Process (GP)\nto assimilate different information into a single QCD EOS for symmetric nuclear\nmatter. In the present implementation, this mixed EOS is informed solely by\nthose of $\\chi$EFT and pQCD, together with the associated truncation errors.\nThe GP is trained on the pressure as a function of number density in the\nlow-density and high-density regions where $\\chi$EFT and pQCD are,\nrespectively, valid. We impose priors on the GP kernel hyperparameters to\nsuppress unphysical correlations between these regimes. This results in smooth\n$\\chi$EFT-to-pQCD curves for both the pressure and the speed of sound. We show\nthat using uncorrelated pointwise mixing requires uncontrolled extrapolation of\nat least one of $\\chi$EFT or pQCD into regions where the perturbative series\nbreaks down and leads to an acausal EOS. We also discuss future extensions and\napplications to neutron star matter guided by recent EOS constraints from\nnuclear theory, nuclear experiment, and multi-messenger astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the equation of state (EOS) of strongly interacting, dense\nmatter is the focus of intense experimental, observational, and theoretical\neffort. Chiral effective field theory ($\\chi$EFT) can describe the EOS between\nthe typical densities of nuclei and those in the outer cores of neutron stars\nwhile perturbative QCD (pQCD) can be applied to properties of deconfined quark\nmatter, both with quantified theoretical uncertainties. However, describing the\ncomplete range of densities between nuclear saturation and an almost-free quark\ngas with a single EOS that has well-quantified uncertainties is a challenging\nproblem. In this work, we argue that Bayesian multi-model inference from\n$\\chi$EFT and pQCD can help bridge the gap between the two theories. We develop\na correlated Bayesian model mixing approach that uses a Gaussian Process (GP)\nto assimilate different information into a single QCD EOS for symmetric nuclear\nmatter. In the present implementation, this mixed EOS is informed solely by\nthose of $\\chi$EFT and pQCD, together with the associated truncation errors.\nThe GP is trained on the pressure as a function of number density in the\nlow-density and high-density regions where $\\chi$EFT and pQCD are,\nrespectively, valid. We impose priors on the GP kernel hyperparameters to\nsuppress unphysical correlations between these regimes. This results in smooth\n$\\chi$EFT-to-pQCD curves for both the pressure and the speed of sound. We show\nthat using uncorrelated pointwise mixing requires uncontrolled extrapolation of\nat least one of $\\chi$EFT or pQCD into regions where the perturbative series\nbreaks down and leads to an acausal EOS. We also discuss future extensions and\napplications to neutron star matter guided by recent EOS constraints from\nnuclear theory, nuclear experiment, and multi-messenger astronomy."
                },
                "authors": [
                    {
                        "name": "A. C. Semposki"
                    },
                    {
                        "name": "C. Drischler"
                    },
                    {
                        "name": "R. J. Furnstahl"
                    },
                    {
                        "name": "J. A. Melendez"
                    },
                    {
                        "name": "D. R. Phillips"
                    }
                ],
                "author_detail": {
                    "name": "D. R. Phillips"
                },
                "author": "D. R. Phillips",
                "arxiv_comment": "22 pages, 9 figures, comments are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05141v1",
                "updated": "2024-08-09T15:53:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:53:55Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    53,
                    55,
                    4,
                    222,
                    0
                ],
                "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}."
                },
                "authors": [
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Chengwu Liu"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05136v1",
                "updated": "2024-08-09T15:45:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    45,
                    41,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:45:41Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    45,
                    41,
                    4,
                    222,
                    0
                ],
                "title": "Cycle-Configuration: A Novel Graph-theoretic Descriptor Set for\n  Molecular Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cycle-Configuration: A Novel Graph-theoretic Descriptor Set for\n  Molecular Inference"
                },
                "summary": "In this paper, we propose a novel family of descriptors of chemical graphs,\nnamed cycle-configuration (CC), that can be used in the standard \"two-layered\n(2L) model\" of mol-infer, a molecular inference framework based on mixed\ninteger linear programming (MILP) and machine learning (ML). Proposed\ndescriptors capture the notion of ortho/meta/para patterns that appear in\naromatic rings, which has been impossible in the framework so far.\nComputational experiments show that, when the new descriptors are supplied, we\ncan construct prediction functions of similar or better performance for all of\nthe 27 tested chemical properties. We also provide an MILP formulation that\nasks for a chemical graph with desired properties under the 2L model with CC\ndescriptors (2L+CC model). We show that a chemical graph with up to 50\nnon-hydrogen vertices can be inferred in a practical time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel family of descriptors of chemical graphs,\nnamed cycle-configuration (CC), that can be used in the standard \"two-layered\n(2L) model\" of mol-infer, a molecular inference framework based on mixed\ninteger linear programming (MILP) and machine learning (ML). Proposed\ndescriptors capture the notion of ortho/meta/para patterns that appear in\naromatic rings, which has been impossible in the framework so far.\nComputational experiments show that, when the new descriptors are supplied, we\ncan construct prediction functions of similar or better performance for all of\nthe 27 tested chemical properties. We also provide an MILP formulation that\nasks for a chemical graph with desired properties under the 2L model with CC\ndescriptors (2L+CC model). We show that a chemical graph with up to 50\nnon-hydrogen vertices can be inferred in a practical time."
                },
                "authors": [
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Jianshen Zhu"
                    },
                    {
                        "name": "Naveed Ahmed Azam"
                    },
                    {
                        "name": "Kazuya Haraguchi"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Tatsuya Akutsu"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Akutsu"
                },
                "author": "Tatsuya Akutsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05131v1",
                "updated": "2024-08-09T15:39:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    39,
                    6,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:39:06Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    39,
                    6,
                    4,
                    222,
                    0
                ],
                "title": "Range Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Range Membership Inference Attacks"
                },
                "summary": "Machine learning models can leak private information about their training\ndata, but the standard methods to measure this risk, based on membership\ninference attacks (MIAs), have a major limitation. They only check if a given\ndata point \\textit{exactly} matches a training point, neglecting the potential\nof similar or partially overlapping data revealing the same private\ninformation. To address this issue, we introduce the class of range membership\ninference attacks (RaMIAs), testing if the model was trained on any data in a\nspecified range (defined based on the semantics of privacy). We formulate the\nRaMIAs game and design a principled statistical test for its complex\nhypotheses. We show that RaMIAs can capture privacy loss more accurately and\ncomprehensively than MIAs on various types of data, such as tabular, image, and\nlanguage. RaMIA paves the way for a more comprehensive and meaningful privacy\nauditing of machine learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models can leak private information about their training\ndata, but the standard methods to measure this risk, based on membership\ninference attacks (MIAs), have a major limitation. They only check if a given\ndata point \\textit{exactly} matches a training point, neglecting the potential\nof similar or partially overlapping data revealing the same private\ninformation. To address this issue, we introduce the class of range membership\ninference attacks (RaMIAs), testing if the model was trained on any data in a\nspecified range (defined based on the semantics of privacy). We formulate the\nRaMIAs game and design a principled statistical test for its complex\nhypotheses. We show that RaMIAs can capture privacy loss more accurately and\ncomprehensively than MIAs on various types of data, such as tabular, image, and\nlanguage. RaMIA paves the way for a more comprehensive and meaningful privacy\nauditing of machine learning algorithms."
                },
                "authors": [
                    {
                        "name": "Jiashu Tao"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07311v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07311v8",
                "updated": "2024-08-09T15:39:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    39,
                    5,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-12T04:47:29Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    4,
                    47,
                    29,
                    1,
                    72,
                    0
                ],
                "title": "Knowledge Graph Large Language Model (KG-LLM) for Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph Large Language Model (KG-LLM) for Link Prediction"
                },
                "summary": "The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Tianle Chen"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07311v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07311v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05128v1",
                "updated": "2024-08-09T15:36:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    36,
                    59,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    36,
                    59,
                    4,
                    222,
                    0
                ],
                "title": "Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of\n  ChatGPT for Software Library Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of\n  ChatGPT for Software Library Recommendations"
                },
                "summary": "Software libraries play a critical role in the functionality, efficiency, and\nmaintainability of software systems. As developers increasingly rely on Large\nLanguage Models (LLMs) to streamline their coding processes, the effectiveness\nof these models in recommending appropriate libraries becomes crucial yet\nremains largely unexplored. In this paper, we assess the effectiveness of\nChatGPT as a software librarian and identify areas for improvement. We\nconducted an empirical study using GPT-3.5 Turbo to generate Python code for\n10,000 Stack Overflow questions. Our findings show that ChatGPT uses\nthird-party libraries nearly 10% more often than human developers, favoring\nwidely adopted and well-established options. However, 14.2% of the recommended\nlibraries had restrictive copyleft licenses, which were not explicitly\ncommunicated by ChatGPT. Additionally, 6.5% of the libraries did not work out\nof the box, leading to potential developer confusion and wasted time. While\nChatGPT can be an effective software librarian, it should be improved by\nproviding more explicit information on maintainability metrics and licensing.\nWe recommend that developers implement rigorous dependency management practices\nand double-check library licenses before integrating LLM-generated code into\ntheir projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software libraries play a critical role in the functionality, efficiency, and\nmaintainability of software systems. As developers increasingly rely on Large\nLanguage Models (LLMs) to streamline their coding processes, the effectiveness\nof these models in recommending appropriate libraries becomes crucial yet\nremains largely unexplored. In this paper, we assess the effectiveness of\nChatGPT as a software librarian and identify areas for improvement. We\nconducted an empirical study using GPT-3.5 Turbo to generate Python code for\n10,000 Stack Overflow questions. Our findings show that ChatGPT uses\nthird-party libraries nearly 10% more often than human developers, favoring\nwidely adopted and well-established options. However, 14.2% of the recommended\nlibraries had restrictive copyleft licenses, which were not explicitly\ncommunicated by ChatGPT. Additionally, 6.5% of the libraries did not work out\nof the box, leading to potential developer confusion and wasted time. While\nChatGPT can be an effective software librarian, it should be improved by\nproviding more explicit information on maintainability metrics and licensing.\nWe recommend that developers implement rigorous dependency management practices\nand double-check library licenses before integrating LLM-generated code into\ntheir projects."
                },
                "authors": [
                    {
                        "name": "Jasmine Latendresse"
                    },
                    {
                        "name": "SayedHassan Khatoonabadi"
                    },
                    {
                        "name": "Ahmad Abdellatif"
                    },
                    {
                        "name": "Emad Shihab"
                    }
                ],
                "author_detail": {
                    "name": "Emad Shihab"
                },
                "author": "Emad Shihab",
                "arxiv_comment": "Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10463v2",
                "updated": "2024-08-09T15:35:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    35,
                    41,
                    4,
                    222,
                    0
                ],
                "published": "2023-12-16T14:42:46Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    14,
                    42,
                    46,
                    5,
                    350,
                    0
                ],
                "title": "RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large\n  Language Models"
                },
                "summary": "In the evolving field of personalized news recommendation, understanding the\nsemantics of the underlying data is crucial. Large Language Models (LLMs) like\nGPT-4 have shown promising performance in understanding natural language.\nHowever, the extent of their applicability in news recommendation systems\nremains to be validated. This paper introduces RecPrompt, the first framework\nfor news recommendation that leverages the capabilities of LLMs through prompt\nengineering. This system incorporates a prompt optimizer that applies an\niterative bootstrapping process, enhancing the LLM-based recommender's ability\nto align news content with user preferences and interests more effectively.\nMoreover, this study offers insights into the effective use of LLMs in news\nrecommendation, emphasizing both the advantages and the challenges of\nincorporating LLMs into recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving field of personalized news recommendation, understanding the\nsemantics of the underlying data is crucial. Large Language Models (LLMs) like\nGPT-4 have shown promising performance in understanding natural language.\nHowever, the extent of their applicability in news recommendation systems\nremains to be validated. This paper introduces RecPrompt, the first framework\nfor news recommendation that leverages the capabilities of LLMs through prompt\nengineering. This system incorporates a prompt optimizer that applies an\niterative bootstrapping process, enhancing the LLM-based recommender's ability\nto align news content with user preferences and interests more effectively.\nMoreover, this study offers insights into the effective use of LLMs in news\nrecommendation, emphasizing both the advantages and the challenges of\nincorporating LLMs into recommendation systems."
                },
                "authors": [
                    {
                        "name": "Dairui Liu"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Honghui Du"
                    },
                    {
                        "name": "Derek Greene"
                    },
                    {
                        "name": "Aonghus Lawlor"
                    },
                    {
                        "name": "Ruihai Dong"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_doi": "10.1145/3627673.3679987",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679987",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.10463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 3 figures, and 8 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05126v1",
                "updated": "2024-08-09T15:34:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    34,
                    41,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:34:41Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    34,
                    41,
                    4,
                    222,
                    0
                ],
                "title": "Large Language Models and Thematic Analysis: Human-AI Synergy in\n  Researching Hate Speech on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Thematic Analysis: Human-AI Synergy in\n  Researching Hate Speech on Social Media"
                },
                "summary": "In the dynamic field of artificial intelligence (AI), the development and\napplication of Large Language Models (LLMs) for text analysis are of\nsignificant academic interest. Despite the promising capabilities of various\nLLMs in conducting qualitative analysis, their use in the humanities and social\nsciences has not been thoroughly examined. This article contributes to the\nemerging literature on LLMs in qualitative analysis by documenting an\nexperimental study involving GPT-4. The study focuses on performing thematic\nanalysis (TA) using a YouTube dataset derived from an EU-funded project, which\nwas previously analyzed by other researchers. This dataset is about the\nrepresentation of Roma migrants in Sweden during 2016, a period marked by the\naftermath of the 2015 refugee crisis and preceding the Swedish national\nelections in 2017. Our study seeks to understand the potential of combining\nhuman intelligence with AI's scalability and efficiency, examining the\nadvantages and limitations of employing LLMs in qualitative research within the\nhumanities and social sciences. Additionally, we discuss future directions for\napplying LLMs in these fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the dynamic field of artificial intelligence (AI), the development and\napplication of Large Language Models (LLMs) for text analysis are of\nsignificant academic interest. Despite the promising capabilities of various\nLLMs in conducting qualitative analysis, their use in the humanities and social\nsciences has not been thoroughly examined. This article contributes to the\nemerging literature on LLMs in qualitative analysis by documenting an\nexperimental study involving GPT-4. The study focuses on performing thematic\nanalysis (TA) using a YouTube dataset derived from an EU-funded project, which\nwas previously analyzed by other researchers. This dataset is about the\nrepresentation of Roma migrants in Sweden during 2016, a period marked by the\naftermath of the 2015 refugee crisis and preceding the Swedish national\nelections in 2017. Our study seeks to understand the potential of combining\nhuman intelligence with AI's scalability and efficiency, examining the\nadvantages and limitations of employing LLMs in qualitative research within the\nhumanities and social sciences. Additionally, we discuss future directions for\napplying LLMs in these fields."
                },
                "authors": [
                    {
                        "name": "Petre Breazu"
                    },
                    {
                        "name": "Miriam Schirmer"
                    },
                    {
                        "name": "Songbo Hu"
                    },
                    {
                        "name": "Napoleon Kastos"
                    }
                ],
                "author_detail": {
                    "name": "Napoleon Kastos"
                },
                "author": "Napoleon Kastos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05123v1",
                "updated": "2024-08-09T15:30:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    30,
                    10,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T15:30:10Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    30,
                    10,
                    4,
                    222,
                    0
                ],
                "title": "Sportify: Question Answering with Embedded Visualizations and\n  Personified Narratives for Sports Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sportify: Question Answering with Embedded Visualizations and\n  Personified Narratives for Sports Video"
                },
                "summary": "As basketball's popularity surges, fans often find themselves confused and\noverwhelmed by the rapid game pace and complexity. Basketball tactics,\ninvolving a complex series of actions, require substantial knowledge to be\nfully understood. This complexity leads to a need for additional information\nand explanation, which can distract fans from the game. To tackle these\nchallenges, we present Sportify, a Visual Question Answering system that\nintegrates narratives and embedded visualization for demystifying basketball\ntactical questions, aiding fans in understanding various game aspects. We\npropose three novel action visualizations (i.e., Pass, Cut, and Screen) to\ndemonstrate critical action sequences. To explain the reasoning and logic\nbehind players' actions, we leverage a large-language model (LLM) to generate\nnarratives. We adopt a storytelling approach for complex scenarios from both\nfirst and third-person perspectives, integrating action visualizations. We\nevaluated Sportify with basketball fans to investigate its impact on\nunderstanding of tactics, and how different personal perspectives of narratives\nimpact the understanding of complex tactic with action visualizations. Our\nevaluation with basketball fans demonstrates Sportify's capability to deepen\ntactical insights and amplify the viewing experience. Furthermore, third-person\nnarration assists people in getting in-depth game explanations while\nfirst-person narration enhances fans' game engagement",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As basketball's popularity surges, fans often find themselves confused and\noverwhelmed by the rapid game pace and complexity. Basketball tactics,\ninvolving a complex series of actions, require substantial knowledge to be\nfully understood. This complexity leads to a need for additional information\nand explanation, which can distract fans from the game. To tackle these\nchallenges, we present Sportify, a Visual Question Answering system that\nintegrates narratives and embedded visualization for demystifying basketball\ntactical questions, aiding fans in understanding various game aspects. We\npropose three novel action visualizations (i.e., Pass, Cut, and Screen) to\ndemonstrate critical action sequences. To explain the reasoning and logic\nbehind players' actions, we leverage a large-language model (LLM) to generate\nnarratives. We adopt a storytelling approach for complex scenarios from both\nfirst and third-person perspectives, integrating action visualizations. We\nevaluated Sportify with basketball fans to investigate its impact on\nunderstanding of tactics, and how different personal perspectives of narratives\nimpact the understanding of complex tactic with action visualizations. Our\nevaluation with basketball fans demonstrates Sportify's capability to deepen\ntactical insights and amplify the viewing experience. Furthermore, third-person\nnarration assists people in getting in-depth game explanations while\nfirst-person narration enhances fans' game engagement"
                },
                "authors": [
                    {
                        "name": "Chunggi Lee"
                    },
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Chen Zhu-Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhu-Tian"
                },
                "author": "Chen Zhu-Tian",
                "arxiv_comment": "14 pages, 8 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10306v3",
                "updated": "2024-08-09T15:10:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    15,
                    10,
                    35,
                    4,
                    222,
                    0
                ],
                "published": "2024-01-18T13:51:48Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    13,
                    51,
                    48,
                    3,
                    18,
                    0
                ],
                "title": "Physics-constrained convolutional neural networks for inverse problems\n  in spatiotemporal partial differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-constrained convolutional neural networks for inverse problems\n  in spatiotemporal partial differential equations"
                },
                "summary": "We propose a physics-constrained convolutional neural network (PC-CNN) to\nsolve two types of inverse problems in partial differential equations (PDEs),\nwhich are nonlinear and vary both in space and time. In the first inverse\nproblem, we are given data that is offset by spatially varying systematic error\n(i.e., the bias, also known as the epistemic uncertainty). The task is to\nuncover the true state, which is the solution of the PDE, from the biased data.\nIn the second inverse problem, we are given sparse information on the solution\nof a PDE. The task is to reconstruct the solution in space with\nhigh-resolution. First, we present the PC-CNN, which constrains the PDE with a\ntime-windowing scheme to handle sequential data. Second, we analyse the\nperformance of the PC-CNN for uncovering solutions from biased data. We analyse\nboth linear and nonlinear convection-diffusion equations, and the Navier-Stokes\nequations, which govern the spatiotemporally chaotic dynamics of turbulent\nflows. We find that the PC-CNN correctly recovers the true solution for a\nvariety of biases, which are parameterised as non-convex functions. Third, we\nanalyse the performance of the PC-CNN for reconstructing solutions from sparse\ninformation for the turbulent flow. We reconstruct the spatiotemporal chaotic\nsolution on a high-resolution grid from only < 1\\% of the information contained\nin it. For both tasks, we further analyse the Navier-Stokes solutions. We find\nthat the inferred solutions have a physical spectral energy content, whereas\ntraditional methods, such as interpolation, do not. This work opens\nopportunities for solving inverse problems with partial differential equations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a physics-constrained convolutional neural network (PC-CNN) to\nsolve two types of inverse problems in partial differential equations (PDEs),\nwhich are nonlinear and vary both in space and time. In the first inverse\nproblem, we are given data that is offset by spatially varying systematic error\n(i.e., the bias, also known as the epistemic uncertainty). The task is to\nuncover the true state, which is the solution of the PDE, from the biased data.\nIn the second inverse problem, we are given sparse information on the solution\nof a PDE. The task is to reconstruct the solution in space with\nhigh-resolution. First, we present the PC-CNN, which constrains the PDE with a\ntime-windowing scheme to handle sequential data. Second, we analyse the\nperformance of the PC-CNN for uncovering solutions from biased data. We analyse\nboth linear and nonlinear convection-diffusion equations, and the Navier-Stokes\nequations, which govern the spatiotemporally chaotic dynamics of turbulent\nflows. We find that the PC-CNN correctly recovers the true solution for a\nvariety of biases, which are parameterised as non-convex functions. Third, we\nanalyse the performance of the PC-CNN for reconstructing solutions from sparse\ninformation for the turbulent flow. We reconstruct the spatiotemporal chaotic\nsolution on a high-resolution grid from only < 1\\% of the information contained\nin it. For both tasks, we further analyse the Navier-Stokes solutions. We find\nthat the inferred solutions have a physical spectral energy content, whereas\ntraditional methods, such as interpolation, do not. This work opens\nopportunities for solving inverse problems with partial differential equations."
                },
                "authors": [
                    {
                        "name": "Daniel Kelshaw"
                    },
                    {
                        "name": "Luca Magri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Magri"
                },
                "author": "Luca Magri",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2306.04600,\n  arXiv:2306.10990",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05365v2",
                "updated": "2024-08-09T14:59:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    59,
                    43,
                    4,
                    222,
                    0
                ],
                "published": "2023-12-08T20:55:18Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    20,
                    55,
                    18,
                    4,
                    342,
                    0
                ],
                "title": "Product Centered Dirichlet Processes for Dependent Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product Centered Dirichlet Processes for Dependent Clustering"
                },
                "summary": "While there is an immense literature on Bayesian methods for clustering, the\nmultiview case has received little attention. This problem focuses on obtaining\ndistinct but statistically dependent clusterings in a common set of entities\nfor different data types. For example, clustering patients into subgroups with\nsubgroup membership varying according to the domain of the patient variables. A\nchallenge is how to model the across-view dependence between the partitions of\npatients into subgroups. The complexities of the partition space make standard\nmethods to model dependence, such as correlation, infeasible. In this article,\nwe propose CLustering with Independence Centering (CLIC), a clustering prior\nthat uses a single parameter to explicitly model dependence between clusterings\nacross views. CLIC is induced by the product centered Dirichlet process (PCDP),\na novel hierarchical prior that bridges between independent and equivalent\npartitions. We show appealing theoretic properties, provide a finite\napproximation and prove its accuracy, present a marginal Gibbs sampler for\nposterior computation, and derive closed form expressions for the marginal and\njoint partition distributions for the CLIC model. On synthetic data and in an\napplication to epidemiology, CLIC accurately characterizes view-specific\npartitions while providing inference on the dependence level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there is an immense literature on Bayesian methods for clustering, the\nmultiview case has received little attention. This problem focuses on obtaining\ndistinct but statistically dependent clusterings in a common set of entities\nfor different data types. For example, clustering patients into subgroups with\nsubgroup membership varying according to the domain of the patient variables. A\nchallenge is how to model the across-view dependence between the partitions of\npatients into subgroups. The complexities of the partition space make standard\nmethods to model dependence, such as correlation, infeasible. In this article,\nwe propose CLustering with Independence Centering (CLIC), a clustering prior\nthat uses a single parameter to explicitly model dependence between clusterings\nacross views. CLIC is induced by the product centered Dirichlet process (PCDP),\na novel hierarchical prior that bridges between independent and equivalent\npartitions. We show appealing theoretic properties, provide a finite\napproximation and prove its accuracy, present a marginal Gibbs sampler for\nposterior computation, and derive closed form expressions for the marginal and\njoint partition distributions for the CLIC model. On synthetic data and in an\napplication to epidemiology, CLIC accurately characterizes view-specific\npartitions while providing inference on the dependence level."
                },
                "authors": [
                    {
                        "name": "Alexander Dombowsky"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05109v1",
                "updated": "2024-08-09T14:59:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    59,
                    36,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:59:36Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    59,
                    36,
                    4,
                    222,
                    0
                ],
                "title": "A Survey of NL2SQL with Large Language Models: Where are we, and where\n  are we going?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of NL2SQL with Large Language Models: Where are we, and where\n  are we going?"
                },
                "summary": "Translating users' natural language queries (NL) into SQL queries (i.e.,\nNL2SQL) can significantly reduce barriers to accessing relational databases and\nsupport various commercial applications. The performance of NL2SQL has been\ngreatly enhanced with the emergence of Large Language Models (LLMs). In this\nsurvey, we provide a comprehensive review of NL2SQL techniques powered by LLMs,\ncovering its entire lifecycle from the following four aspects: (1) Model:\nNL2SQL translation techniques that tackle not only NL ambiguity and\nunder-specification, but also properly map NL with database schema and\ninstances; (2) Data: From the collection of training data, data synthesis due\nto training data scarcity, to NL2SQL benchmarks; (3) Evaluation: Evaluating\nNL2SQL methods from multiple angles using different metrics and granularities;\nand (4) Error Analysis: analyzing NL2SQL errors to find the root cause and\nguiding NL2SQL models to evolve. Moreover, we provide a rule of thumb for\ndeveloping NL2SQL solutions. Finally, we discuss the research challenges and\nopen problems of NL2SQL in the LLMs era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating users' natural language queries (NL) into SQL queries (i.e.,\nNL2SQL) can significantly reduce barriers to accessing relational databases and\nsupport various commercial applications. The performance of NL2SQL has been\ngreatly enhanced with the emergence of Large Language Models (LLMs). In this\nsurvey, we provide a comprehensive review of NL2SQL techniques powered by LLMs,\ncovering its entire lifecycle from the following four aspects: (1) Model:\nNL2SQL translation techniques that tackle not only NL ambiguity and\nunder-specification, but also properly map NL with database schema and\ninstances; (2) Data: From the collection of training data, data synthesis due\nto training data scarcity, to NL2SQL benchmarks; (3) Evaluation: Evaluating\nNL2SQL methods from multiple angles using different metrics and granularities;\nand (4) Error Analysis: analyzing NL2SQL errors to find the root cause and\nguiding NL2SQL models to evolve. Moreover, we provide a rule of thumb for\ndeveloping NL2SQL solutions. Finally, we discuss the research challenges and\nopen problems of NL2SQL in the LLMs era."
                },
                "authors": [
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Shuyu Shen"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Peixian Ma"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05107v1",
                "updated": "2024-08-09T14:58:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    58,
                    3,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:58:03Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    58,
                    3,
                    4,
                    222,
                    0
                ],
                "title": "Depth Helps: Improving Pre-trained RGB-based Policy with Depth\n  Information Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth Helps: Improving Pre-trained RGB-based Policy with Depth\n  Information Injection"
                },
                "summary": "3D perception ability is crucial for generalizable robotic manipulation.\nWhile recent foundation models have made significant strides in perception and\ndecision-making with RGB-based input, their lack of 3D perception limits their\neffectiveness in fine-grained robotic manipulation tasks. To address these\nlimitations, we propose a Depth Information Injection ($\\bold{DI}^{\\bold{2}}$)\nframework that leverages the RGB-Depth modality for policy fine-tuning, while\nrelying solely on RGB images for robust and efficient deployment. Concretely,\nwe introduce the Depth Completion Module (DCM) to extract the spatial prior\nknowledge related to depth information and generate virtual depth information\nfrom RGB inputs to aid policy deployment. Further, we propose the Depth-Aware\nCodebook (DAC) to eliminate noise and reduce the cumulative error from the\ndepth prediction. In the inference phase, this framework employs RGB inputs and\naccurately predicted depth data to generate the manipulation action. We conduct\nexperiments on simulated LIBERO environments and real-world scenarios, and the\nexperiment results prove that our method could effectively enhance the\npre-trained RGB-based policy with 3D perception ability for robotic\nmanipulation. The website is released at\nhttps://gewu-lab.github.io/DepthHelps-IROS2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D perception ability is crucial for generalizable robotic manipulation.\nWhile recent foundation models have made significant strides in perception and\ndecision-making with RGB-based input, their lack of 3D perception limits their\neffectiveness in fine-grained robotic manipulation tasks. To address these\nlimitations, we propose a Depth Information Injection ($\\bold{DI}^{\\bold{2}}$)\nframework that leverages the RGB-Depth modality for policy fine-tuning, while\nrelying solely on RGB images for robust and efficient deployment. Concretely,\nwe introduce the Depth Completion Module (DCM) to extract the spatial prior\nknowledge related to depth information and generate virtual depth information\nfrom RGB inputs to aid policy deployment. Further, we propose the Depth-Aware\nCodebook (DAC) to eliminate noise and reduce the cumulative error from the\ndepth prediction. In the inference phase, this framework employs RGB inputs and\naccurately predicted depth data to generate the manipulation action. We conduct\nexperiments on simulated LIBERO environments and real-world scenarios, and the\nexperiment results prove that our method could effectively enhance the\npre-trained RGB-based policy with 3D perception ability for robotic\nmanipulation. The website is released at\nhttps://gewu-lab.github.io/DepthHelps-IROS2024."
                },
                "authors": [
                    {
                        "name": "Xincheng Pang"
                    },
                    {
                        "name": "Wenke Xia"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Di Hu"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "accepted by IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05106v1",
                "updated": "2024-08-09T14:57:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    57,
                    47,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:57:47Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    57,
                    47,
                    4,
                    222,
                    0
                ],
                "title": "Spatial Deconfounding is Reasonable Statistical Practice:\n  Interpretations, Clarifications, and New Benefits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Deconfounding is Reasonable Statistical Practice:\n  Interpretations, Clarifications, and New Benefits"
                },
                "summary": "The spatial linear mixed model (SLMM) consists of fixed and spatial random\neffects that can be confounded. Restricted spatial regression (RSR) models\nrestrict the spatial random effects to be in the orthogonal column space of the\ncovariates, which \"deconfounds\" the SLMM. Recent articles have shown that the\nRSR generally performs worse than the SLMM under a certain interpretation of\nthe RSR. We show that every additive model can be reparameterized as a\ndeconfounded model leading to what we call the linear reparameterization of\nadditive models (LRAM). Under this reparameterization the coefficients of the\ncovariates (referred to as deconfounded regression effects) are different from\nthe (confounded) regression effects in the SLMM. It is shown that under the\nLRAM interpretation, existing deconfounded spatial models produce estimated\ndeconfounded regression effects, spatial prediction, and spatial prediction\nvariances equivalent to that of SLMM in Bayesian contexts. Furthermore, a\ngeneral RSR (GRSR) and the SLMM produce identical inferences on confounded\nregression effects. While our results are in complete agreement with recent\ncriticisms, our new results under the LRAM interpretation provide\nclarifications that lead to different and sometimes contrary conclusions.\nAdditionally, we discuss the inferential and computational benefits to\ndeconfounding, which we illustrate via a simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spatial linear mixed model (SLMM) consists of fixed and spatial random\neffects that can be confounded. Restricted spatial regression (RSR) models\nrestrict the spatial random effects to be in the orthogonal column space of the\ncovariates, which \"deconfounds\" the SLMM. Recent articles have shown that the\nRSR generally performs worse than the SLMM under a certain interpretation of\nthe RSR. We show that every additive model can be reparameterized as a\ndeconfounded model leading to what we call the linear reparameterization of\nadditive models (LRAM). Under this reparameterization the coefficients of the\ncovariates (referred to as deconfounded regression effects) are different from\nthe (confounded) regression effects in the SLMM. It is shown that under the\nLRAM interpretation, existing deconfounded spatial models produce estimated\ndeconfounded regression effects, spatial prediction, and spatial prediction\nvariances equivalent to that of SLMM in Bayesian contexts. Furthermore, a\ngeneral RSR (GRSR) and the SLMM produce identical inferences on confounded\nregression effects. While our results are in complete agreement with recent\ncriticisms, our new results under the LRAM interpretation provide\nclarifications that lead to different and sometimes contrary conclusions.\nAdditionally, we discuss the inferential and computational benefits to\ndeconfounding, which we illustrate via a simulation."
                },
                "authors": [
                    {
                        "name": "Jonathan R. Bradley"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Bradley"
                },
                "author": "Jonathan R. Bradley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03301v2",
                "updated": "2024-08-09T14:50:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    50,
                    37,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-05T14:11:30Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    14,
                    11,
                    30,
                    2,
                    157,
                    0
                ],
                "title": "Effects of Mosaic Crystal Instrument Functions on X-ray Thomson\n  Scattering Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of Mosaic Crystal Instrument Functions on X-ray Thomson\n  Scattering Diagnostics"
                },
                "summary": "Mosaic crystals, with their high integrated reflectivities, are\nwidely-employed in spectrometers used to diagnose high energy density systems.\nX-ray Thomson scattering (XRTS) has emerged as a powerful diagnostic tool of\nthese systems, providing in principle direct access to important properties\nsuch as the temperature via detailed balance. However, the measured XRTS\nspectrum is broadened by the spectrometer instrument function (IF), and without\ncareful consideration of the IF one risks misdiagnosing system conditions.\nHere, we consider in detail the IF of 40 $\\mu$m and 100 $\\mu$m mosaic HAPG\ncrystals, and how the broadening varies across the spectrometer in an energy\nrange of 6.7-8.6 keV. Notably, we find a strong asymmetry in the shape of the\nIF towards higher energies. As an example, we consider the effect of the\nasymmetry in the IF on the temperature inferred via XRTS for simulated 80 eV CH\nplasmas, and find that the temperature can be overestimated if an approximate\nsymmetric IF is used. We therefore expect a detailed consideration of the full\nIF will have an important impact on system properties inferred via XRTS in both\nforward modelling and model-free approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mosaic crystals, with their high integrated reflectivities, are\nwidely-employed in spectrometers used to diagnose high energy density systems.\nX-ray Thomson scattering (XRTS) has emerged as a powerful diagnostic tool of\nthese systems, providing in principle direct access to important properties\nsuch as the temperature via detailed balance. However, the measured XRTS\nspectrum is broadened by the spectrometer instrument function (IF), and without\ncareful consideration of the IF one risks misdiagnosing system conditions.\nHere, we consider in detail the IF of 40 $\\mu$m and 100 $\\mu$m mosaic HAPG\ncrystals, and how the broadening varies across the spectrometer in an energy\nrange of 6.7-8.6 keV. Notably, we find a strong asymmetry in the shape of the\nIF towards higher energies. As an example, we consider the effect of the\nasymmetry in the IF on the temperature inferred via XRTS for simulated 80 eV CH\nplasmas, and find that the temperature can be overestimated if an approximate\nsymmetric IF is used. We therefore expect a detailed consideration of the full\nIF will have an important impact on system properties inferred via XRTS in both\nforward modelling and model-free approaches."
                },
                "authors": [
                    {
                        "name": "Thomas Gawne"
                    },
                    {
                        "name": "Hannah Bellenbaum"
                    },
                    {
                        "name": "Luke B. Fletcher"
                    },
                    {
                        "name": "Karen Appel"
                    },
                    {
                        "name": "Carsten Baehtz"
                    },
                    {
                        "name": "Victorien Bouffetier"
                    },
                    {
                        "name": "Erik Brambrink"
                    },
                    {
                        "name": "Danielle Brown"
                    },
                    {
                        "name": "Attila Cangi"
                    },
                    {
                        "name": "Adrien Descamps"
                    },
                    {
                        "name": "Sebastian Göde"
                    },
                    {
                        "name": "Nicholas J. Hartley"
                    },
                    {
                        "name": "Marie-Luise Herbert"
                    },
                    {
                        "name": "Philipp Hesselbach"
                    },
                    {
                        "name": "Hauke Höppner"
                    },
                    {
                        "name": "Oliver S. Humphries"
                    },
                    {
                        "name": "Zuzana Konôpková"
                    },
                    {
                        "name": "Alejandro Laso Garcia"
                    },
                    {
                        "name": "Björn Lindqvist"
                    },
                    {
                        "name": "Julian Lütgert"
                    },
                    {
                        "name": "Michael J. MacDonald"
                    },
                    {
                        "name": "Mikako Makita"
                    },
                    {
                        "name": "Willow Martin"
                    },
                    {
                        "name": "Mikhail Mishchenko"
                    },
                    {
                        "name": "Zhandos A. Moldabekov"
                    },
                    {
                        "name": "Motoaki Nakatsutsumi"
                    },
                    {
                        "name": "Jean-Paul Naedler"
                    },
                    {
                        "name": "Paul Neumayer"
                    },
                    {
                        "name": "Alexander Pelka"
                    },
                    {
                        "name": "Chongbing Qu"
                    },
                    {
                        "name": "Lisa Randolph"
                    },
                    {
                        "name": "Johannes Rips"
                    },
                    {
                        "name": "Toma Toncian"
                    },
                    {
                        "name": "Jan Vorberger"
                    },
                    {
                        "name": "Lennart Wollenweber"
                    },
                    {
                        "name": "Ulf Zastrau"
                    },
                    {
                        "name": "Dominik Kraus"
                    },
                    {
                        "name": "Thomas R. Preston"
                    },
                    {
                        "name": "Tobias Dornheim"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Dornheim"
                },
                "author": "Tobias Dornheim",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05102v1",
                "updated": "2024-08-09T14:45:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    45,
                    22,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:45:22Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    45,
                    22,
                    4,
                    222,
                    0
                ],
                "title": "How Well Do LLMs Identify Cultural Unity in Diversity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do LLMs Identify Cultural Unity in Diversity?"
                },
                "summary": "Much work on the cultural awareness of large language models (LLMs) focuses\non the models' sensitivity to geo-cultural diversity. However, in addition to\ncross-cultural differences, there also exists common ground across cultures.\nFor instance, a bridal veil in the United States plays a similar\ncultural-relevant role as a honggaitou in China. In this study, we introduce a\nbenchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the\ncultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation\nexamples building upon 285 traditional cultural-specific concepts across 10\ncountries. Based on a systematic manual annotation of cultural-relevant\nfeatures per concept, we calculate the cultural association between any pair of\ncross-cultural concepts. Built upon this dataset, we design a contrastive\nmatching task to evaluate the LLMs' capability to identify highly associated\ncross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular\nprompting strategies, under the settings of either giving all extracted concept\nfeatures or no features at all on CUNIT Interestingly, we find that cultural\nassociations across countries regarding clothing concepts largely differ from\nfood. Our analysis shows that LLMs are still limited to capturing\ncross-cultural associations between concepts compared to humans. Moreover,\ngeo-cultural proximity shows a weak influence on model performance in capturing\ncross-cultural associations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much work on the cultural awareness of large language models (LLMs) focuses\non the models' sensitivity to geo-cultural diversity. However, in addition to\ncross-cultural differences, there also exists common ground across cultures.\nFor instance, a bridal veil in the United States plays a similar\ncultural-relevant role as a honggaitou in China. In this study, we introduce a\nbenchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the\ncultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation\nexamples building upon 285 traditional cultural-specific concepts across 10\ncountries. Based on a systematic manual annotation of cultural-relevant\nfeatures per concept, we calculate the cultural association between any pair of\ncross-cultural concepts. Built upon this dataset, we design a contrastive\nmatching task to evaluate the LLMs' capability to identify highly associated\ncross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular\nprompting strategies, under the settings of either giving all extracted concept\nfeatures or no features at all on CUNIT Interestingly, we find that cultural\nassociations across countries regarding clothing concepts largely differ from\nfood. Our analysis shows that LLMs are still limited to capturing\ncross-cultural associations between concepts compared to humans. Moreover,\ngeo-cultural proximity shows a weak influence on model performance in capturing\ncross-cultural associations."
                },
                "authors": [
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Junli Wang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Ming Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jiang"
                },
                "author": "Ming Jiang",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05101v1",
                "updated": "2024-08-09T14:43:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    43,
                    56,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    43,
                    56,
                    4,
                    222,
                    0
                ],
                "title": "MooER: LLM-based Speech Recognition and Translation Models from Moore\n  Threads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MooER: LLM-based Speech Recognition and Translation Models from Moore\n  Threads"
                },
                "summary": "In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on."
                },
                "authors": [
                    {
                        "name": "Junhao Xu"
                    },
                    {
                        "name": "Zhenlin Liang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yichao Hu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yajun Zheng"
                    },
                    {
                        "name": "Meng Cai"
                    },
                    {
                        "name": "Hua Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wang"
                },
                "author": "Hua Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05093v1",
                "updated": "2024-08-09T14:34:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    34,
                    32,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:34:32Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    34,
                    32,
                    4,
                    222,
                    0
                ],
                "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models"
                },
                "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."
                },
                "authors": [
                    {
                        "name": "Zikai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zikai Xie"
                },
                "author": "Zikai Xie",
                "arxiv_comment": "7 pages, submitted to AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19793v2",
                "updated": "2024-08-09T14:18:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    18,
                    23,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-30T08:01:20Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    1,
                    20,
                    3,
                    151,
                    0
                ],
                "title": "PDDLEGO: Iterative Planning in Textual Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDDLEGO: Iterative Planning in Textual Environments"
                },
                "summary": "Planning in textual environments have been shown to be a long-standing\nchallenge even for current models. A recent, promising line of work uses LLMs\nto generate a formal representation of the environment that can be solved by a\nsymbolic planner. However, existing methods rely on a fully-observed\nenvironment where all entity states are initially known, so a one-off\nrepresentation can be constructed, leading to a complete plan. In contrast, we\ntackle partially-observed environments where there is initially no sufficient\ninformation to plan for the end-goal. We propose PDDLEGO that iteratively\nconstruct a planning representation that can lead to a partial plan for a given\nsub-goal. By accomplishing the sub-goal, more information is acquired to\naugment the representation, eventually achieving the end-goal. We show that\nplans produced by few-shot PDDLEGO are 43% more efficient than generating plans\nend-to-end on the Coin Collector simulation, with strong performance (98%) on\nthe more complex Cooking World simulation where end-to-end LLMs fail to\ngenerate coherent plans (4%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning in textual environments have been shown to be a long-standing\nchallenge even for current models. A recent, promising line of work uses LLMs\nto generate a formal representation of the environment that can be solved by a\nsymbolic planner. However, existing methods rely on a fully-observed\nenvironment where all entity states are initially known, so a one-off\nrepresentation can be constructed, leading to a complete plan. In contrast, we\ntackle partially-observed environments where there is initially no sufficient\ninformation to plan for the end-goal. We propose PDDLEGO that iteratively\nconstruct a planning representation that can lead to a partial plan for a given\nsub-goal. By accomplishing the sub-goal, more information is acquired to\naugment the representation, eventually achieving the end-goal. We show that\nplans produced by few-shot PDDLEGO are 43% more efficient than generating plans\nend-to-end on the Coin Collector simulation, with strong performance (98%) on\nthe more complex Cooking World simulation where end-to-end LLMs fail to\ngenerate coherent plans (4%)."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Peter Jansen"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Niket Tandon"
                    }
                ],
                "author_detail": {
                    "name": "Niket Tandon"
                },
                "author": "Niket Tandon",
                "arxiv_comment": "In *SEM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05085v1",
                "updated": "2024-08-09T14:16:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    16,
                    21,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:16:21Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    16,
                    21,
                    4,
                    222,
                    0
                ],
                "title": "On expected signatures and signature cumulants in semimartingale models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On expected signatures and signature cumulants in semimartingale models"
                },
                "summary": "The concept of signatures and expected signatures is vital in data science,\nespecially for sequential data analysis. The signature transform, a Cartan type\ndevelopment, translates paths into high-dimensional feature vectors, capturing\ntheir intrinsic characteristics. Under natural conditions, the expectation of\nthe signature determines the law of the signature, providing a statistical\nsummary of the data distribution. This property facilitates robust modeling and\ninference in machine learning and stochastic processes. Building on previous\nwork by the present authors [Unified signature cumulants and generalized Magnus\nexpansions, FoM Sigma '22] we here revisit the actual computation of expected\nsignatures, in a general semimartingale setting. Several new formulae are\ngiven. A log-transform of (expected) signatures leads to log-signatures\n(signature cumulants), offering a significant reduction in complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of signatures and expected signatures is vital in data science,\nespecially for sequential data analysis. The signature transform, a Cartan type\ndevelopment, translates paths into high-dimensional feature vectors, capturing\ntheir intrinsic characteristics. Under natural conditions, the expectation of\nthe signature determines the law of the signature, providing a statistical\nsummary of the data distribution. This property facilitates robust modeling and\ninference in machine learning and stochastic processes. Building on previous\nwork by the present authors [Unified signature cumulants and generalized Magnus\nexpansions, FoM Sigma '22] we here revisit the actual computation of expected\nsignatures, in a general semimartingale setting. Several new formulae are\ngiven. A log-transform of (expected) signatures leads to log-signatures\n(signature cumulants), offering a significant reduction in complexity."
                },
                "authors": [
                    {
                        "name": "Peter K. Friz"
                    },
                    {
                        "name": "Paul P. Hager"
                    },
                    {
                        "name": "Nikolas Tapia"
                    }
                ],
                "author_detail": {
                    "name": "Nikolas Tapia"
                },
                "author": "Nikolas Tapia",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2102.03345",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60L10, 60L90, 60E10, 60G44, 60G48, 60G51, 60J76",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02993v2",
                "updated": "2024-08-09T14:12:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    12,
                    49,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-06T06:59:15Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    6,
                    59,
                    15,
                    1,
                    219,
                    0
                ],
                "title": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model"
                },
                "summary": "Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents. In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models. In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM."
                },
                "authors": [
                    {
                        "name": "Yiming Zhong"
                    },
                    {
                        "name": "Xiaolin Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yunchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Yunchao Wei"
                },
                "author": "Yunchao Wei",
                "arxiv_comment": "15 pages, 9 figures, ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v1",
                "updated": "2024-08-09T14:02:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08919v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08919v3",
                "updated": "2024-08-09T13:49:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    49,
                    20,
                    4,
                    222,
                    0
                ],
                "published": "2024-01-17T02:04:59Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    2,
                    4,
                    59,
                    2,
                    17,
                    0
                ],
                "title": "A Context-Contrastive Inference Approach To Partial Diacritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Context-Contrastive Inference Approach To Partial Diacritization"
                },
                "summary": "Diacritization plays a pivotal role in improving readability and\ndisambiguating the meaning of Arabic texts. Efforts have so far focused on\nmarking every eligible character (Full Diacritization). Comparatively\noverlooked, Partial Diacritzation (PD) is the selection of a subset of\ncharacters to be marked to aid comprehension where needed. Research has\nindicated that excessive diacritic marks can hinder skilled readers -- reducing\nreading speed and accuracy. We conduct a behavioral experiment and show that\npartially marked text is often easier to read than fully marked text, and\nsometimes easier than plain text. In this light, we introduce\nContext-Contrastive Partial Diacritization (CCPD) -- a novel approach to PD\nwhich integrates seamlessly with existing Arabic diacritization systems. CCPD\nprocesses each word twice, once with context and once without, and diacritizes\nonly the characters with disparities between the two inferences. Further, we\nintroduce novel indicators for measuring partial diacritization quality,\nessential for establishing this as a machine learning task. Lastly, we\nintroduce TD2, a Transformer-variant of an established model which offers a\nmarkedly different performance profile on our proposed indicators compared to\nall other known systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diacritization plays a pivotal role in improving readability and\ndisambiguating the meaning of Arabic texts. Efforts have so far focused on\nmarking every eligible character (Full Diacritization). Comparatively\noverlooked, Partial Diacritzation (PD) is the selection of a subset of\ncharacters to be marked to aid comprehension where needed. Research has\nindicated that excessive diacritic marks can hinder skilled readers -- reducing\nreading speed and accuracy. We conduct a behavioral experiment and show that\npartially marked text is often easier to read than fully marked text, and\nsometimes easier than plain text. In this light, we introduce\nContext-Contrastive Partial Diacritization (CCPD) -- a novel approach to PD\nwhich integrates seamlessly with existing Arabic diacritization systems. CCPD\nprocesses each word twice, once with context and once without, and diacritizes\nonly the characters with disparities between the two inferences. Further, we\nintroduce novel indicators for measuring partial diacritization quality,\nessential for establishing this as a machine learning task. Lastly, we\nintroduce TD2, a Transformer-variant of an established model which offers a\nmarkedly different performance profile on our proposed indicators compared to\nall other known systems."
                },
                "authors": [
                    {
                        "name": "Muhammad ElNokrashy"
                    },
                    {
                        "name": "Badr AlKhamissi"
                    }
                ],
                "author_detail": {
                    "name": "Badr AlKhamissi"
                },
                "author": "Badr AlKhamissi",
                "arxiv_comment": "14 equations, 5 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08919v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08919v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05065v1",
                "updated": "2024-08-09T13:46:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    46,
                    28,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T13:46:28Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    46,
                    28,
                    4,
                    222,
                    0
                ],
                "title": "Masked adversarial neural network for cell type deconvolution in spatial\n  transcriptomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked adversarial neural network for cell type deconvolution in spatial\n  transcriptomics"
                },
                "summary": "Accurately determining cell type composition in disease-relevant tissues is\ncrucial for identifying disease targets. Most existing spatial transcriptomics\n(ST) technologies cannot achieve single-cell resolution, making it challenging\nto accurately determine cell types. To address this issue, various\ndeconvolution methods have been developed. Most of these methods use\nsingle-cell RNA sequencing (scRNA-seq) data from the same tissue as a reference\nto infer cell types in ST data spots. However, they often overlook the\ndifferences between scRNA-seq and ST data. To overcome this limitation, we\npropose a Masked Adversarial Neural Network (MACD). MACD employs adversarial\nlearning to align real ST data with simulated ST data generated from scRNA-seq\ndata. By mapping them into a unified latent space, it can minimize the\ndifferences between the two types of data. Additionally, MACD uses masking\ntechniques to effectively learn the features of real ST data and mitigate\nnoise. We evaluated MACD on 32 simulated datasets and 2 real datasets,\ndemonstrating its accuracy in performing cell type deconvolution. All code and\npublic datasets used in this paper are available at\nhttps://github.com/wenwenmin/MACD and https://zenodo.org/records/12804822.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately determining cell type composition in disease-relevant tissues is\ncrucial for identifying disease targets. Most existing spatial transcriptomics\n(ST) technologies cannot achieve single-cell resolution, making it challenging\nto accurately determine cell types. To address this issue, various\ndeconvolution methods have been developed. Most of these methods use\nsingle-cell RNA sequencing (scRNA-seq) data from the same tissue as a reference\nto infer cell types in ST data spots. However, they often overlook the\ndifferences between scRNA-seq and ST data. To overcome this limitation, we\npropose a Masked Adversarial Neural Network (MACD). MACD employs adversarial\nlearning to align real ST data with simulated ST data generated from scRNA-seq\ndata. By mapping them into a unified latent space, it can minimize the\ndifferences between the two types of data. Additionally, MACD uses masking\ntechniques to effectively learn the features of real ST data and mitigate\nnoise. We evaluated MACD on 32 simulated datasets and 2 real datasets,\ndemonstrating its accuracy in performing cell type deconvolution. All code and\npublic datasets used in this paper are available at\nhttps://github.com/wenwenmin/MACD and https://zenodo.org/records/12804822."
                },
                "authors": [
                    {
                        "name": "Lin Huang"
                    },
                    {
                        "name": "Xiaofei Liu"
                    },
                    {
                        "name": "Shunfang Wang"
                    },
                    {
                        "name": "Wenwen Min"
                    }
                ],
                "author_detail": {
                    "name": "Wenwen Min"
                },
                "author": "Wenwen Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06566v3",
                "updated": "2024-08-09T13:35:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    35,
                    43,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-03T07:44:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    7,
                    44,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin"
                },
                "summary": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis."
                },
                "authors": [
                    {
                        "name": "Carolina Fortuna"
                    },
                    {
                        "name": "Vid Hanžel"
                    },
                    {
                        "name": "Blaž Bertalanič"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Bertalanič"
                },
                "author": "Blaž Bertalanič",
                "arxiv_comment": "Accepted at IEEE SmartGridComm'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05061v1",
                "updated": "2024-08-09T13:32:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    32,
                    50,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T13:32:50Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    32,
                    50,
                    4,
                    222,
                    0
                ],
                "title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered\n  Applications are Vulnerable to PromptWares",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered\n  Applications are Vulnerable to PromptWares"
                },
                "summary": "In this paper we argue that a jailbroken GenAI model can cause substantial\nharm to GenAI-powered applications and facilitate PromptWare, a new type of\nattack that flips the GenAI model's behavior from serving an application to\nattacking it. PromptWare exploits user inputs to jailbreak a GenAI model to\nforce/perform malicious activity within the context of a GenAI-powered\napplication. First, we introduce a naive implementation of PromptWare that\nbehaves as malware that targets Plan & Execute architectures (a.k.a., ReAct,\nfunction calling). We show that attackers could force a desired execution flow\nby creating a user input that produces desired outputs given that the logic of\nthe GenAI-powered application is known to attackers. We demonstrate the\napplication of a DoS attack that triggers the execution of a GenAI-powered\nassistant to enter an infinite loop that wastes money and computational\nresources on redundant API calls to a GenAI engine, preventing the application\nfrom providing service to a user. Next, we introduce a more sophisticated\nimplementation of PromptWare that we name Advanced PromptWare Threat (APwT)\nthat targets GenAI-powered applications whose logic is unknown to attackers. We\nshow that attackers could create user input that exploits the GenAI engine's\nadvanced AI capabilities to launch a kill chain in inference time consisting of\nsix steps intended to escalate privileges, analyze the application's context,\nidentify valuable assets, reason possible malicious activities, decide on one\nof them, and execute it. We demonstrate the application of APwT against a\nGenAI-powered e-commerce chatbot and show that it can trigger the modification\nof SQL tables, potentially leading to unauthorized discounts on the items sold\nto the user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we argue that a jailbroken GenAI model can cause substantial\nharm to GenAI-powered applications and facilitate PromptWare, a new type of\nattack that flips the GenAI model's behavior from serving an application to\nattacking it. PromptWare exploits user inputs to jailbreak a GenAI model to\nforce/perform malicious activity within the context of a GenAI-powered\napplication. First, we introduce a naive implementation of PromptWare that\nbehaves as malware that targets Plan & Execute architectures (a.k.a., ReAct,\nfunction calling). We show that attackers could force a desired execution flow\nby creating a user input that produces desired outputs given that the logic of\nthe GenAI-powered application is known to attackers. We demonstrate the\napplication of a DoS attack that triggers the execution of a GenAI-powered\nassistant to enter an infinite loop that wastes money and computational\nresources on redundant API calls to a GenAI engine, preventing the application\nfrom providing service to a user. Next, we introduce a more sophisticated\nimplementation of PromptWare that we name Advanced PromptWare Threat (APwT)\nthat targets GenAI-powered applications whose logic is unknown to attackers. We\nshow that attackers could create user input that exploits the GenAI engine's\nadvanced AI capabilities to launch a kill chain in inference time consisting of\nsix steps intended to escalate privileges, analyze the application's context,\nidentify valuable assets, reason possible malicious activities, decide on one\nof them, and execute it. We demonstrate the application of APwT against a\nGenAI-powered e-commerce chatbot and show that it can trigger the modification\nof SQL tables, potentially leading to unauthorized discounts on the items sold\nto the user."
                },
                "authors": [
                    {
                        "name": "Stav Cohen"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Ben Nassi"
                    }
                ],
                "author_detail": {
                    "name": "Ben Nassi"
                },
                "author": "Ben Nassi",
                "arxiv_comment": "Website, see https://sites.google.com/view/promptware",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05058v1",
                "updated": "2024-08-09T13:29:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    29,
                    8,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T13:29:08Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    13,
                    29,
                    8,
                    4,
                    222,
                    0
                ],
                "title": "Variational Bayesian Phylogenetic Inference with Semi-implicit Branch\n  Length Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Phylogenetic Inference with Semi-implicit Branch\n  Length Distributions"
                },
                "summary": "Reconstructing the evolutionary history relating a collection of molecular\nsequences is the main subject of modern Bayesian phylogenetic inference.\nHowever, the commonly used Markov chain Monte Carlo methods can be inefficient\ndue to the complicated space of phylogenetic trees, especially when the number\nof sequences is large. An alternative approach is variational Bayesian\nphylogenetic inference (VBPI) which transforms the inference problem into an\noptimization problem. While effective, the default diagonal lognormal\napproximation for the branch lengths of the tree used in VBPI is often\ninsufficient to capture the complexity of the exact posterior. In this work, we\npropose a more flexible family of branch length variational posteriors based on\nsemi-implicit hierarchical distributions using graph neural networks. We show\nthat this semi-implicit construction emits straightforward permutation\nequivariant distributions, and therefore can handle the non-Euclidean branch\nlength space across different tree topologies with ease. To deal with the\nintractable marginal probability of semi-implicit variational distributions, we\ndevelop several alternative lower bounds for stochastic optimization. We\ndemonstrate the effectiveness of our proposed method over baseline methods on\nbenchmark data examples, in terms of both marginal likelihood estimation and\nbranch length posterior approximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the evolutionary history relating a collection of molecular\nsequences is the main subject of modern Bayesian phylogenetic inference.\nHowever, the commonly used Markov chain Monte Carlo methods can be inefficient\ndue to the complicated space of phylogenetic trees, especially when the number\nof sequences is large. An alternative approach is variational Bayesian\nphylogenetic inference (VBPI) which transforms the inference problem into an\noptimization problem. While effective, the default diagonal lognormal\napproximation for the branch lengths of the tree used in VBPI is often\ninsufficient to capture the complexity of the exact posterior. In this work, we\npropose a more flexible family of branch length variational posteriors based on\nsemi-implicit hierarchical distributions using graph neural networks. We show\nthat this semi-implicit construction emits straightforward permutation\nequivariant distributions, and therefore can handle the non-Euclidean branch\nlength space across different tree topologies with ease. To deal with the\nintractable marginal probability of semi-implicit variational distributions, we\ndevelop several alternative lower bounds for stochastic optimization. We\ndemonstrate the effectiveness of our proposed method over baseline methods on\nbenchmark data examples, in terms of both marginal likelihood estimation and\nbranch length posterior approximation."
                },
                "authors": [
                    {
                        "name": "Tianyu Xie"
                    },
                    {
                        "name": "Frederick A. Matsen IV"
                    },
                    {
                        "name": "Marc A. Suchard"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.13220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.13220v3",
                "updated": "2024-08-09T12:57:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    57,
                    51,
                    4,
                    222,
                    0
                ],
                "published": "2022-11-23T18:58:33Z",
                "published_parsed": [
                    2022,
                    11,
                    23,
                    18,
                    58,
                    33,
                    2,
                    327,
                    0
                ],
                "title": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TetraDiffusion: Tetrahedral Diffusion Models for 3D Shape Generation"
                },
                "summary": "Probabilistic denoising diffusion models (DDMs) have set a new standard for\n2D image generation. Extending DDMs for 3D content creation is an active field\nof research. Here, we propose TetraDiffusion, a diffusion model that operates\non a tetrahedral partitioning of 3D space to enable efficient, high-resolution\n3D shape generation. Our model introduces operators for convolution and\ntranspose convolution that act directly on the tetrahedral partition, and\nseamlessly includes additional attributes such as color. Remarkably,\nTetraDiffusion enables rapid sampling of detailed 3D objects in nearly\nreal-time with unprecedented resolution. It's also adaptable for generating 3D\nshapes conditioned on 2D images. Compared to existing 3D mesh diffusion\ntechniques, our method is up to 200 times faster in inference speed, works on\nstandard consumer hardware, and delivers superior results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic denoising diffusion models (DDMs) have set a new standard for\n2D image generation. Extending DDMs for 3D content creation is an active field\nof research. Here, we propose TetraDiffusion, a diffusion model that operates\non a tetrahedral partitioning of 3D space to enable efficient, high-resolution\n3D shape generation. Our model introduces operators for convolution and\ntranspose convolution that act directly on the tetrahedral partition, and\nseamlessly includes additional attributes such as color. Remarkably,\nTetraDiffusion enables rapid sampling of detailed 3D objects in nearly\nreal-time with unprecedented resolution. It's also adaptable for generating 3D\nshapes conditioned on 2D images. Compared to existing 3D mesh diffusion\ntechniques, our method is up to 200 times faster in inference speed, works on\nstandard consumer hardware, and delivers superior results."
                },
                "authors": [
                    {
                        "name": "Nikolai Kalischek"
                    },
                    {
                        "name": "Torben Peters"
                    },
                    {
                        "name": "Jan D. Wegner"
                    },
                    {
                        "name": "Konrad Schindler"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Schindler"
                },
                "author": "Konrad Schindler",
                "arxiv_comment": "This version introduces further improvements compared to v2. Project\n  page https://tetradiffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.13220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.13220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14840v2",
                "updated": "2024-08-09T12:53:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    53,
                    3,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-23T17:55:09Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    55,
                    9,
                    3,
                    144,
                    0
                ],
                "title": "Differentiable Annealed Importance Sampling Minimizes The Symmetrized\n  Kullback-Leibler Divergence Between Initial and Target Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Annealed Importance Sampling Minimizes The Symmetrized\n  Kullback-Leibler Divergence Between Initial and Target Distribution"
                },
                "summary": "Differentiable annealed importance sampling (DAIS), proposed by Geffner &\nDomke (2021) and Zhang et al. (2021), allows optimizing over the initial\ndistribution of AIS. In this paper, we show that, in the limit of many\ntransitions, DAIS minimizes the symmetrized Kullback-Leibler divergence between\nthe initial and target distribution. Thus, DAIS can be seen as a form of\nvariational inference (VI) as its initial distribution is a parametric fit to\nan intractable target distribution. We empirically evaluate the usefulness of\nthe initial distribution as a variational distribution on synthetic and\nreal-world data, observing that it often provides more accurate uncertainty\nestimates than VI (optimizing the reverse KL divergence), importance weighted\nVI, and Markovian score climbing (optimizing the forward KL divergence).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable annealed importance sampling (DAIS), proposed by Geffner &\nDomke (2021) and Zhang et al. (2021), allows optimizing over the initial\ndistribution of AIS. In this paper, we show that, in the limit of many\ntransitions, DAIS minimizes the symmetrized Kullback-Leibler divergence between\nthe initial and target distribution. Thus, DAIS can be seen as a form of\nvariational inference (VI) as its initial distribution is a parametric fit to\nan intractable target distribution. We empirically evaluate the usefulness of\nthe initial distribution as a variational distribution on synthetic and\nreal-world data, observing that it often provides more accurate uncertainty\nestimates than VI (optimizing the reverse KL divergence), importance weighted\nVI, and Markovian score climbing (optimizing the forward KL divergence)."
                },
                "authors": [
                    {
                        "name": "Johannes Zenn"
                    },
                    {
                        "name": "Robert Bamler"
                    }
                ],
                "author_detail": {
                    "name": "Robert Bamler"
                },
                "author": "Robert Bamler",
                "arxiv_comment": "22 pages, including 9 pages of main text and 11 pages of appendix,\n  conference paper at ICML 2024, updated terminology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05035v1",
                "updated": "2024-08-09T12:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    47,
                    28,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T12:47:28Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    47,
                    28,
                    4,
                    222,
                    0
                ],
                "title": "Examining the Behavior of LLM Architectures Within the Framework of\n  Standardized National Exams in Brazil",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Behavior of LLM Architectures Within the Framework of\n  Standardized National Exams in Brazil"
                },
                "summary": "The Exame Nacional do Ensino M\\'edio (ENEM) is a pivotal test for Brazilian\nstudents, required for admission to a significant number of universities in\nBrazil. The test consists of four objective high-school level tests on Math,\nHumanities, Natural Sciences and Languages, and one writing essay. Students'\nanswers to the test and to the accompanying socioeconomic status questionnaire\nare made public every year (albeit anonymized) due to transparency policies\nfrom the Brazilian Government. In the context of large language models (LLMs),\nthese data lend themselves nicely to comparing different groups of humans with\nAI, as we can have access to human and machine answer distributions. We\nleverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,\nand MariTalk, a model trained using Portuguese data, to humans, aiming to\nascertain how their answers relate to real societal groups and what that may\nreveal about the model biases. We divide the human groups by using\nsocioeconomic status (SES), and compare their answer distribution with LLMs for\neach question and for the essay. We find no significant biases when comparing\nLLM performance to humans on the multiple-choice Brazilian Portuguese tests, as\nthe distance between model and human answers is mostly determined by the human\naccuracy. A similar conclusion is found by looking at the generated text as,\nwhen analyzing the essays, we observe that human and LLM essays differ in a few\nkey factors, one being the choice of words where model essays were easily\nseparable from human ones. The texts also differ syntactically, with LLM\ngenerated essays exhibiting, on average, smaller sentences and less thought\nunits, among other differences. These results suggest that, for Brazilian\nPortuguese in the ENEM context, LLM outputs represent no group of humans, being\nsignificantly different from the answers from Brazilian students across all\ntests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Exame Nacional do Ensino M\\'edio (ENEM) is a pivotal test for Brazilian\nstudents, required for admission to a significant number of universities in\nBrazil. The test consists of four objective high-school level tests on Math,\nHumanities, Natural Sciences and Languages, and one writing essay. Students'\nanswers to the test and to the accompanying socioeconomic status questionnaire\nare made public every year (albeit anonymized) due to transparency policies\nfrom the Brazilian Government. In the context of large language models (LLMs),\nthese data lend themselves nicely to comparing different groups of humans with\nAI, as we can have access to human and machine answer distributions. We\nleverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,\nand MariTalk, a model trained using Portuguese data, to humans, aiming to\nascertain how their answers relate to real societal groups and what that may\nreveal about the model biases. We divide the human groups by using\nsocioeconomic status (SES), and compare their answer distribution with LLMs for\neach question and for the essay. We find no significant biases when comparing\nLLM performance to humans on the multiple-choice Brazilian Portuguese tests, as\nthe distance between model and human answers is mostly determined by the human\naccuracy. A similar conclusion is found by looking at the generated text as,\nwhen analyzing the essays, we observe that human and LLM essays differ in a few\nkey factors, one being the choice of words where model essays were easily\nseparable from human ones. The texts also differ syntactically, with LLM\ngenerated essays exhibiting, on average, smaller sentences and less thought\nunits, among other differences. These results suggest that, for Brazilian\nPortuguese in the ENEM context, LLM outputs represent no group of humans, being\nsignificantly different from the answers from Brazilian students across all\ntests."
                },
                "authors": [
                    {
                        "name": "Marcelo Sartori Locatelli"
                    },
                    {
                        "name": "Matheus Prado Miranda"
                    },
                    {
                        "name": "Igor Joaquim da Silva Costa"
                    },
                    {
                        "name": "Matheus Torres Prates"
                    },
                    {
                        "name": "Victor Thomé"
                    },
                    {
                        "name": "Mateus Zaparoli Monteiro"
                    },
                    {
                        "name": "Tomas Lacerda"
                    },
                    {
                        "name": "Adriana Pagano"
                    },
                    {
                        "name": "Eduardo Rios Neto"
                    },
                    {
                        "name": "Wagner Meira Jr."
                    },
                    {
                        "name": "Virgilio Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Virgilio Almeida"
                },
                "author": "Virgilio Almeida",
                "arxiv_comment": "Accepted at the Seventh AAAI/ACM Conference on AI, Ethics and Society\n  (AIES 2024). 14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.12921v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.12921v4",
                "updated": "2024-08-09T12:46:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    46,
                    36,
                    4,
                    222,
                    0
                ],
                "published": "2022-12-25T15:40:05Z",
                "published_parsed": [
                    2022,
                    12,
                    25,
                    15,
                    40,
                    5,
                    6,
                    359,
                    0
                ],
                "title": "Learning k-Level Structured Sparse Neural Networks Using Group Envelope\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning k-Level Structured Sparse Neural Networks Using Group Envelope\n  Regularization"
                },
                "summary": "The extensive need for computational resources poses a significant obstacle\nto deploying large-scale Deep Neural Networks (DNN) on devices with constrained\nresources. At the same time, studies have demonstrated that a significant\nnumber of these DNN parameters are redundant and extraneous. In this paper, we\nintroduce a novel approach for learning structured sparse neural networks,\naimed at bridging the DNN hardware deployment challenges. We develop a novel\nregularization technique, termed Weighted Group Sparse Envelope Function\n(WGSEF), generalizing the Sparse Envelop Function (SEF), to select (or nullify)\nneuron groups, thereby reducing redundancy and enhancing computational\nefficiency. The method speeds up inference time and aims to reduce memory\ndemand and power consumption, thanks to its adaptability which lets any\nhardware specify group definitions, such as filters, channels, filter shapes,\nlayer depths, a single parameter (unstructured), etc. The properties of the\nWGSEF enable the pre-definition of a desired sparsity level to be achieved at\nthe training convergence. In the case of redundant parameters, this approach\nmaintains negligible network accuracy degradation or can even lead to\nimprovements in accuracy. Our method efficiently computes the WGSEF regularizer\nand its proximal operator, in a worst-case linear complexity relative to the\nnumber of group variables. Employing a proximal-gradient-based optimization\ntechnique, to train the model, it tackles the non-convex minimization problem\nincorporating the neural network loss and the WGSEF. Finally, we experiment and\nillustrate the efficiency of our proposed method in terms of the compression\nratio, accuracy, and inference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extensive need for computational resources poses a significant obstacle\nto deploying large-scale Deep Neural Networks (DNN) on devices with constrained\nresources. At the same time, studies have demonstrated that a significant\nnumber of these DNN parameters are redundant and extraneous. In this paper, we\nintroduce a novel approach for learning structured sparse neural networks,\naimed at bridging the DNN hardware deployment challenges. We develop a novel\nregularization technique, termed Weighted Group Sparse Envelope Function\n(WGSEF), generalizing the Sparse Envelop Function (SEF), to select (or nullify)\nneuron groups, thereby reducing redundancy and enhancing computational\nefficiency. The method speeds up inference time and aims to reduce memory\ndemand and power consumption, thanks to its adaptability which lets any\nhardware specify group definitions, such as filters, channels, filter shapes,\nlayer depths, a single parameter (unstructured), etc. The properties of the\nWGSEF enable the pre-definition of a desired sparsity level to be achieved at\nthe training convergence. In the case of redundant parameters, this approach\nmaintains negligible network accuracy degradation or can even lead to\nimprovements in accuracy. Our method efficiently computes the WGSEF regularizer\nand its proximal operator, in a worst-case linear complexity relative to the\nnumber of group variables. Employing a proximal-gradient-based optimization\ntechnique, to train the model, it tackles the non-convex minimization problem\nincorporating the neural network loss and the WGSEF. Finally, we experiment and\nillustrate the efficiency of our proposed method in terms of the compression\nratio, accuracy, and inference latency."
                },
                "authors": [
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Iftach Arbel"
                    },
                    {
                        "name": "Wasim Huleihel"
                    }
                ],
                "author_detail": {
                    "name": "Wasim Huleihel"
                },
                "author": "Wasim Huleihel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.12921v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.12921v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05026v1",
                "updated": "2024-08-09T12:26:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    26,
                    57,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T12:26:57Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    26,
                    57,
                    4,
                    222,
                    0
                ],
                "title": "Retrieval-augmented code completion for local projects using large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented code completion for local projects using large\n  language models"
                },
                "summary": "The use of large language models (LLMs) is becoming increasingly widespread\namong software developers. However, privacy and computational requirements are\nproblematic with commercial solutions and the use of LLMs. In this work, we\nfocus on using LLMs with around 160 million parameters that are suitable for\nlocal execution and augmentation with retrieval from local projects. We train\ntwo models based on the transformer architecture, the generative model GPT-2\nand the retrieval-adapted RETRO model, on open-source Python files, and\nempirically evaluate and compare them, confirming the benefits of vector\nembedding based retrieval. Further, we improve our models' performance with\nIn-context retrieval-augmented generation, which retrieves code snippets based\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\ngeneration on larger models and conclude that, despite its simplicity, the\napproach is more suitable than using the RETRO architecture. We highlight the\nkey role of proper tokenization in achieving the full potential of LLMs in code\ncompletion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) is becoming increasingly widespread\namong software developers. However, privacy and computational requirements are\nproblematic with commercial solutions and the use of LLMs. In this work, we\nfocus on using LLMs with around 160 million parameters that are suitable for\nlocal execution and augmentation with retrieval from local projects. We train\ntwo models based on the transformer architecture, the generative model GPT-2\nand the retrieval-adapted RETRO model, on open-source Python files, and\nempirically evaluate and compare them, confirming the benefits of vector\nembedding based retrieval. Further, we improve our models' performance with\nIn-context retrieval-augmented generation, which retrieves code snippets based\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\ngeneration on larger models and conclude that, despite its simplicity, the\napproach is more suitable than using the RETRO architecture. We highlight the\nkey role of proper tokenization in achieving the full potential of LLMs in code\ncompletion."
                },
                "authors": [
                    {
                        "name": "Marko Hostnik"
                    },
                    {
                        "name": "Marko Robnik-Šikonja"
                    }
                ],
                "author_detail": {
                    "name": "Marko Robnik-Šikonja"
                },
                "author": "Marko Robnik-Šikonja",
                "arxiv_comment": "28 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05025v2",
                "updated": "2024-08-12T13:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    57,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-09T12:26:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    26,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality."
                },
                "authors": [
                    {
                        "name": "Gianluca De Stefano"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Giancarlo Pellegrino"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Pellegrino"
                },
                "author": "Giancarlo Pellegrino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05024v1",
                "updated": "2024-08-09T12:25:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    25,
                    23,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T12:25:23Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    25,
                    23,
                    4,
                    222,
                    0
                ],
                "title": "MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling"
                },
                "summary": "Guitar tablatures enrich the structure of traditional music notation by\nassigning each note to a string and fret of a guitar in a particular tuning,\nindicating precisely where to play the note on the instrument. The problem of\ngenerating tablature from a symbolic music representation involves inferring\nthis string and fret assignment per note across an entire composition or\nperformance. On the guitar, multiple string-fret assignments are possible for\nmost pitches, which leads to a large combinatorial space that prevents\nexhaustive search approaches. Most modern methods use constraint-based dynamic\nprogramming to minimize some cost function (e.g.\\ hand position movement). In\nthis work, we introduce a novel deep learning solution to symbolic guitar\ntablature estimation. We train an encoder-decoder Transformer model in a masked\nlanguage modeling paradigm to assign notes to strings. The model is first\npre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on\na curated set of professionally transcribed guitar performances. Given the\nsubjective nature of assessing tablature quality, we conduct a user study\namongst guitarists, wherein we ask participants to rate the playability of\nmultiple versions of tablature for the same four-bar excerpt. The results\nindicate our system significantly outperforms competing algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guitar tablatures enrich the structure of traditional music notation by\nassigning each note to a string and fret of a guitar in a particular tuning,\nindicating precisely where to play the note on the instrument. The problem of\ngenerating tablature from a symbolic music representation involves inferring\nthis string and fret assignment per note across an entire composition or\nperformance. On the guitar, multiple string-fret assignments are possible for\nmost pitches, which leads to a large combinatorial space that prevents\nexhaustive search approaches. Most modern methods use constraint-based dynamic\nprogramming to minimize some cost function (e.g.\\ hand position movement). In\nthis work, we introduce a novel deep learning solution to symbolic guitar\ntablature estimation. We train an encoder-decoder Transformer model in a masked\nlanguage modeling paradigm to assign notes to strings. The model is first\npre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on\na curated set of professionally transcribed guitar performances. Given the\nsubjective nature of assessing tablature quality, we conduct a user study\namongst guitarists, wherein we ask participants to rate the playability of\nmultiple versions of tablature for the same four-bar excerpt. The results\nindicate our system significantly outperforms competing algorithms."
                },
                "authors": [
                    {
                        "name": "Drew Edwards"
                    },
                    {
                        "name": "Xavier Riley"
                    },
                    {
                        "name": "Pedro Sarmento"
                    },
                    {
                        "name": "Simon Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Simon Dixon"
                },
                "author": "Simon Dixon",
                "arxiv_comment": "Reviewed pre-print accepted for publication at ISMIR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03136v2",
                "updated": "2024-08-09T12:24:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    24,
                    45,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-06T12:23:58Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    12,
                    23,
                    58,
                    1,
                    219,
                    0
                ],
                "title": "A Dust-Scattering Model for M1-92: A Revised Estimate of the Mass\n  Distribution and Inclination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dust-Scattering Model for M1-92: A Revised Estimate of the Mass\n  Distribution and Inclination"
                },
                "summary": "Preplanetary nebulae (PPNe) are formed from mass-ejecting late-stage AGB\nstars. Much of the light from the star gets scattered or absorbed by dust\nparticles, giving rise to the observed reflection nebula seen at visible and\nnear-IR wavelengths. Precursors to planetary nebulae (PNe), PPNe generally have\nnot yet undergone any ionization by UV radiation from the still-buried stellar\ncore. Bipolar PPNe are a common form of observed PPNe. This study lays the\ngroundwork for future dynamical studies by reconstructing the dust density\ndistribution of a particularly symmetric bipolar PPN, M1-92 (Minkowski's\nFootprint, IRAS 19343$+$2926). For this purpose, we develop an efficient\nsingle-scattering radiative transfer model with corrections for\ndouble-scattering. Using a V-band image from the Hubble Space Telescope (HST),\nwe infer the dust density profile and orientation of M1-92. These results\nindicate that M1-92's slowly expanding equatorial torus exhibits an outer\nradial cutoff in its density, which implicates the influence of a binary\ncompanion during the formation of the nebula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preplanetary nebulae (PPNe) are formed from mass-ejecting late-stage AGB\nstars. Much of the light from the star gets scattered or absorbed by dust\nparticles, giving rise to the observed reflection nebula seen at visible and\nnear-IR wavelengths. Precursors to planetary nebulae (PNe), PPNe generally have\nnot yet undergone any ionization by UV radiation from the still-buried stellar\ncore. Bipolar PPNe are a common form of observed PPNe. This study lays the\ngroundwork for future dynamical studies by reconstructing the dust density\ndistribution of a particularly symmetric bipolar PPN, M1-92 (Minkowski's\nFootprint, IRAS 19343$+$2926). For this purpose, we develop an efficient\nsingle-scattering radiative transfer model with corrections for\ndouble-scattering. Using a V-band image from the Hubble Space Telescope (HST),\nwe infer the dust density profile and orientation of M1-92. These results\nindicate that M1-92's slowly expanding equatorial torus exhibits an outer\nradial cutoff in its density, which implicates the influence of a binary\ncompanion during the formation of the nebula."
                },
                "authors": [
                    {
                        "name": "Yun Qi Li"
                    },
                    {
                        "name": "Mark R. Morris"
                    },
                    {
                        "name": "Raghvendra Sahai"
                    }
                ],
                "author_detail": {
                    "name": "Raghvendra Sahai"
                },
                "author": "Raghvendra Sahai",
                "arxiv_doi": "10.3390/galaxies12040044",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/galaxies12040044",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.03136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 8 figures, accepted for publications in Galaxies",
                "arxiv_journal_ref": "Galaxies 2024, 12(4), 44",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05019v1",
                "updated": "2024-08-09T12:13:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    13,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T12:13:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    13,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Instruction Tuning-free Visual Token Complement for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning-free Visual Token Complement for Multimodal LLMs"
                },
                "summary": "As the open community of large language models (LLMs) matures, multimodal\nLLMs (MLLMs) have promised an elegant bridge between vision and language.\nHowever, current research is inherently constrained by challenges such as the\nneed for high-quality instruction pairs and the loss of visual information in\nimage-to-text training objectives. To this end, we propose a Visual Token\nComplement framework (VTC) that helps MLLMs regain the missing visual features\nand thus improve response accuracy. Specifically, our VTC integrates\ntext-to-image generation as a guide to identifying the text-irrelevant\nfeatures, and a visual selector is then developed to generate complementary\nvisual tokens to enrich the original visual input. Moreover, an iterative\nstrategy is further designed to extract more visual information by iteratively\nusing the visual selector without any additional training. Notably, the\ntraining pipeline requires no additional image-text pairs, resulting in a\ndesired instruction tuning-free property. Both qualitative and quantitative\nexperiments demonstrate the superiority and efficiency of our VTC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the open community of large language models (LLMs) matures, multimodal\nLLMs (MLLMs) have promised an elegant bridge between vision and language.\nHowever, current research is inherently constrained by challenges such as the\nneed for high-quality instruction pairs and the loss of visual information in\nimage-to-text training objectives. To this end, we propose a Visual Token\nComplement framework (VTC) that helps MLLMs regain the missing visual features\nand thus improve response accuracy. Specifically, our VTC integrates\ntext-to-image generation as a guide to identifying the text-irrelevant\nfeatures, and a visual selector is then developed to generate complementary\nvisual tokens to enrich the original visual input. Moreover, an iterative\nstrategy is further designed to extract more visual information by iteratively\nusing the visual selector without any additional training. Notably, the\ntraining pipeline requires no additional image-text pairs, resulting in a\ndesired instruction tuning-free property. Both qualitative and quantitative\nexperiments demonstrate the superiority and efficiency of our VTC."
                },
                "authors": [
                    {
                        "name": "Dongsheng Wang"
                    },
                    {
                        "name": "Jiequan Cui"
                    },
                    {
                        "name": "Miaoge Li"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Hanwang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanwang Zhang"
                },
                "author": "Hanwang Zhang",
                "arxiv_comment": "Accepted by ECCV2024 (20pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14224v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14224v3",
                "updated": "2024-08-09T12:12:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    12,
                    51,
                    4,
                    222,
                    0
                ],
                "published": "2023-11-23T23:24:45Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    23,
                    24,
                    45,
                    3,
                    327,
                    0
                ],
                "title": "A master-slave coupling scheme for synchronization and parameter\n  estimation in the generalized Kuramoto-Sivashinsky equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A master-slave coupling scheme for synchronization and parameter\n  estimation in the generalized Kuramoto-Sivashinsky equation"
                },
                "summary": "The problem of estimating the constant parameters of the Kuramoto-Sivashinsky\n(KS) equation from observed data has received attention from researchers in\nphysics, applied mathematics and statistics. This is motivated by the various\nphysical applications of the equation and, also, because it often serves as a\ntest model for the study of space-time pattern formation. Remarkably, most\nexisting inference techniques rely on statistical tools, which are\ncomputationally very costly yet do not exploit the dynamical features of the\nsystem. In this paper we introduce a simple, online parameter estimation method\nthat relies on the synchronization properties of the KS equation. In\nparticular, we describe a master-slave setup where the slave model is driven by\nobservations from the master system. The slave dynamics are data-driven and\ndesigned to continuously adapt the model parameters until identical\nsynchronization with the master system is achieved. We provide a simple\nanalysis that supports the proposed approach and also present and discuss the\nresults of an extensive set of computer simulations. Our numerical study shows\nthat the proposed method is computationally fast and also robust to\ninitialization errors, observational noise and variations in the spatial\nresolution of the numerical scheme used to integrate the KS equation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of estimating the constant parameters of the Kuramoto-Sivashinsky\n(KS) equation from observed data has received attention from researchers in\nphysics, applied mathematics and statistics. This is motivated by the various\nphysical applications of the equation and, also, because it often serves as a\ntest model for the study of space-time pattern formation. Remarkably, most\nexisting inference techniques rely on statistical tools, which are\ncomputationally very costly yet do not exploit the dynamical features of the\nsystem. In this paper we introduce a simple, online parameter estimation method\nthat relies on the synchronization properties of the KS equation. In\nparticular, we describe a master-slave setup where the slave model is driven by\nobservations from the master system. The slave dynamics are data-driven and\ndesigned to continuously adapt the model parameters until identical\nsynchronization with the master system is achieved. We provide a simple\nanalysis that supports the proposed approach and also present and discuss the\nresults of an extensive set of computer simulations. Our numerical study shows\nthat the proposed method is computationally fast and also robust to\ninitialization errors, observational noise and variations in the spatial\nresolution of the numerical scheme used to integrate the KS equation."
                },
                "authors": [
                    {
                        "name": "Joaquin Miguez"
                    },
                    {
                        "name": "Harold Molina-Bulla"
                    },
                    {
                        "name": "Inés P. Mariño"
                    }
                ],
                "author_detail": {
                    "name": "Inés P. Mariño"
                },
                "author": "Inés P. Mariño",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14224v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14224v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35A24, 35R30, 34H10, 34D06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21424v2",
                "updated": "2024-08-09T11:58:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    58,
                    55,
                    4,
                    222,
                    0
                ],
                "published": "2024-07-31T08:19:06Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    8,
                    19,
                    6,
                    2,
                    213,
                    0
                ],
                "title": "Cost-Effective Hallucination Detection for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective Hallucination Detection for LLMs"
                },
                "summary": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead."
                },
                "authors": [
                    {
                        "name": "Simon Valentin"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Gianluca Detommaso"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Giovanni Zappella"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "arxiv_comment": "Accepted to GenAI Evaluation Workshop at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05013v1",
                "updated": "2024-08-09T11:49:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    49,
                    18,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T11:49:18Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    49,
                    18,
                    4,
                    222,
                    0
                ],
                "title": "Galaxy-dark matter connection from weak lensing in imaging surveys:\n  Impact of photometric redshift errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy-dark matter connection from weak lensing in imaging surveys:\n  Impact of photometric redshift errors"
                },
                "summary": "The uncertainties in photometric redshifts and stellar masses from imaging\nsurveys affect galaxy sample selection, their abundance measurements, as well\nas the measured weak lensing signals. We develop a framework to assess the\nsystematic effects arising from the use of redshifts and stellar masses derived\nfrom photometric data, and explore their impact on the inferred galaxy-dark\nmatter connection. We use galaxy catalogues from the UniverseMachine (UM)\ngalaxy formation model to create Pz-mock galaxy samples that approximately\nfollow the redshift errors in the Subaru HSC survey. We focus on galaxy\nstellar-mass thresholds ranging from $\\log\\left[M_*/(h^{-2}M_\\odot)\\right]$\nfrom $8.6$ to $11.2$ in steps of 0.2 dex within two redshift bins $0.30-0.55$\nand $0.55-0.80$. A comparison of the Pz-mock samples to true galaxy samples in\nUM shows a relatively mild sample contamination for thresholds with\n$\\log\\left[M_{*,\\rm limit}/(h^{-2}M_\\odot)\\right]<10.6$, while an increasing\ncontamination towards the more massive end. We show how such contamination\naffects the measured abundance and the lensing signal. A joint HOD modelling of\nthe observables from the Pz-mock compared to the truth in the UM informs the\nsystematic biases on the average halo masses of central galaxies in the HSC\nsurvey. Even with a reasonably conservative choice of photo-$z$ errors in\nPz-mock, we show that the inferred halo masses deduced from the HSC galaxies\nfor low-mass thresholds will have a systematic bias smaller than 0.05 dex.\nBeyond $\\log\\left[M_{*,\\rm limit}/(h^{-2}M_\\odot)\\right]=10.6$, the inferred\nhalo masses show an increasing systematic bias with stellar mass, reaching\nvalues of order $0.2$ dex, larger than the statistical error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The uncertainties in photometric redshifts and stellar masses from imaging\nsurveys affect galaxy sample selection, their abundance measurements, as well\nas the measured weak lensing signals. We develop a framework to assess the\nsystematic effects arising from the use of redshifts and stellar masses derived\nfrom photometric data, and explore their impact on the inferred galaxy-dark\nmatter connection. We use galaxy catalogues from the UniverseMachine (UM)\ngalaxy formation model to create Pz-mock galaxy samples that approximately\nfollow the redshift errors in the Subaru HSC survey. We focus on galaxy\nstellar-mass thresholds ranging from $\\log\\left[M_*/(h^{-2}M_\\odot)\\right]$\nfrom $8.6$ to $11.2$ in steps of 0.2 dex within two redshift bins $0.30-0.55$\nand $0.55-0.80$. A comparison of the Pz-mock samples to true galaxy samples in\nUM shows a relatively mild sample contamination for thresholds with\n$\\log\\left[M_{*,\\rm limit}/(h^{-2}M_\\odot)\\right]<10.6$, while an increasing\ncontamination towards the more massive end. We show how such contamination\naffects the measured abundance and the lensing signal. A joint HOD modelling of\nthe observables from the Pz-mock compared to the truth in the UM informs the\nsystematic biases on the average halo masses of central galaxies in the HSC\nsurvey. Even with a reasonably conservative choice of photo-$z$ errors in\nPz-mock, we show that the inferred halo masses deduced from the HSC galaxies\nfor low-mass thresholds will have a systematic bias smaller than 0.05 dex.\nBeyond $\\log\\left[M_{*,\\rm limit}/(h^{-2}M_\\odot)\\right]=10.6$, the inferred\nhalo masses show an increasing systematic bias with stellar mass, reaching\nvalues of order $0.2$ dex, larger than the statistical error."
                },
                "authors": [
                    {
                        "name": "Navin Chaurasiya"
                    },
                    {
                        "name": "Surhud More"
                    },
                    {
                        "name": "Daichi Kashino"
                    },
                    {
                        "name": "Shogo Masaki"
                    },
                    {
                        "name": "Shogo Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Shogo Ishikawa"
                },
                "author": "Shogo Ishikawa",
                "arxiv_comment": "20 pages, 20 figures, Submitted to MNRAS Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05006v1",
                "updated": "2024-08-09T11:35:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    35,
                    44,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T11:35:44Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    35,
                    44,
                    4,
                    222,
                    0
                ],
                "title": "Enhancing the Code Debugging Ability of LLMs via Communicative Agent\n  Based Data Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Code Debugging Ability of LLMs via Communicative Agent\n  Based Data Refinement"
                },
                "summary": "Debugging is a vital aspect of software development, yet the debugging\ncapabilities of Large Language Models (LLMs) remain largely unexplored. This\npaper first introduces DEBUGEVAL, a comprehensive benchmark designed to\nevaluate the debugging capabilities of LLMs. DEBUGEVAL collects data from\nexisting high-quality datasets and designs four different tasks to evaluate the\ndebugging effectiveness, including BUG Localization, BUG Identification, Code\nReview, and Code Repair. Additionally, to enhance the code debugging ability of\nLLMs, this paper proposes a CoMmunicative Agent BaSed DaTa REfinement FRamework\n(MASTER), which generates the refined code debugging data for supervised\nfinetuning. Specifically, MASTER employs the Code Quizzer to generate refined\ndata according to the defined tasks of DEBUGEVAL. Then the Code Learner acts as\na critic and reserves the generated problems that it can not solve. Finally,\nthe Code Teacher provides a detailed Chain-of-Thought based solution to deal\nwith the generated problem. We collect the synthesized data and finetune the\nCode Learner to enhance the debugging ability and conduct the NeuDebugger\nmodel. Our experiments evaluate various LLMs and NeuDebugger in the zero-shot\nsetting on DEBUGEVAL. Experimental results demonstrate that these 7B-scale LLMs\nhave weaker debugging capabilities, even these code-oriented LLMs. On the\ncontrary, these larger models (over 70B) show convincing debugging ability. Our\nfurther analyses illustrate that MASTER is an effective method to enhance the\ncode debugging ability by synthesizing data for Supervised Fine-Tuning (SFT)\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a vital aspect of software development, yet the debugging\ncapabilities of Large Language Models (LLMs) remain largely unexplored. This\npaper first introduces DEBUGEVAL, a comprehensive benchmark designed to\nevaluate the debugging capabilities of LLMs. DEBUGEVAL collects data from\nexisting high-quality datasets and designs four different tasks to evaluate the\ndebugging effectiveness, including BUG Localization, BUG Identification, Code\nReview, and Code Repair. Additionally, to enhance the code debugging ability of\nLLMs, this paper proposes a CoMmunicative Agent BaSed DaTa REfinement FRamework\n(MASTER), which generates the refined code debugging data for supervised\nfinetuning. Specifically, MASTER employs the Code Quizzer to generate refined\ndata according to the defined tasks of DEBUGEVAL. Then the Code Learner acts as\na critic and reserves the generated problems that it can not solve. Finally,\nthe Code Teacher provides a detailed Chain-of-Thought based solution to deal\nwith the generated problem. We collect the synthesized data and finetune the\nCode Learner to enhance the debugging ability and conduct the NeuDebugger\nmodel. Our experiments evaluate various LLMs and NeuDebugger in the zero-shot\nsetting on DEBUGEVAL. Experimental results demonstrate that these 7B-scale LLMs\nhave weaker debugging capabilities, even these code-oriented LLMs. On the\ncontrary, these larger models (over 70B) show convincing debugging ability. Our\nfurther analyses illustrate that MASTER is an effective method to enhance the\ncode debugging ability by synthesizing data for Supervised Fine-Tuning (SFT)\nLLMs."
                },
                "authors": [
                    {
                        "name": "Weiqing Yang"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Minghe Yu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14319v2",
                "updated": "2024-08-09T11:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    20,
                    58,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-23T08:49:55Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    8,
                    49,
                    55,
                    3,
                    144,
                    0
                ],
                "title": "Variational Signal Separation for Automotive Radar Interference\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Signal Separation for Automotive Radar Interference\n  Mitigation"
                },
                "summary": "Algorithms for mutual interference mitigation and object parameter estimation\nare a key enabler for automotive applications of frequency-modulated continuous\nwave (FMCW) radar. In this paper, we introduce a signal separation method to\ndetect and estimate radar object parameters while jointly estimating and\nsuccessively canceling the interference signal. The underlying signal model\nposes a challenge, since both the coherent radar echo and the non-coherent\ninterference influenced by individual multipath propagation channels must be\nconsidered. Under certain assumptions, the model is described as a\nsuperposition of multipath channels weighted by parametric interference chirp\nenvelopes. Inspired by sparse Bayesian learning (SBL), we employ an augmented\nprobabilistic model that uses a hierarchical Gamma-Gaussian prior model for\neach multipath channel. Based on this, an iterative inference algorithm is\nderived using the variational expectation-maximization (EM) methodology. The\nalgorithm is statistically evaluated in terms of object parameter estimation\naccuracy and robustness, indicating that it is fundamentally capable of\nachieving the Cramer-Rao lower bound (CRLB) with respect to the accuracy of\nobject estimates and it closely follows the radar performance achieved when no\ninterference is present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms for mutual interference mitigation and object parameter estimation\nare a key enabler for automotive applications of frequency-modulated continuous\nwave (FMCW) radar. In this paper, we introduce a signal separation method to\ndetect and estimate radar object parameters while jointly estimating and\nsuccessively canceling the interference signal. The underlying signal model\nposes a challenge, since both the coherent radar echo and the non-coherent\ninterference influenced by individual multipath propagation channels must be\nconsidered. Under certain assumptions, the model is described as a\nsuperposition of multipath channels weighted by parametric interference chirp\nenvelopes. Inspired by sparse Bayesian learning (SBL), we employ an augmented\nprobabilistic model that uses a hierarchical Gamma-Gaussian prior model for\neach multipath channel. Based on this, an iterative inference algorithm is\nderived using the variational expectation-maximization (EM) methodology. The\nalgorithm is statistically evaluated in terms of object parameter estimation\naccuracy and robustness, indicating that it is fundamentally capable of\nachieving the Cramer-Rao lower bound (CRLB) with respect to the accuracy of\nobject estimates and it closely follows the radar performance achieved when no\ninterference is present."
                },
                "authors": [
                    {
                        "name": "Mate Toth"
                    },
                    {
                        "name": "Erik Leitinger"
                    },
                    {
                        "name": "Klaus Witrisal"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Witrisal"
                },
                "author": "Klaus Witrisal",
                "arxiv_comment": "19 pages, 8 figures; submitted to IEEE Transactions on Radar Systems\n  on May 23, 2024; major revision on Aug. 8, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04998v1",
                "updated": "2024-08-09T11:18:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    18,
                    29,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T11:18:29Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    18,
                    29,
                    4,
                    222,
                    0
                ],
                "title": "ProFuser: Progressive Fusion of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProFuser: Progressive Fusion of Large Language Models"
                },
                "summary": "While fusing the capacities and advantages of various large language models\n(LLMs) offers a pathway to construct more powerful and versatile models, a\nfundamental challenge is to properly select advantageous model during the\ntraining. Existing fusion methods primarily focus on the training mode that\nuses cross entropy on ground truth in a teacher-forcing setup to measure a\nmodel's advantage, which may provide limited insight towards model advantage.\nIn this paper, we introduce a novel approach that enhances the fusion process\nby incorporating both the training and inference modes. Our method evaluates\nmodel advantage not only through cross entropy during training but also by\nconsidering inference outputs, providing a more comprehensive assessment. To\ncombine the two modes effectively, we introduce ProFuser to progressively\ntransition from inference mode to training mode. To validate ProFuser's\neffectiveness, we fused three models, including vicuna-7b-v1.5,\nLlama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance\nin knowledge, reasoning, and safety compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While fusing the capacities and advantages of various large language models\n(LLMs) offers a pathway to construct more powerful and versatile models, a\nfundamental challenge is to properly select advantageous model during the\ntraining. Existing fusion methods primarily focus on the training mode that\nuses cross entropy on ground truth in a teacher-forcing setup to measure a\nmodel's advantage, which may provide limited insight towards model advantage.\nIn this paper, we introduce a novel approach that enhances the fusion process\nby incorporating both the training and inference modes. Our method evaluates\nmodel advantage not only through cross entropy during training but also by\nconsidering inference outputs, providing a more comprehensive assessment. To\ncombine the two modes effectively, we introduce ProFuser to progressively\ntransition from inference mode to training mode. To validate ProFuser's\neffectiveness, we fused three models, including vicuna-7b-v1.5,\nLlama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance\nin knowledge, reasoning, and safety compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Canbin Huang"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10199v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10199v4",
                "updated": "2024-08-09T11:06:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    6,
                    2,
                    4,
                    222,
                    0
                ],
                "published": "2024-04-16T00:50:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    0,
                    50,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting"
                },
                "summary": "As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/"
                },
                "authors": [
                    {
                        "name": "Huihan Li"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Sebastin Santy"
                    },
                    {
                        "name": "Taylor Sorensen"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10199v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10199v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19760v2",
                "updated": "2024-08-09T10:43:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    10,
                    43,
                    18,
                    4,
                    222,
                    0
                ],
                "published": "2024-07-29T07:51:43Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    7,
                    51,
                    43,
                    0,
                    211,
                    0
                ],
                "title": "Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional\n  Principles in Complex Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional\n  Principles in Complex Scenarios"
                },
                "summary": "In this paper, we conduct an empirical analysis of how large language models\n(LLMs), specifically GPT-4, interpret constitutional principles in complex\ndecision-making scenarios. We examine rulings from the Italian Constitutional\nCourt on bioethics issues that involve trade-offs between competing values and\ncompare model-generated legal arguments on these issues to those presented by\nthe State, the Court, and the applicants. Our results indicate that GPT-4\nconsistently aligns more closely with progressive interpretations of the\nConstitution, often overlooking competing values and mirroring the applicants'\nviews rather than the more conservative perspectives of the State or the\nCourt's moderate positions. Our experiments reveal a distinct tendency of GPT-4\nto favor progressive legal interpretations, underscoring the influence of\nunderlying data biases. We thus underscore the importance of testing alignment\nin real-world scenarios and considering the implications of deploying LLMs in\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we conduct an empirical analysis of how large language models\n(LLMs), specifically GPT-4, interpret constitutional principles in complex\ndecision-making scenarios. We examine rulings from the Italian Constitutional\nCourt on bioethics issues that involve trade-offs between competing values and\ncompare model-generated legal arguments on these issues to those presented by\nthe State, the Court, and the applicants. Our results indicate that GPT-4\nconsistently aligns more closely with progressive interpretations of the\nConstitution, often overlooking competing values and mirroring the applicants'\nviews rather than the more conservative perspectives of the State or the\nCourt's moderate positions. Our experiments reveal a distinct tendency of GPT-4\nto favor progressive legal interpretations, underscoring the influence of\nunderlying data biases. We thus underscore the importance of testing alignment\nin real-world scenarios and considering the implications of deploying LLMs in\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Camilla Bignotti"
                    },
                    {
                        "name": "Carolina Camassa"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Camassa"
                },
                "author": "Carolina Camassa",
                "arxiv_comment": "Accepted at AIES24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04983v1",
                "updated": "2024-08-09T10:26:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    10,
                    26,
                    11,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T10:26:11Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    10,
                    26,
                    11,
                    4,
                    222,
                    0
                ],
                "title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with\n  Selective Entropy Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Get Confused Cautiously: Textual Sequence Memorization Erasure with\n  Selective Entropy Maximization"
                },
                "summary": "Large Language Models (LLMs) have been found to memorize and recite some of\nthe textual sequences from their training set verbatim, raising broad concerns\nabout privacy and copyright issues when using LLMs. This Textual Sequence\nMemorization (TSM) phenomenon leads to a high demand to regulate LLM output to\nprevent it from generating certain memorized text to meet user requirements.\nHowever, our empirical study reveals that existing methods for TSM erasure fail\nto forget massive memorized samples without substantially jeopardizing the\nmodel utility. To achieve a better trade-off between the effectiveness of TSM\nerasure and model utility in LLMs, our paper proposes a new framework based on\nEntropy Maximization with Selective Optimization (EMSO), where the updated\nweights are chosen with a novel contrastive gradient metric without any\nparticipation of additional model or data. Our analysis shows that training\nwith the entropy maximization loss has a more stable optimization process and\nbetter keeps model utility than existing methods. The contrastive gradient\nmetric localizes the most influential weight for TSM erasure by taking both the\ngradient magnitude and direction into consideration. Extensive experiments\nacross three model scales demonstrate that our method excels in handling\nlarge-scale forgetting requests while preserving model ability in language\ngeneration and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been found to memorize and recite some of\nthe textual sequences from their training set verbatim, raising broad concerns\nabout privacy and copyright issues when using LLMs. This Textual Sequence\nMemorization (TSM) phenomenon leads to a high demand to regulate LLM output to\nprevent it from generating certain memorized text to meet user requirements.\nHowever, our empirical study reveals that existing methods for TSM erasure fail\nto forget massive memorized samples without substantially jeopardizing the\nmodel utility. To achieve a better trade-off between the effectiveness of TSM\nerasure and model utility in LLMs, our paper proposes a new framework based on\nEntropy Maximization with Selective Optimization (EMSO), where the updated\nweights are chosen with a novel contrastive gradient metric without any\nparticipation of additional model or data. Our analysis shows that training\nwith the entropy maximization loss has a more stable optimization process and\nbetter keeps model utility than existing methods. The contrastive gradient\nmetric localizes the most influential weight for TSM erasure by taking both the\ngradient magnitude and direction into consideration. Extensive experiments\nacross three model scales demonstrate that our method excels in handling\nlarge-scale forgetting requests while preserving model ability in language\ngeneration and reasoning."
                },
                "authors": [
                    {
                        "name": "Zhaohan Zhang"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04974v1",
                "updated": "2024-08-09T09:54:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    54,
                    11,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T09:54:11Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    54,
                    11,
                    4,
                    222,
                    0
                ],
                "title": "XNN: Paradigm Shift in Mitigating Identity Leakage within Cloud-Enabled\n  Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XNN: Paradigm Shift in Mitigating Identity Leakage within Cloud-Enabled\n  Deep Learning"
                },
                "summary": "In the domain of cloud-based deep learning, the imperative for external\ncomputational resources coexists with acute privacy concerns, particularly\nidentity leakage. To address this challenge, we introduce XNN and XNN-d,\npioneering methodologies that infuse neural network features with randomized\nperturbations, striking a harmonious balance between utility and privacy. XNN,\ndesigned for the training phase, ingeniously blends random permutation with\nmatrix multiplication techniques to obfuscate feature maps, effectively\nshielding private data from potential breaches without compromising training\nintegrity. Concurrently, XNN-d, devised for the inference phase, employs\nadversarial training to integrate generative adversarial noise. This technique\neffectively counters black-box access attacks aimed at identity extraction,\nwhile a distilled face recognition network adeptly processes the perturbed\nfeatures, ensuring accurate identification. Our evaluation demonstrates XNN's\neffectiveness, significantly outperforming existing methods in reducing\nidentity leakage while maintaining a high model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of cloud-based deep learning, the imperative for external\ncomputational resources coexists with acute privacy concerns, particularly\nidentity leakage. To address this challenge, we introduce XNN and XNN-d,\npioneering methodologies that infuse neural network features with randomized\nperturbations, striking a harmonious balance between utility and privacy. XNN,\ndesigned for the training phase, ingeniously blends random permutation with\nmatrix multiplication techniques to obfuscate feature maps, effectively\nshielding private data from potential breaches without compromising training\nintegrity. Concurrently, XNN-d, devised for the inference phase, employs\nadversarial training to integrate generative adversarial noise. This technique\neffectively counters black-box access attacks aimed at identity extraction,\nwhile a distilled face recognition network adeptly processes the perturbed\nfeatures, ensuring accurate identification. Our evaluation demonstrates XNN's\neffectiveness, significantly outperforming existing methods in reducing\nidentity leakage while maintaining a high model accuracy."
                },
                "authors": [
                    {
                        "name": "Kaixin Liu"
                    },
                    {
                        "name": "Huixin Xiong"
                    },
                    {
                        "name": "Bingyu Duan"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Wanqian Zhang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04971v1",
                "updated": "2024-08-09T09:43:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    43,
                    40,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T09:43:40Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    43,
                    40,
                    4,
                    222,
                    0
                ],
                "title": "Neutron multiplicity counting distribution reconstruction from moments\n  using Meixner polynomial expansion and N-forked branching approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron multiplicity counting distribution reconstruction from moments\n  using Meixner polynomial expansion and N-forked branching approximations"
                },
                "summary": "Methods used to infer nuclear parameters from neutron count statistics fall\ninto two categories depending on whether they use moments or count number\nprobabilities. As probabilities are in general more difficult to calculate, we\nare interested here in the reconstruction of distributions from their first\nmoments. For this, we explore two approaches. The first one relies on a\ngeneralization of the 2-forked branching correlation (quadratic) approximation\nused in the PMZBB and Poisson radical distributions, the second one is based on\nthe expansion of the distribution on a basis of Meixner discrete orthogonal\npolynomials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods used to infer nuclear parameters from neutron count statistics fall\ninto two categories depending on whether they use moments or count number\nprobabilities. As probabilities are in general more difficult to calculate, we\nare interested here in the reconstruction of distributions from their first\nmoments. For this, we explore two approaches. The first one relies on a\ngeneralization of the 2-forked branching correlation (quadratic) approximation\nused in the PMZBB and Poisson radical distributions, the second one is based on\nthe expansion of the distribution on a basis of Meixner discrete orthogonal\npolynomials."
                },
                "authors": [
                    {
                        "name": "Philippe Humbert"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Humbert"
                },
                "author": "Philippe Humbert",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.08573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.08573v2",
                "updated": "2024-08-09T09:36:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    36,
                    0,
                    4,
                    222,
                    0
                ],
                "published": "2023-09-15T17:38:41Z",
                "published_parsed": [
                    2023,
                    9,
                    15,
                    17,
                    38,
                    41,
                    4,
                    258,
                    0
                ],
                "title": "Indian-BhED: A Dataset for Measuring India-Centric Biases in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indian-BhED: A Dataset for Measuring India-Centric Biases in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs), now used daily by millions, can encode societal\nbiases, exposing their users to representational harms. A large body of\nscholarship on LLM bias exists but it predominantly adopts a Western-centric\nframe and attends comparatively less to bias levels and potential harms in the\nGlobal South. In this paper, we quantify stereotypical bias in popular LLMs\naccording to an Indian-centric frame through Indian-BhED, a first of its kind\ndataset, containing stereotypical and anti-stereotypical examples in the\ncontext of caste and religious stereotypes in India. We find that the majority\nof LLMs tested have a strong propensity to output stereotypes in the Indian\ncontext, especially when compared to axes of bias traditionally studied in the\nWestern context, such as gender and race. Notably, we find that GPT-2, GPT-2\nLarge, and GPT 3.5 have a particularly high propensity for preferring\nstereotypical outputs as a percent of all sentences for the axes of caste\n(63-79%) and religion (69-72%). We finally investigate potential causes for\nsuch harmful behaviour in LLMs, and posit intervention techniques to reduce\nboth stereotypical and anti-stereotypical biases. The findings of this work\nhighlight the need for including more diverse voices when researching fairness\nin AI and evaluating LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), now used daily by millions, can encode societal\nbiases, exposing their users to representational harms. A large body of\nscholarship on LLM bias exists but it predominantly adopts a Western-centric\nframe and attends comparatively less to bias levels and potential harms in the\nGlobal South. In this paper, we quantify stereotypical bias in popular LLMs\naccording to an Indian-centric frame through Indian-BhED, a first of its kind\ndataset, containing stereotypical and anti-stereotypical examples in the\ncontext of caste and religious stereotypes in India. We find that the majority\nof LLMs tested have a strong propensity to output stereotypes in the Indian\ncontext, especially when compared to axes of bias traditionally studied in the\nWestern context, such as gender and race. Notably, we find that GPT-2, GPT-2\nLarge, and GPT 3.5 have a particularly high propensity for preferring\nstereotypical outputs as a percent of all sentences for the axes of caste\n(63-79%) and religion (69-72%). We finally investigate potential causes for\nsuch harmful behaviour in LLMs, and posit intervention techniques to reduce\nboth stereotypical and anti-stereotypical biases. The findings of this work\nhighlight the need for including more diverse voices when researching fairness\nin AI and evaluating LLMs."
                },
                "authors": [
                    {
                        "name": "Khyati Khandelwal"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Hannah Rose Kirk"
                    },
                    {
                        "name": "Scott A. Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A. Hale"
                },
                "author": "Scott A. Hale",
                "arxiv_doi": "10.1145/3677525.3678666",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677525.3678666",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.08573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.08573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in GoodIT '24, doi:10.1145/3677525.3678666. 14 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04966v1",
                "updated": "2024-08-09T09:31:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    31,
                    49,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T09:31:49Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    31,
                    49,
                    4,
                    222,
                    0
                ],
                "title": "Predicting RSO Populations Using a Neighbouring Orbits Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting RSO Populations Using a Neighbouring Orbits Technique"
                },
                "summary": "The determination of the full population of Resident Space Objects (RSOs) in\nLow Earth Orbit (LEO) is a key issue in the field of space situational\nawareness that will only increase in importance in the coming years. We\nendeavour to describe a novel method of inferring the population of RSOs as a\nfunction of orbital height and inclination for a range of magnitudes. The\nmethod described uses observations of an orbit of known height and inclination\nto detect RSOs on neighbouring orbits. These neighbouring orbit targets move\nslowly relative to our tracked orbit, and are thus detectable down to faint\nmagnitudes. We conduct simulations to show that, by observing multiple passes\nof a known orbit, we can infer the population of RSOs within a defined region\nof orbital parameter space. Observing a range of orbits from different orbital\nsites will allow for the inference of a population of LEO RSOs as a function of\ntheir orbital parameters and object magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The determination of the full population of Resident Space Objects (RSOs) in\nLow Earth Orbit (LEO) is a key issue in the field of space situational\nawareness that will only increase in importance in the coming years. We\nendeavour to describe a novel method of inferring the population of RSOs as a\nfunction of orbital height and inclination for a range of magnitudes. The\nmethod described uses observations of an orbit of known height and inclination\nto detect RSOs on neighbouring orbits. These neighbouring orbit targets move\nslowly relative to our tracked orbit, and are thus detectable down to faint\nmagnitudes. We conduct simulations to show that, by observing multiple passes\nof a known orbit, we can infer the population of RSOs within a defined region\nof orbital parameter space. Observing a range of orbits from different orbital\nsites will allow for the inference of a population of LEO RSOs as a function of\ntheir orbital parameters and object magnitude."
                },
                "authors": [
                    {
                        "name": "Benjamin F. Cooke"
                    },
                    {
                        "name": "James A. Blake"
                    },
                    {
                        "name": "Paul Chote"
                    },
                    {
                        "name": "James McCormac"
                    },
                    {
                        "name": "Don Pollacco"
                    }
                ],
                "author_detail": {
                    "name": "Don Pollacco"
                },
                "arxiv_affiliation": "Department of Physics, University of Warwick, UK",
                "author": "Don Pollacco",
                "arxiv_comment": "15 pages, 12 figures. Accepted for publication in RAS Techniques and\n  Instruments (RASTI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10739v2",
                "updated": "2024-08-09T09:28:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    28,
                    14,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-17T12:37:10Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    12,
                    37,
                    10,
                    4,
                    138,
                    0
                ],
                "title": "Efficient Multimodal Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multimodal Large Language Models: A Survey"
                },
                "summary": "In the past year, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable performance in tasks such as visual question answering, visual\nunderstanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs has\nenormous potential, especially in edge computing scenarios. In this survey, we\nprovide a comprehensive and systematic review of the current state of efficient\nMLLMs. Specifically, we summarize the timeline of representative efficient\nMLLMs, research state of efficient structures and strategies, and the\napplications. Finally, we discuss the limitations of current efficient MLLM\nresearch and promising future directions. Please refer to our GitHub repository\nfor more details:\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past year, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable performance in tasks such as visual question answering, visual\nunderstanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs has\nenormous potential, especially in edge computing scenarios. In this survey, we\nprovide a comprehensive and systematic review of the current state of efficient\nMLLMs. Specifically, we summarize the timeline of representative efficient\nMLLMs, research state of efficient structures and strategies, and the\napplications. Finally, we discuss the limitations of current efficient MLLM\nresearch and promising future directions. Please refer to our GitHub repository\nfor more details:\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Yizhang Jin"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Tianjun Gu"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Zhengkai Jiang"
                    },
                    {
                        "name": "Muyang He"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04948v1",
                "updated": "2024-08-09T09:07:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    7,
                    48,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T09:07:48Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    7,
                    48,
                    4,
                    222,
                    0
                ],
                "title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented\n  Generation for Efficient Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented\n  Generation for Efficient Information Extraction"
                },
                "summary": "Extraction and interpretation of intricate information from unstructured text\ndata arising in financial applications, such as earnings call transcripts,\npresent substantial challenges to large language models (LLMs) even using the\ncurrent best practices to use Retrieval Augmented Generation (RAG) (referred to\nas VectorRAG techniques which utilize vector databases for information\nretrieval) due to challenges such as domain specific terminology and complex\nformats of the documents. We introduce a novel approach based on a combination,\ncalled HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called\nGraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for\ninformation extraction from financial documents that is shown to be capable of\ngenerating accurate and contextually relevant answers. Using experiments on a\nset of financial earning call transcripts documents which come in the form of\nQ&A format, and hence provide a natural set of pairs of ground-truth Q&As, we\nshow that HybridRAG which retrieves context from both vector database and KG\noutperforms both traditional VectorRAG and GraphRAG individually when evaluated\nat both the retrieval and generation stages in terms of retrieval accuracy and\nanswer generation. The proposed technique has applications beyond the financial\ndomain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extraction and interpretation of intricate information from unstructured text\ndata arising in financial applications, such as earnings call transcripts,\npresent substantial challenges to large language models (LLMs) even using the\ncurrent best practices to use Retrieval Augmented Generation (RAG) (referred to\nas VectorRAG techniques which utilize vector databases for information\nretrieval) due to challenges such as domain specific terminology and complex\nformats of the documents. We introduce a novel approach based on a combination,\ncalled HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called\nGraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for\ninformation extraction from financial documents that is shown to be capable of\ngenerating accurate and contextually relevant answers. Using experiments on a\nset of financial earning call transcripts documents which come in the form of\nQ&A format, and hence provide a natural set of pairs of ground-truth Q&As, we\nshow that HybridRAG which retrieves context from both vector database and KG\noutperforms both traditional VectorRAG and GraphRAG individually when evaluated\nat both the retrieval and generation stages in terms of retrieval accuracy and\nanswer generation. The proposed technique has applications beyond the financial\ndomain"
                },
                "authors": [
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Benika Hall"
                    },
                    {
                        "name": "Rohan Rao"
                    },
                    {
                        "name": "Sunil Patel"
                    },
                    {
                        "name": "Stefano Pasquali"
                    },
                    {
                        "name": "Dhagash Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Dhagash Mehta"
                },
                "author": "Dhagash Mehta",
                "arxiv_comment": "9 pages, 2 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13544v2",
                "updated": "2024-08-09T09:07:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    7,
                    35,
                    4,
                    222,
                    0
                ],
                "published": "2023-07-25T14:47:36Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    14,
                    47,
                    36,
                    1,
                    206,
                    0
                ],
                "title": "A model for efficient dynamical ranking in networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model for efficient dynamical ranking in networks"
                },
                "summary": "We present a physics-inspired method for inferring dynamic rankings in\ndirected temporal networks - networks in which each directed and timestamped\nedge reflects the outcome and timing of a pairwise interaction. The inferred\nranking of each node is real-valued and varies in time as each new edge,\nencoding an outcome like a win or loss, raises or lowers the node's estimated\nstrength or prestige, as is often observed in real scenarios including\nsequences of games, tournaments, or interactions in animal hierarchies. Our\nmethod works by solving a linear system of equations and requires only one\nparameter to be tuned. As a result, the corresponding algorithm is scalable and\nefficient. We test our method by evaluating its ability to predict interactions\n(edges' existence) and their outcomes (edges' directions) in a variety of\napplications, including both synthetic and real data. Our analysis shows that\nin many cases our method's performance is better than existing methods for\npredicting dynamic rankings and interaction outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a physics-inspired method for inferring dynamic rankings in\ndirected temporal networks - networks in which each directed and timestamped\nedge reflects the outcome and timing of a pairwise interaction. The inferred\nranking of each node is real-valued and varies in time as each new edge,\nencoding an outcome like a win or loss, raises or lowers the node's estimated\nstrength or prestige, as is often observed in real scenarios including\nsequences of games, tournaments, or interactions in animal hierarchies. Our\nmethod works by solving a linear system of equations and requires only one\nparameter to be tuned. As a result, the corresponding algorithm is scalable and\nefficient. We test our method by evaluating its ability to predict interactions\n(edges' existence) and their outcomes (edges' directions) in a variety of\napplications, including both synthetic and real data. Our analysis shows that\nin many cases our method's performance is better than existing methods for\npredicting dynamic rankings and interaction outcomes."
                },
                "authors": [
                    {
                        "name": "Andrea Della Vecchia"
                    },
                    {
                        "name": "Kibidi Neocosmos"
                    },
                    {
                        "name": "Daniel B. Larremore"
                    },
                    {
                        "name": "Cristopher Moore"
                    },
                    {
                        "name": "Caterina De Bacco"
                    }
                ],
                "author_detail": {
                    "name": "Caterina De Bacco"
                },
                "author": "Caterina De Bacco",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11796v2",
                "updated": "2024-08-09T09:00:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    9,
                    0,
                    21,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-18T13:53:48Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    13,
                    53,
                    48,
                    0,
                    78,
                    0
                ],
                "title": "OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy\n  Representation"
                },
                "summary": "3D reconstruction has been widely used in autonomous navigation fields of\nmobile robotics. However, the former research can only provide the basic\ngeometry structure without the capability of open-world scene understanding,\nlimiting advanced tasks like human interaction and visual navigation. Moreover,\ntraditional 3D scene understanding approaches rely on expensive labeled 3D\ndatasets to train a model for a single task with supervision. Thus, geometric\nreconstruction with zero-shot scene understanding i.e. Open vocabulary 3D\nUnderstanding and Reconstruction, is crucial for the future development of\nmobile robots. In this paper, we propose OpenOcc, a novel framework unifying\nthe 3D scene reconstruction and open vocabulary understanding with neural\nradiance fields. We model the geometric structure of the scene with occupancy\nrepresentation and distill the pre-trained open vocabulary model into a 3D\nlanguage field via volume rendering for zero-shot inference. Furthermore, a\nnovel semantic-aware confidence propagation (SCP) method has been proposed to\nrelieve the issue of language field representation degeneracy caused by\ninconsistent measurements in distilled features. Experimental results show that\nour approach achieves competitive performance in 3D scene understanding tasks,\nespecially for small and long-tail objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction has been widely used in autonomous navigation fields of\nmobile robotics. However, the former research can only provide the basic\ngeometry structure without the capability of open-world scene understanding,\nlimiting advanced tasks like human interaction and visual navigation. Moreover,\ntraditional 3D scene understanding approaches rely on expensive labeled 3D\ndatasets to train a model for a single task with supervision. Thus, geometric\nreconstruction with zero-shot scene understanding i.e. Open vocabulary 3D\nUnderstanding and Reconstruction, is crucial for the future development of\nmobile robots. In this paper, we propose OpenOcc, a novel framework unifying\nthe 3D scene reconstruction and open vocabulary understanding with neural\nradiance fields. We model the geometric structure of the scene with occupancy\nrepresentation and distill the pre-trained open vocabulary model into a 3D\nlanguage field via volume rendering for zero-shot inference. Furthermore, a\nnovel semantic-aware confidence propagation (SCP) method has been proposed to\nrelieve the issue of language field representation degeneracy caused by\ninconsistent measurements in distilled features. Experimental results show that\nour approach achieves competitive performance in 3D scene understanding tasks,\nespecially for small and long-tail objects."
                },
                "authors": [
                    {
                        "name": "Haochen Jiang"
                    },
                    {
                        "name": "Yueming Xu"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jianfeng Feng"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "This work is accepted by IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07153v2",
                "updated": "2024-08-09T08:42:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    42,
                    34,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-11T10:57:28Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    10,
                    57,
                    28,
                    1,
                    163,
                    0
                ],
                "title": "EEG classification for visual brain decoding with spatio-temporal and\n  transformer based paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG classification for visual brain decoding with spatio-temporal and\n  transformer based paradigms"
                },
                "summary": "In this work, we delve into the EEG classification task in the domain of\nvisual brain decoding via two frameworks, involving two different learning\nparadigms. Considering the spatio-temporal nature of EEG data, one of our\nframeworks is based on a CNN-BiLSTM model. The other involves a CNN-Transformer\narchitecture which inherently involves the more versatile attention based\nlearning paradigm. In both cases, a special 1D-CNN feature extraction module is\nused to generate the initial embeddings with 1D convolutions in the time and\nthe EEG channel domains. Considering the EEG signals are noisy, non stationary\nand the discriminative features are even less clear (than in semantically\nstructured data such as text or image), we also follow a window-based\nclassification followed by majority voting during inference, to yield labels at\na signal level. To illustrate how brain patterns correlate with different image\nclasses, we visualize t-SNE plots of the BiLSTM embeddings alongside brain\nactivation maps for the top 10 classes. These visualizations provide insightful\nrevelations into the distinct neural signatures associated with each visual\ncategory, showcasing the BiLSTM's capability to capture and represent the\ndiscriminative brain activity linked to visual stimuli. We demonstrate the\nperformance of our approach on the updated EEG-Imagenet dataset with positive\ncomparisons with state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we delve into the EEG classification task in the domain of\nvisual brain decoding via two frameworks, involving two different learning\nparadigms. Considering the spatio-temporal nature of EEG data, one of our\nframeworks is based on a CNN-BiLSTM model. The other involves a CNN-Transformer\narchitecture which inherently involves the more versatile attention based\nlearning paradigm. In both cases, a special 1D-CNN feature extraction module is\nused to generate the initial embeddings with 1D convolutions in the time and\nthe EEG channel domains. Considering the EEG signals are noisy, non stationary\nand the discriminative features are even less clear (than in semantically\nstructured data such as text or image), we also follow a window-based\nclassification followed by majority voting during inference, to yield labels at\na signal level. To illustrate how brain patterns correlate with different image\nclasses, we visualize t-SNE plots of the BiLSTM embeddings alongside brain\nactivation maps for the top 10 classes. These visualizations provide insightful\nrevelations into the distinct neural signatures associated with each visual\ncategory, showcasing the BiLSTM's capability to capture and represent the\ndiscriminative brain activity linked to visual stimuli. We demonstrate the\nperformance of our approach on the updated EEG-Imagenet dataset with positive\ncomparisons with state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Akanksha Sharma"
                    },
                    {
                        "name": "Jyoti Nigam"
                    },
                    {
                        "name": "Abhishek Rathore"
                    },
                    {
                        "name": "Arnav Bhavsar"
                    }
                ],
                "author_detail": {
                    "name": "Arnav Bhavsar"
                },
                "author": "Arnav Bhavsar",
                "arxiv_comment": "The paper has been submitted at ICPR 2024. It contains 17 pages with\n  9 images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04927v1",
                "updated": "2024-08-09T08:16:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    16,
                    8,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T08:16:08Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    16,
                    8,
                    4,
                    222,
                    0
                ],
                "title": "Large Models for Aerial Edges: An Edge-Cloud Model Evolution and\n  Communication Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Models for Aerial Edges: An Edge-Cloud Model Evolution and\n  Communication Paradigm"
                },
                "summary": "The future sixth-generation (6G) of wireless networks is expected to surpass\nits predecessors by offering ubiquitous coverage through integrated air-ground\nfacility deployments in both communication and computing domains. In this\nnetwork, aerial facilities, such as unmanned aerial vehicles (UAVs), conduct\nartificial intelligence (AI) computations based on multi-modal data to support\ndiverse applications including surveillance and environment construction.\nHowever, these multi-domain inference and content generation tasks require\nlarge AI models, demanding powerful computing capabilities, thus posing\nsignificant challenges for UAVs. To tackle this problem, we propose an\nintegrated edge-cloud model evolution framework, where UAVs serve as edge nodes\nfor data collection and edge model computation. Through wireless channels, UAVs\ncollaborate with ground cloud servers, providing cloud model computation and\nmodel updating for edge UAVs. With limited wireless communication bandwidth,\nthe proposed framework faces the challenge of information exchange scheduling\nbetween the edge UAVs and the cloud server. To tackle this, we present joint\ntask allocation, transmission resource allocation, transmission data\nquantization design, and edge model update design to enhance the inference\naccuracy of the integrated air-ground edge-cloud model evolution framework by\nmean average precision (mAP) maximization. A closed-form lower bound on the mAP\nof the proposed framework is derived, and the solution to the mAP maximization\nproblem is optimized accordingly. Simulations, based on results from\nvision-based classification experiments, consistently demonstrate that the mAP\nof the proposed framework outperforms both a centralized cloud model framework\nand a distributed edge model framework across various communication bandwidths\nand data sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future sixth-generation (6G) of wireless networks is expected to surpass\nits predecessors by offering ubiquitous coverage through integrated air-ground\nfacility deployments in both communication and computing domains. In this\nnetwork, aerial facilities, such as unmanned aerial vehicles (UAVs), conduct\nartificial intelligence (AI) computations based on multi-modal data to support\ndiverse applications including surveillance and environment construction.\nHowever, these multi-domain inference and content generation tasks require\nlarge AI models, demanding powerful computing capabilities, thus posing\nsignificant challenges for UAVs. To tackle this problem, we propose an\nintegrated edge-cloud model evolution framework, where UAVs serve as edge nodes\nfor data collection and edge model computation. Through wireless channels, UAVs\ncollaborate with ground cloud servers, providing cloud model computation and\nmodel updating for edge UAVs. With limited wireless communication bandwidth,\nthe proposed framework faces the challenge of information exchange scheduling\nbetween the edge UAVs and the cloud server. To tackle this, we present joint\ntask allocation, transmission resource allocation, transmission data\nquantization design, and edge model update design to enhance the inference\naccuracy of the integrated air-ground edge-cloud model evolution framework by\nmean average precision (mAP) maximization. A closed-form lower bound on the mAP\nof the proposed framework is derived, and the solution to the mAP maximization\nproblem is optimized accordingly. Simulations, based on results from\nvision-based classification experiments, consistently demonstrate that the mAP\nof the proposed framework outperforms both a centralized cloud model framework\nand a distributed edge model framework across various communication bandwidths\nand data sizes."
                },
                "authors": [
                    {
                        "name": "Shuhang Zhang"
                    },
                    {
                        "name": "Qingyu Liu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Boya Di"
                    },
                    {
                        "name": "Hongliang Zhang"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhu Han"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03747v2",
                "updated": "2024-08-09T08:10:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    10,
                    52,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-07T13:01:10Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    13,
                    1,
                    10,
                    2,
                    220,
                    0
                ],
                "title": "Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions"
                },
                "summary": "Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work."
                },
                "authors": [
                    {
                        "name": "Lucas Correia"
                    },
                    {
                        "name": "Jan-Christoph Goos"
                    },
                    {
                        "name": "Philipp Klein"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Anna V. Kononova"
                    }
                ],
                "author_detail": {
                    "name": "Anna V. Kononova"
                },
                "author": "Anna V. Kononova",
                "arxiv_comment": "Submitted to Engineering Applications of Artificial Intelligence\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15741v2",
                "updated": "2024-08-09T08:06:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    6,
                    39,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-22T05:33:35Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    5,
                    33,
                    35,
                    5,
                    174,
                    0
                ],
                "title": "MT-Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level"
                },
                "summary": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/Ladder",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/Ladder"
                },
                "authors": [
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zijie Meng"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Data and code are available at https://github.com/fzp0424/Ladder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04919v1",
                "updated": "2024-08-09T08:01:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    1,
                    37,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T08:01:37Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    1,
                    37,
                    4,
                    222,
                    0
                ],
                "title": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost."
                },
                "authors": [
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Yingxia Shao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12459v2",
                "updated": "2024-08-09T07:54:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    54,
                    41,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-21T02:33:17Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    2,
                    33,
                    17,
                    1,
                    142,
                    0
                ],
                "title": "TrajCogn: Leveraging LLMs for Cognizing Movement Patterns and Travel\n  Purposes from Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajCogn: Leveraging LLMs for Cognizing Movement Patterns and Travel\n  Purposes from Trajectories"
                },
                "summary": "Spatio-temporal trajectories are crucial in various data mining tasks. It is\nimportant to develop a versatile trajectory learning method that performs\ndifferent tasks with high accuracy. This involves effectively extracting two\ncore aspects of information--movement patterns and travel purposes--from\ntrajectories. However, this is challenging due to limitations in model capacity\nand the quality and scale of trajectory datasets. Meanwhile, large language\nmodels (LLMs) have shown great success in versatility by training on\nlarge-scale, high-quality datasets. Given the similarities between trajectories\nand sentences, there's potential to leverage LLMs to develop an effective\ntrajectory learning method. However, standard LLMs are not designed to handle\nthe unique spatio-temporal features of trajectories and cannot extract movement\npatterns and travel purposes.\n  To address these challenges, we propose a model called TrajCogn that\neffectively utilizes LLMs to model trajectories. TrajCogn leverages the\nstrengths of LLMs to create a versatile trajectory learning approach while\naddressing the limitations of standard LLMs. First, TrajCogn incorporates a\nnovel trajectory semantic embedder that enables LLMs to process spatio-temporal\nfeatures and extract movement patterns and travel purposes. Second, TrajCogn\nintroduces a new trajectory prompt that integrates these patterns and purposes\ninto LLMs, allowing the model to adapt to various tasks. Extensive experiments\non two real-world datasets and two representative tasks demonstrate that\nTrajCogn successfully achieves its design goals. Codes are available at\nhttps://anonymous.4open.science/r/TrajCogn-5021.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal trajectories are crucial in various data mining tasks. It is\nimportant to develop a versatile trajectory learning method that performs\ndifferent tasks with high accuracy. This involves effectively extracting two\ncore aspects of information--movement patterns and travel purposes--from\ntrajectories. However, this is challenging due to limitations in model capacity\nand the quality and scale of trajectory datasets. Meanwhile, large language\nmodels (LLMs) have shown great success in versatility by training on\nlarge-scale, high-quality datasets. Given the similarities between trajectories\nand sentences, there's potential to leverage LLMs to develop an effective\ntrajectory learning method. However, standard LLMs are not designed to handle\nthe unique spatio-temporal features of trajectories and cannot extract movement\npatterns and travel purposes.\n  To address these challenges, we propose a model called TrajCogn that\neffectively utilizes LLMs to model trajectories. TrajCogn leverages the\nstrengths of LLMs to create a versatile trajectory learning approach while\naddressing the limitations of standard LLMs. First, TrajCogn incorporates a\nnovel trajectory semantic embedder that enables LLMs to process spatio-temporal\nfeatures and extract movement patterns and travel purposes. Second, TrajCogn\nintroduces a new trajectory prompt that integrates these patterns and purposes\ninto LLMs, allowing the model to adapt to various tasks. Extensive experiments\non two real-world datasets and two representative tasks demonstrate that\nTrajCogn successfully achieves its design goals. Codes are available at\nhttps://anonymous.4open.science/r/TrajCogn-5021."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhou"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Qisen Xu"
                    },
                    {
                        "name": "Shengnan Guo"
                    },
                    {
                        "name": "Jilin Hu"
                    },
                    {
                        "name": "Youfang Lin"
                    },
                    {
                        "name": "Huaiyu Wan"
                    }
                ],
                "author_detail": {
                    "name": "Huaiyu Wan"
                },
                "author": "Huaiyu Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04907v1",
                "updated": "2024-08-09T07:24:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    24,
                    12,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T07:24:12Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    24,
                    12,
                    4,
                    222,
                    0
                ],
                "title": "Causal Discovery of Linear Non-Gaussian Causal Models with Unobserved\n  Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Discovery of Linear Non-Gaussian Causal Models with Unobserved\n  Confounding"
                },
                "summary": "We consider linear non-Gaussian structural equation models that involve\nlatent confounding. In this setting, the causal structure is identifiable, but,\nin general, it is not possible to identify the specific causal effects.\nInstead, a finite number of different causal effects result in the same\nobservational distribution. Most existing algorithms for identifying these\ncausal effects use overcomplete independent component analysis (ICA), which\noften suffers from convergence to local optima. Furthermore, the number of\nlatent variables must be known a priori. To address these issues, we propose an\nalgorithm that operates recursively rather than using overcomplete ICA. The\nalgorithm first infers a source, estimates the effect of the source and its\nlatent parents on their descendants, and then eliminates their influence from\nthe data. For both source identification and effect size estimation, we use\nrank conditions on matrices formed from higher-order cumulants. We prove\nasymptotic correctness under the mild assumption that locally, the number of\nlatent variables never exceeds the number of observed variables. Simulation\nstudies demonstrate that our method achieves comparable performance to\novercomplete ICA even though it does not know the number of latents in advance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider linear non-Gaussian structural equation models that involve\nlatent confounding. In this setting, the causal structure is identifiable, but,\nin general, it is not possible to identify the specific causal effects.\nInstead, a finite number of different causal effects result in the same\nobservational distribution. Most existing algorithms for identifying these\ncausal effects use overcomplete independent component analysis (ICA), which\noften suffers from convergence to local optima. Furthermore, the number of\nlatent variables must be known a priori. To address these issues, we propose an\nalgorithm that operates recursively rather than using overcomplete ICA. The\nalgorithm first infers a source, estimates the effect of the source and its\nlatent parents on their descendants, and then eliminates their influence from\nthe data. For both source identification and effect size estimation, we use\nrank conditions on matrices formed from higher-order cumulants. We prove\nasymptotic correctness under the mild assumption that locally, the number of\nlatent variables never exceeds the number of observed variables. Simulation\nstudies demonstrate that our method achieves comparable performance to\novercomplete ICA even though it does not know the number of latents in advance."
                },
                "authors": [
                    {
                        "name": "Daniela Schkoda"
                    },
                    {
                        "name": "Elina Robeva"
                    },
                    {
                        "name": "Mathias Drton"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Drton"
                },
                "author": "Mathias Drton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04906v1",
                "updated": "2024-08-09T07:20:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    20,
                    15,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T07:20:15Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    20,
                    15,
                    4,
                    222,
                    0
                ],
                "title": "Towards a Generative Approach for Emotion Detection and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Generative Approach for Emotion Detection and Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance in\nmathematical and commonsense reasoning tasks using chain-of-thought (CoT)\nprompting techniques. But can they perform emotional reasoning by concatenating\n`Let's think step-by-step' to the input prompt? In this paper we investigate\nthis question along with introducing a novel approach to zero-shot emotion\ndetection and emotional reasoning using LLMs. Existing state of the art\nzero-shot approaches rely on textual entailment models to choose the most\nappropriate emotion label for an input text. We argue that this strongly\nrestricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead,\nwe propose framing the problem of emotion analysis as a generative\nquestion-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion\ndetection question step-by-step. Our paper is the first work on using a\ngenerative approach to jointly address the tasks of emotion detection and\nemotional reasoning for texts. We evaluate our approach on two popular emotion\ndetection datasets and also release the fine-grained emotion labels and\nexplanations for further training and fine-tuning of emotional reasoning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance in\nmathematical and commonsense reasoning tasks using chain-of-thought (CoT)\nprompting techniques. But can they perform emotional reasoning by concatenating\n`Let's think step-by-step' to the input prompt? In this paper we investigate\nthis question along with introducing a novel approach to zero-shot emotion\ndetection and emotional reasoning using LLMs. Existing state of the art\nzero-shot approaches rely on textual entailment models to choose the most\nappropriate emotion label for an input text. We argue that this strongly\nrestricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead,\nwe propose framing the problem of emotion analysis as a generative\nquestion-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion\ndetection question step-by-step. Our paper is the first work on using a\ngenerative approach to jointly address the tasks of emotion detection and\nemotional reasoning for texts. We evaluate our approach on two popular emotion\ndetection datasets and also release the fine-grained emotion labels and\nexplanations for further training and fine-tuning of emotional reasoning\nsystems."
                },
                "authors": [
                    {
                        "name": "Ankita Bhaumik"
                    },
                    {
                        "name": "Tomek Strzalkowski"
                    }
                ],
                "author_detail": {
                    "name": "Tomek Strzalkowski"
                },
                "author": "Tomek Strzalkowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04905v1",
                "updated": "2024-08-09T07:19:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    19,
                    53,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T07:19:53Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    19,
                    53,
                    4,
                    222,
                    0
                ],
                "title": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch\n  Tokens in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch\n  Tokens in Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs."
                },
                "authors": [
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Wuxia Bai"
                    },
                    {
                        "name": "Yuxi Li"
                    },
                    {
                        "name": "Mark Huasong Meng"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09007v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09007v4",
                "updated": "2024-08-09T07:07:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    7,
                    49,
                    4,
                    222,
                    0
                ],
                "published": "2023-12-14T14:57:58Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    14,
                    57,
                    58,
                    3,
                    348,
                    0
                ],
                "title": "LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution"
                },
                "summary": "Task-oriented communications are an important element in future intelligent\nIoT systems. Existing IoT systems, however, are limited in their capacity to\nhandle complex tasks, particularly in their interactions with humans to\naccomplish these tasks. In this paper, we present LLMind, an LLM-based\ntask-oriented AI agent framework that enables effective collaboration among IoT\ndevices, with humans communicating high-level verbal instructions, to perform\ncomplex tasks. Inspired by the functional specialization theory of the brain,\nour framework integrates an LLM with domain-specific AI modules, enhancing its\ncapabilities. Complex tasks, which may involve collaborations of multiple\ndomain-specific AI modules and IoT devices, are executed through a control\nscript generated by the LLM using a Language-Code transformation approach,\nwhich first converts language descriptions to an intermediate finite-state\nmachine (FSM) before final precise transformation to code. Furthermore, the\nframework incorporates a novel experience accumulation mechanism to enhance\nresponse speed and effectiveness, allowing the framework to evolve and become\nprogressively sophisticated through continuing user and machine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented communications are an important element in future intelligent\nIoT systems. Existing IoT systems, however, are limited in their capacity to\nhandle complex tasks, particularly in their interactions with humans to\naccomplish these tasks. In this paper, we present LLMind, an LLM-based\ntask-oriented AI agent framework that enables effective collaboration among IoT\ndevices, with humans communicating high-level verbal instructions, to perform\ncomplex tasks. Inspired by the functional specialization theory of the brain,\nour framework integrates an LLM with domain-specific AI modules, enhancing its\ncapabilities. Complex tasks, which may involve collaborations of multiple\ndomain-specific AI modules and IoT devices, are executed through a control\nscript generated by the LLM using a Language-Code transformation approach,\nwhich first converts language descriptions to an intermediate finite-state\nmachine (FSM) before final precise transformation to code. Furthermore, the\nframework incorporates a novel experience accumulation mechanism to enhance\nresponse speed and effectiveness, allowing the framework to evolve and become\nprogressively sophisticated through continuing user and machine interactions."
                },
                "authors": [
                    {
                        "name": "Hongwei Cui"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Qun Yang"
                    },
                    {
                        "name": "Yulin Shao"
                    },
                    {
                        "name": "Soung Chang Liew"
                    }
                ],
                "author_detail": {
                    "name": "Soung Chang Liew"
                },
                "author": "Soung Chang Liew",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09007v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09007v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04900v1",
                "updated": "2024-08-09T07:02:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    2,
                    18,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T07:02:18Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    2,
                    18,
                    4,
                    222,
                    0
                ],
                "title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural\n  Communication in Codenames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural\n  Communication in Codenames"
                },
                "summary": "Cultural differences in common ground may result in pragmatic failure and\nmisunderstandings during communication. We develop our method Rational Speech\nActs for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural\ndifferences in common ground. To measure the success of our method, we study\nRSA+C3 in the collaborative referential game of Codenames Duet and show that\nour method successfully improves collaboration between simulated players of\ndifferent cultures. Our contributions are threefold: (1) creating Codenames\nplayers using contrastive learning of an embedding space and LLM prompting that\nare aligned with human patterns of play, (2) studying culturally induced\ndifferences in common ground reflected in our trained models, and (3)\ndemonstrating that our method RSA+C3 can ease cross-cultural communication in\ngameplay by inferring sociocultural context from interaction. Our code is\npublicly available at github.com/icwhite/codenames.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural differences in common ground may result in pragmatic failure and\nmisunderstandings during communication. We develop our method Rational Speech\nActs for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural\ndifferences in common ground. To measure the success of our method, we study\nRSA+C3 in the collaborative referential game of Codenames Duet and show that\nour method successfully improves collaboration between simulated players of\ndifferent cultures. Our contributions are threefold: (1) creating Codenames\nplayers using contrastive learning of an embedding space and LLM prompting that\nare aligned with human patterns of play, (2) studying culturally induced\ndifferences in common ground reflected in our trained models, and (3)\ndemonstrating that our method RSA+C3 can ease cross-cultural communication in\ngameplay by inferring sociocultural context from interaction. Our code is\npublicly available at github.com/icwhite/codenames."
                },
                "authors": [
                    {
                        "name": "Isadora White"
                    },
                    {
                        "name": "Sashrika Pandey"
                    },
                    {
                        "name": "Michelle Pan"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Pan"
                },
                "author": "Michelle Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01258v2",
                "updated": "2024-08-09T06:49:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    6,
                    49,
                    50,
                    4,
                    222,
                    0
                ],
                "published": "2024-05-02T13:04:26Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    13,
                    4,
                    26,
                    3,
                    123,
                    0
                ],
                "title": "Towards Consistent Object Detection via LiDAR-Camera Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Consistent Object Detection via LiDAR-Camera Synergy"
                },
                "summary": "As human-machine interaction continues to evolve, the capacity for\nenvironmental perception is becoming increasingly crucial. Integrating the two\nmost common types of sensory data, images, and point clouds, can enhance\ndetection accuracy. Currently, there is no existing model capable of detecting\nan object's position in both point clouds and images while also determining\ntheir corresponding relationship. This information is invaluable for\nhuman-machine interactions, offering new possibilities for their enhancement.\nIn light of this, this paper introduces an end-to-end Consistency Object\nDetection (COD) algorithm framework that requires only a single forward\ninference to simultaneously obtain an object's position in both point clouds\nand images and establish their correlation. Furthermore, to assess the accuracy\nof the object correlation between point clouds and images, this paper proposes\na new evaluation metric, Consistency Precision (CP). To verify the\neffectiveness of the proposed framework, an extensive set of experiments has\nbeen conducted on the KITTI and DAIR-V2X datasets. The study also explored how\nthe proposed consistency detection method performs on images when the\ncalibration parameters between images and point clouds are disturbed, compared\nto existing post-processing methods. The experimental results demonstrate that\nthe proposed method exhibits excellent detection performance and robustness,\nachieving end-to-end consistency detection. The source code will be made\npublicly available at https://github.com/xifen523/COD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As human-machine interaction continues to evolve, the capacity for\nenvironmental perception is becoming increasingly crucial. Integrating the two\nmost common types of sensory data, images, and point clouds, can enhance\ndetection accuracy. Currently, there is no existing model capable of detecting\nan object's position in both point clouds and images while also determining\ntheir corresponding relationship. This information is invaluable for\nhuman-machine interactions, offering new possibilities for their enhancement.\nIn light of this, this paper introduces an end-to-end Consistency Object\nDetection (COD) algorithm framework that requires only a single forward\ninference to simultaneously obtain an object's position in both point clouds\nand images and establish their correlation. Furthermore, to assess the accuracy\nof the object correlation between point clouds and images, this paper proposes\na new evaluation metric, Consistency Precision (CP). To verify the\neffectiveness of the proposed framework, an extensive set of experiments has\nbeen conducted on the KITTI and DAIR-V2X datasets. The study also explored how\nthe proposed consistency detection method performs on images when the\ncalibration parameters between images and point clouds are disturbed, compared\nto existing post-processing methods. The experimental results demonstrate that\nthe proposed method exhibits excellent detection performance and robustness,\nachieving end-to-end consistency detection. The source code will be made\npublicly available at https://github.com/xifen523/COD."
                },
                "authors": [
                    {
                        "name": "Kai Luo"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Kefu Yi"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Wei Hao"
                    },
                    {
                        "name": "Rongdong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Rongdong Hu"
                },
                "author": "Rongdong Hu",
                "arxiv_comment": "Accepted to IEEE SMC 2024. The source code will be made publicly\n  available at https://github.com/xifen523/COD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07339v2",
                "updated": "2024-08-09T06:16:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    6,
                    16,
                    55,
                    4,
                    222,
                    0
                ],
                "published": "2024-01-14T18:12:03Z",
                "published_parsed": [
                    2024,
                    1,
                    14,
                    18,
                    12,
                    3,
                    6,
                    14,
                    0
                ],
                "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems\n  for Real-World Repo-level Coding Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems\n  for Real-World Repo-level Coding Challenges"
                },
                "summary": "Large Language Models (LLMs) have shown promise in automated code generation\nbut typically excel only in simpler tasks such as generating standalone code\nunits. Real-world software development, however, often involves complex code\nrepositories (named repo) with complex dependencies and extensive\ndocumentation. To fill this gap, our research pivots towards evaluating LLMs in\na more realistic setting -- real-world repo-level code generation. We introduce\nCodeAgentBench, a manually curated benchmark for repo-level code generation.\nThis benchmark comprises five high-quality Python projects, encompassing a\ntotal of 101 samples. We assess nine leading LLMs on repo-level tasks and\nobserve a decline in their performance. To tackle this, we present CodeAgent, a\nnovel LLM-based agent framework that employs external tools for effective\nrepo-level code generation. CodeAgent integrates five programming tools,\nenabling interaction with software artifacts for information retrieval, code\nsymbol navigation, and code testing. We implement four agent strategies to\noptimize these tools' usage. Our experiments on CodeAgentBench show that\nCodeAgent enhances LLM performance significantly, with improvements ranging\nfrom 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm\nCodeAgent's adaptability and efficacy across various code generation tasks.\nNotably, CodeAgent outperforms commercial products like Github Copilot,\nshowcasing superior accuracy and efficiency. These results demonstrate\nCodeAgent's robust capabilities in code generation, highlighting its potential\nfor real-world repo-level coding challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in automated code generation\nbut typically excel only in simpler tasks such as generating standalone code\nunits. Real-world software development, however, often involves complex code\nrepositories (named repo) with complex dependencies and extensive\ndocumentation. To fill this gap, our research pivots towards evaluating LLMs in\na more realistic setting -- real-world repo-level code generation. We introduce\nCodeAgentBench, a manually curated benchmark for repo-level code generation.\nThis benchmark comprises five high-quality Python projects, encompassing a\ntotal of 101 samples. We assess nine leading LLMs on repo-level tasks and\nobserve a decline in their performance. To tackle this, we present CodeAgent, a\nnovel LLM-based agent framework that employs external tools for effective\nrepo-level code generation. CodeAgent integrates five programming tools,\nenabling interaction with software artifacts for information retrieval, code\nsymbol navigation, and code testing. We implement four agent strategies to\noptimize these tools' usage. Our experiments on CodeAgentBench show that\nCodeAgent enhances LLM performance significantly, with improvements ranging\nfrom 18.1\\% to 250\\%. Further tests on the HumanEval benchmark confirm\nCodeAgent's adaptability and efficacy across various code generation tasks.\nNotably, CodeAgent outperforms commercial products like Github Copilot,\nshowcasing superior accuracy and efficiency. These results demonstrate\nCodeAgent's robust capabilities in code generation, highlighting its potential\nfor real-world repo-level coding challenges."
                },
                "authors": [
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xianjie Shi"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Camera Ready version for ACL 2024 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19115v2",
                "updated": "2024-08-09T06:15:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    6,
                    15,
                    19,
                    4,
                    222,
                    0
                ],
                "published": "2024-03-28T03:11:38Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    3,
                    11,
                    38,
                    3,
                    88,
                    0
                ],
                "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position"
                },
                "summary": "Addressing the limitation of context length in large language models for\ncode-related tasks is the primary focus of this paper. Existing LLMs are\nconstrained by their pre-trained context lengths, leading to performance issues\nin handling long complex code sequences. Inspired by how human programmers\nnavigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a\nnovel approach that enhances the traditional rotary position embedding into a\nhierarchical format based on the hierarchical structure of source code. HiRoPE\noffers easy integration into existing LLMs without extra training costs. Our\nmethod is extensively evaluated with various LLMs, demonstrating stable\nperformance in tasks such as language modeling and long code completion. We\nalso introduce a new long code understanding task with real-world code\nprojects, in hopes of promoting further development in this code-related field.\nTheoretically and experimentally, we find that HiRoPE also addresses the\nout-of-distribution issue in position encoding. Our HiRoPE significantly\nexpands the context length capabilities of LLMs, enabling inference at lengths\nexponentially greater than the training length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the limitation of context length in large language models for\ncode-related tasks is the primary focus of this paper. Existing LLMs are\nconstrained by their pre-trained context lengths, leading to performance issues\nin handling long complex code sequences. Inspired by how human programmers\nnavigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a\nnovel approach that enhances the traditional rotary position embedding into a\nhierarchical format based on the hierarchical structure of source code. HiRoPE\noffers easy integration into existing LLMs without extra training costs. Our\nmethod is extensively evaluated with various LLMs, demonstrating stable\nperformance in tasks such as language modeling and long code completion. We\nalso introduce a new long code understanding task with real-world code\nprojects, in hopes of promoting further development in this code-related field.\nTheoretically and experimentally, we find that HiRoPE also addresses the\nout-of-distribution issue in position encoding. Our HiRoPE significantly\nexpands the context length capabilities of LLMs, enabling inference at lengths\nexponentially greater than the training length."
                },
                "authors": [
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Huangzhao Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Camera Ready version for ACL 2024 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04872v1",
                "updated": "2024-08-09T05:25:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    25,
                    17,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:25:17Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    25,
                    17,
                    4,
                    222,
                    0
                ],
                "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation"
                },
                "summary": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "16 pages, 2 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v1",
                "updated": "2024-08-09T05:20:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04868v1",
                "updated": "2024-08-09T05:13:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    13,
                    7,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:13:07Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    13,
                    7,
                    4,
                    222,
                    0
                ],
                "title": "ChatGPT Meets Iris Biometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Meets Iris Biometrics"
                },
                "summary": "This study utilizes the advanced capabilities of the GPT-4 multimodal Large\nLanguage Model (LLM) to explore its potential in iris recognition - a field\nless common and more specialized than face recognition. By focusing on this\nniche yet crucial area, we investigate how well AI tools like ChatGPT can\nunderstand and analyze iris images. Through a series of meticulously designed\nexperiments employing a zero-shot learning approach, the capabilities of\nChatGPT-4 was assessed across various challenging conditions including diverse\ndatasets, presentation attacks, occlusions such as glasses, and other\nreal-world variations. The findings convey ChatGPT-4's remarkable adaptability\nand precision, revealing its proficiency in identifying distinctive iris\nfeatures, while also detecting subtle effects like makeup on iris recognition.\nA comparative analysis with Gemini Advanced - Google's AI model - highlighted\nChatGPT-4's better performance and user experience in complex iris analysis\ntasks. This research not only validates the use of LLMs for specialized\nbiometric applications but also emphasizes the importance of nuanced query\nframing and interaction design in extracting significant insights from\nbiometric data. Our findings suggest a promising path for future research and\nthe development of more adaptable, efficient, robust and interactive biometric\nsecurity solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study utilizes the advanced capabilities of the GPT-4 multimodal Large\nLanguage Model (LLM) to explore its potential in iris recognition - a field\nless common and more specialized than face recognition. By focusing on this\nniche yet crucial area, we investigate how well AI tools like ChatGPT can\nunderstand and analyze iris images. Through a series of meticulously designed\nexperiments employing a zero-shot learning approach, the capabilities of\nChatGPT-4 was assessed across various challenging conditions including diverse\ndatasets, presentation attacks, occlusions such as glasses, and other\nreal-world variations. The findings convey ChatGPT-4's remarkable adaptability\nand precision, revealing its proficiency in identifying distinctive iris\nfeatures, while also detecting subtle effects like makeup on iris recognition.\nA comparative analysis with Gemini Advanced - Google's AI model - highlighted\nChatGPT-4's better performance and user experience in complex iris analysis\ntasks. This research not only validates the use of LLMs for specialized\nbiometric applications but also emphasizes the importance of nuanced query\nframing and interaction design in extracting significant insights from\nbiometric data. Our findings suggest a promising path for future research and\nthe development of more adaptable, efficient, robust and interactive biometric\nsecurity solutions."
                },
                "authors": [
                    {
                        "name": "Parisa Farmanifard"
                    },
                    {
                        "name": "Arun Ross"
                    }
                ],
                "author_detail": {
                    "name": "Arun Ross"
                },
                "author": "Arun Ross",
                "arxiv_comment": "Published at IJCB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04867v1",
                "updated": "2024-08-09T05:13:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    13,
                    3,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:13:03Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    13,
                    3,
                    4,
                    222,
                    0
                ],
                "title": "An Evaluation of Standard Statistical Models and LLMs on Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of Standard Statistical Models and LLMs on Time Series\n  Forecasting"
                },
                "summary": "This research examines the use of Large Language Models (LLMs) in predicting\ntime series, with a specific focus on the LLMTIME model. Despite the\nestablished effectiveness of LLMs in tasks such as text generation, language\ntranslation, and sentiment analysis, this study highlights the key challenges\nthat large language models encounter in the context of time series prediction.\nWe assess the performance of LLMTIME across multiple datasets and introduce\nclassical almost periodic functions as time series to gauge its effectiveness.\nThe empirical results indicate that while large language models can perform\nwell in zero-shot forecasting for certain datasets, their predictive accuracy\ndiminishes notably when confronted with diverse time series data and\ntraditional signals. The primary finding of this study is that the predictive\ncapacity of LLMTIME, similar to other LLMs, significantly deteriorates when\ndealing with time series data that contain both periodic and trend components,\nas well as when the signal comprises complex frequency components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research examines the use of Large Language Models (LLMs) in predicting\ntime series, with a specific focus on the LLMTIME model. Despite the\nestablished effectiveness of LLMs in tasks such as text generation, language\ntranslation, and sentiment analysis, this study highlights the key challenges\nthat large language models encounter in the context of time series prediction.\nWe assess the performance of LLMTIME across multiple datasets and introduce\nclassical almost periodic functions as time series to gauge its effectiveness.\nThe empirical results indicate that while large language models can perform\nwell in zero-shot forecasting for certain datasets, their predictive accuracy\ndiminishes notably when confronted with diverse time series data and\ntraditional signals. The primary finding of this study is that the predictive\ncapacity of LLMTIME, similar to other LLMs, significantly deteriorates when\ndealing with time series data that contain both periodic and trend components,\nas well as when the signal comprises complex frequency components."
                },
                "authors": [
                    {
                        "name": "Rui Cao"
                    },
                    {
                        "name": "Qiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiao Wang"
                },
                "author": "Qiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04866v1",
                "updated": "2024-08-09T05:05:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    5,
                    39,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:05:39Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    5,
                    39,
                    4,
                    222,
                    0
                ],
                "title": "Network and interaction models for data with hierarchical granularity\n  via fragmentation and coagulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network and interaction models for data with hierarchical granularity\n  via fragmentation and coagulation"
                },
                "summary": "We introduce a nested family of Bayesian nonparametric models for network and\ninteraction data with a hierarchical granularity structure that naturally\narises through finer and coarser population labelings. In the case of network\ndata, the structure is easily visualized by merging and shattering vertices,\nwhile respecting the edge structure. We further develop Bayesian inference\nprocedures for the model family, and apply them to synthetic and real data. The\nfamily provides a connection of practical and theoretical interest between the\nHollywood model of Crane and Dempsey, and the generalized-gamma graphex model\nof Caron and Fox. A key ingredient for the construction of the family is\nfragmentation and coagulation duality for integer partitions, and for this we\ndevelop novel duality relations that generalize those of Pitman and Dong,\nGoldschmidt and Martin. The duality is also crucially used in our inferential\nprocedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a nested family of Bayesian nonparametric models for network and\ninteraction data with a hierarchical granularity structure that naturally\narises through finer and coarser population labelings. In the case of network\ndata, the structure is easily visualized by merging and shattering vertices,\nwhile respecting the edge structure. We further develop Bayesian inference\nprocedures for the model family, and apply them to synthetic and real data. The\nfamily provides a connection of practical and theoretical interest between the\nHollywood model of Crane and Dempsey, and the generalized-gamma graphex model\nof Caron and Fox. A key ingredient for the construction of the family is\nfragmentation and coagulation duality for integer partitions, and for this we\ndevelop novel duality relations that generalize those of Pitman and Dong,\nGoldschmidt and Martin. The duality is also crucially used in our inferential\nprocedures."
                },
                "authors": [
                    {
                        "name": "Lancelot F. James"
                    },
                    {
                        "name": "Juho Lee"
                    },
                    {
                        "name": "Nathan Ross"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Ross"
                },
                "author": "Nathan Ross",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00575v2",
                "updated": "2024-08-09T04:33:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    4,
                    33,
                    58,
                    4,
                    222,
                    0
                ],
                "published": "2023-12-01T13:31:02Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    13,
                    31,
                    2,
                    4,
                    335,
                    0
                ],
                "title": "Instruction-tuning Aligns LLMs to the Human Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuning Aligns LLMs to the Human Brain"
                },
                "summary": "Instruction-tuning is a widely adopted finetuning method that enables large\nlanguage models (LLMs) to generate output that more closely resembles human\nresponses. However, no studies have shown that instruction-tuning actually\nteaches LLMs to process language in a similar manner as humans. We investigate\nthe effect of instruction-tuning on aligning LLM and human language processing\nmechanisms in two ways: (1) brain alignment, the similarity of LLM internal\nrepresentations to neural activity in the human language system, and (2)\nbehavioral alignment, the similarity of LLM and human behavior on a reading\ntask. We assess 25 vanilla and instruction-tuned LLMs on three datasets\ninvolving humans reading naturalistic stories and sentences, and find that\ninstruction-tuning generally enhances brain alignment (~6%), but has no similar\neffect on behavioral alignment. To identify factors underlying this improvement\nin brain alignment, we compute correlations between brain alignment and various\nLLM properties, such as model size, problem-solving, and world knowledge\nunderstanding. Notably, we find a strong positive correlation between brain\nalignment and model size (r = 0.95), as well as performance on tasks requiring\nworld knowledge (r = 0.81). Our results demonstrate that instruction-tuning\nLLMs improves both world knowledge representations and brain alignment,\nsuggesting that the mechanisms that encode world knowledge in LLMs also improve\nrepresentational alignment to the human brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuning is a widely adopted finetuning method that enables large\nlanguage models (LLMs) to generate output that more closely resembles human\nresponses. However, no studies have shown that instruction-tuning actually\nteaches LLMs to process language in a similar manner as humans. We investigate\nthe effect of instruction-tuning on aligning LLM and human language processing\nmechanisms in two ways: (1) brain alignment, the similarity of LLM internal\nrepresentations to neural activity in the human language system, and (2)\nbehavioral alignment, the similarity of LLM and human behavior on a reading\ntask. We assess 25 vanilla and instruction-tuned LLMs on three datasets\ninvolving humans reading naturalistic stories and sentences, and find that\ninstruction-tuning generally enhances brain alignment (~6%), but has no similar\neffect on behavioral alignment. To identify factors underlying this improvement\nin brain alignment, we compute correlations between brain alignment and various\nLLM properties, such as model size, problem-solving, and world knowledge\nunderstanding. Notably, we find a strong positive correlation between brain\nalignment and model size (r = 0.95), as well as performance on tasks requiring\nworld knowledge (r = 0.81). Our results demonstrate that instruction-tuning\nLLMs improves both world knowledge representations and brain alignment,\nsuggesting that the mechanisms that encode world knowledge in LLMs also improve\nrepresentational alignment to the human brain."
                },
                "authors": [
                    {
                        "name": "Khai Loong Aw"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Martin Schrimpf"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20076v3",
                "updated": "2024-08-09T03:52:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    3,
                    52,
                    31,
                    4,
                    222,
                    0
                ],
                "published": "2024-06-28T17:38:18Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    38,
                    18,
                    4,
                    180,
                    0
                ],
                "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model"
                },
                "summary": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Longjin Ran"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Preprint. Update: SAM-2 and Visualizations. Code and models are\n  available at: https://github.com/hustvl/EVF-SAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.06767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.06767v2",
                "updated": "2024-08-09T03:44:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    3,
                    44,
                    50,
                    4,
                    222,
                    0
                ],
                "published": "2023-08-13T13:34:04Z",
                "published_parsed": [
                    2023,
                    8,
                    13,
                    13,
                    34,
                    4,
                    6,
                    225,
                    0
                ],
                "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis,\n  and Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis,\n  and Recommendations"
                },
                "summary": "Modern deep neural networks, particularly recent large language models, come\nwith massive model sizes that require significant computational and storage\nresources. To enable the deployment of modern models on resource-constrained\nenvironments and accelerate inference time, researchers have increasingly\nexplored pruning techniques as a popular research direction in neural network\ncompression. However, there is a dearth of up-to-date comprehensive review\npapers on pruning. To address this issue, in this survey, we provide a\ncomprehensive review of existing research works on deep neural network pruning\nin a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to\nprune, and 4) fusion of pruning and other compression techniques. We then\nprovide a thorough comparative analysis of eight pairs of contrast settings for\npruning and explore emerging topics, including pruning for large language\nmodels, large multimodal models, post-training pruning, and different\nsupervision levels for pruning to shed light on the commonalities and\ndifferences of existing methods and lay the foundation for further method\ndevelopment. To facilitate future research, we build a curated collection of\ndatasets, networks, and evaluations on different applications. Finally, we\nprovide valuable recommendations on selecting pruning methods and prospect\nseveral promising research directions. We build a repository at\nhttps://github.com/hrcheng1066/awesome-pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern deep neural networks, particularly recent large language models, come\nwith massive model sizes that require significant computational and storage\nresources. To enable the deployment of modern models on resource-constrained\nenvironments and accelerate inference time, researchers have increasingly\nexplored pruning techniques as a popular research direction in neural network\ncompression. However, there is a dearth of up-to-date comprehensive review\npapers on pruning. To address this issue, in this survey, we provide a\ncomprehensive review of existing research works on deep neural network pruning\nin a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to\nprune, and 4) fusion of pruning and other compression techniques. We then\nprovide a thorough comparative analysis of eight pairs of contrast settings for\npruning and explore emerging topics, including pruning for large language\nmodels, large multimodal models, post-training pruning, and different\nsupervision levels for pruning to shed light on the commonalities and\ndifferences of existing methods and lay the foundation for further method\ndevelopment. To facilitate future research, we build a curated collection of\ndatasets, networks, and evaluations on different applications. Finally, we\nprovide valuable recommendations on selecting pruning methods and prospect\nseveral promising research directions. We build a repository at\nhttps://github.com/hrcheng1066/awesome-pruning."
                },
                "authors": [
                    {
                        "name": "Hongrong Cheng"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Javen Qinfeng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Javen Qinfeng Shi"
                },
                "author": "Javen Qinfeng Shi",
                "arxiv_comment": "IEEE TPAMI major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.06767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.06767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17150v2",
                "updated": "2024-08-09T03:14:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    3,
                    14,
                    51,
                    4,
                    222,
                    0
                ],
                "published": "2024-07-24T10:49:19Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    10,
                    49,
                    19,
                    2,
                    206,
                    0
                ],
                "title": "SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle"
                },
                "summary": "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components."
                },
                "authors": [
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Guoqiang Jin"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Jiangheng Huang"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04835v1",
                "updated": "2024-08-09T03:12:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    3,
                    12,
                    53,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T03:12:53Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    3,
                    12,
                    53,
                    4,
                    222,
                    0
                ],
                "title": "Next-Generation Wi-Fi Networks with Generative AI: Design and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Generation Wi-Fi Networks with Generative AI: Design and Insights"
                },
                "summary": "Generative artificial intelligence (GAI), known for its powerful capabilities\nin image and text processing, also holds significant promise for the design and\nperformance enhancement of future wireless networks. In this article, we\nexplore the transformative potential of GAI in next-generation Wi-Fi networks,\nexploiting its advanced capabilities to address key challenges and improve\noverall network performance. We begin by reviewing the development of major\nWi-Fi generations and illustrating the challenges that future Wi-Fi networks\nmay encounter. We then introduce typical GAI models and detail their potential\ncapabilities in Wi-Fi network optimization, performance enhancement, and other\napplications. Furthermore, we present a case study wherein we propose a\nretrieval-augmented LLM (RA-LLM)-enabled Wi-Fi design framework that aids in\nproblem formulation, which is subsequently solved using a generative diffusion\nmodel (GDM)-based deep reinforcement learning (DRL) framework to optimize\nvarious network parameters. Numerical results demonstrate the effectiveness of\nour proposed algorithm in high-density deployment scenarios. Finally, we\nprovide some potential future research directions for GAI-assisted Wi-Fi\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GAI), known for its powerful capabilities\nin image and text processing, also holds significant promise for the design and\nperformance enhancement of future wireless networks. In this article, we\nexplore the transformative potential of GAI in next-generation Wi-Fi networks,\nexploiting its advanced capabilities to address key challenges and improve\noverall network performance. We begin by reviewing the development of major\nWi-Fi generations and illustrating the challenges that future Wi-Fi networks\nmay encounter. We then introduce typical GAI models and detail their potential\ncapabilities in Wi-Fi network optimization, performance enhancement, and other\napplications. Furthermore, we present a case study wherein we propose a\nretrieval-augmented LLM (RA-LLM)-enabled Wi-Fi design framework that aids in\nproblem formulation, which is subsequently solved using a generative diffusion\nmodel (GDM)-based deep reinforcement learning (DRL) framework to optimize\nvarious network parameters. Numerical results demonstrate the effectiveness of\nour proposed algorithm in high-density deployment scenarios. Finally, we\nprovide some potential future research directions for GAI-assisted Wi-Fi\nnetworks."
                },
                "authors": [
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Xuming Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Tie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tie Liu"
                },
                "author": "Tie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04826v1",
                "updated": "2024-08-09T02:55:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    55,
                    25,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T02:55:25Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    55,
                    25,
                    4,
                    222,
                    0
                ],
                "title": "Geo-UNet: A Geometrically Constrained Neural Framework for\n  Clinical-Grade Lumen Segmentation in Intravascular Ultrasound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-UNet: A Geometrically Constrained Neural Framework for\n  Clinical-Grade Lumen Segmentation in Intravascular Ultrasound"
                },
                "summary": "Precisely estimating lumen boundaries in intravascular ultrasound (IVUS) is\nneeded for sizing interventional stents to treat deep vein thrombosis (DVT).\nUnfortunately, current segmentation networks like the UNet lack the precision\nneeded for clinical adoption in IVUS workflows. This arises due to the\ndifficulty of automatically learning accurate lumen contour from limited\ntraining data while accounting for the radial geometry of IVUS imaging. We\npropose the Geo-UNet framework to address these issues via a design informed by\nthe geometry of the lumen contour segmentation task. We first convert the input\ndata and segmentation targets from Cartesian to polar coordinates. Starting\nfrom a convUNet feature extractor, we propose a two-task setup, one for\nconventional pixel-wise labeling and the other for single boundary\nlumen-contour localization. We directly combine the two predictions by passing\nthe predicted lumen contour through a new activation (named CDFeLU) to filter\nout spurious pixel-wise predictions. Our unified loss function carefully\nbalances area-based, distance-based, and contour-based penalties to provide\nnear clinical-grade generalization in unseen patient data. We also introduce a\nlightweight, inference-time technique to enhance segmentation smoothness. The\nefficacy of our framework on a venous IVUS dataset is shown against\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precisely estimating lumen boundaries in intravascular ultrasound (IVUS) is\nneeded for sizing interventional stents to treat deep vein thrombosis (DVT).\nUnfortunately, current segmentation networks like the UNet lack the precision\nneeded for clinical adoption in IVUS workflows. This arises due to the\ndifficulty of automatically learning accurate lumen contour from limited\ntraining data while accounting for the radial geometry of IVUS imaging. We\npropose the Geo-UNet framework to address these issues via a design informed by\nthe geometry of the lumen contour segmentation task. We first convert the input\ndata and segmentation targets from Cartesian to polar coordinates. Starting\nfrom a convUNet feature extractor, we propose a two-task setup, one for\nconventional pixel-wise labeling and the other for single boundary\nlumen-contour localization. We directly combine the two predictions by passing\nthe predicted lumen contour through a new activation (named CDFeLU) to filter\nout spurious pixel-wise predictions. Our unified loss function carefully\nbalances area-based, distance-based, and contour-based penalties to provide\nnear clinical-grade generalization in unseen patient data. We also introduce a\nlightweight, inference-time technique to enhance segmentation smoothness. The\nefficacy of our framework on a venous IVUS dataset is shown against\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Niharika S. D'Souza"
                    },
                    {
                        "name": "Akshith Mandepally"
                    },
                    {
                        "name": "Patrick Henninger"
                    },
                    {
                        "name": "Satyananda Kashyap"
                    },
                    {
                        "name": "Neerav Karani"
                    },
                    {
                        "name": "Neel Dey"
                    },
                    {
                        "name": "Marcos Zachary"
                    },
                    {
                        "name": "Raed Rizq"
                    },
                    {
                        "name": "Paul Chouinard"
                    },
                    {
                        "name": "Polina Golland"
                    },
                    {
                        "name": "Tanveer F. Syeda-Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Tanveer F. Syeda-Mahmood"
                },
                "author": "Tanveer F. Syeda-Mahmood",
                "arxiv_comment": "Accepted into the 15th workshop on Machine Learning in Medical\n  Imaging at MICCAI 2024. (* indicates equal contribution)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04821v1",
                "updated": "2024-08-09T02:27:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    27,
                    25,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T02:27:25Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    27,
                    25,
                    4,
                    222,
                    0
                ],
                "title": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive\n  Controller (MPC) for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive\n  Controller (MPC) for Autonomous Driving"
                },
                "summary": "Motivated by the emergent reasoning capabilities of Vision Language Models\n(VLMs) and its potential to improve the comprehensibility of autonomous driving\nsystems, this paper introduces a closed-loop autonomous driving controller\ncalled VLM-MPC, which combines a VLM for high-level decision-making and a Model\nPredictive Controller (MPC) for low-level vehicle control. The proposed VLM-MPC\nsystem is structurally divided into two asynchronous components: an upper-level\nVLM and a lower-level MPC. The upper layer VLM generates driving parameters for\nlower-level control based on front camera images, ego vehicle state, traffic\nenvironment conditions, and reference memory. The lower-level MPC controls the\nvehicle in real-time using these parameters, considering engine lag and\nproviding state feedback to the entire system. Experiments based on the\nnuScenes dataset validated the effectiveness of the proposed VLM-MPC system\nacross various scenarios (e.g., night, rain, intersections). Results showed\nthat the VLM-MPC system consistently outperformed baseline models in terms of\nsafety and driving comfort. By comparing behaviors under different weather\nconditions and scenarios, we demonstrated the VLM's ability to understand the\nenvironment and make reasonable inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the emergent reasoning capabilities of Vision Language Models\n(VLMs) and its potential to improve the comprehensibility of autonomous driving\nsystems, this paper introduces a closed-loop autonomous driving controller\ncalled VLM-MPC, which combines a VLM for high-level decision-making and a Model\nPredictive Controller (MPC) for low-level vehicle control. The proposed VLM-MPC\nsystem is structurally divided into two asynchronous components: an upper-level\nVLM and a lower-level MPC. The upper layer VLM generates driving parameters for\nlower-level control based on front camera images, ego vehicle state, traffic\nenvironment conditions, and reference memory. The lower-level MPC controls the\nvehicle in real-time using these parameters, considering engine lag and\nproviding state feedback to the entire system. Experiments based on the\nnuScenes dataset validated the effectiveness of the proposed VLM-MPC system\nacross various scenarios (e.g., night, rain, intersections). Results showed\nthat the VLM-MPC system consistently outperformed baseline models in terms of\nsafety and driving comfort. By comparing behaviors under different weather\nconditions and scenarios, we demonstrated the VLM's ability to understand the\nenvironment and make reasonable inferences."
                },
                "authors": [
                    {
                        "name": "Keke Long"
                    },
                    {
                        "name": "Haotian Shi"
                    },
                    {
                        "name": "Jiaxi Liu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Li"
                },
                "author": "Xiaopeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04820v1",
                "updated": "2024-08-09T02:22:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    22,
                    51,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T02:22:51Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    2,
                    22,
                    51,
                    4,
                    222,
                    0
                ],
                "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Outlines for Code: Literate Programming in the LLM Era"
                },
                "summary": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and the difficult task of malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and the difficult task of malware detection."
                },
                "authors": [
                    {
                        "name": "Kensen Shi"
                    },
                    {
                        "name": "Deniz Altınbüken"
                    },
                    {
                        "name": "Saswat Anand"
                    },
                    {
                        "name": "Mihai Christodorescu"
                    },
                    {
                        "name": "Katja Grünwedel"
                    },
                    {
                        "name": "Alexa Koenings"
                    },
                    {
                        "name": "Sai Naidu"
                    },
                    {
                        "name": "Anurag Pathak"
                    },
                    {
                        "name": "Marc Rasi"
                    },
                    {
                        "name": "Fredde Ribeiro"
                    },
                    {
                        "name": "Brandon Ruffin"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Maxim Tabachnyk"
                    },
                    {
                        "name": "Sara Toth"
                    },
                    {
                        "name": "Roy Tu"
                    },
                    {
                        "name": "Tobias Welp"
                    },
                    {
                        "name": "Pengcheng Yin"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Charles Sutton"
                    }
                ],
                "author_detail": {
                    "name": "Charles Sutton"
                },
                "author": "Charles Sutton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]