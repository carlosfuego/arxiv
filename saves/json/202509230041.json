[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v3",
                "updated": "2025-09-19T17:04:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    4,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Bálint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v1",
                "updated": "2025-09-15T11:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.16203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16203v1",
                "updated": "2025-09-19T17:59:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    57,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:59:57Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    57,
                    4,
                    262,
                    0
                ],
                "title": "Inverting Trojans in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverting Trojans in LLMs"
                },
                "summary": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases."
                },
                "authors": [
                    {
                        "name": "Zhengxing Li"
                    },
                    {
                        "name": "Guangmingmei Yang"
                    },
                    {
                        "name": "Jayaram Raghuram"
                    },
                    {
                        "name": "David J. Miller"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16202v1",
                "updated": "2025-09-19T17:59:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    12,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:59:12Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    12,
                    4,
                    262,
                    0
                ],
                "title": "Sound-Horizon-Agnostic Inference of the Hubble Constant and Neutrino\n  Mass from BAO, CMB Lensing, and Galaxy Weak Lensing and Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-Horizon-Agnostic Inference of the Hubble Constant and Neutrino\n  Mass from BAO, CMB Lensing, and Galaxy Weak Lensing and Clustering"
                },
                "summary": "We present a sound-horizon-agnostic determination of the Hubble constant,\n$H_0$, by combining DESI DR2 baryon acoustic oscillation (BAO) data with the\nlatest cosmic microwave background (CMB) lensing measurements from Planck, ACT,\nand SPT-3G, the angular size of the CMB acoustic scale, Dark Energy Survey\nYear-3 ($3\\times2$-pt) galaxy weak lensing and clustering correlations, and the\nPantheon+ supernova sample. In this analysis, the sound horizon at the drag\nepoch, $r_d$, is treated as a free parameter, avoiding assumptions about\nearly-Universe physics. By combining uncalibrated comoving distances from BAO\nand supernovae with constraints on the matter density $\\Omega_m h^2$ from CMB\nand galaxy lensing/clustering, we break the $r_d$-$H_0$ degeneracy and obtain\n$H_0 = 70.0 \\pm 1.7$ km/s/Mpc when the sum of the neutrino masses is fixed at\n$\\Sigma m_\\nu = 0.06$ eV. With a conservative prior on the amplitude of\nprimordial fluctuations, $A_s$, we find $H_0 = 70.03 \\pm 0.97$ km/s/Mpc and\n$r_d = 144.8 \\pm 1.6$ Mpc. Allowing $\\Sigma m_\\nu$ to vary yields $H_0 =\n75.3^{+3.3}_{-4.0}$ km/s/Mpc and $\\Sigma m_\\nu = 0.55^{+0.23}_{-0.37}$ ($<1.11$\neV) at 68% (95%) CL, and $H_0 = 73.9 \\pm 2.2$ km/s/Mpc with $\\Sigma m_\\nu =\n0.46^{+0.21}_{-0.25}$ ($=0.46^{+0.40}_{-0.45}$ eV) at 68% (95%) CL when a prior\non $A_s$ is applied. Forecasts for the completed DESI BAO program, combined\nwith Simons-Observatory-like CMB lensing, next-generation $3\\times2$-pt data,\nand expanded supernova samples predict $\\sigma(H_0) \\simeq 0.67$ km/s/Mpc with\nfixed $\\Sigma m_\\nu$, and $\\sigma(H_0) \\simeq 1.1$ km/s/Mpc with $\\Sigma m_\\nu\n< 0.133$ ($<0.263$) eV at 68% (95%) CL when the neutrino mass is varied. As the\nprecision of BAO, CMB lensing, and galaxy lensing/clustering improve, this\n$r_d$-agnostic framework will provide an independent test of the need for new\nphysics at recombination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a sound-horizon-agnostic determination of the Hubble constant,\n$H_0$, by combining DESI DR2 baryon acoustic oscillation (BAO) data with the\nlatest cosmic microwave background (CMB) lensing measurements from Planck, ACT,\nand SPT-3G, the angular size of the CMB acoustic scale, Dark Energy Survey\nYear-3 ($3\\times2$-pt) galaxy weak lensing and clustering correlations, and the\nPantheon+ supernova sample. In this analysis, the sound horizon at the drag\nepoch, $r_d$, is treated as a free parameter, avoiding assumptions about\nearly-Universe physics. By combining uncalibrated comoving distances from BAO\nand supernovae with constraints on the matter density $\\Omega_m h^2$ from CMB\nand galaxy lensing/clustering, we break the $r_d$-$H_0$ degeneracy and obtain\n$H_0 = 70.0 \\pm 1.7$ km/s/Mpc when the sum of the neutrino masses is fixed at\n$\\Sigma m_\\nu = 0.06$ eV. With a conservative prior on the amplitude of\nprimordial fluctuations, $A_s$, we find $H_0 = 70.03 \\pm 0.97$ km/s/Mpc and\n$r_d = 144.8 \\pm 1.6$ Mpc. Allowing $\\Sigma m_\\nu$ to vary yields $H_0 =\n75.3^{+3.3}_{-4.0}$ km/s/Mpc and $\\Sigma m_\\nu = 0.55^{+0.23}_{-0.37}$ ($<1.11$\neV) at 68% (95%) CL, and $H_0 = 73.9 \\pm 2.2$ km/s/Mpc with $\\Sigma m_\\nu =\n0.46^{+0.21}_{-0.25}$ ($=0.46^{+0.40}_{-0.45}$ eV) at 68% (95%) CL when a prior\non $A_s$ is applied. Forecasts for the completed DESI BAO program, combined\nwith Simons-Observatory-like CMB lensing, next-generation $3\\times2$-pt data,\nand expanded supernova samples predict $\\sigma(H_0) \\simeq 0.67$ km/s/Mpc with\nfixed $\\Sigma m_\\nu$, and $\\sigma(H_0) \\simeq 1.1$ km/s/Mpc with $\\Sigma m_\\nu\n< 0.133$ ($<0.263$) eV at 68% (95%) CL when the neutrino mass is varied. As the\nprecision of BAO, CMB lensing, and galaxy lensing/clustering improve, this\n$r_d$-agnostic framework will provide an independent test of the need for new\nphysics at recombination."
                },
                "authors": [
                    {
                        "name": "Helena García Escudero"
                    },
                    {
                        "name": "Seyed Hamidreza Mirpoorian"
                    },
                    {
                        "name": "Levon Pogosian"
                    }
                ],
                "author_detail": {
                    "name": "Levon Pogosian"
                },
                "author": "Levon Pogosian",
                "arxiv_comment": "14 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16198v1",
                "updated": "2025-09-19T17:58:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:58:14Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    14,
                    4,
                    262,
                    0
                ],
                "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation"
                },
                "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization."
                },
                "authors": [
                    {
                        "name": "Jane Luo"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Steven Liu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Chengyu Yin"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16197v1",
                "updated": "2025-09-19T17:58:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    0,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:58:00Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    0,
                    4,
                    262,
                    0
                ],
                "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer"
                },
                "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer."
                },
                "authors": [
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Jialing Tong"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Zhenbang Wang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Wenze Hu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Zhifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Chen"
                },
                "author": "Zhifeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09723v3",
                "updated": "2025-09-19T17:56:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    56,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-13T21:10:56Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    21,
                    10,
                    56,
                    6,
                    103,
                    0
                ],
                "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM\n  Agents"
                },
                "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns."
                },
                "authors": [
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Ting-Yao Hsu"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Limeng Cui"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "William Headean"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Akash Veeragouni"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "Jessie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jessie Wang"
                },
                "author": "Jessie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09407v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09407v3",
                "updated": "2025-09-19T17:52:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    52,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-13T02:34:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    2,
                    34,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents"
                },
                "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate their new designs. But what about\nevaluating and iterating the usability testing study design itself? Recent\nadvances in Large Language Model-simulated Agent (LLM Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and iterating\ntheir study design before they conduct the real human-subject study. Our system\nfeatures a Persona Generator module, an LLM Agent module, and a Universal\nBrowser Connector module to automatically generate thousands of simulated users\nand to interactively test the target website. The system also provides a Result\nViewer Interface so that the UX researchers can easily review and analyze the\ngenerated qualitative (e.g., agents' post-study surveys) and quantitative data\n(e.g., agents' interaction logs), or even interview agents directly. Through a\nheuristic evaluation with 16 UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent usage in UX studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate their new designs. But what about\nevaluating and iterating the usability testing study design itself? Recent\nadvances in Large Language Model-simulated Agent (LLM Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and iterating\ntheir study design before they conduct the real human-subject study. Our system\nfeatures a Persona Generator module, an LLM Agent module, and a Universal\nBrowser Connector module to automatically generate thousands of simulated users\nand to interactively test the target website. The system also provides a Result\nViewer Interface so that the UX researchers can easily review and analyze the\ngenerated qualitative (e.g., agents' post-study surveys) and quantitative data\n(e.g., agents' interaction logs), or even interview agents directly. Through a\nheuristic evaluation with 16 UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent usage in UX studies."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09407v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09407v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13951v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13951v3",
                "updated": "2025-09-19T17:50:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    50,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-20T03:22:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    3,
                    22,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments"
                },
                "summary": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening."
                },
                "authors": [
                    {
                        "name": "Jinwen Tang"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Yi Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Shang"
                },
                "author": "Yi Shang",
                "arxiv_doi": "10.1109/CAI64502.2025.00080",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CAI64502.2025.00080",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13951v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13951v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. 2025 IEEE Conference on Artificial Intelligence (CAI), Santa\n  Clara, CA, USA, 2025, pp. 435-440",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16191v1",
                "updated": "2025-09-19T17:50:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    50,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:50:44Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    50,
                    44,
                    4,
                    262,
                    0
                ],
                "title": "Binary-lens Microlensing Degeneracy: Impact on Planetary Sensitivity and\n  Mass-ratio Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary-lens Microlensing Degeneracy: Impact on Planetary Sensitivity and\n  Mass-ratio Function"
                },
                "summary": "Gravitational microlensing is a unique method for discovering cold planets\nacross a broad mass range. Reliable statistics of the microlensing planets\nrequire accurate sensitivity estimates. However, the impact of the degeneracies\nin binary-lens single-source (2L1S) models that affect many actual planet\ndetections is often omitted in sensitivity estimates, leading to potential\nself-inconsistency of the statistics studies. In this work, we evaluate the\neffect of the 2L1S degeneracies on planetary sensitivity by simulating a series\nof typical microlensing events and comprehensively replicating a realistic\nplanet detection pipeline, including the anomaly identification, global 2L1S\nmodel search, and degenerate model comparison. We find that for a pure-survey\nstatistical sample, the 2L1S degeneracies reduce the overall planetary\nsensitivity by $5\\sim10\\%$, with the effect increasing at higher planet-host\nmass ratios. This bias leads to an underestimation of planet occurrence rates\nand a flattening of the inferred mass-ratio function slope. This effect will be\ncritical for upcoming space-based microlensing surveys like the Roman or Earth\n2.0 missions, which are expected to discover $\\mathcal{O}(10^3)$ planets. We\nalso discuss the computational challenges and propose potential approaches for\nfuture applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational microlensing is a unique method for discovering cold planets\nacross a broad mass range. Reliable statistics of the microlensing planets\nrequire accurate sensitivity estimates. However, the impact of the degeneracies\nin binary-lens single-source (2L1S) models that affect many actual planet\ndetections is often omitted in sensitivity estimates, leading to potential\nself-inconsistency of the statistics studies. In this work, we evaluate the\neffect of the 2L1S degeneracies on planetary sensitivity by simulating a series\nof typical microlensing events and comprehensively replicating a realistic\nplanet detection pipeline, including the anomaly identification, global 2L1S\nmodel search, and degenerate model comparison. We find that for a pure-survey\nstatistical sample, the 2L1S degeneracies reduce the overall planetary\nsensitivity by $5\\sim10\\%$, with the effect increasing at higher planet-host\nmass ratios. This bias leads to an underestimation of planet occurrence rates\nand a flattening of the inferred mass-ratio function slope. This effect will be\ncritical for upcoming space-based microlensing surveys like the Roman or Earth\n2.0 missions, which are expected to discover $\\mathcal{O}(10^3)$ planets. We\nalso discuss the computational challenges and propose potential approaches for\nfuture applications."
                },
                "authors": [
                    {
                        "name": "Yuxin Shang"
                    },
                    {
                        "name": "Hongjing Yang"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Shude Mao"
                    },
                    {
                        "name": "Andrew Gould"
                    },
                    {
                        "name": "Weicheng Zang"
                    },
                    {
                        "name": "Qiyue Qian"
                    },
                    {
                        "name": "Jennifer C. Yee"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer C. Yee"
                },
                "author": "Jennifer C. Yee",
                "arxiv_comment": "submitted to AAS, 18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16188v1",
                "updated": "2025-09-19T17:47:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    47,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:47:48Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    47,
                    48,
                    4,
                    262,
                    0
                ],
                "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture"
                },
                "authors": [
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Sihang Jiang"
                    },
                    {
                        "name": "Shiwei Guo"
                    },
                    {
                        "name": "Shisong Chen"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Minggui HE"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Hongxia Ma"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Ma"
                },
                "author": "Hongxia Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16187v1",
                "updated": "2025-09-19T17:46:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    46,
                    13,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:46:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    46,
                    13,
                    4,
                    262,
                    0
                ],
                "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code\n  Translation Validation and Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code\n  Translation Validation and Repair"
                },
                "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Brandon Paulsen"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Joey Dodds"
                    },
                    {
                        "name": "Daniel Kroening"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kroening"
                },
                "author": "Daniel Kroening",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08415v2",
                "updated": "2025-09-19T17:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    45,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-12T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "FSLI: An Interpretable Formal Semantic System for One-Dimensional\n  Ordering Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FSLI: An Interpretable Formal Semantic System for One-Dimensional\n  Ordering Inference"
                },
                "summary": "We develop a system for solving logical deduction one-dimensional ordering\nproblems by transforming natural language premises and candidate statements\ninto first-order logic. Building on Heim and Kratzer's syntax-based\ncompositional semantic rules which utilizes lambda calculus, we develop a\nsemantic parsing algorithm with abstract types, templated rules, and a dynamic\ncomponent for interpreting entities within a context constructed from the\ninput. The resulting logical forms are executed via constraint logic\nprogramming to determine which candidate statements can be logically deduced\nfrom the premises.\n  The symbolic system, the Formal Semantic Logic Inferer (FSLI), provides a\nformally grounded, linguistically driven system for natural language logical\ndeduction. We evaluate it on both synthetic and derived logical deduction\nproblems. FSLI achieves 100% accuracy on BIG-bench's logical deduction task and\n88% on a syntactically simplified subset of AR-LSAT outperforming an LLM\nbaseline, o1-preview.\n  While current research in natural language reasoning emphasizes neural\nlanguage models, FSLI highlights the potential of principled, interpretable\nsystems for symbolic logical deduction in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a system for solving logical deduction one-dimensional ordering\nproblems by transforming natural language premises and candidate statements\ninto first-order logic. Building on Heim and Kratzer's syntax-based\ncompositional semantic rules which utilizes lambda calculus, we develop a\nsemantic parsing algorithm with abstract types, templated rules, and a dynamic\ncomponent for interpreting entities within a context constructed from the\ninput. The resulting logical forms are executed via constraint logic\nprogramming to determine which candidate statements can be logically deduced\nfrom the premises.\n  The symbolic system, the Formal Semantic Logic Inferer (FSLI), provides a\nformally grounded, linguistically driven system for natural language logical\ndeduction. We evaluate it on both synthetic and derived logical deduction\nproblems. FSLI achieves 100% accuracy on BIG-bench's logical deduction task and\n88% on a syntactically simplified subset of AR-LSAT outperforming an LLM\nbaseline, o1-preview.\n  While current research in natural language reasoning emphasizes neural\nlanguage models, FSLI highlights the potential of principled, interpretable\nsystems for symbolic logical deduction in NLP."
                },
                "authors": [
                    {
                        "name": "Maha Alkhairy"
                    },
                    {
                        "name": "Vincent Homer"
                    },
                    {
                        "name": "Brendan O'Connor"
                    }
                ],
                "author_detail": {
                    "name": "Brendan O'Connor"
                },
                "author": "Brendan O'Connor",
                "arxiv_comment": "3 figures, 9 pages main paper and 8 pages references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07820v4",
                "updated": "2025-09-19T17:38:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    38,
                    31,
                    4,
                    262,
                    0
                ],
                "published": "2024-11-12T14:12:45Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    14,
                    12,
                    45,
                    1,
                    317,
                    0
                ],
                "title": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models"
                },
                "summary": "We introduce the \\textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a\nnovel approach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the \\textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a\nnovel approach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Youan Cong"
                    },
                    {
                        "name": "Pritom Saha Akash"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Kevin Chen-Chuan Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Chen-Chuan Chang"
                },
                "author": "Kevin Chen-Chuan Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16176v1",
                "updated": "2025-09-19T17:35:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:35:51Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "title": "Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\n  Trajectories"
                },
                "summary": "We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\nTrajectories (ACDC), an autonomous drone cinematography system driven by\nnatural language communication between human directors and drones. The main\nlimitation of previous drone cinematography workflows is that they require\nmanual selection of waypoints and view angles based on predefined human intent,\nwhich is labor-intensive and yields inconsistent performance. In this paper, we\npropose employing large language models (LLMs) and vision foundation models\n(VFMs) to convert free-form natural language prompts directly into executable\nindoor UAV video tours. Specifically, our method comprises a vision-language\nretrieval pipeline for initial waypoint selection, a preference-based Bayesian\noptimization framework that refines poses using aesthetic feedback, and a\nmotion planner that generates safe quadrotor trajectories. We validate ACDC\nthrough both simulation and hardware-in-the-loop experiments, demonstrating\nthat it robustly produces professional-quality footage across diverse indoor\nscenes without requiring expertise in robotics or cinematography. These results\nhighlight the potential of embodied AI agents to close the loop from\nopen-vocabulary dialogue to real-world autonomous aerial cinematography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\nTrajectories (ACDC), an autonomous drone cinematography system driven by\nnatural language communication between human directors and drones. The main\nlimitation of previous drone cinematography workflows is that they require\nmanual selection of waypoints and view angles based on predefined human intent,\nwhich is labor-intensive and yields inconsistent performance. In this paper, we\npropose employing large language models (LLMs) and vision foundation models\n(VFMs) to convert free-form natural language prompts directly into executable\nindoor UAV video tours. Specifically, our method comprises a vision-language\nretrieval pipeline for initial waypoint selection, a preference-based Bayesian\noptimization framework that refines poses using aesthetic feedback, and a\nmotion planner that generates safe quadrotor trajectories. We validate ACDC\nthrough both simulation and hardware-in-the-loop experiments, demonstrating\nthat it robustly produces professional-quality footage across diverse indoor\nscenes without requiring expertise in robotics or cinematography. These results\nhighlight the potential of embodied AI agents to close the loop from\nopen-vocabulary dialogue to real-world autonomous aerial cinematography."
                },
                "authors": [
                    {
                        "name": "Yifan Lin"
                    },
                    {
                        "name": "Sophie Ziyu Liu"
                    },
                    {
                        "name": "Ran Qi"
                    },
                    {
                        "name": "George Z. Xue"
                    },
                    {
                        "name": "Xinping Song"
                    },
                    {
                        "name": "Chao Qin"
                    },
                    {
                        "name": "Hugh H. -T. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hugh H. -T. Liu"
                },
                "author": "Hugh H. -T. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16170v1",
                "updated": "2025-09-19T17:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    29,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    29,
                    25,
                    4,
                    262,
                    0
                ],
                "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical\n  Self-Supervised Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical\n  Self-Supervised Compensation"
                },
                "summary": "Multi-modal image segmentation faces real-world deployment challenges from\nincomplete/corrupted modalities degrading performance. While existing methods\naddress training-inference modality gaps via specialized per-combination\nmodels, they introduce high deployment costs by requiring exhaustive model\nsubsets and model-modality matching. In this work, we propose a unified\nmodality-relax segmentation network (UniMRSeg) through hierarchical\nself-supervised compensation (HSSC). Our approach hierarchically bridges\nrepresentation gaps between complete and incomplete modalities across input,\nfeature and output levels. % First, we adopt modality reconstruction with the\nhybrid shuffled-masking augmentation, encouraging the model to learn the\nintrinsic modality characteristics and generate meaningful representations for\nmissing modalities through cross-modal fusion. % Next, modality-invariant\ncontrastive learning implicitly compensates the feature space distance among\nincomplete-complete modality pairs. Furthermore, the proposed lightweight\nreverse attention adapter explicitly compensates for the weak perceptual\nsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid\nconsistency constraint to ensure stable prediction under all modality\ncombinations without large performance fluctuations. Without bells and\nwhistles, UniMRSeg significantly outperforms the state-of-the-art methods under\ndiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D\nsemantic segmentation, RGB-D/T salient object segmentation. The code will be\nreleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal image segmentation faces real-world deployment challenges from\nincomplete/corrupted modalities degrading performance. While existing methods\naddress training-inference modality gaps via specialized per-combination\nmodels, they introduce high deployment costs by requiring exhaustive model\nsubsets and model-modality matching. In this work, we propose a unified\nmodality-relax segmentation network (UniMRSeg) through hierarchical\nself-supervised compensation (HSSC). Our approach hierarchically bridges\nrepresentation gaps between complete and incomplete modalities across input,\nfeature and output levels. % First, we adopt modality reconstruction with the\nhybrid shuffled-masking augmentation, encouraging the model to learn the\nintrinsic modality characteristics and generate meaningful representations for\nmissing modalities through cross-modal fusion. % Next, modality-invariant\ncontrastive learning implicitly compensates the feature space distance among\nincomplete-complete modality pairs. Furthermore, the proposed lightweight\nreverse attention adapter explicitly compensates for the weak perceptual\nsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid\nconsistency constraint to ensure stable prediction under all modality\ncombinations without large performance fluctuations. Without bells and\nwhistles, UniMRSeg significantly outperforms the state-of-the-art methods under\ndiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D\nsemantic segmentation, RGB-D/T salient object segmentation. The code will be\nreleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg."
                },
                "authors": [
                    {
                        "name": "Xiaoqi Zhao"
                    },
                    {
                        "name": "Youwei Pang"
                    },
                    {
                        "name": "Chenyang Yu"
                    },
                    {
                        "name": "Lihe Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Shijian Lu"
                    },
                    {
                        "name": "Georges El Fakhri"
                    },
                    {
                        "name": "Xiaofeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Liu"
                },
                "author": "Xiaofeng Liu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v4",
                "updated": "2025-09-19T17:17:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    17,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "On the Security of Tool-Invocation Prompts for LLM-Based Agentic\n  Systems: An Empirical Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Security of Tool-Invocation Prompts for LLM-Based Agentic\n  Systems: An Empirical Risk Assessment"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11759v2",
                "updated": "2025-09-19T17:12:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    12,
                    21,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-15T18:09:53Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    18,
                    9,
                    53,
                    4,
                    227,
                    0
                ],
                "title": "Using Natural Language for Human-Robot Collaboration in the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Natural Language for Human-Robot Collaboration in the Real World"
                },
                "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans."
                },
                "authors": [
                    {
                        "name": "Peter Lindes"
                    },
                    {
                        "name": "Kaoutar Skiker"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar Skiker"
                },
                "author": "Kaoutar Skiker",
                "arxiv_comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24544v2",
                "updated": "2025-09-19T17:11:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    11,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T12:52:35Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    52,
                    35,
                    4,
                    150,
                    0
                ],
                "title": "Cross-Attention Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Attention Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wei Zhong"
                    },
                    {
                        "name": "Manasa Bharadwaj"
                    },
                    {
                        "name": "Yixiao Wang"
                    },
                    {
                        "name": "Nikhil Verma"
                    },
                    {
                        "name": "Yipeng Ji"
                    },
                    {
                        "name": "Chul Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chul Lee"
                },
                "author": "Chul Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16232v2",
                "updated": "2025-09-19T17:11:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    11,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-22T05:05:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    5,
                    5,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "MuseScorer: Idea Originality Scoring At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuseScorer: Idea Originality Scoring At Scale"
                },
                "summary": "An objective, face-valid method for scoring idea originality is to measure\neach idea's statistical infrequency within a population -- an approach long\nused in creativity research. Yet, computing these frequencies requires manually\nbucketing idea rephrasings, a process that is subjective, labor-intensive,\nerror-prone, and brittle at scale. We introduce MuseScorer, a fully automated,\npsychometrically validated system for frequency-based originality scoring.\nMuseScorer integrates a Large Language Model (LLM) with externally orchestrated\nretrieval: given a new idea, it retrieves semantically similar prior\nidea-buckets and zero-shot prompts the LLM to judge whether the idea fits an\nexisting bucket or forms a new one. These buckets enable frequency-based\noriginality scoring without human annotation. Across five datasets\nN_{participants}=1143, n_{ideas}=16,294), MuseScorer matches human annotators\nin idea clustering structure (AMI = 0.59) and participant-level scoring (r =\n0.89), while demonstrating strong convergent and external validity. The system\nenables scalable, intent-sensitive, and human-aligned originality assessment\nfor creativity research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An objective, face-valid method for scoring idea originality is to measure\neach idea's statistical infrequency within a population -- an approach long\nused in creativity research. Yet, computing these frequencies requires manually\nbucketing idea rephrasings, a process that is subjective, labor-intensive,\nerror-prone, and brittle at scale. We introduce MuseScorer, a fully automated,\npsychometrically validated system for frequency-based originality scoring.\nMuseScorer integrates a Large Language Model (LLM) with externally orchestrated\nretrieval: given a new idea, it retrieves semantically similar prior\nidea-buckets and zero-shot prompts the LLM to judge whether the idea fits an\nexisting bucket or forms a new one. These buckets enable frequency-based\noriginality scoring without human annotation. Across five datasets\nN_{participants}=1143, n_{ideas}=16,294), MuseScorer matches human annotators\nin idea clustering structure (AMI = 0.59) and participant-level scoring (r =\n0.89), while demonstrating strong convergent and external validity. The system\nenables scalable, intent-sensitive, and human-aligned originality assessment\nfor creativity research."
                },
                "authors": [
                    {
                        "name": "Ali Sarosh Bangash"
                    },
                    {
                        "name": "Krish Veera"
                    },
                    {
                        "name": "Ishfat Abrar Islam"
                    },
                    {
                        "name": "Raiyan Abdul Baten"
                    }
                ],
                "author_detail": {
                    "name": "Raiyan Abdul Baten"
                },
                "author": "Raiyan Abdul Baten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12158v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12158v3",
                "updated": "2025-09-19T17:07:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    7,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-13T18:24:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    18,
                    24,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models."
                },
                "authors": [
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12158v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16149v1",
                "updated": "2025-09-19T16:57:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    57,
                    20,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:57:20Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    57,
                    20,
                    4,
                    262,
                    0
                ],
                "title": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions."
                },
                "authors": [
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Kehao Miao"
                    },
                    {
                        "name": "Li Peihang"
                    },
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16136v1",
                "updated": "2025-09-19T16:35:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    35,
                    27,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:35:27Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    35,
                    27,
                    4,
                    262,
                    0
                ],
                "title": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model\n  Framework for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model\n  Framework for Reinforcement Learning"
                },
                "summary": "Designing effective reward functions remains a major challenge in\nreinforcement learning (RL), often requiring considerable human expertise and\niterative refinement. Recent advances leverage Large Language Models (LLMs) for\nautomated reward design, but these approaches are limited by hallucinations,\nreliance on human feedback, and challenges with handling complex, multi-step\ntasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts\n(RE-GoT), a novel bi-level framework that enhances LLMs with structured\ngraph-based reasoning and integrates Visual Language Models (VLMs) for\nautomated rollout evaluation. RE-GoT first decomposes tasks into\ntext-attributed graphs, enabling comprehensive analysis and reward function\ngeneration, and then iteratively refines rewards using visual feedback from\nVLMs without human intervention. Extensive experiments on 10 RoboGen and 4\nManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing\nLLM-based baselines. On RoboGen, our method improves average task success rates\nby 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,\nRE-GoT achieves an average success rate of 93.73% across four diverse\nmanipulation tasks, significantly surpassing prior LLM-based approaches and\neven exceeding expert-designed rewards. Our results indicate that combining\nLLMs and VLMs with graph-of-thoughts reasoning provides a scalable and\neffective solution for autonomous reward evolution in RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective reward functions remains a major challenge in\nreinforcement learning (RL), often requiring considerable human expertise and\niterative refinement. Recent advances leverage Large Language Models (LLMs) for\nautomated reward design, but these approaches are limited by hallucinations,\nreliance on human feedback, and challenges with handling complex, multi-step\ntasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts\n(RE-GoT), a novel bi-level framework that enhances LLMs with structured\ngraph-based reasoning and integrates Visual Language Models (VLMs) for\nautomated rollout evaluation. RE-GoT first decomposes tasks into\ntext-attributed graphs, enabling comprehensive analysis and reward function\ngeneration, and then iteratively refines rewards using visual feedback from\nVLMs without human intervention. Extensive experiments on 10 RoboGen and 4\nManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing\nLLM-based baselines. On RoboGen, our method improves average task success rates\nby 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,\nRE-GoT achieves an average success rate of 93.73% across four diverse\nmanipulation tasks, significantly surpassing prior LLM-based approaches and\neven exceeding expert-designed rewards. Our results indicate that combining\nLLMs and VLMs with graph-of-thoughts reasoning provides a scalable and\neffective solution for autonomous reward evolution in RL."
                },
                "authors": [
                    {
                        "name": "Changwei Yao"
                    },
                    {
                        "name": "Xinzi Liu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16132v1",
                "updated": "2025-09-19T16:31:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    31,
                    5,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:31:05Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    31,
                    5,
                    4,
                    262,
                    0
                ],
                "title": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering Parametric Scenes from Very Few Time-of-Flight Pixels"
                },
                "summary": "We aim to recover the geometry of 3D parametric scenes using very few depth\nmeasurements from low-cost, commercially available time-of-flight sensors.\nThese sensors offer very low spatial resolution (i.e., a single pixel), but\nimage a wide field-of-view per pixel and capture detailed time-of-flight data\nin the form of time-resolved photon counts. This time-of-flight data encodes\nrich scene information and thus enables recovery of simple scenes from sparse\nmeasurements. We investigate the feasibility of using a distributed set of few\nmeasurements (e.g., as few as 15 pixels) to recover the geometry of simple\nparametric scenes with a strong prior, such as estimating the 6D pose of a\nknown object. To achieve this, we design a method that utilizes both\nfeed-forward prediction to infer scene parameters, and differentiable rendering\nwithin an analysis-by-synthesis framework to refine the scene parameter\nestimate. We develop hardware prototypes and demonstrate that our method\neffectively recovers object pose given an untextured 3D model in both\nsimulations and controlled real-world captures, and show promising initial\nresults for other parametric scenes. We additionally conduct experiments to\nexplore the limits and capabilities of our imaging solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to recover the geometry of 3D parametric scenes using very few depth\nmeasurements from low-cost, commercially available time-of-flight sensors.\nThese sensors offer very low spatial resolution (i.e., a single pixel), but\nimage a wide field-of-view per pixel and capture detailed time-of-flight data\nin the form of time-resolved photon counts. This time-of-flight data encodes\nrich scene information and thus enables recovery of simple scenes from sparse\nmeasurements. We investigate the feasibility of using a distributed set of few\nmeasurements (e.g., as few as 15 pixels) to recover the geometry of simple\nparametric scenes with a strong prior, such as estimating the 6D pose of a\nknown object. To achieve this, we design a method that utilizes both\nfeed-forward prediction to infer scene parameters, and differentiable rendering\nwithin an analysis-by-synthesis framework to refine the scene parameter\nestimate. We develop hardware prototypes and demonstrate that our method\neffectively recovers object pose given an untextured 3D model in both\nsimulations and controlled real-world captures, and show promising initial\nresults for other parametric scenes. We additionally conduct experiments to\nexplore the limits and capabilities of our imaging solution."
                },
                "authors": [
                    {
                        "name": "Carter Sifferman"
                    },
                    {
                        "name": "Yiquan Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Fangzhou Mu"
                    },
                    {
                        "name": "Michael Gleicher"
                    },
                    {
                        "name": "Mohit Gupta"
                    },
                    {
                        "name": "Yin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yin Li"
                },
                "author": "Yin Li",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24708v2",
                "updated": "2025-09-19T16:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    28,
                    9,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T15:29:36Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    29,
                    36,
                    4,
                    150,
                    0
                ],
                "title": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions"
                },
                "summary": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels."
                },
                "authors": [
                    {
                        "name": "Jonas Nitzler"
                    },
                    {
                        "name": "Bugrahan Z. Temür"
                    },
                    {
                        "name": "Phaedon-Stelios Koutsourelakis"
                    },
                    {
                        "name": "Wolfgang A. Wall"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang A. Wall"
                },
                "author": "Wolfgang A. Wall",
                "arxiv_comment": "40 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16128v1",
                "updated": "2025-09-19T16:25:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    30,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:25:30Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    30,
                    4,
                    262,
                    0
                ],
                "title": "AnchoredAI: Contextual Anchoring of AI Comments Improves Writer Agency\n  and Ownership",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchoredAI: Contextual Anchoring of AI Comments Improves Writer Agency\n  and Ownership"
                },
                "summary": "Generative AI is increasingly integrated into writing support, yet current\nchat-based interfaces often obscure referential context and risk amplifying\nautomation bias and overreliance. We introduce AnchoredAI, a novel system that\nanchors AI feedback directly to relevant text spans. AnchoredAI implements two\nkey mechanisms: (1) an Anchoring Context Window (ACW) that maintains unique,\ncontext-rich references, and (2) an update-aware context retrieval method that\npreserves the intent of prior comments after document edits. In a controlled\nuser study, we compared AnchoredAI to a chat-based LLM interface. Results show\nthat AnchoredAI led to more targeted revisions while fostering a stronger\nagency metrics (e.g., control and ownership) among writers. These findings\nhighlight how interface design shapes AI-assisted writing, suggesting that\nanchoring can mitigate overreliance and enable more precise, user-driven\nrevision practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is increasingly integrated into writing support, yet current\nchat-based interfaces often obscure referential context and risk amplifying\nautomation bias and overreliance. We introduce AnchoredAI, a novel system that\nanchors AI feedback directly to relevant text spans. AnchoredAI implements two\nkey mechanisms: (1) an Anchoring Context Window (ACW) that maintains unique,\ncontext-rich references, and (2) an update-aware context retrieval method that\npreserves the intent of prior comments after document edits. In a controlled\nuser study, we compared AnchoredAI to a chat-based LLM interface. Results show\nthat AnchoredAI led to more targeted revisions while fostering a stronger\nagency metrics (e.g., control and ownership) among writers. These findings\nhighlight how interface design shapes AI-assisted writing, suggesting that\nanchoring can mitigate overreliance and enable more precise, user-driven\nrevision practices."
                },
                "authors": [
                    {
                        "name": "Martin Lou"
                    },
                    {
                        "name": "Jackie Crowley"
                    },
                    {
                        "name": "Samuel Dodson"
                    },
                    {
                        "name": "Dongwook Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Dongwook Yoon"
                },
                "author": "Dongwook Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v3",
                "updated": "2025-09-19T16:25:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    3,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Automated CGRA Design with Multi-Agent LLMs: A Unified Hardware-Software\n  Co-Design Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated CGRA Design with Multi-Agent LLMs: A Unified Hardware-Software\n  Co-Design Framework"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v4",
                "updated": "2025-09-19T16:18:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    18,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight the performance gap between open-source models and top\nstudents, the strong reasoning abilities of closed-source models, and the\nremaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for\nmultimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO\nwith a public leaderboard at https://phyarena.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight the performance gap between open-source models and top\nstudents, the strong reasoning abilities of closed-source models, and the\nremaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for\nmultimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO\nwith a public leaderboard at https://phyarena.github.io/."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16119v1",
                "updated": "2025-09-19T16:13:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    13,
                    9,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:13:09Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    13,
                    9,
                    4,
                    262,
                    0
                ],
                "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars"
                },
                "summary": "4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Weiyi Xiong"
                    },
                    {
                        "name": "Bing Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Zewei Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zewei Zheng"
                },
                "author": "Zewei Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09885v2",
                "updated": "2025-09-19T16:12:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    12,
                    6,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-14T03:46:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    3,
                    46,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via\n  Grayscale-Aware Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via\n  Grayscale-Aware Attention"
                },
                "summary": "Reconstructing hyperspectral images (HSIs) from RGB inputs provides a\ncost-effective alternative to hyperspectral cameras, but reconstructing\nhigh-dimensional spectra from three channels is inherently ill-posed. Existing\nmethods typically directly regress RGB-to-HSI mappings using large attention\nnetworks, which are computationally expensive and handle ill-posedness only\nimplicitly. We propose MCGA, a Mixture-of-Codebooks with Grayscale-aware\nAttention framework that explicitly addresses these challenges using spectral\npriors and photometric consistency. MCGA first learns transferable spectral\npriors via a mixture-of-codebooks (MoC) from heterogeneous HSI datasets, then\naligns RGB features with these priors through grayscale-aware photometric\nattention (GANet). Efficiency and robustness are further improved via top-K\nattention design and test-time adaptation (TTA). Experiments on benchmarks and\nreal-world data demonstrate the state-of-the-art accuracy, strong cross-dataset\ngeneralization, and 4-5x faster inference. Codes will be available once\nacceptance at https://github.com/Fibonaccirabbit/MCGA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing hyperspectral images (HSIs) from RGB inputs provides a\ncost-effective alternative to hyperspectral cameras, but reconstructing\nhigh-dimensional spectra from three channels is inherently ill-posed. Existing\nmethods typically directly regress RGB-to-HSI mappings using large attention\nnetworks, which are computationally expensive and handle ill-posedness only\nimplicitly. We propose MCGA, a Mixture-of-Codebooks with Grayscale-aware\nAttention framework that explicitly addresses these challenges using spectral\npriors and photometric consistency. MCGA first learns transferable spectral\npriors via a mixture-of-codebooks (MoC) from heterogeneous HSI datasets, then\naligns RGB features with these priors through grayscale-aware photometric\nattention (GANet). Efficiency and robustness are further improved via top-K\nattention design and test-time adaptation (TTA). Experiments on benchmarks and\nreal-world data demonstrate the state-of-the-art accuracy, strong cross-dataset\ngeneralization, and 4-5x faster inference. Codes will be available once\nacceptance at https://github.com/Fibonaccirabbit/MCGA."
                },
                "authors": [
                    {
                        "name": "Zhanjiang Yang"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Jiawei Dong"
                    },
                    {
                        "name": "Xiaoxin An"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16112v1",
                "updated": "2025-09-19T15:57:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    57,
                    40,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:57:40Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    57,
                    40,
                    4,
                    262,
                    0
                ],
                "title": "CodeRAG: Finding Relevant and Necessary Knowledge for\n  Retrieval-Augmented Repository-Level Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeRAG: Finding Relevant and Necessary Knowledge for\n  Retrieval-Augmented Repository-Level Code Completion"
                },
                "summary": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG."
                },
                "authors": [
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Shuquan Lian"
                    },
                    {
                        "name": "Shun Song"
                    },
                    {
                        "name": "Hui Li"
                    }
                ],
                "author_detail": {
                    "name": "Hui Li"
                },
                "author": "Hui Li",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18931v2",
                "updated": "2025-09-19T15:55:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    55,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-25T01:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    1,
                    50,
                    5,
                    6,
                    145,
                    0
                ],
                "title": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?"
                },
                "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work evaluating LLM causal\nreasoning primarily focuses on synthetically generated texts which involve\nstraightforward causal relationships that are explicitly mentioned in the text.\nThis fails to reflect the complexities of real-world tasks. In this paper, we\ninvestigate whether LLMs are capable of inferring causal relationships from\nreal-world texts. We develop a benchmark drawn from real-world academic\nliterature which includes diverse texts with respect to length, complexity of\nrelationships (different levels of explicitness, number of nodes, and causal\nrelationships), and domains and sub-domains. To the best of our knowledge, our\nbenchmark is the first-ever real-world dataset for this task. Our experiments\non this dataset show that LLMs face significant challenges in inferring causal\nrelationships from real-world text, with the best-performing model achieving an\naverage F1 score of only 0.477. Through systematic analysis across aspects of\nreal-world text (degree of confounding, size of graph, length of text, domain),\nour benchmark offers targeted insights for further research into advancing LLM\ncausal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work evaluating LLM causal\nreasoning primarily focuses on synthetically generated texts which involve\nstraightforward causal relationships that are explicitly mentioned in the text.\nThis fails to reflect the complexities of real-world tasks. In this paper, we\ninvestigate whether LLMs are capable of inferring causal relationships from\nreal-world texts. We develop a benchmark drawn from real-world academic\nliterature which includes diverse texts with respect to length, complexity of\nrelationships (different levels of explicitness, number of nodes, and causal\nrelationships), and domains and sub-domains. To the best of our knowledge, our\nbenchmark is the first-ever real-world dataset for this task. Our experiments\non this dataset show that LLMs face significant challenges in inferring causal\nrelationships from real-world text, with the best-performing model achieving an\naverage F1 score of only 0.477. Through systematic analysis across aspects of\nreal-world text (degree of confounding, size of graph, length of text, domain),\nour benchmark offers targeted insights for further research into advancing LLM\ncausal reasoning."
                },
                "authors": [
                    {
                        "name": "Ryan Saklad"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Oleg Pavlov"
                    },
                    {
                        "name": "Raha Moraffah"
                    }
                ],
                "author_detail": {
                    "name": "Raha Moraffah"
                },
                "author": "Raha Moraffah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16111v1",
                "updated": "2025-09-19T15:54:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    54,
                    41,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:54:41Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    54,
                    41,
                    4,
                    262,
                    0
                ],
                "title": "The Nature of High-Redshift Massive Quiescent Galaxies -- Searching for\n  RUBIES-UDS-QG-z7 in FLARES",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of High-Redshift Massive Quiescent Galaxies -- Searching for\n  RUBIES-UDS-QG-z7 in FLARES"
                },
                "summary": "RUBIES-UDS-QG-z7 (RQG) is the earliest massive quiescent galaxy identified to\ndate, inferred to have formed its abundant stellar mass in a single burst that\nceases rapidly before $z \\sim 8$. An object of such extreme nature challenges\nour understanding of galaxy formation, requiring rapid growth and quenching\nmechanisms only 0.6 Gyr after the Big Bang and implying number densities 2 dex\nhigher than currently predicted by simulations. We use synthetic observables to\nidentify analogous systems within the First Light And Reionisation Epoch\nSimulations (FLARES) and find two massive galaxies dominated by rapidly\nquenched bursts. Beyond demonstrating that the current FLARES model is capable\nof producing RQG-like systems, these analogues provide a laboratory within\nwhich to study the underlying physics. Their active galactic nuclei (AGN) heat\nand expel gas, inducing rapid quenching and preventing timely rejuvenation.\nThis causes above-average chemical enrichment at a given stellar mass, with\nsuper solar levels predicted for RQG. These metallicities are underestimated by\nspectral energy distribution fitting and we show that $\\alpha$-enhancement\ncannot be solely responsible. Degeneracies with age and dust attenuation appear\nthe more likely causes. Tensions between observed and simulated number\ndensities can be alleviated in part by considering systematics, but adjustments\nto AGN feedback, such as allowing super-Eddington accretion rates, may be\nrequired for full agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUBIES-UDS-QG-z7 (RQG) is the earliest massive quiescent galaxy identified to\ndate, inferred to have formed its abundant stellar mass in a single burst that\nceases rapidly before $z \\sim 8$. An object of such extreme nature challenges\nour understanding of galaxy formation, requiring rapid growth and quenching\nmechanisms only 0.6 Gyr after the Big Bang and implying number densities 2 dex\nhigher than currently predicted by simulations. We use synthetic observables to\nidentify analogous systems within the First Light And Reionisation Epoch\nSimulations (FLARES) and find two massive galaxies dominated by rapidly\nquenched bursts. Beyond demonstrating that the current FLARES model is capable\nof producing RQG-like systems, these analogues provide a laboratory within\nwhich to study the underlying physics. Their active galactic nuclei (AGN) heat\nand expel gas, inducing rapid quenching and preventing timely rejuvenation.\nThis causes above-average chemical enrichment at a given stellar mass, with\nsuper solar levels predicted for RQG. These metallicities are underestimated by\nspectral energy distribution fitting and we show that $\\alpha$-enhancement\ncannot be solely responsible. Degeneracies with age and dust attenuation appear\nthe more likely causes. Tensions between observed and simulated number\ndensities can be alleviated in part by considering systematics, but adjustments\nto AGN feedback, such as allowing super-Eddington accretion rates, may be\nrequired for full agreement."
                },
                "authors": [
                    {
                        "name": "Jack C. Turner"
                    },
                    {
                        "name": "Will J. Roper"
                    },
                    {
                        "name": "Aswin P. Vijayan"
                    },
                    {
                        "name": "Sophie L. Newman"
                    },
                    {
                        "name": "Stephen M. Wilkins"
                    },
                    {
                        "name": "Christopher C. Lovell"
                    },
                    {
                        "name": "Shihong Liao"
                    },
                    {
                        "name": "Louise T. C. Seeyave"
                    }
                ],
                "author_detail": {
                    "name": "Louise T. C. Seeyave"
                },
                "author": "Louise T. C. Seeyave",
                "arxiv_comment": "15 pages, 12 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16107v1",
                "updated": "2025-09-19T15:49:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    49,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:49:26Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    49,
                    26,
                    4,
                    262,
                    0
                ],
                "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with\n  Commonsense Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with\n  Commonsense Knowledge"
                },
                "summary": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles."
                },
                "authors": [
                    {
                        "name": "Lukas Ellinger"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "arxiv_comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03455v2",
                "updated": "2025-09-19T15:48:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    48,
                    29,
                    4,
                    262,
                    0
                ],
                "published": "2024-12-04T16:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    43,
                    51,
                    2,
                    339,
                    0
                ],
                "title": "A High Incidence of Central Star Formation Inferred from the Color\n  Gradients of Galaxies at $z>4$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High Incidence of Central Star Formation Inferred from the Color\n  Gradients of Galaxies at $z>4$"
                },
                "summary": "We study the rest-frame ultraviolet-optical color gradients of 441 galaxies\nat $4<z<8$ by characterizing the wavelength dependence of their structural\nparameters derived from simultaneously fitting the seven-band NIRCam images\nacquired with the James Webb Space Telescope. Distinct from trends observed at\nlower redshifts, where most galaxies exhibit negative color gradients whereby\ngalaxy centers are redder than their outskirts, in high-redshift galaxies\npositive color gradients are just as common as or even outnumber negative color\ngradients. Varying stellar population, dust, and active galactic nuclei can\ncontribute to the observed color gradient. We show that for the majority of our\nsample, the observed color gradients principally reflect radial variations in\nstellar population, without strong contribution from dust reddening or\ncontamination from active galactic nuclei. The sign and magnitude of the color\nprofile depend systematically on the global properties of the galaxy: positive\ncolor gradients, characteristic of centrally concentrated star formation or\noutside-in growth, are found preferentially in galaxies of lower stellar mass,\nsmaller size, and bluer spectral energy distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the rest-frame ultraviolet-optical color gradients of 441 galaxies\nat $4<z<8$ by characterizing the wavelength dependence of their structural\nparameters derived from simultaneously fitting the seven-band NIRCam images\nacquired with the James Webb Space Telescope. Distinct from trends observed at\nlower redshifts, where most galaxies exhibit negative color gradients whereby\ngalaxy centers are redder than their outskirts, in high-redshift galaxies\npositive color gradients are just as common as or even outnumber negative color\ngradients. Varying stellar population, dust, and active galactic nuclei can\ncontribute to the observed color gradient. We show that for the majority of our\nsample, the observed color gradients principally reflect radial variations in\nstellar population, without strong contribution from dust reddening or\ncontamination from active galactic nuclei. The sign and magnitude of the color\nprofile depend systematically on the global properties of the galaxy: positive\ncolor gradients, characteristic of centrally concentrated star formation or\noutside-in growth, are found preferentially in galaxies of lower stellar mass,\nsmaller size, and bluer spectral energy distribution."
                },
                "authors": [
                    {
                        "name": "Bingcheng Jin"
                    },
                    {
                        "name": "Luis C. Ho"
                    },
                    {
                        "name": "Wen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wen Sun"
                },
                "arxiv_affiliation": "PKU, KIAA",
                "author": "Wen Sun",
                "arxiv_comment": "21 pages, 16 figures, submitted to ApJ; v2 updated to match the\n  accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12385v2",
                "updated": "2025-09-19T15:48:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    48,
                    12,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-15T19:26:17Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    19,
                    26,
                    17,
                    0,
                    258,
                    0
                ],
                "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection"
                },
                "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting."
                },
                "authors": [
                    {
                        "name": "Mitchell Plyler"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Alexander Tuzhilin"
                    },
                    {
                        "name": "Saoud Khalifah"
                    },
                    {
                        "name": "Sen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Sen Tian"
                },
                "author": "Sen Tian",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15405v2",
                "updated": "2025-09-19T15:40:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    40,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2024-12-19T21:18:43Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    21,
                    18,
                    43,
                    3,
                    354,
                    0
                ],
                "title": "Cosmology with Persistent Homology: Parameter Inference via Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology with Persistent Homology: Parameter Inference via Machine\n  Learning"
                },
                "summary": "Building upon [2308.02636], we investigate the constraining power of\npersistent homology on cosmological parameters and primordial non-Gaussianity\nin a likelihood-free inference pipeline utilizing machine learning. We evaluate\nthe ability of Persistence Images (PIs) to infer parameters, comparing them to\nthe combined Power Spectrum and Bispectrum (PS/BS). We also compare two classes\nof models: neural-based and tree-based. PIs consistently lead to better\npredictions compared to the combined PS/BS for parameters that can be\nconstrained, i.e., for $\\{\\Omega_{\\rm m}, \\sigma_8, n_{\\rm s}, f_{\\rm NL}^{\\rm\nloc}\\}$. PIs perform particularly well for $f_{\\rm NL}^{\\rm loc}$, highlighting\nthe potential of persistent homology for constraining primordial\nnon-Gaussianity. Our results indicate that combining PIs with PS/BS provides\nonly marginal gains, indicating that the PS/BS contains little additional or\ncomplementary information to the PIs. Finally, we provide a visualization of\nthe most important topological features for $f_{\\rm NL}^{\\rm loc}$ and for\n$\\Omega_{\\rm m}$. This reveals that clusters and voids (0-cycles and 2-cycles)\nare most informative for $\\Omega_{\\rm m}$, while $f_{\\rm NL}^{\\rm loc}$ is\nadditionally informed by filaments (1-cycles).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building upon [2308.02636], we investigate the constraining power of\npersistent homology on cosmological parameters and primordial non-Gaussianity\nin a likelihood-free inference pipeline utilizing machine learning. We evaluate\nthe ability of Persistence Images (PIs) to infer parameters, comparing them to\nthe combined Power Spectrum and Bispectrum (PS/BS). We also compare two classes\nof models: neural-based and tree-based. PIs consistently lead to better\npredictions compared to the combined PS/BS for parameters that can be\nconstrained, i.e., for $\\{\\Omega_{\\rm m}, \\sigma_8, n_{\\rm s}, f_{\\rm NL}^{\\rm\nloc}\\}$. PIs perform particularly well for $f_{\\rm NL}^{\\rm loc}$, highlighting\nthe potential of persistent homology for constraining primordial\nnon-Gaussianity. Our results indicate that combining PIs with PS/BS provides\nonly marginal gains, indicating that the PS/BS contains little additional or\ncomplementary information to the PIs. Finally, we provide a visualization of\nthe most important topological features for $f_{\\rm NL}^{\\rm loc}$ and for\n$\\Omega_{\\rm m}$. This reveals that clusters and voids (0-cycles and 2-cycles)\nare most informative for $\\Omega_{\\rm m}$, while $f_{\\rm NL}^{\\rm loc}$ is\nadditionally informed by filaments (1-cycles)."
                },
                "authors": [
                    {
                        "name": "Juan Calles"
                    },
                    {
                        "name": "Jacky H. T. Yip"
                    },
                    {
                        "name": "Gabriella Contardo"
                    },
                    {
                        "name": "Jorge Noreña"
                    },
                    {
                        "name": "Adam Rouhiainen"
                    },
                    {
                        "name": "Gary Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Gary Shiu"
                },
                "author": "Gary Shiu",
                "arxiv_doi": "10.1088/1475-7516/2025/09/064",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/09/064",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 9 figures, 4 tables. Accepted for publication in JCAP.\n  Replaced with the accepted version (minor changes)",
                "arxiv_journal_ref": "JCAP 09 (2025) 064",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16094v1",
                "updated": "2025-09-19T15:38:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    38,
                    13,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:38:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    38,
                    13,
                    4,
                    262,
                    0
                ],
                "title": "The GUAPOS project. VI: the chemical inventory of shocked gas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GUAPOS project. VI: the chemical inventory of shocked gas"
                },
                "summary": "The study of the chemical composition of star-forming regions is key to\nunderstand the chemical ingredients available during the formation of planetary\nsystems. Given that the chemical inventory on interstellar dust grains in the\nprestellar phases might be altered due to the prostostellar warm-up, an\nalternative to infer the chemical composition on the grains could be to observe\nregions affected by shocks associated with molecular outflows. Such shocks can\ndesorb the molecules, and might produce less chemical processing due to shorter\ntimescales. We present here a detailed study of the chemical reservoir of a\nshocked region located in the G31.41+0.31 protocluster using GUAPOS data\n(G31.41+0.31 Unbiased ALMA sPectral Observational Survey). We report here the\ndetection of 30 molecular species (plus 18 isotopologues). We performed a\ncomparison of the molecular ratios in the shocked region with those derived\ntowards the hot core of G31.41+0.31, finding that they are poorly correlated,\nexcepting N-bearing species. Our results confirm observationally that a\ndifferent level of chemical alteration is present in hot cores and in shocks.\nWhile the former likely alter the molecular ratios due to thermal processing\nduring longer timescales, the latter might represent freshly desorbed material\nthat constitutes a better proxy of the icy mantle composition. The similarity\nof molecular ratios between the N-bearing species in the G31.41 shock and the\nhot core suggests that these species are desorbed at early evolutionary stages.\nInterestingly, we have found that the abundances in the G31.41 shock show\nbetter correlations with other shock-dominated regions (two protostellar\noutflows and a Galactic Center molecular cloud). This suggests a negligible\ngas-phase chemistry after shock-induced ejection from grains, and that the\nice-mantle composition is similar regardless of the Galactic environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of the chemical composition of star-forming regions is key to\nunderstand the chemical ingredients available during the formation of planetary\nsystems. Given that the chemical inventory on interstellar dust grains in the\nprestellar phases might be altered due to the prostostellar warm-up, an\nalternative to infer the chemical composition on the grains could be to observe\nregions affected by shocks associated with molecular outflows. Such shocks can\ndesorb the molecules, and might produce less chemical processing due to shorter\ntimescales. We present here a detailed study of the chemical reservoir of a\nshocked region located in the G31.41+0.31 protocluster using GUAPOS data\n(G31.41+0.31 Unbiased ALMA sPectral Observational Survey). We report here the\ndetection of 30 molecular species (plus 18 isotopologues). We performed a\ncomparison of the molecular ratios in the shocked region with those derived\ntowards the hot core of G31.41+0.31, finding that they are poorly correlated,\nexcepting N-bearing species. Our results confirm observationally that a\ndifferent level of chemical alteration is present in hot cores and in shocks.\nWhile the former likely alter the molecular ratios due to thermal processing\nduring longer timescales, the latter might represent freshly desorbed material\nthat constitutes a better proxy of the icy mantle composition. The similarity\nof molecular ratios between the N-bearing species in the G31.41 shock and the\nhot core suggests that these species are desorbed at early evolutionary stages.\nInterestingly, we have found that the abundances in the G31.41 shock show\nbetter correlations with other shock-dominated regions (two protostellar\noutflows and a Galactic Center molecular cloud). This suggests a negligible\ngas-phase chemistry after shock-induced ejection from grains, and that the\nice-mantle composition is similar regardless of the Galactic environment."
                },
                "authors": [
                    {
                        "name": "Á. López-Gallifa"
                    },
                    {
                        "name": "V. M. Rivilla"
                    },
                    {
                        "name": "M. T. Beltrán"
                    },
                    {
                        "name": "L. Colzi"
                    },
                    {
                        "name": "F. Fontani"
                    },
                    {
                        "name": "Á. Sánchez-Monge"
                    },
                    {
                        "name": "C. Mininni"
                    },
                    {
                        "name": "R. Cesaroni"
                    },
                    {
                        "name": "I. Jiménez-Serra"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "A. Lorenzani"
                    }
                ],
                "author_detail": {
                    "name": "A. Lorenzani"
                },
                "author": "A. Lorenzani",
                "arxiv_comment": "Accepted for publication in Astronomy and Astrophysics (A&A)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16093v1",
                "updated": "2025-09-19T15:36:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    36,
                    2,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:36:02Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    36,
                    2,
                    4,
                    262,
                    0
                ],
                "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM\n  Responses"
                },
                "summary": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains."
                },
                "authors": [
                    {
                        "name": "Fangyi Yu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Dasha Herrmannova"
                    },
                    {
                        "name": "Frank Schilder"
                    },
                    {
                        "name": "Jonathan Richard Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Richard Schwarz"
                },
                "author": "Jonathan Richard Schwarz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v6",
                "updated": "2025-09-19T15:35:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    35,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05439v2",
                "updated": "2025-09-19T15:33:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    33,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-05T12:04:59Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    4,
                    59,
                    3,
                    156,
                    0
                ],
                "title": "LLMs Can Compensate for Deficiencies in Visual Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Compensate for Deficiencies in Visual Representations"
                },
                "summary": "Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder."
                },
                "authors": [
                    {
                        "name": "Sho Takishita"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Abdelrahman Mohamed"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Yova Kementchedjhieva"
                    }
                ],
                "author_detail": {
                    "name": "Yova Kementchedjhieva"
                },
                "author": "Yova Kementchedjhieva",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16084v1",
                "updated": "2025-09-19T15:29:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    29,
                    57,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:29:57Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    29,
                    57,
                    4,
                    262,
                    0
                ],
                "title": "Rethinking Molecule Synthesizability with Chain-of-Reaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Molecule Synthesizability with Chain-of-Reaction"
                },
                "summary": "A well-known pitfall of molecular generative models is that they are not\nguaranteed to generate synthesizable molecules. There have been considerable\nattempts to address this problem, but given the exponentially large\ncombinatorial space of synthesizable molecules, existing methods have shown\nlimited coverage of the space and poor molecular optimization performance. To\ntackle these problems, we introduce ReaSyn, a generative framework for\nsynthesizable projection where the model explores the neighborhood of given\nmolecules in the synthesizable space by generating pathways that result in\nsynthesizable analogs. To fully utilize the chemical knowledge contained in the\nsynthetic pathways, we propose a novel perspective that views synthetic\npathways akin to reasoning paths in large language models (LLMs). Specifically,\ninspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the\nchain-of-reaction (CoR) notation that explicitly states reactants, reaction\ntypes, and intermediate products for each step in a pathway. With the CoR\nnotation, ReaSyn can get dense supervision in every reaction step to explicitly\nlearn chemical reaction rules during supervised training and perform\nstep-by-step reasoning. In addition, to further enhance the reasoning\ncapability of ReaSyn, we propose reinforcement learning (RL)-based finetuning\nand goal-directed test-time compute scaling tailored for synthesizable\nprojection. ReaSyn achieves the highest reconstruction rate and pathway\ndiversity in synthesizable molecule reconstruction and the highest optimization\nperformance in synthesizable goal-directed molecular optimization, and\nsignificantly outperforms previous synthesizable projection methods in\nsynthesizable hit expansion. These results highlight ReaSyn's superior ability\nto navigate combinatorially-large synthesizable chemical space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known pitfall of molecular generative models is that they are not\nguaranteed to generate synthesizable molecules. There have been considerable\nattempts to address this problem, but given the exponentially large\ncombinatorial space of synthesizable molecules, existing methods have shown\nlimited coverage of the space and poor molecular optimization performance. To\ntackle these problems, we introduce ReaSyn, a generative framework for\nsynthesizable projection where the model explores the neighborhood of given\nmolecules in the synthesizable space by generating pathways that result in\nsynthesizable analogs. To fully utilize the chemical knowledge contained in the\nsynthetic pathways, we propose a novel perspective that views synthetic\npathways akin to reasoning paths in large language models (LLMs). Specifically,\ninspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the\nchain-of-reaction (CoR) notation that explicitly states reactants, reaction\ntypes, and intermediate products for each step in a pathway. With the CoR\nnotation, ReaSyn can get dense supervision in every reaction step to explicitly\nlearn chemical reaction rules during supervised training and perform\nstep-by-step reasoning. In addition, to further enhance the reasoning\ncapability of ReaSyn, we propose reinforcement learning (RL)-based finetuning\nand goal-directed test-time compute scaling tailored for synthesizable\nprojection. ReaSyn achieves the highest reconstruction rate and pathway\ndiversity in synthesizable molecule reconstruction and the highest optimization\nperformance in synthesizable goal-directed molecular optimization, and\nsignificantly outperforms previous synthesizable projection methods in\nsynthesizable hit expansion. These results highlight ReaSyn's superior ability\nto navigate combinatorially-large synthesizable chemical space."
                },
                "authors": [
                    {
                        "name": "Seul Lee"
                    },
                    {
                        "name": "Karsten Kreis"
                    },
                    {
                        "name": "Srimukh Prasad Veccham"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Danny Reidenbach"
                    },
                    {
                        "name": "Saee Paliwal"
                    },
                    {
                        "name": "Weili Nie"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13252v2",
                "updated": "2025-09-19T15:19:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    24,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-19T15:35:17Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "title": "Are LLMs Better Formalizers than Solvers on Complex Problems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better Formalizers than Solvers on Complex Problems?"
                },
                "summary": "A trending line of recent work advocates for using large language models\n(LLMs) as formalizers instead of as end-to-end solvers for logical reasoning\nproblems. Instead of generating the solution, the LLM generates a formal\nprogram that derives a solution via an external solver. While performance gain\nof the seemingly scalable LLM-as-formalizer over the seemingly unscalable\nLLM-as-solver has been widely reported, we show that this superiority does not\nhold on real-life constraint satisfaction problems. On 4 domains, we\nsystematically evaluate 6 LLMs including 4 large reasoning models with\ninference-time scaling, paired with 5 pipelines including 2 types of formalism.\nWe show that in few-shot settings, LLM-as-formalizer underperforms\nLLM-as-solver. While LLM-as-formalizer promises accuracy, robustness,\nfaithfulness, and efficiency, we observe that the present LLMs do not yet\ndeliver any of those, as their limited ability to generate formal programs\nleads to failure to scale with complexity, hard-coded solutions, and excessive\nreasoning tokens. We present our detailed analysis and actionable remedies to\ndrive future research that improves LLM-as-formalizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A trending line of recent work advocates for using large language models\n(LLMs) as formalizers instead of as end-to-end solvers for logical reasoning\nproblems. Instead of generating the solution, the LLM generates a formal\nprogram that derives a solution via an external solver. While performance gain\nof the seemingly scalable LLM-as-formalizer over the seemingly unscalable\nLLM-as-solver has been widely reported, we show that this superiority does not\nhold on real-life constraint satisfaction problems. On 4 domains, we\nsystematically evaluate 6 LLMs including 4 large reasoning models with\ninference-time scaling, paired with 5 pipelines including 2 types of formalism.\nWe show that in few-shot settings, LLM-as-formalizer underperforms\nLLM-as-solver. While LLM-as-formalizer promises accuracy, robustness,\nfaithfulness, and efficiency, we observe that the present LLMs do not yet\ndeliver any of those, as their limited ability to generate formal programs\nleads to failure to scale with complexity, hard-coded solutions, and excessive\nreasoning tokens. We present our detailed analysis and actionable remedies to\ndrive future research that improves LLM-as-formalizer."
                },
                "authors": [
                    {
                        "name": "Rikhil Amonkar"
                    },
                    {
                        "name": "May Lai"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16070v1",
                "updated": "2025-09-19T15:18:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    18,
                    41,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:18:41Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    18,
                    41,
                    4,
                    262,
                    0
                ],
                "title": "A Hypothesis-First Framework for Mechanistic Model Evaluation and\n  Selection in Neuroimaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hypothesis-First Framework for Mechanistic Model Evaluation and\n  Selection in Neuroimaging"
                },
                "summary": "Neuroimaging provides rich measurements of brain structure and neural\nactivity, but turning these data into mechanistic insight remains difficult.\nStatistical models quantify associations without much considerations for how\nthey arise, whereas bio-realistic models directly embody candidate mechanisms\nbut remain hard to deploy rigorously without specialized training. We present a\nframework that recasts modeling choices as testable mechanistic hypotheses and\nsupplies a simple protocol for rejecting inappropriate model specifications,\nsuch as under-/over-parameterization or invalid simplifying assumptions, based\non predefined criteria before any parameter inference. The key idea is expected\nmodel behavior under feature generalization constraints: instead of judging a\nmodel solely by how well it fits a specific target feature of interest Y at an\noptimal parameter set, we evaluate the model's expected Y output when the model\nis constrained to reproduce a broader, or distinct, feature Z over the entire\nparameter space. We then assess whether a mirror statistical model, derived\nfrom the model's expected Y outputs, to the empirical statistical model using\nstandard statistics. In synthetic experiments with known ground truth (Wilson\nCowan dynamics), the framework correctly rejects mis-specified hypotheses,\npenalizes unnecessary degrees of freedom, and preserves valid specifications.\nThis provides a practical, hypothesis-first route to using mechanistic models\nfor neuroimaging without requiring expert-level methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroimaging provides rich measurements of brain structure and neural\nactivity, but turning these data into mechanistic insight remains difficult.\nStatistical models quantify associations without much considerations for how\nthey arise, whereas bio-realistic models directly embody candidate mechanisms\nbut remain hard to deploy rigorously without specialized training. We present a\nframework that recasts modeling choices as testable mechanistic hypotheses and\nsupplies a simple protocol for rejecting inappropriate model specifications,\nsuch as under-/over-parameterization or invalid simplifying assumptions, based\non predefined criteria before any parameter inference. The key idea is expected\nmodel behavior under feature generalization constraints: instead of judging a\nmodel solely by how well it fits a specific target feature of interest Y at an\noptimal parameter set, we evaluate the model's expected Y output when the model\nis constrained to reproduce a broader, or distinct, feature Z over the entire\nparameter space. We then assess whether a mirror statistical model, derived\nfrom the model's expected Y outputs, to the empirical statistical model using\nstandard statistics. In synthetic experiments with known ground truth (Wilson\nCowan dynamics), the framework correctly rejects mis-specified hypotheses,\npenalizes unnecessary degrees of freedom, and preserves valid specifications.\nThis provides a practical, hypothesis-first route to using mechanistic models\nfor neuroimaging without requiring expert-level methodology."
                },
                "authors": [
                    {
                        "name": "Dominic Boutet"
                    },
                    {
                        "name": "Sylvain Baillet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Baillet"
                },
                "author": "Sylvain Baillet",
                "arxiv_comment": "20 pages, 7 figures, 5 supplemental figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16067v1",
                "updated": "2025-09-19T15:14:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    14,
                    28,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:14:28Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    14,
                    28,
                    4,
                    262,
                    0
                ],
                "title": "Misspecified learning and evolutionary stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misspecified learning and evolutionary stability"
                },
                "summary": "We extend the indirect evolutionary approach to the selection of (possibly\nmisspecified) models. Agents with different models match in pairs to play a\nstage game, where models define feasible beliefs about game parameters and\nabout others' strategies. In equilibrium, each agent adopts the feasible belief\nthat best fits their data and plays optimally given their beliefs. We define\nthe stability of the resident model by comparing its equilibrium payoff with\nthat of the entrant model, and provide conditions under which the correctly\nspecified resident model can only be destabilized by misspecified entrant\nmodels that contain multiple feasible beliefs (that is, entrant models that\npermit inference). We also show that entrants may do well in their matches\nagainst the residents only when the entrant population is large, due to the\nendogeneity of misspecified beliefs. Applications include the selection of\ndemand-elasticity misperception in Cournot duopoly and the emergence of\nanalogy-based reasoning in centipede games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the indirect evolutionary approach to the selection of (possibly\nmisspecified) models. Agents with different models match in pairs to play a\nstage game, where models define feasible beliefs about game parameters and\nabout others' strategies. In equilibrium, each agent adopts the feasible belief\nthat best fits their data and plays optimally given their beliefs. We define\nthe stability of the resident model by comparing its equilibrium payoff with\nthat of the entrant model, and provide conditions under which the correctly\nspecified resident model can only be destabilized by misspecified entrant\nmodels that contain multiple feasible beliefs (that is, entrant models that\npermit inference). We also show that entrants may do well in their matches\nagainst the residents only when the entrant population is large, due to the\nendogeneity of misspecified beliefs. Applications include the selection of\ndemand-elasticity misperception in Cournot duopoly and the emergence of\nanalogy-based reasoning in centipede games."
                },
                "authors": [
                    {
                        "name": "Kevin He"
                    },
                    {
                        "name": "Jonathan Libgober"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Libgober"
                },
                "author": "Jonathan Libgober",
                "arxiv_doi": "10.1016/j.jet.2025.106082",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jet.2025.106082",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.16067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This material was previously part of a larger paper titled\n  \"Evolutionarily Stable (Mis)specifications: Theory and Applications,\" which\n  split into two smaller papers: \"Misspecified Learning and Evolutionary\n  Stability\" and \"Higher-Order Beliefs and (Mis)learning from Prices.\". arXiv\n  admin note: text overlap with arXiv:2012.15007",
                "arxiv_journal_ref": "Journal of Economic Theory 230:106082, 2025",
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16064v1",
                "updated": "2025-09-19T15:12:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    12,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:12:56Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    12,
                    56,
                    4,
                    262,
                    0
                ],
                "title": "Generating Detailed Character Motion from Blocking Poses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Detailed Character Motion from Blocking Poses"
                },
                "summary": "We focus on the problem of using generative diffusion models for the task of\nmotion detailing: converting a rough version of a character animation,\nrepresented by a sparse set of coarsely posed, and imprecisely timed blocking\nposes, into a detailed, natural looking character animation. Current diffusion\nmodels can address the problem of correcting the timing of imprecisely timed\nposes, but we find that no good solution exists for leveraging the diffusion\nprior to enhance a sparse set of blocking poses with additional pose detail. We\novercome this challenge using a simple inference-time trick. At certain\ndiffusion steps, we blend the outputs of an unconditioned diffusion model with\ninput blocking pose constraints using per-blocking-pose tolerance weights, and\npass this result in as the input condition to an pre-existing motion retiming\nmodel. We find this approach works significantly better than existing\napproaches that attempt to add detail by blending model outputs or via\nexpressing blocking pose constraints as guidance. The result is the first\ndiffusion model that can robustly convert blocking-level poses into plausible\ndetailed character animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We focus on the problem of using generative diffusion models for the task of\nmotion detailing: converting a rough version of a character animation,\nrepresented by a sparse set of coarsely posed, and imprecisely timed blocking\nposes, into a detailed, natural looking character animation. Current diffusion\nmodels can address the problem of correcting the timing of imprecisely timed\nposes, but we find that no good solution exists for leveraging the diffusion\nprior to enhance a sparse set of blocking poses with additional pose detail. We\novercome this challenge using a simple inference-time trick. At certain\ndiffusion steps, we blend the outputs of an unconditioned diffusion model with\ninput blocking pose constraints using per-blocking-pose tolerance weights, and\npass this result in as the input condition to an pre-existing motion retiming\nmodel. We find this approach works significantly better than existing\napproaches that attempt to add detail by blending model outputs or via\nexpressing blocking pose constraints as guidance. The result is the first\ndiffusion model that can robustly convert blocking-level poses into plausible\ndetailed character animations."
                },
                "authors": [
                    {
                        "name": "Purvi Goel"
                    },
                    {
                        "name": "Guy Tevet"
                    },
                    {
                        "name": "C. K. Liu"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    }
                ],
                "author_detail": {
                    "name": "Kayvon Fatahalian"
                },
                "author": "Kayvon Fatahalian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16062v1",
                "updated": "2025-09-19T15:12:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    12,
                    10,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:12:10Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    12,
                    10,
                    4,
                    262,
                    0
                ],
                "title": "Transient regime of piecewise deterministic Monte Carlo algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transient regime of piecewise deterministic Monte Carlo algorithms"
                },
                "summary": "Piecewise Deterministic Markov Processes (PDMPs) such as the Bouncy Particle\nSampler and the Zig-Zag Sampler, have gained attention as continuous-time\ncounterparts of classical Markov chain Monte Carlo. We study their transient\nregime under convex potentials, namely how trajectories that start in\nlow-probability regions move toward higher-probability sets. Using fluid-limit\narguments with a decomposition of the generator into fast and slow parts, we\nobtain deterministic ordinary differential equation descriptions of early-stage\nbehaviour. The fast dynamics alone are non-ergodic because once the event rate\nreaches zero it does not restart. The slow component reactivates the dynamics,\nso averaging remains valid when taken over short micro-cycles rather than with\nrespect to an invariant law.\n  Using the expected number of jump events as a cost proxy for gradient\nevaluations, we find that for Gaussian targets the transient cost of PDMP\nmethods is comparable to that of random-walk Metropolis. For convex\nheavy-tailed families with subquadratic growth, PDMP methods can be more\nefficient when event simulation is implemented well. Forward Event-Chain and\nCoordinate Samplers can, under the same assumptions, reach the typical set with\nan order-one expected number of jumps. For the Zig-Zag Sampler we show that,\nunder a diagonal-dominance condition, the transient choice of direction\ncoincides with the solution of a box-constrained quadratic program; outside\nthat regime we give a formal derivation and a piecewise-smooth update rule that\nclarifies the roles of the gradient and the Hessian. These results provide\ntheoretical insight and practical guidance for the use of PDMP samplers in\nlarge-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise Deterministic Markov Processes (PDMPs) such as the Bouncy Particle\nSampler and the Zig-Zag Sampler, have gained attention as continuous-time\ncounterparts of classical Markov chain Monte Carlo. We study their transient\nregime under convex potentials, namely how trajectories that start in\nlow-probability regions move toward higher-probability sets. Using fluid-limit\narguments with a decomposition of the generator into fast and slow parts, we\nobtain deterministic ordinary differential equation descriptions of early-stage\nbehaviour. The fast dynamics alone are non-ergodic because once the event rate\nreaches zero it does not restart. The slow component reactivates the dynamics,\nso averaging remains valid when taken over short micro-cycles rather than with\nrespect to an invariant law.\n  Using the expected number of jump events as a cost proxy for gradient\nevaluations, we find that for Gaussian targets the transient cost of PDMP\nmethods is comparable to that of random-walk Metropolis. For convex\nheavy-tailed families with subquadratic growth, PDMP methods can be more\nefficient when event simulation is implemented well. Forward Event-Chain and\nCoordinate Samplers can, under the same assumptions, reach the typical set with\nan order-one expected number of jumps. For the Zig-Zag Sampler we show that,\nunder a diagonal-dominance condition, the transient choice of direction\ncoincides with the solution of a box-constrained quadratic program; outside\nthat regime we give a formal derivation and a piecewise-smooth update rule that\nclarifies the roles of the gradient and the Hessian. These results provide\ntheoretical insight and practical guidance for the use of PDMP samplers in\nlarge-scale inference."
                },
                "authors": [
                    {
                        "name": "Sanket Agrawal"
                    },
                    {
                        "name": "Joris Bierkens"
                    },
                    {
                        "name": "Kengo Kamatani"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Gareth O. Roberts"
                },
                "author": "Gareth O. Roberts",
                "arxiv_comment": "39 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16060v1",
                "updated": "2025-09-19T15:10:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:10:19Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    19,
                    4,
                    262,
                    0
                ],
                "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer\n  Residual Connection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer\n  Residual Connection"
                },
                "summary": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER."
                },
                "authors": [
                    {
                        "name": "Maithili Joshi"
                    },
                    {
                        "name": "Palash Nandi"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "Accepted in EMNLP'25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09627v2",
                "updated": "2025-09-19T15:02:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    2,
                    54,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T11:37:02Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    37,
                    2,
                    2,
                    162,
                    0
                ],
                "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates"
                },
                "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions. First, we\nstudy how each methods performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions. First, we\nstudy how each methods performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples."
                },
                "authors": [
                    {
                        "name": "Nicolas Audinet de Pieuchon"
                    },
                    {
                        "name": "Adel Daoud"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Richard Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Johansson"
                },
                "author": "Richard Johansson",
                "arxiv_comment": "To appear as: Nicolas Audinet de Pieuchon, Adel Daoud, Connor T.\n  Jerzak, Moa Johansson, Richard Johansson. Benchmarking Debiasing Methods for\n  LLM-based Parameter Estimates. In: Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08454v2",
                "updated": "2025-09-19T15:01:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    1,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-14T21:55:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    55,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack"
                },
                "summary": "Large language models (LLMs) have become essential tools for digital task\nassistance. Their training relies heavily on the collection of vast amounts of\ndata, which may include copyright-protected or sensitive information. Recent\nstudies on detecting pretraining data in LLMs have primarily focused on\nsentence- or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model's predicted tokens. However,\nthese methods often exhibit poor accuracy, failing to account for the semantic\nimportance of textual content and word significance. To address these\nshortcomings, we propose Tag&Tab, a novel approach for detecting data used in\nLLM pretraining. Our method leverages established natural language processing\n(NLP) techniques to tag keywords in the input text, a process we term Tagging.\nThen, the LLM is used to obtain probabilities for these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on four benchmark datasets (BookMIA,\nMIMIR, PatentMIA, and the Pile) and several open-source LLMs of varying sizes\ndemonstrate an average increase in AUC scores ranging from 5.3% to 17.6% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential tools for digital task\nassistance. Their training relies heavily on the collection of vast amounts of\ndata, which may include copyright-protected or sensitive information. Recent\nstudies on detecting pretraining data in LLMs have primarily focused on\nsentence- or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model's predicted tokens. However,\nthese methods often exhibit poor accuracy, failing to account for the semantic\nimportance of textual content and word significance. To address these\nshortcomings, we propose Tag&Tab, a novel approach for detecting data used in\nLLM pretraining. Our method leverages established natural language processing\n(NLP) techniques to tag keywords in the input text, a process we term Tagging.\nThen, the LLM is used to obtain probabilities for these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on four benchmark datasets (BookMIA,\nMIMIR, PatentMIA, and the Pile) and several open-source LLMs of varying sizes\ndemonstrate an average increase in AUC scores ranging from 5.3% to 17.6% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."
                },
                "authors": [
                    {
                        "name": "Sagiv Antebi"
                    },
                    {
                        "name": "Edan Habler"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Elovici"
                },
                "author": "Yuval Elovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21741v2",
                "updated": "2025-09-19T14:53:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    53,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-29T16:07:33Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance"
                },
                "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines."
                },
                "authors": [
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "Minlong Peng"
                    }
                ],
                "author_detail": {
                    "name": "Minlong Peng"
                },
                "author": "Minlong Peng",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16028v1",
                "updated": "2025-09-19T14:34:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    34,
                    22,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:34:22Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    34,
                    22,
                    4,
                    262,
                    0
                ],
                "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech"
                },
                "summary": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT"
                },
                "authors": [
                    {
                        "name": "Sang Hoon Woo"
                    },
                    {
                        "name": "Sehun Lee"
                    },
                    {
                        "name": "Kang-wook Kim"
                    },
                    {
                        "name": "Gunhee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gunhee Kim"
                },
                "author": "Gunhee Kim",
                "arxiv_comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16027v1",
                "updated": "2025-09-19T14:33:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    33,
                    55,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:33:55Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    33,
                    55,
                    4,
                    262,
                    0
                ],
                "title": "What is a good matching of probability measures? A counterfactual lens\n  on transport maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is a good matching of probability measures? A counterfactual lens\n  on transport maps"
                },
                "summary": "Coupling probability measures lies at the core of many problems in statistics\nand machine learning, from domain adaptation to transfer learning and causal\ninference. Yet, even when restricted to deterministic transports, such\ncouplings are not identifiable: two atomless marginals admit infinitely many\ntransport maps. The common recourse to optimal transport, motivated by cost\nminimization and cyclical monotonicity, obscures the fact that several distinct\nnotions of multivariate monotone matchings coexist. In this work, we first\ncarry a comparative analysis of three constructions of transport maps:\ncyclically monotone, quantile-preserving and triangular monotone maps. We\nestablish necessary and sufficient conditions for their equivalence, thereby\nclarifying their respective structural properties. In parallel, we formulate\ncounterfactual reasoning within the framework of structural causal models as a\nproblem of selecting transport maps between fixed marginals, which makes\nexplicit the role of untestable assumptions in counterfactual reasoning. Then,\nwe are able to connect these two perspectives by identifying conditions on\ncausal graphs and structural equations under which counterfactual maps coincide\nwith classical statistical transports. In this way, we delineate the\ncircumstances in which causal assumptions support the use of a specific\nstructure of transport map. Taken together, our results aim to enrich the\ntheoretical understanding of families of transport maps and to clarify their\npossible causal interpretations. We hope this work contributes to establishing\nnew bridges between statistical transport and causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coupling probability measures lies at the core of many problems in statistics\nand machine learning, from domain adaptation to transfer learning and causal\ninference. Yet, even when restricted to deterministic transports, such\ncouplings are not identifiable: two atomless marginals admit infinitely many\ntransport maps. The common recourse to optimal transport, motivated by cost\nminimization and cyclical monotonicity, obscures the fact that several distinct\nnotions of multivariate monotone matchings coexist. In this work, we first\ncarry a comparative analysis of three constructions of transport maps:\ncyclically monotone, quantile-preserving and triangular monotone maps. We\nestablish necessary and sufficient conditions for their equivalence, thereby\nclarifying their respective structural properties. In parallel, we formulate\ncounterfactual reasoning within the framework of structural causal models as a\nproblem of selecting transport maps between fixed marginals, which makes\nexplicit the role of untestable assumptions in counterfactual reasoning. Then,\nwe are able to connect these two perspectives by identifying conditions on\ncausal graphs and structural equations under which counterfactual maps coincide\nwith classical statistical transports. In this way, we delineate the\ncircumstances in which causal assumptions support the use of a specific\nstructure of transport map. Taken together, our results aim to enrich the\ntheoretical understanding of families of transport maps and to clarify their\npossible causal interpretations. We hope this work contributes to establishing\nnew bridges between statistical transport and causal inference."
                },
                "authors": [
                    {
                        "name": "Lucas De Lara"
                    },
                    {
                        "name": "Luca Ganassali"
                    }
                ],
                "author_detail": {
                    "name": "Luca Ganassali"
                },
                "author": "Luca Ganassali",
                "arxiv_comment": "37 pages; comments most welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16022v1",
                "updated": "2025-09-19T14:31:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    31,
                    40,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:31:40Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    31,
                    40,
                    4,
                    262,
                    0
                ],
                "title": "Generalized Deep Multi-view Clustering via Causal Learning with\n  Partially Aligned Cross-view Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Deep Multi-view Clustering via Causal Learning with\n  Partially Aligned Cross-view Correspondence"
                },
                "summary": "Multi-view clustering (MVC) aims to explore the common clustering structure\nacross multiple views. Many existing MVC methods heavily rely on the assumption\nof view consistency, where alignments for corresponding samples across\ndifferent views are ordered in advance. However, real-world scenarios often\npresent a challenge as only partial data is consistently aligned across\ndifferent views, restricting the overall clustering performance. In this work,\nwe consider the model performance decreasing phenomenon caused by data order\nshift (i.e., from fully to partially aligned) as a generalized multi-view\nclustering problem. To tackle this problem, we design a causal multi-view\nclustering network, termed CauMVC. We adopt a causal modeling approach to\nunderstand multi-view clustering procedure. To be specific, we formulate the\npartially aligned data as an intervention and multi-view clustering with\npartially aligned data as an post-intervention inference. However, obtaining\ninvariant features directly can be challenging. Thus, we design a Variational\nAuto-Encoder for causal learning by incorporating an encoder from existing\ninformation to estimate the invariant features. Moreover, a decoder is designed\nto perform the post-intervention inference. Lastly, we design a contrastive\nregularizer to capture sample correlations. To the best of our knowledge, this\npaper is the first work to deal generalized multi-view clustering via causal\nlearning. Empirical experiments on both fully and partially aligned data\nillustrate the strong generalization and effectiveness of CauMVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view clustering (MVC) aims to explore the common clustering structure\nacross multiple views. Many existing MVC methods heavily rely on the assumption\nof view consistency, where alignments for corresponding samples across\ndifferent views are ordered in advance. However, real-world scenarios often\npresent a challenge as only partial data is consistently aligned across\ndifferent views, restricting the overall clustering performance. In this work,\nwe consider the model performance decreasing phenomenon caused by data order\nshift (i.e., from fully to partially aligned) as a generalized multi-view\nclustering problem. To tackle this problem, we design a causal multi-view\nclustering network, termed CauMVC. We adopt a causal modeling approach to\nunderstand multi-view clustering procedure. To be specific, we formulate the\npartially aligned data as an intervention and multi-view clustering with\npartially aligned data as an post-intervention inference. However, obtaining\ninvariant features directly can be challenging. Thus, we design a Variational\nAuto-Encoder for causal learning by incorporating an encoder from existing\ninformation to estimate the invariant features. Moreover, a decoder is designed\nto perform the post-intervention inference. Lastly, we design a contrastive\nregularizer to capture sample correlations. To the best of our knowledge, this\npaper is the first work to deal generalized multi-view clustering via causal\nlearning. Empirical experiments on both fully and partially aligned data\nillustrate the strong generalization and effectiveness of CauMVC."
                },
                "authors": [
                    {
                        "name": "Xihong Yang"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Jiaqi Jin"
                    },
                    {
                        "name": "Fangdi Wang"
                    },
                    {
                        "name": "Tianrui Liu"
                    },
                    {
                        "name": "Yueming Jin"
                    },
                    {
                        "name": "Xinwang Liu"
                    },
                    {
                        "name": "En Zhu"
                    },
                    {
                        "name": "Kunlun He"
                    }
                ],
                "author_detail": {
                    "name": "Kunlun He"
                },
                "author": "Kunlun He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08177v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08177v4",
                "updated": "2025-09-19T14:30:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    30,
                    28,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-12T07:32:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "SycEval: Evaluating LLM Sycophancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SycEval: Evaluating LLM Sycophancy"
                },
                "summary": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications."
                },
                "authors": [
                    {
                        "name": "Aaron Fanous"
                    },
                    {
                        "name": "Jacob Goldberg"
                    },
                    {
                        "name": "Ank A. Agarwal"
                    },
                    {
                        "name": "Joanna Lin"
                    },
                    {
                        "name": "Anson Zhou"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "arxiv_affiliation": "Stanford University",
                "author": "Sanmi Koyejo",
                "arxiv_comment": "AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08177v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08177v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10371v2",
                "updated": "2025-09-19T14:28:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    28,
                    47,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-12T16:05:07Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective"
                },
                "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT."
                },
                "authors": [
                    {
                        "name": "Seokjin Go"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Spandan More"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Aaron Jezghani"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14395v2",
                "updated": "2025-09-19T14:26:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    26,
                    2,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-20T14:14:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language"
                },
                "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy for successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy for successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Dongkwan Kim"
                    },
                    {
                        "name": "Jay Shin"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "To appear in Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16011v1",
                "updated": "2025-09-19T14:24:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    24,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    24,
                    48,
                    4,
                    262,
                    0
                ],
                "title": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision"
                },
                "summary": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning."
                },
                "authors": [
                    {
                        "name": "Xiwei Liu"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16006v1",
                "updated": "2025-09-19T14:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    19,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    19,
                    44,
                    4,
                    262,
                    0
                ],
                "title": "Defining and Monitoring Complex Robot Activities via LLMs and Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Monitoring Complex Robot Activities via LLMs and Symbolic\n  Reasoning"
                },
                "summary": "Recent years have witnessed a growing interest in automating labor-intensive\nand complex activities, i.e., those consisting of multiple atomic tasks, by\ndeploying robots in dynamic and unpredictable environments such as industrial\nand agricultural settings. A key characteristic of these contexts is that\nactivities are not predefined: while they involve a limited set of possible\ntasks, their combinations may vary depending on the situation. Moreover,\ndespite recent advances in robotics, the ability for humans to monitor the\nprogress of high-level activities - in terms of past, present, and future\nactions - remains fundamental to ensure the correct execution of\nsafety-critical processes. In this paper, we introduce a general architecture\nthat integrates Large Language Models (LLMs) with automated planning, enabling\nhumans to specify high-level activities (also referred to as processes) using\nnatural language, and to monitor their execution by querying a robot. We also\npresent an implementation of this architecture using state-of-the-art\ncomponents and quantitatively evaluate the approach in a real-world precision\nagriculture scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed a growing interest in automating labor-intensive\nand complex activities, i.e., those consisting of multiple atomic tasks, by\ndeploying robots in dynamic and unpredictable environments such as industrial\nand agricultural settings. A key characteristic of these contexts is that\nactivities are not predefined: while they involve a limited set of possible\ntasks, their combinations may vary depending on the situation. Moreover,\ndespite recent advances in robotics, the ability for humans to monitor the\nprogress of high-level activities - in terms of past, present, and future\nactions - remains fundamental to ensure the correct execution of\nsafety-critical processes. In this paper, we introduce a general architecture\nthat integrates Large Language Models (LLMs) with automated planning, enabling\nhumans to specify high-level activities (also referred to as processes) using\nnatural language, and to monitor their execution by querying a robot. We also\npresent an implementation of this architecture using state-of-the-art\ncomponents and quantitatively evaluate the approach in a real-world precision\nagriculture scenario."
                },
                "authors": [
                    {
                        "name": "Francesco Argenziano"
                    },
                    {
                        "name": "Elena Umili"
                    },
                    {
                        "name": "Francesco Leotta"
                    },
                    {
                        "name": "Daniele Nardi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Nardi"
                },
                "author": "Daniele Nardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06845v2",
                "updated": "2025-09-19T14:00:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    0,
                    6,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-09T13:02:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    2,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "Probability density function for dispersion measure of fast radio burst\n  from extragalactic medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability density function for dispersion measure of fast radio burst\n  from extragalactic medium"
                },
                "summary": "Fast Radio Bursts (FRBs) have emerged as powerful probes in cosmology. An\noptimized method was recently proposed to extract the cosmic baryon density\nfrom localized FRBs by maximizing the joint likelihood function of the\nextragalactic dispersion measure ($\\mathrm{DM}_{\\mathrm{ext}}$). In this paper,\nwe identify a crucial factor that was omitted in the probability density\nfunction (PDF) for $\\mathrm{DM}_{\\mathrm{ext}}$ in that method. Using simulated\nFRB data, we demonstrate that neglecting this factor leads to a systematic bias\nin the inferred cosmic baryon density, with deviations exceeding the $1\\sigma$\nconfidence level. This highlights the necessity of including the missing factor\nfor reliable cosmological applications of FRBs. Furthermore, applying our\ncorrected PDF to a sample of 88 real localized FRBs, we find that the baryon\ndensity inferred with the original PDF is inconsistent with the Planck 2018 CMB\nresults, whereas our corrected PDF yields excellent agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Radio Bursts (FRBs) have emerged as powerful probes in cosmology. An\noptimized method was recently proposed to extract the cosmic baryon density\nfrom localized FRBs by maximizing the joint likelihood function of the\nextragalactic dispersion measure ($\\mathrm{DM}_{\\mathrm{ext}}$). In this paper,\nwe identify a crucial factor that was omitted in the probability density\nfunction (PDF) for $\\mathrm{DM}_{\\mathrm{ext}}$ in that method. Using simulated\nFRB data, we demonstrate that neglecting this factor leads to a systematic bias\nin the inferred cosmic baryon density, with deviations exceeding the $1\\sigma$\nconfidence level. This highlights the necessity of including the missing factor\nfor reliable cosmological applications of FRBs. Furthermore, applying our\ncorrected PDF to a sample of 88 real localized FRBs, we find that the baryon\ndensity inferred with the original PDF is inconsistent with the Planck 2018 CMB\nresults, whereas our corrected PDF yields excellent agreement."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hongwei Yu"
                    },
                    {
                        "name": "Puxun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Puxun Wu"
                },
                "author": "Puxun Wu",
                "arxiv_comment": "15 pages, 2 figures. To appear in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15989v1",
                "updated": "2025-09-19T13:57:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    57,
                    17,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:57:17Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    57,
                    17,
                    4,
                    262,
                    0
                ],
                "title": "Model-free algorithms for fast node clustering in SBM type graphs and\n  application to social role inference in animals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-free algorithms for fast node clustering in SBM type graphs and\n  application to social role inference in animals"
                },
                "summary": "We propose a novel family of model-free algorithms for node clustering and\nparameter inference in graphs generated from the Stochastic Block Model (SBM),\na fundamental framework in community detection. Drawing inspiration from the\nLloyd algorithm for the $k$-means problem, our approach extends to SBMs with\ngeneral edge weight distributions. We establish the consistency of our\nestimator under a natural identifiability condition. Through extensive\nnumerical experiments, we benchmark our methods against state-of-the-art\ntechniques, demonstrating significantly faster computation times with the lower\norder of estimation error. Finally, we validate the practical relevance of our\nalgorithms by applying them to empirical network data from behavioral ecology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel family of model-free algorithms for node clustering and\nparameter inference in graphs generated from the Stochastic Block Model (SBM),\na fundamental framework in community detection. Drawing inspiration from the\nLloyd algorithm for the $k$-means problem, our approach extends to SBMs with\ngeneral edge weight distributions. We establish the consistency of our\nestimator under a natural identifiability condition. Through extensive\nnumerical experiments, we benchmark our methods against state-of-the-art\ntechniques, demonstrating significantly faster computation times with the lower\norder of estimation error. Finally, we validate the practical relevance of our\nalgorithms by applying them to empirical network data from behavioral ecology."
                },
                "authors": [
                    {
                        "name": "Bertrand Cloez"
                    },
                    {
                        "name": "Adrien Cotil"
                    },
                    {
                        "name": "Jean-Baptiste Menassol"
                    },
                    {
                        "name": "Nicolas Verzelen"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Verzelen"
                },
                "author": "Nicolas Verzelen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62Fxx, 62Lxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15389v2",
                "updated": "2025-09-19T13:54:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    54,
                    9,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-21T11:26:40Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    26,
                    40,
                    2,
                    141,
                    0
                ],
                "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study"
                },
                "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jihae Jeong"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13090v2",
                "updated": "2025-09-19T13:54:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    54,
                    4,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-19T13:24:01Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    24,
                    1,
                    0,
                    139,
                    0
                ],
                "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation"
                },
                "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."
                },
                "authors": [
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "EMNLP 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03455v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03455v3",
                "updated": "2025-09-19T13:47:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    47,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2024-11-05T19:13:22Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    13,
                    22,
                    1,
                    310,
                    0
                ],
                "title": "Watson: A Cognitive Observability Framework for the Reasoning of\n  LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watson: A Cognitive Observability Framework for the Reasoning of\n  LLM-Powered Agents"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into autonomous\nsystems, giving rise to a new class of software known as Agentware, where\nLLM-powered agents perform complex, open-ended tasks in domains such as\nsoftware engineering, customer service, and data analysis. However, their high\nautonomy and opaque reasoning processes pose significant challenges for\ntraditional software observability methods. To address this, we introduce the\nconcept of cognitive observability - the ability to recover and inspect the\nimplicit reasoning behind agent decisions. We present Watson, a general-purpose\nframework for observing the reasoning processes of fast-thinking LLM agents\nwithout altering their behavior. Watson retroactively infers reasoning traces\nusing prompt attribution techniques. We evaluate Watson in both manual\ndebugging and automated correction scenarios across the MMLU benchmark and the\nAutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both\nstatic and dynamic settings, Watson surfaces actionable reasoning insights and\nsupports targeted interventions, demonstrating its practical utility for\nimproving transparency and reliability in Agentware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into autonomous\nsystems, giving rise to a new class of software known as Agentware, where\nLLM-powered agents perform complex, open-ended tasks in domains such as\nsoftware engineering, customer service, and data analysis. However, their high\nautonomy and opaque reasoning processes pose significant challenges for\ntraditional software observability methods. To address this, we introduce the\nconcept of cognitive observability - the ability to recover and inspect the\nimplicit reasoning behind agent decisions. We present Watson, a general-purpose\nframework for observing the reasoning processes of fast-thinking LLM agents\nwithout altering their behavior. Watson retroactively infers reasoning traces\nusing prompt attribution techniques. We evaluate Watson in both manual\ndebugging and automated correction scenarios across the MMLU benchmark and the\nAutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both\nstatic and dynamic settings, Watson surfaces actionable reasoning insights and\nsupports targeted interventions, demonstrating its practical utility for\nimproving transparency and reliability in Agentware systems."
                },
                "authors": [
                    {
                        "name": "Benjamin Rombaut"
                    },
                    {
                        "name": "Sogol Masoumzadeh"
                    },
                    {
                        "name": "Kirill Vasilevski"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03455v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03455v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15756v2",
                "updated": "2025-09-19T13:45:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    45,
                    43,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-22T10:09:33Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    9,
                    33,
                    1,
                    112,
                    0
                ],
                "title": "DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy"
                },
                "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster than\nthe second-best method, highlighting its practical advantages. We provide an\nanonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster than\nthe second-best method, highlighting its practical advantages. We provide an\nanonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/."
                },
                "authors": [
                    {
                        "name": "Qirui Yang"
                    },
                    {
                        "name": "Fangpu Zhang"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Qihua Cheng"
                    },
                    {
                        "name": "Peng-Tao Jiang"
                    },
                    {
                        "name": "Huanjing Yue"
                    },
                    {
                        "name": "Jingyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyu Yang"
                },
                "author": "Jingyu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10392v2",
                "updated": "2025-09-19T13:45:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    45,
                    24,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-15T15:14:02Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    14,
                    2,
                    3,
                    135,
                    0
                ],
                "title": "Schreier-Coset Graph Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schreier-Coset Graph Propagation"
                },
                "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications."
                },
                "authors": [
                    {
                        "name": "Aryan Mishra"
                    },
                    {
                        "name": "Lizhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Lin"
                },
                "author": "Lizhen Lin",
                "arxiv_comment": "The paper has been updated and now utilizes a more comprehensive\n  methodology, we felt that the name does not do justice to it as their is no\n  graph rewiring involved. Our method adds embeddings at the every beginning of\n  before the propagation begins which is essentially feature augmentation. We\n  have a more comprehensive method including graph rewiring which we will\n  release in due course of time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14091v2",
                "updated": "2025-09-19T13:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    40,
                    31,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-18T22:15:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    15,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "DataMaestro: A Versatile and Efficient Data Streaming Engine Bringing\n  Decoupled Memory Access To Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMaestro: A Versatile and Efficient Data Streaming Engine Bringing\n  Decoupled Memory Access To Dataflow Accelerators"
                },
                "summary": "Deep Neural Networks (DNNs) have achieved remarkable success across various\nintelligent tasks but encounter performance and energy challenges in inference\nexecution due to data movement bottlenecks. We introduce DataMaestro, a\nversatile and efficient data streaming unit that brings the decoupled\naccess/execute architecture to DNN dataflow accelerators to address this issue.\nDataMaestro supports flexible and programmable access patterns to accommodate\ndiverse workload types and dataflows, incorporates fine-grained prefetch and\naddressing mode switching to mitigate bank conflicts, and enables customizable\non-the-fly data manipulation to reduce memory footprints and access counts. We\nintegrate five DataMaestros with a Tensor Core-like GeMM accelerator and a\nQuantization accelerator into a RISC-V host system for evaluation. The FPGA\nprototype and VLSI synthesis results demonstrate that DataMaestro helps the\nGeMM core achieve nearly 100% utilization, which is 1.05-21.39x better than\nstate-of-the-art solutions, while minimizing area and energy consumption to\nmerely 6.43% and 15.06% of the total system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks (DNNs) have achieved remarkable success across various\nintelligent tasks but encounter performance and energy challenges in inference\nexecution due to data movement bottlenecks. We introduce DataMaestro, a\nversatile and efficient data streaming unit that brings the decoupled\naccess/execute architecture to DNN dataflow accelerators to address this issue.\nDataMaestro supports flexible and programmable access patterns to accommodate\ndiverse workload types and dataflows, incorporates fine-grained prefetch and\naddressing mode switching to mitigate bank conflicts, and enables customizable\non-the-fly data manipulation to reduce memory footprints and access counts. We\nintegrate five DataMaestros with a Tensor Core-like GeMM accelerator and a\nQuantization accelerator into a RISC-V host system for evaluation. The FPGA\nprototype and VLSI synthesis results demonstrate that DataMaestro helps the\nGeMM core achieve nearly 100% utilization, which is 1.05-21.39x better than\nstate-of-the-art solutions, while minimizing area and energy consumption to\nmerely 6.43% and 15.06% of the total system."
                },
                "authors": [
                    {
                        "name": "Xiaoling Yi"
                    },
                    {
                        "name": "Yunhao Deng"
                    },
                    {
                        "name": "Ryan Antonio"
                    },
                    {
                        "name": "Fanchen Kong"
                    },
                    {
                        "name": "Guilherme Paim"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21943v2",
                "updated": "2025-09-19T13:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    39,
                    47,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-27T06:35:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    6,
                    35,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "Single-Trajectory Bayesian Modeling Reveals Multi-State Diffusion of the\n  MSH Sliding Clamp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-Trajectory Bayesian Modeling Reveals Multi-State Diffusion of the\n  MSH Sliding Clamp"
                },
                "summary": "DNA mismatch repair (MMR) is the essential mechanism for preserving genomic\nintegrity in various living organisms. In this process, MutS homologs (MSH)\nplay crucial roles in identifying mismatched basepairs and recruiting\ndownstream MMR proteins. The MSH protein exhibits distinct functions and\ndiffusion dynamics before and after the recognition of mismatches while\ntraversing along DNA. An ADP-bound MSH, known as the MSH searching clamp, scans\nDNA sequences via rotational diffusion along the DNA backbone. Upon recognizing\na mismatch, the MSH combines with ATP molecules, forming a stable sliding\nclamp. Recent experimental evidence challenges the conventional view that the\nsliding clamp performs a simple Brownian motion. In this study, we explore the\ndiffusion dynamics of the ATP-bound MSH sliding clamp through single-particle\ntracking experiments and introduce a Bayesian single-trajectory modeling\nframework to analyze its motion. Our quantitative analysis reveals that the\ndiffusion characteristics defy explanation by a single-state diffusion\nmechanism. Instead, our in-depth model inference uncovers three distinct\ndiffusion states, each characterized by specific diffusion coefficients. These\nstates alternate over time, with cross-state transitions predominantly\ninvolving one intermediate state, and direct transitions between the slowest\nand the fastest states being scarce. We propose that these multi-state dynamics\nreflect underlying conformational changes in the MSH sliding clamp,\nhighlighting a more intricate diffusion mechanism than previously appreciated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNA mismatch repair (MMR) is the essential mechanism for preserving genomic\nintegrity in various living organisms. In this process, MutS homologs (MSH)\nplay crucial roles in identifying mismatched basepairs and recruiting\ndownstream MMR proteins. The MSH protein exhibits distinct functions and\ndiffusion dynamics before and after the recognition of mismatches while\ntraversing along DNA. An ADP-bound MSH, known as the MSH searching clamp, scans\nDNA sequences via rotational diffusion along the DNA backbone. Upon recognizing\na mismatch, the MSH combines with ATP molecules, forming a stable sliding\nclamp. Recent experimental evidence challenges the conventional view that the\nsliding clamp performs a simple Brownian motion. In this study, we explore the\ndiffusion dynamics of the ATP-bound MSH sliding clamp through single-particle\ntracking experiments and introduce a Bayesian single-trajectory modeling\nframework to analyze its motion. Our quantitative analysis reveals that the\ndiffusion characteristics defy explanation by a single-state diffusion\nmechanism. Instead, our in-depth model inference uncovers three distinct\ndiffusion states, each characterized by specific diffusion coefficients. These\nstates alternate over time, with cross-state transitions predominantly\ninvolving one intermediate state, and direct transitions between the slowest\nand the fastest states being scarce. We propose that these multi-state dynamics\nreflect underlying conformational changes in the MSH sliding clamp,\nhighlighting a more intricate diffusion mechanism than previously appreciated."
                },
                "authors": [
                    {
                        "name": "Seongyu Park"
                    },
                    {
                        "name": "Inho Yang"
                    },
                    {
                        "name": "Jinseob Lee"
                    },
                    {
                        "name": "Sinwoo Kim"
                    },
                    {
                        "name": "Juana Martín-López"
                    },
                    {
                        "name": "Richard Fishel"
                    },
                    {
                        "name": "Jong-Bong Lee"
                    },
                    {
                        "name": "Jae-Hyung Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Hyung Jeon"
                },
                "author": "Jae-Hyung Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15976v1",
                "updated": "2025-09-19T13:36:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    36,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:36:25Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    36,
                    25,
                    4,
                    262,
                    0
                ],
                "title": "The GECKOS Survey: revealing the formation history of a barred galaxy\n  via structural decomposition and resolved spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GECKOS Survey: revealing the formation history of a barred galaxy\n  via structural decomposition and resolved spectroscopy"
                },
                "summary": "Disentangling the (co-)evolution of individual galaxy structural components\nremains a difficult task owing to the inability to cleanly isolate light from\nspatially overlapping components. In this pilot study of PGC 044931, observed\nas part of the GECKOS survey, we utilise VIRCAM $H$-band imaging to decompose\nthe galaxy into five photometric components, three of which contribute $>50\\%$\nof light in given regions: a main disc, a boxy/peanut bulge, and a nuclear\ndisc. When the photometric decompositions are mapped onto MUSE observations, we\nfind remarkably good separation in stellar kinematic space. All three\nstructures occupy unique locations in the parameter space of the ratio of\nstellar line-of-sight velocity ($\\rm{V}_{\\star}$) and dispersion,\n($\\sigma_{\\star}$), and high order stellar skew, ($h_{3}$). These clear and\ndistinct kinematic signatures give us confidence to make inferences about the\nformation history of the individual components from observations of the mean\nlight-weighted stellar age and metallicity of the three components. A clear\nstory emerges: one in which an extended galactic disc hosted continued star\nformation, possibly with the addition of accreted pristine gas. Within the\ndisc, a bar formed and buckled early, building a nuclear disc that continued to\nenrich via multiple generations of star formation. These results exemplify how\ncareful photometric decompositions, combined with well-resolved stellar\nkinematic information, can help separate out age-metallicity relations of\ndifferent components and therefore disentangle the formation history of the\ngalaxy. The results of this pilot survey are applicable to modern,\nwell-resolved spectroscopic galaxy surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling the (co-)evolution of individual galaxy structural components\nremains a difficult task owing to the inability to cleanly isolate light from\nspatially overlapping components. In this pilot study of PGC 044931, observed\nas part of the GECKOS survey, we utilise VIRCAM $H$-band imaging to decompose\nthe galaxy into five photometric components, three of which contribute $>50\\%$\nof light in given regions: a main disc, a boxy/peanut bulge, and a nuclear\ndisc. When the photometric decompositions are mapped onto MUSE observations, we\nfind remarkably good separation in stellar kinematic space. All three\nstructures occupy unique locations in the parameter space of the ratio of\nstellar line-of-sight velocity ($\\rm{V}_{\\star}$) and dispersion,\n($\\sigma_{\\star}$), and high order stellar skew, ($h_{3}$). These clear and\ndistinct kinematic signatures give us confidence to make inferences about the\nformation history of the individual components from observations of the mean\nlight-weighted stellar age and metallicity of the three components. A clear\nstory emerges: one in which an extended galactic disc hosted continued star\nformation, possibly with the addition of accreted pristine gas. Within the\ndisc, a bar formed and buckled early, building a nuclear disc that continued to\nenrich via multiple generations of star formation. These results exemplify how\ncareful photometric decompositions, combined with well-resolved stellar\nkinematic information, can help separate out age-metallicity relations of\ndifferent components and therefore disentangle the formation history of the\ngalaxy. The results of this pilot survey are applicable to modern,\nwell-resolved spectroscopic galaxy surveys."
                },
                "authors": [
                    {
                        "name": "A. Fraser-McKelvie"
                    },
                    {
                        "name": "D. A. Gadotti"
                    },
                    {
                        "name": "F. Fragkoudi"
                    },
                    {
                        "name": "C. de Sá-Freitas"
                    },
                    {
                        "name": "M. Martig"
                    },
                    {
                        "name": "T. Davis"
                    },
                    {
                        "name": "R. Elliott"
                    },
                    {
                        "name": "D. Fisher"
                    },
                    {
                        "name": "M. R. Hayden"
                    },
                    {
                        "name": "J. van de Sande"
                    },
                    {
                        "name": "A. B. Watts"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Watts"
                },
                "author": "A. B. Watts",
                "arxiv_comment": "4 pages, 3 figures, submitted to A&A Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15975v1",
                "updated": "2025-09-19T13:35:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:35:51Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "title": "Extremal Steklov-Neumann Eigenvalues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremal Steklov-Neumann Eigenvalues"
                },
                "summary": "Let $\\Omega$ be a bounded open planar domain with smooth connected boundary,\n$\\Gamma$, that has been partitioned into two disjoint components, $\\Gamma =\n\\Gamma_S \\sqcup \\Gamma_N$. We consider the Steklov-Neumann eigenproblem on\n$\\Omega$, where a harmonic function is sought that satisfies the Steklov\nboundary condition on $\\Gamma_S$ and the Neumann boundary condition on\n$\\Gamma_N$. We pose the extremal eigenvalue problems (EEPs) of\nminimizing/maximizing the $k$-th non-trivial Steklov-Neumann eigenvalue among\nboundary partitions of prescribed measure. We formulate a relaxation of these\nEEPs in terms of weighted Steklov eigenvalues where an $L^\\infty(\\Gamma)$\ndensity replaces the boundary partition. For these relaxed EEPs, we establish\nexistence, prove optimality conditions, show that the maximization problem is\nconvex for $k=1$ and non-convex for $k\\geq 2$, and establish symmetry\nproperties for the maximizing densities for $k=1$. We also prove a\nhomogenization result that allows us to use solutions to the relaxed EEPs to\ninfer properties of solutions to the original EEPs. For a disk, we provide\nnumerical and asymptotic evidence that the minimizing arrangement of\n$\\Gamma_S\\sqcup \\Gamma_N$ for the $k$-th eigenvalue consists of $k+1$ connected\ncomponents that are symmetrically arranged on the boundary. For a disk, we\nprove that for $k = 1$, the constant density is a maximizer for the relaxed\nproblem; we also provide numerical and asymptotic evidence that for $k\\ge 2$,\nthe maximizing density for the relaxed problem is a non-trivial function; a\nsequence of rapidly oscillating Steklov/Neumann boundary conditions approach\nthe supremum value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $\\Omega$ be a bounded open planar domain with smooth connected boundary,\n$\\Gamma$, that has been partitioned into two disjoint components, $\\Gamma =\n\\Gamma_S \\sqcup \\Gamma_N$. We consider the Steklov-Neumann eigenproblem on\n$\\Omega$, where a harmonic function is sought that satisfies the Steklov\nboundary condition on $\\Gamma_S$ and the Neumann boundary condition on\n$\\Gamma_N$. We pose the extremal eigenvalue problems (EEPs) of\nminimizing/maximizing the $k$-th non-trivial Steklov-Neumann eigenvalue among\nboundary partitions of prescribed measure. We formulate a relaxation of these\nEEPs in terms of weighted Steklov eigenvalues where an $L^\\infty(\\Gamma)$\ndensity replaces the boundary partition. For these relaxed EEPs, we establish\nexistence, prove optimality conditions, show that the maximization problem is\nconvex for $k=1$ and non-convex for $k\\geq 2$, and establish symmetry\nproperties for the maximizing densities for $k=1$. We also prove a\nhomogenization result that allows us to use solutions to the relaxed EEPs to\ninfer properties of solutions to the original EEPs. For a disk, we provide\nnumerical and asymptotic evidence that the minimizing arrangement of\n$\\Gamma_S\\sqcup \\Gamma_N$ for the $k$-th eigenvalue consists of $k+1$ connected\ncomponents that are symmetrically arranged on the boundary. For a disk, we\nprove that for $k = 1$, the constant density is a maximizer for the relaxed\nproblem; we also provide numerical and asymptotic evidence that for $k\\ge 2$,\nthe maximizing density for the relaxed problem is a non-trivial function; a\nsequence of rapidly oscillating Steklov/Neumann boundary conditions approach\nthe supremum value."
                },
                "authors": [
                    {
                        "name": "Chiu-Yen Kao"
                    },
                    {
                        "name": "Braxton Osting"
                    },
                    {
                        "name": "Chee Han Tan"
                    },
                    {
                        "name": "Robert Viator"
                    }
                ],
                "author_detail": {
                    "name": "Robert Viator"
                },
                "author": "Robert Viator",
                "arxiv_comment": "23 pages, 6 figures, 2 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "31A25, 35P15, 49M41, 65K10, 65N25, 49R05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23386v2",
                "updated": "2025-09-19T13:35:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-31T10:01:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models"
                },
                "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods."
                },
                "authors": [
                    {
                        "name": "Ailiang Lin"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Kotaro Funakoshi"
                    },
                    {
                        "name": "Manabu Okumura"
                    }
                ],
                "author_detail": {
                    "name": "Manabu Okumura"
                },
                "author": "Manabu Okumura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15974v1",
                "updated": "2025-09-19T13:35:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    7,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:35:07Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    7,
                    4,
                    262,
                    0
                ],
                "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEFT: Bias-Efficient Fine-Tuning of Language Models"
                },
                "summary": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks."
                },
                "authors": [
                    {
                        "name": "Baichuan Huang"
                    },
                    {
                        "name": "Ananth Balashankar"
                    },
                    {
                        "name": "Amir Aminifar"
                    }
                ],
                "author_detail": {
                    "name": "Amir Aminifar"
                },
                "author": "Amir Aminifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18352v2",
                "updated": "2025-09-19T13:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    5,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-24T12:25:12Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    12,
                    25,
                    12,
                    3,
                    205,
                    0
                ],
                "title": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny is not small enough: High-quality, low-resource facial animation\n  models through hybrid knowledge distillation"
                },
                "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters."
                },
                "authors": [
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Mattias Teye"
                    },
                    {
                        "name": "Derek Yadgaroff"
                    },
                    {
                        "name": "Judith Bütepage"
                    }
                ],
                "author_detail": {
                    "name": "Judith Bütepage"
                },
                "author": "Judith Bütepage",
                "arxiv_doi": "10.1145/3730929",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730929",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.18352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM TOG 2025 (SIGGRAPH journal track); Project page:\n  https://electronicarts.github.io/tiny-voice2face/",
                "arxiv_journal_ref": "ACM Transactions on Graphics, Vol. 44, No. 4, Article 104, July\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01322v2",
                "updated": "2025-09-19T13:34:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    34,
                    47,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-01T10:05:45Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    5,
                    45,
                    0,
                    244,
                    0
                ],
                "title": "LongCat-Flash Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCat-Flash Technical Report"
                },
                "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat"
                },
                "authors": [
                    {
                        "name": "Meituan LongCat Team"
                    },
                    {
                        "name": "Bayan"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Bingye Lei"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bolin Rong"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Chong Peng"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Chuyu Zhang"
                    },
                    {
                        "name": "Cong Chen"
                    },
                    {
                        "name": "Congkui Wang"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Daoru Pan"
                    },
                    {
                        "name": "Defei Bu"
                    },
                    {
                        "name": "Dengchang Zhao"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Dishan Liu"
                    },
                    {
                        "name": "Feiye Huo"
                    },
                    {
                        "name": "Fengcun Li"
                    },
                    {
                        "name": "Fubao Zhang"
                    },
                    {
                        "name": "Gan Dong"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Gang Xu"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Guoqiang Tan"
                    },
                    {
                        "name": "Guoyuan Lin"
                    },
                    {
                        "name": "Haihang Jing"
                    },
                    {
                        "name": "Haomin Fu"
                    },
                    {
                        "name": "Haonan Yan"
                    },
                    {
                        "name": "Haoxing Wen"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Hongmei Shi"
                    },
                    {
                        "name": "Hongyan Hao"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Huantian Lv"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Jiajun Yang"
                    },
                    {
                        "name": "Jiaming Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Sun"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Jiawei Fu"
                    },
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Jiayu Qin"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Jiyuan He"
                    },
                    {
                        "name": "Jun Kuang"
                    },
                    {
                        "name": "Junhui Mei"
                    },
                    {
                        "name": "Kai Liang"
                    },
                    {
                        "name": "Ke He"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Keheng Wang"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Liang Gao"
                    },
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Lianhui Ma"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Lingbin Kong"
                    },
                    {
                        "name": "Lingtong Si"
                    },
                    {
                        "name": "Linkun Lyu"
                    },
                    {
                        "name": "Linsen Guo"
                    },
                    {
                        "name": "Liqi Yang"
                    },
                    {
                        "name": "Lizhi Yan"
                    },
                    {
                        "name": "Mai Xia"
                    },
                    {
                        "name": "Man Gao"
                    },
                    {
                        "name": "Manyuan Zhang"
                    },
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Mengxia Shen"
                    },
                    {
                        "name": "Mingxiang Tuo"
                    },
                    {
                        "name": "Mingyang Zhu"
                    },
                    {
                        "name": "Peiguang Li"
                    },
                    {
                        "name": "Peng Pei"
                    },
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Pengcheng Jia"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Qi Gu"
                    },
                    {
                        "name": "Qianyun Li"
                    },
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Qiong Huang"
                    },
                    {
                        "name": "Qiyuan Duan"
                    },
                    {
                        "name": "Ran Meng"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Ruichen Shao"
                    },
                    {
                        "name": "Rumei Li"
                    },
                    {
                        "name": "Shizhe Wu"
                    },
                    {
                        "name": "Shuai Liang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Suogui Dang"
                    },
                    {
                        "name": "Tao Fang"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Tefeng Chen"
                    },
                    {
                        "name": "Tianhao Bai"
                    },
                    {
                        "name": "Tianhao Zhou"
                    },
                    {
                        "name": "Tingwen Xie"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Weikang Zhao"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Wenjie Shi"
                    },
                    {
                        "name": "Xi Nan"
                    },
                    {
                        "name": "Xi Su"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Xiang Mei"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Xiangyu Xi"
                    },
                    {
                        "name": "Xiangzhou Huang"
                    },
                    {
                        "name": "Xianpeng Li"
                    },
                    {
                        "name": "Xiao Fu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xiao Wei"
                    },
                    {
                        "name": "Xiaodong Cai"
                    },
                    {
                        "name": "Xiaolong Chen"
                    },
                    {
                        "name": "Xiaoqing Liu"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Xiaowei Shi"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Xili Wang"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xingyu Miao"
                    },
                    {
                        "name": "Xinyan He"
                    },
                    {
                        "name": "Xuemiao Zhang"
                    },
                    {
                        "name": "Xueyuan Hao"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Xurui Yang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yaqi Huo"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yipeng Zang"
                    },
                    {
                        "name": "Yitao Zhai"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Yongjing Yin"
                    },
                    {
                        "name": "Yongkang Lv"
                    },
                    {
                        "name": "Yongwei Zhou"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Yueqing Sun"
                    },
                    {
                        "name": "Yuewen Zheng"
                    },
                    {
                        "name": "Yuhuai Wei"
                    },
                    {
                        "name": "Yulei Qian"
                    },
                    {
                        "name": "Yunfan Liang"
                    },
                    {
                        "name": "Yunfang Tai"
                    },
                    {
                        "name": "Yunke Zhao"
                    },
                    {
                        "name": "Zeyang Yu"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Zhaohua Yang"
                    },
                    {
                        "name": "Zhenchao Zhang"
                    },
                    {
                        "name": "Zhikang Xia"
                    },
                    {
                        "name": "Zhiye Zou"
                    },
                    {
                        "name": "Zhizhao Zeng"
                    },
                    {
                        "name": "Zhongda Su"
                    },
                    {
                        "name": "Zhuofan Chen"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Zixu Jiang"
                    },
                    {
                        "name": "Zizhe Zhao"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Zunhai Su"
                    }
                ],
                "author_detail": {
                    "name": "Zunhai Su"
                },
                "author": "Zunhai Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15971v1",
                "updated": "2025-09-19T13:27:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    27,
                    27,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:27:27Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    27,
                    27,
                    4,
                    262,
                    0
                ],
                "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine\n  Learning Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine\n  Learning Pipelines"
                },
                "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines."
                },
                "authors": [
                    {
                        "name": "Owen Truong"
                    },
                    {
                        "name": "Terrence Zhang"
                    },
                    {
                        "name": "Arnav Marchareddy"
                    },
                    {
                        "name": "Ryan Lee"
                    },
                    {
                        "name": "Jeffery Busold"
                    },
                    {
                        "name": "Michael Socas"
                    },
                    {
                        "name": "Eman Abdullah AlOmar"
                    }
                ],
                "author_detail": {
                    "name": "Eman Abdullah AlOmar"
                },
                "author": "Eman Abdullah AlOmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21589v3",
                "updated": "2025-09-19T13:25:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    25,
                    52,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-29T12:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning"
                },
                "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are coming soon. Our datasets,\nmodels, and code are publicly available at https://github.com/Word2VecT/Middo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are coming soon. Our datasets,\nmodels, and code are publicly available at https://github.com/Word2VecT/Middo."
                },
                "authors": [
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Mengzhang Cai"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "Accepted by EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13786v2",
                "updated": "2025-09-19T13:24:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    24,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-17T07:55:58Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    58,
                    2,
                    260,
                    0
                ],
                "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization"
                },
                "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. In this paper, we extend Post-Training\nQuantization (PTQ) by focusing on Quantization-Aware Training (QAT), which\nincorporates low-precision simulation during training for robustness at\nultra-low bitwidths. In particular, we develop a QAT methodology for a neural\nreceiver architecture and benchmark it against a PTQ approach across diverse\n3GPP Clustered Delay Line (CDL) channel profiles under both Line-of-Sight (LoS)\nand Non-LoS (NLoS) conditions, with user velocities up to 40 m/s. Results show\nthat 4-bit and 8-bit QAT models achieve BLERs comparable to FP32 models at a\n10% target BLER. Moreover, QAT models succeed in NLoS scenarios where PTQ\nmodels fail to reach the 10% BLER target, while also yielding an 8x\ncompression. These results with respect to full-precision demonstrate that QAT\nis a key enabler of low-complexity and latency-constrained inference at the PHY\nlayer, facilitating real-time processing in 6G edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. In this paper, we extend Post-Training\nQuantization (PTQ) by focusing on Quantization-Aware Training (QAT), which\nincorporates low-precision simulation during training for robustness at\nultra-low bitwidths. In particular, we develop a QAT methodology for a neural\nreceiver architecture and benchmark it against a PTQ approach across diverse\n3GPP Clustered Delay Line (CDL) channel profiles under both Line-of-Sight (LoS)\nand Non-LoS (NLoS) conditions, with user velocities up to 40 m/s. Results show\nthat 4-bit and 8-bit QAT models achieve BLERs comparable to FP32 models at a\n10% target BLER. Moreover, QAT models succeed in NLoS scenarios where PTQ\nmodels fail to reach the 10% BLER target, while also yielding an 8x\ncompression. These results with respect to full-precision demonstrate that QAT\nis a key enabler of low-complexity and latency-constrained inference at the PHY\nlayer, facilitating real-time processing in 6G edge devices."
                },
                "authors": [
                    {
                        "name": "SaiKrishna Saketh Yellapragada"
                    },
                    {
                        "name": "Esa Ollila"
                    },
                    {
                        "name": "Mario Costa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Costa"
                },
                "author": "Mario Costa",
                "arxiv_comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15963v1",
                "updated": "2025-09-19T13:21:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    21,
                    39,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:21:39Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    21,
                    39,
                    4,
                    262,
                    0
                ],
                "title": "Going with the Flow: Solving for Symmetry-Driven PDE dynamics with\n  Physics-informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going with the Flow: Solving for Symmetry-Driven PDE dynamics with\n  Physics-informed Neural Networks"
                },
                "summary": "In the past, we have presented a systematic computational framework for\nanalyzing self-similar and traveling wave dynamics in nonlinear partial\ndifferential equations (PDEs) by dynamically factoring out continuous\nsymmetries such as translation and scaling. This is achieved through the use of\ntime-dependent transformations -- what can be viewed as dynamic pinning\nconditions -- that render the symmetry-invariant solution stationary or slowly\nvarying in rescaled coordinates. The transformation process yields a modified\nevolution equation coupled with algebraic constraints on the symmetry\nparameters, resulting in index-2 differential-algebraic equation (DAE) systems.\nThe framework accommodates both first-kind and second-kind self-similarity, and\ndirectly recovers the self-similarity exponents or wave speeds as part of the\nsolution, upon considering steady-state solutions in the rescaled coordinate\nframe. To solve the resulting high-index DAE systems, we employ\nPhysics-Informed Neural Networks (PINNs), which naturally integrate PDE\nresiduals and algebraic constraints into a unified loss function. This allows\nsimultaneous inference of both the invariant solution and the transformation\nproperties (such as the speed or the scaling rate without the need for large\ncomputational domains, mesh adaptivity, or front tracking. We demonstrate the\neffectiveness of the method on four canonical problems: (i) the Nagumo equation\nexhibiting traveling waves, (ii) the diffusion equation (1D and 2D) with\nfirst-kind self-similarity, (iii) the 2D axisymmetric porous medium equation\nshowcasing second-kind self-similarity, and (iv) the Burgers equation, which\ninvolves both translational and scaling invariance. The results demonstrate the\ncapability of PINNs to effectively solve these complex PDE-DAE systems,\nproviding a promising tool for studying nonlinear wave and scaling phenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past, we have presented a systematic computational framework for\nanalyzing self-similar and traveling wave dynamics in nonlinear partial\ndifferential equations (PDEs) by dynamically factoring out continuous\nsymmetries such as translation and scaling. This is achieved through the use of\ntime-dependent transformations -- what can be viewed as dynamic pinning\nconditions -- that render the symmetry-invariant solution stationary or slowly\nvarying in rescaled coordinates. The transformation process yields a modified\nevolution equation coupled with algebraic constraints on the symmetry\nparameters, resulting in index-2 differential-algebraic equation (DAE) systems.\nThe framework accommodates both first-kind and second-kind self-similarity, and\ndirectly recovers the self-similarity exponents or wave speeds as part of the\nsolution, upon considering steady-state solutions in the rescaled coordinate\nframe. To solve the resulting high-index DAE systems, we employ\nPhysics-Informed Neural Networks (PINNs), which naturally integrate PDE\nresiduals and algebraic constraints into a unified loss function. This allows\nsimultaneous inference of both the invariant solution and the transformation\nproperties (such as the speed or the scaling rate without the need for large\ncomputational domains, mesh adaptivity, or front tracking. We demonstrate the\neffectiveness of the method on four canonical problems: (i) the Nagumo equation\nexhibiting traveling waves, (ii) the diffusion equation (1D and 2D) with\nfirst-kind self-similarity, (iii) the 2D axisymmetric porous medium equation\nshowcasing second-kind self-similarity, and (iv) the Burgers equation, which\ninvolves both translational and scaling invariance. The results demonstrate the\ncapability of PINNs to effectively solve these complex PDE-DAE systems,\nproviding a promising tool for studying nonlinear wave and scaling phenomena."
                },
                "authors": [
                    {
                        "name": "Michail Kavousanakis"
                    },
                    {
                        "name": "Gianluca Fabiani"
                    },
                    {
                        "name": "Anastasia Georgiou"
                    },
                    {
                        "name": "Constantinos Siettos"
                    },
                    {
                        "name": "Panagiotis Kevrekidis"
                    },
                    {
                        "name": "Ioannis Kevrekidis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Kevrekidis"
                },
                "author": "Ioannis Kevrekidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22777v3",
                "updated": "2025-09-19T13:18:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    18,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-28T18:45:42Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    45,
                    42,
                    2,
                    148,
                    0
                ],
                "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators"
                },
                "summary": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Alon Lavie"
                    },
                    {
                        "name": "Isabel Trancoso"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Trancoso"
                },
                "author": "Isabel Trancoso",
                "arxiv_comment": "October ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15957v1",
                "updated": "2025-09-19T13:17:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    17,
                    16,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:17:16Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    17,
                    16,
                    4,
                    262,
                    0
                ],
                "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol"
                },
                "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice."
                },
                "authors": [
                    {
                        "name": "Kanato Masayoshi"
                    },
                    {
                        "name": "Masahiro Hashimoto"
                    },
                    {
                        "name": "Ryoichi Yokoyama"
                    },
                    {
                        "name": "Naoki Toda"
                    },
                    {
                        "name": "Yoshifumi Uwamino"
                    },
                    {
                        "name": "Shogo Fukuda"
                    },
                    {
                        "name": "Ho Namkoong"
                    },
                    {
                        "name": "Masahiro Jinzaki"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Jinzaki"
                },
                "author": "Masahiro Jinzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v5",
                "updated": "2025-09-19T13:12:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    12,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Published at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15953v1",
                "updated": "2025-09-19T13:09:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    9,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:09:56Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    9,
                    56,
                    4,
                    262,
                    0
                ],
                "title": "Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal"
                },
                "summary": "Turning garments right-side out is a challenging manipulation task: it is\nhighly dynamic, entails rapid contact changes, and is subject to severe visual\nocclusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that\neffectively solves this challenge by exploiting task structures. We decompose\nthe task into Drag/Fling to create and stabilize an access opening, followed by\nInsert&Pull to invert the garment. Each step uses a depth-inferred,\nkeypoint-parameterized bimanual primitive that sharply reduces the action space\nwhile preserving robustness. Efficient data generation is enabled by our\ncustom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator\nthat models thin-shell deformation and provides robust and efficient contact\nhandling for batched rollouts. Built on the simulator, our fully automated\npipeline scales data generation by randomizing garment geometry, material\nparameters, and viewpoints, producing depth, masks, and per-primitive keypoint\nlabels without any human annotations. With a single depth camera, policies\ntrained entirely in simulation deploy zero-shot on real hardware, achieving up\nto 81.3% success rate. By employing task decomposition and high fidelity\nsimulation, our framework enables tackling highly dynamic, severely occluded\ntasks without laborious human demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning garments right-side out is a challenging manipulation task: it is\nhighly dynamic, entails rapid contact changes, and is subject to severe visual\nocclusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that\neffectively solves this challenge by exploiting task structures. We decompose\nthe task into Drag/Fling to create and stabilize an access opening, followed by\nInsert&Pull to invert the garment. Each step uses a depth-inferred,\nkeypoint-parameterized bimanual primitive that sharply reduces the action space\nwhile preserving robustness. Efficient data generation is enabled by our\ncustom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator\nthat models thin-shell deformation and provides robust and efficient contact\nhandling for batched rollouts. Built on the simulator, our fully automated\npipeline scales data generation by randomizing garment geometry, material\nparameters, and viewpoints, producing depth, masks, and per-primitive keypoint\nlabels without any human annotations. With a single depth camera, policies\ntrained entirely in simulation deploy zero-shot on real hardware, achieving up\nto 81.3% success rate. By employing task decomposition and high fidelity\nsimulation, our framework enables tackling highly dynamic, severely occluded\ntasks without laborious human demonstrations."
                },
                "authors": [
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Siyu Ma"
                    },
                    {
                        "name": "Wenxin Du"
                    },
                    {
                        "name": "Zeshun Zong"
                    },
                    {
                        "name": "Han Xue"
                    },
                    {
                        "name": "Wendi Chen"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Xuchen Han"
                    },
                    {
                        "name": "Joseph Masterjohn"
                    },
                    {
                        "name": "Alejandro Castro"
                    },
                    {
                        "name": "Chenfanfu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Chenfanfu Jiang"
                },
                "author": "Chenfanfu Jiang",
                "arxiv_comment": "More details and supplementary material are on the website:\n  https://right-side-out.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01349v3",
                "updated": "2025-09-19T13:06:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    6,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-03T13:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    39,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation."
                },
                "authors": [
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Konstantinos Thomas"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15432v2",
                "updated": "2025-09-19T12:53:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    53,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-21T10:35:41Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    35,
                    41,
                    3,
                    233,
                    0
                ],
                "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data"
                },
                "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines."
                },
                "authors": [
                    {
                        "name": "Bidyapati Pradhan"
                    },
                    {
                        "name": "Surajit Dasgupta"
                    },
                    {
                        "name": "Amit Kumar Saha"
                    },
                    {
                        "name": "Omkar Anustoop"
                    },
                    {
                        "name": "Sriram Puttagunta"
                    },
                    {
                        "name": "Vipul Mittal"
                    },
                    {
                        "name": "Gopal Sarda"
                    }
                ],
                "author_detail": {
                    "name": "Gopal Sarda"
                },
                "author": "Gopal Sarda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14312v2",
                "updated": "2025-09-19T12:52:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    52,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-18T18:32:17Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    32,
                    17,
                    4,
                    199,
                    0
                ],
                "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation"
                },
                "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts."
                },
                "authors": [
                    {
                        "name": "Marc Lafon"
                    },
                    {
                        "name": "Gustavo Adolfo Vargas Hakim"
                    },
                    {
                        "name": "Clément Rambour"
                    },
                    {
                        "name": "Christian Desrosier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems, NeurIPS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15940v1",
                "updated": "2025-09-19T12:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    52,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:52:32Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    52,
                    32,
                    4,
                    262,
                    0
                ],
                "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication\n  Alignment on More Than 9600 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pre-Training of LLMs via Topology-Aware Communication\n  Alignment on More Than 9600 GPUs"
                },
                "summary": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline."
                },
                "authors": [
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Kaihua Jiang"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zixian Du"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01476v2",
                "updated": "2025-09-19T12:43:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    43,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-01T13:44:15Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    44,
                    15,
                    0,
                    244,
                    0
                ],
                "title": "Do Retrieval Augmented Language Models Know When They Don't Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Retrieval Augmented Language Models Know When They Don't Know?"
                },
                "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems."
                },
                "authors": [
                    {
                        "name": "Youchao Zhou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Yicheng Liu"
                    },
                    {
                        "name": "Rui Dai"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Xingchen Zhang"
                    },
                    {
                        "name": "Shumin Shi"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15935v1",
                "updated": "2025-09-19T12:40:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    40,
                    49,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:40:49Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    40,
                    49,
                    4,
                    262,
                    0
                ],
                "title": "PAN: Pillars-Attention-Based Network for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAN: Pillars-Attention-Based Network for 3D Object Detection"
                },
                "summary": "Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar\nfusion for the 3D object detection task in real-time under adverse weather and\nlighting conditions. However, currently, in the literature, it is possible to\nfind few works focusing on this modality and, most importantly, developing new\narchitectures to explore the advantages of the radar point cloud, such as\naccurate distance estimation and speed information. Therefore, this work\npresents a novel and efficient 3D object detection algorithm using cameras and\nradars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of\nradar before fusing the features into a detection head. A new backbone is\nintroduced, which maps the radar pillar features into an embedded dimension. A\nself-attention mechanism allows the backbone to model the dependencies between\nthe radar points. We are using a simplified convolutional layer to replace the\nFPN-based convolutional layers used in the PointPillars-based architectures\nwith the main goal of reducing inference time. Our results show that with this\nmodification, our approach achieves the new state-of-the-art in the 3D object\ndetection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,\nwhile also setting a new benchmark for inference time on the nuScenes dataset\nfor the same category."
                },
                "authors": [
                    {
                        "name": "Ruan Bispo"
                    },
                    {
                        "name": "Dane Mitrev"
                    },
                    {
                        "name": "Letizia Mariotti"
                    },
                    {
                        "name": "Clément Botty"
                    },
                    {
                        "name": "Denver Humphrey"
                    },
                    {
                        "name": "Anthony Scanlan"
                    },
                    {
                        "name": "Ciarán Eising"
                    }
                ],
                "author_detail": {
                    "name": "Ciarán Eising"
                },
                "author": "Ciarán Eising",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15927v1",
                "updated": "2025-09-19T12:30:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    30,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:30:26Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    30,
                    26,
                    4,
                    262,
                    0
                ],
                "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search"
                },
                "summary": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach."
                },
                "authors": [
                    {
                        "name": "Zhiyu Mou"
                    },
                    {
                        "name": "Yiqin Lv"
                    },
                    {
                        "name": "Miao Xu"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Qichen Ye"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Rongquan Bai"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15926v1",
                "updated": "2025-09-19T12:28:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    28,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:28:50Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    28,
                    50,
                    4,
                    262,
                    0
                ],
                "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay\n  Assessment"
                },
                "summary": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work."
                },
                "authors": [
                    {
                        "name": "Ahmed Karim"
                    },
                    {
                        "name": "Qiao Wang"
                    },
                    {
                        "name": "Zheng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yuan"
                },
                "arxiv_affiliation": "Judy",
                "author": "Zheng Yuan",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11277v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11277v5",
                "updated": "2025-09-19T12:21:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    21,
                    3,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-16T14:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    11,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "Search and Refine During Think: Facilitating Knowledge Refinement for\n  Improved Retrieval-Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search and Refine During Think: Facilitating Knowledge Refinement for\n  Improved Retrieval-Augmented Reasoning"
                },
                "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n\"search-and-refine-during-think\" paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n\"search-and-refine-during-think\" paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."
                },
                "authors": [
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11277v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11277v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15915v1",
                "updated": "2025-09-19T12:10:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    10,
                    28,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:10:28Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    10,
                    28,
                    4,
                    262,
                    0
                ],
                "title": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds"
                },
                "summary": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements."
                },
                "authors": [
                    {
                        "name": "Remo Sasso"
                    },
                    {
                        "name": "Michelangelo Conserva"
                    },
                    {
                        "name": "Dominik Jeurissen"
                    },
                    {
                        "name": "Paulo Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Paulo Rauber"
                },
                "author": "Paulo Rauber",
                "arxiv_comment": "20 pages, 9 figures. Accepted for presentation at the 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied\n  World Models for Decision Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07824v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07824v5",
                "updated": "2025-09-19T11:59:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    59,
                    49,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-14T03:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Efficient Real-time Refinement of Language Model Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Real-time Refinement of Language Model Text Generation"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods."
                },
                "authors": [
                    {
                        "name": "Joonho Ko"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07824v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07824v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19668v4",
                "updated": "2025-09-19T11:58:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T01:29:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    29,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning"
                },
                "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."
                },
                "authors": [
                    {
                        "name": "Mingsheng Cai"
                    },
                    {
                        "name": "Jiuming Jiang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Rossella Arcucci"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Arcucci"
                },
                "author": "Rossella Arcucci",
                "arxiv_comment": "Findings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15901v1",
                "updated": "2025-09-19T11:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    17,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    17,
                    4,
                    262,
                    0
                ],
                "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions"
                },
                "summary": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Sonu Kumar"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00611v2",
                "updated": "2025-09-19T11:43:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    43,
                    18,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-01T13:14:41Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    14,
                    41,
                    4,
                    213,
                    0
                ],
                "title": "Rapid cosmological inference with the two-loop matter power spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid cosmological inference with the two-loop matter power spectrum"
                },
                "summary": "We compute the two-loop effective field theory (EFT) power spectrum of dark\nmatter density fluctuations in $\\Lambda$CDM using the recently proposed COBRA\nmethod (Bakx. et al, 2025). With COBRA, we are able to evaluate the two-loop\nmatter power spectrum in $\\sim 1$ millisecond at $ \\sim 0.1 \\%$ precision on\none CPU for arbitrary redshifts and on scales where perturbation theory\napplies. As an application, we use the nonlinear matter power spectrum from the\nDark Sky simulation to assess the performance of the two-loop EFT power\nspectrum compared to the one-loop EFT power spectrum at $z=0$. We find that,\nfor volumes typical for Stage IV galaxy surveys, $V = 25 \\,(\\text{Gpc}/h)^3$,\nthe two-loop EFT can provide unbiased cosmological constraints on\n$\\Omega_m,H_0$ and $A_s$ using scales up to $k_\\text{max}=0.26\\, h/\\text{Mpc}$,\nthereby outperforming the constraints from the one-loop EFT\n($k_\\text{max}=0.11\\, h/\\text{Mpc}$). The Figure of Merit on these three\nparameters increases by a factor $\\sim 2.6$ and the one-dimensional\nmarginalized constraints improve by $\\sim35\\%$ for $\\Omega_m$, $\\sim20\\%$ for\n$H_0$ and $\\sim 15\\%$ for $A_s$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the two-loop effective field theory (EFT) power spectrum of dark\nmatter density fluctuations in $\\Lambda$CDM using the recently proposed COBRA\nmethod (Bakx. et al, 2025). With COBRA, we are able to evaluate the two-loop\nmatter power spectrum in $\\sim 1$ millisecond at $ \\sim 0.1 \\%$ precision on\none CPU for arbitrary redshifts and on scales where perturbation theory\napplies. As an application, we use the nonlinear matter power spectrum from the\nDark Sky simulation to assess the performance of the two-loop EFT power\nspectrum compared to the one-loop EFT power spectrum at $z=0$. We find that,\nfor volumes typical for Stage IV galaxy surveys, $V = 25 \\,(\\text{Gpc}/h)^3$,\nthe two-loop EFT can provide unbiased cosmological constraints on\n$\\Omega_m,H_0$ and $A_s$ using scales up to $k_\\text{max}=0.26\\, h/\\text{Mpc}$,\nthereby outperforming the constraints from the one-loop EFT\n($k_\\text{max}=0.11\\, h/\\text{Mpc}$). The Figure of Merit on these three\nparameters increases by a factor $\\sim 2.6$ and the one-dimensional\nmarginalized constraints improve by $\\sim35\\%$ for $\\Omega_m$, $\\sim20\\%$ for\n$H_0$ and $\\sim 15\\%$ for $A_s$."
                },
                "authors": [
                    {
                        "name": "Thomas Bakx"
                    },
                    {
                        "name": "Henrique Rubira"
                    },
                    {
                        "name": "Nora Elisa Chisari"
                    },
                    {
                        "name": "Zvonimir Vlah"
                    }
                ],
                "author_detail": {
                    "name": "Zvonimir Vlah"
                },
                "author": "Zvonimir Vlah",
                "arxiv_comment": "15 pages, 8 figures, changed FoM definition, submitted to OJA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15888v1",
                "updated": "2025-09-19T11:35:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    35,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:35:56Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    35,
                    56,
                    4,
                    262,
                    0
                ],
                "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation"
                },
                "summary": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models."
                },
                "authors": [
                    {
                        "name": "Senkang Hu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Jinqi Jiang"
                    },
                    {
                        "name": "Yihang Tao"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Sam Tak Wu Kwong"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "Accepted by NeurIPS'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14442v2",
                "updated": "2025-09-19T11:33:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    34,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-20T14:43:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Creative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Preference Optimization"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Simone A. Luchini"
                    },
                    {
                        "name": "Reet Patel"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Roger Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger Beaty"
                },
                "author": "Roger Beaty",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03693v2",
                "updated": "2025-09-19T11:32:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    32,
                    43,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-05T17:59:56Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    59,
                    56,
                    1,
                    217,
                    0
                ],
                "title": "PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement\n  Learning"
                },
                "summary": "As AI systems become increasingly autonomous, reliably aligning their\ndecision-making with human preferences is essential. Inverse reinforcement\nlearning (IRL) offers a promising approach to infer preferences from\ndemonstrations. These preferences can then be used to produce an apprentice\npolicy that performs well on the demonstrated task. However, in domains like\nautonomous driving or robotics, where errors can have serious consequences, we\nneed not just good average performance but reliable policies with formal\nguarantees -- yet obtaining sufficient human demonstrations for reliability\nguarantees can be costly. Active IRL addresses this challenge by strategically\nselecting the most informative scenarios for human demonstration. We introduce\nPAC-EIG, an information-theoretic acquisition function that directly targets\nprobably-approximately-correct (PAC) guarantees for the learned policy --\nproviding the first such theoretical guarantee for active IRL with noisy expert\ndemonstrations. Our method maximises information gain about the regret of the\napprentice policy, efficiently identifying states requiring further\ndemonstration. We also present Reward-EIG as an alternative when learning the\nreward itself is the primary objective. Focusing on finite state-action spaces,\nwe prove convergence bounds, illustrate failure modes of prior heuristic\nmethods, and demonstrate our method's advantages experimentally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems become increasingly autonomous, reliably aligning their\ndecision-making with human preferences is essential. Inverse reinforcement\nlearning (IRL) offers a promising approach to infer preferences from\ndemonstrations. These preferences can then be used to produce an apprentice\npolicy that performs well on the demonstrated task. However, in domains like\nautonomous driving or robotics, where errors can have serious consequences, we\nneed not just good average performance but reliable policies with formal\nguarantees -- yet obtaining sufficient human demonstrations for reliability\nguarantees can be costly. Active IRL addresses this challenge by strategically\nselecting the most informative scenarios for human demonstration. We introduce\nPAC-EIG, an information-theoretic acquisition function that directly targets\nprobably-approximately-correct (PAC) guarantees for the learned policy --\nproviding the first such theoretical guarantee for active IRL with noisy expert\ndemonstrations. Our method maximises information gain about the regret of the\napprentice policy, efficiently identifying states requiring further\ndemonstration. We also present Reward-EIG as an alternative when learning the\nreward itself is the primary objective. Focusing on finite state-action spaces,\nwe prove convergence bounds, illustrate failure modes of prior heuristic\nmethods, and demonstrate our method's advantages experimentally."
                },
                "authors": [
                    {
                        "name": "Ondrej Bajgar"
                    },
                    {
                        "name": "Dewi S. W. Gould"
                    },
                    {
                        "name": "Jonathon Liu"
                    },
                    {
                        "name": "Alessandro Abate"
                    },
                    {
                        "name": "Konstantinos Gatsis"
                    },
                    {
                        "name": "Michael A. Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Osborne"
                },
                "author": "Michael A. Osborne",
                "arxiv_comment": "Presented at RLC 2025; published in RLJ 2025",
                "arxiv_journal_ref": "Reinforcement Learning Journal 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00288v3",
                "updated": "2025-09-19T11:27:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    27,
                    30,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T22:31:59Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    22,
                    31,
                    59,
                    4,
                    150,
                    0
                ],
                "title": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation"
                },
                "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future."
                },
                "authors": [
                    {
                        "name": "Ahmed Elhady"
                    },
                    {
                        "name": "Eneko Agirre"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "arxiv_comment": "Published as a Conference Paper at the main track of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.16203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16203v1",
                "updated": "2025-09-19T17:59:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    57,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:59:57Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    59,
                    57,
                    4,
                    262,
                    0
                ],
                "title": "Inverting Trojans in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverting Trojans in LLMs"
                },
                "summary": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases."
                },
                "authors": [
                    {
                        "name": "Zhengxing Li"
                    },
                    {
                        "name": "Guangmingmei Yang"
                    },
                    {
                        "name": "Jayaram Raghuram"
                    },
                    {
                        "name": "David J. Miller"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16198v1",
                "updated": "2025-09-19T17:58:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:58:14Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    14,
                    4,
                    262,
                    0
                ],
                "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation"
                },
                "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization."
                },
                "authors": [
                    {
                        "name": "Jane Luo"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Steven Liu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Chengyu Yin"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16197v1",
                "updated": "2025-09-19T17:58:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    0,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:58:00Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    58,
                    0,
                    4,
                    262,
                    0
                ],
                "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid\n  Vision Tokenizer"
                },
                "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer."
                },
                "authors": [
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Jialing Tong"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Zhenbang Wang"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Wenze Hu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Zhifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Chen"
                },
                "author": "Zhifeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09723v3",
                "updated": "2025-09-19T17:56:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    56,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-13T21:10:56Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    21,
                    10,
                    56,
                    6,
                    103,
                    0
                ],
                "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM\n  Agents"
                },
                "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns."
                },
                "authors": [
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Ting-Yao Hsu"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Limeng Cui"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "William Headean"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Akash Veeragouni"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "Jessie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jessie Wang"
                },
                "author": "Jessie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09407v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09407v3",
                "updated": "2025-09-19T17:52:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    52,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-13T02:34:22Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    2,
                    34,
                    22,
                    6,
                    103,
                    0
                ],
                "title": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents"
                },
                "summary": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate their new designs. But what about\nevaluating and iterating the usability testing study design itself? Recent\nadvances in Large Language Model-simulated Agent (LLM Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and iterating\ntheir study design before they conduct the real human-subject study. Our system\nfeatures a Persona Generator module, an LLM Agent module, and a Universal\nBrowser Connector module to automatically generate thousands of simulated users\nand to interactively test the target website. The system also provides a Result\nViewer Interface so that the UX researchers can easily review and analyze the\ngenerated qualitative (e.g., agents' post-study surveys) and quantitative data\n(e.g., agents' interaction logs), or even interview agents directly. Through a\nheuristic evaluation with 16 UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent usage in UX studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate their new designs. But what about\nevaluating and iterating the usability testing study design itself? Recent\nadvances in Large Language Model-simulated Agent (LLM Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and iterating\ntheir study design before they conduct the real human-subject study. Our system\nfeatures a Persona Generator module, an LLM Agent module, and a Universal\nBrowser Connector module to automatically generate thousands of simulated users\nand to interactively test the target website. The system also provides a Result\nViewer Interface so that the UX researchers can easily review and analyze the\ngenerated qualitative (e.g., agents' post-study surveys) and quantitative data\n(e.g., agents' interaction logs), or even interview agents directly. Through a\nheuristic evaluation with 16 UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent usage in UX studies."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09407v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09407v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13951v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13951v3",
                "updated": "2025-09-19T17:50:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    50,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-20T03:22:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    3,
                    22,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Layered Multi-Expert Framework for Long-Context Mental Health\n  Assessments"
                },
                "summary": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form mental health assessments pose unique challenges for large language\nmodels (LLMs), which often exhibit hallucinations or inconsistent reasoning\nwhen handling extended, domain-specific contexts. We introduce Stacked\nMulti-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs\nand specialized smaller models as coequal 'experts'. Early layers isolate\nshort, discrete subtasks, while later layers integrate and refine these partial\noutputs through more advanced long-context models. We evaluate SMMR on the\nDAIC-WOZ depression-screening dataset and 48 curated case studies with\npsychiatric diagnoses, demonstrating consistent improvements over single-model\nbaselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By\nharnessing diverse 'second opinions', SMMR mitigates hallucinations, captures\nsubtle clinical nuances, and enhances reliability in high-stakes mental health\nassessments. Our findings underscore the value of multi-expert frameworks for\nmore trustworthy AI-driven screening."
                },
                "authors": [
                    {
                        "name": "Jinwen Tang"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Yi Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Shang"
                },
                "author": "Yi Shang",
                "arxiv_doi": "10.1109/CAI64502.2025.00080",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CAI64502.2025.00080",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13951v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13951v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. 2025 IEEE Conference on Artificial Intelligence (CAI), Santa\n  Clara, CA, USA, 2025, pp. 435-440",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16188v1",
                "updated": "2025-09-19T17:47:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    47,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:47:48Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    47,
                    48,
                    4,
                    262,
                    0
                ],
                "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in\n  LLMs"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture"
                },
                "authors": [
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Sihang Jiang"
                    },
                    {
                        "name": "Shiwei Guo"
                    },
                    {
                        "name": "Shisong Chen"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Minggui HE"
                    },
                    {
                        "name": "Shimin Tao"
                    },
                    {
                        "name": "Hongxia Ma"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Ma"
                },
                "author": "Hongxia Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16187v1",
                "updated": "2025-09-19T17:46:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    46,
                    13,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:46:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    46,
                    13,
                    4,
                    262,
                    0
                ],
                "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code\n  Translation Validation and Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code\n  Translation Validation and Repair"
                },
                "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Brandon Paulsen"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Joey Dodds"
                    },
                    {
                        "name": "Daniel Kroening"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kroening"
                },
                "author": "Daniel Kroening",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08415v2",
                "updated": "2025-09-19T17:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    45,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-12T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "FSLI: An Interpretable Formal Semantic System for One-Dimensional\n  Ordering Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FSLI: An Interpretable Formal Semantic System for One-Dimensional\n  Ordering Inference"
                },
                "summary": "We develop a system for solving logical deduction one-dimensional ordering\nproblems by transforming natural language premises and candidate statements\ninto first-order logic. Building on Heim and Kratzer's syntax-based\ncompositional semantic rules which utilizes lambda calculus, we develop a\nsemantic parsing algorithm with abstract types, templated rules, and a dynamic\ncomponent for interpreting entities within a context constructed from the\ninput. The resulting logical forms are executed via constraint logic\nprogramming to determine which candidate statements can be logically deduced\nfrom the premises.\n  The symbolic system, the Formal Semantic Logic Inferer (FSLI), provides a\nformally grounded, linguistically driven system for natural language logical\ndeduction. We evaluate it on both synthetic and derived logical deduction\nproblems. FSLI achieves 100% accuracy on BIG-bench's logical deduction task and\n88% on a syntactically simplified subset of AR-LSAT outperforming an LLM\nbaseline, o1-preview.\n  While current research in natural language reasoning emphasizes neural\nlanguage models, FSLI highlights the potential of principled, interpretable\nsystems for symbolic logical deduction in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a system for solving logical deduction one-dimensional ordering\nproblems by transforming natural language premises and candidate statements\ninto first-order logic. Building on Heim and Kratzer's syntax-based\ncompositional semantic rules which utilizes lambda calculus, we develop a\nsemantic parsing algorithm with abstract types, templated rules, and a dynamic\ncomponent for interpreting entities within a context constructed from the\ninput. The resulting logical forms are executed via constraint logic\nprogramming to determine which candidate statements can be logically deduced\nfrom the premises.\n  The symbolic system, the Formal Semantic Logic Inferer (FSLI), provides a\nformally grounded, linguistically driven system for natural language logical\ndeduction. We evaluate it on both synthetic and derived logical deduction\nproblems. FSLI achieves 100% accuracy on BIG-bench's logical deduction task and\n88% on a syntactically simplified subset of AR-LSAT outperforming an LLM\nbaseline, o1-preview.\n  While current research in natural language reasoning emphasizes neural\nlanguage models, FSLI highlights the potential of principled, interpretable\nsystems for symbolic logical deduction in NLP."
                },
                "authors": [
                    {
                        "name": "Maha Alkhairy"
                    },
                    {
                        "name": "Vincent Homer"
                    },
                    {
                        "name": "Brendan O'Connor"
                    }
                ],
                "author_detail": {
                    "name": "Brendan O'Connor"
                },
                "author": "Brendan O'Connor",
                "arxiv_comment": "3 figures, 9 pages main paper and 8 pages references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16183v1",
                "updated": "2025-09-19T17:43:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    43,
                    53,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:43:53Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    43,
                    53,
                    4,
                    262,
                    0
                ],
                "title": "Xona Pulsar Compatibility with GNSS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xona Pulsar Compatibility with GNSS"
                },
                "summary": "At least ten emerging providers are developing satellite navigation systems\nfor low Earth orbit (LEO). Compatibility with existing GNSS in L-band is\ncritical to their successful deployment and for the larger ecosystem. Xona is\ndeploying Pulsar, a near 260-satellite LEO constellation offering dual L-band\nnavigation services near L1 and L5. Designed for interoperability, Pulsar\nprovides centimeter-level accuracy, resilience, and authentication, while\nmaintaining a format that existing GNSS receivers can support through a\nfirmware update. This study examines Pulsar's compatibility with GPS and\nGalileo by evaluating C/N0 degradation caused by the introduction of its X1 and\nX5 signals. Using spectrally compact QPSK modulation, Pulsar minimizes\ninterference despite higher signal power. Theoretical analysis is supported by\nhardware testing across a range of commercial GNSS receivers in both lab-based\nsimulation and in-orbit live-sky conditions. The study confirms Pulsar causes\nno adverse interference effects to existing GNSS, supporting coexistence and\nintegration within the global PNT ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least ten emerging providers are developing satellite navigation systems\nfor low Earth orbit (LEO). Compatibility with existing GNSS in L-band is\ncritical to their successful deployment and for the larger ecosystem. Xona is\ndeploying Pulsar, a near 260-satellite LEO constellation offering dual L-band\nnavigation services near L1 and L5. Designed for interoperability, Pulsar\nprovides centimeter-level accuracy, resilience, and authentication, while\nmaintaining a format that existing GNSS receivers can support through a\nfirmware update. This study examines Pulsar's compatibility with GPS and\nGalileo by evaluating C/N0 degradation caused by the introduction of its X1 and\nX5 signals. Using spectrally compact QPSK modulation, Pulsar minimizes\ninterference despite higher signal power. Theoretical analysis is supported by\nhardware testing across a range of commercial GNSS receivers in both lab-based\nsimulation and in-orbit live-sky conditions. The study confirms Pulsar causes\nno adverse interference effects to existing GNSS, supporting coexistence and\nintegration within the global PNT ecosystem."
                },
                "authors": [
                    {
                        "name": "Tyler G. R. Reid"
                    },
                    {
                        "name": "Matteo Gala"
                    },
                    {
                        "name": "Mathieu Favreau"
                    },
                    {
                        "name": "Argyris Kriezis"
                    },
                    {
                        "name": "Michael O'Meara"
                    },
                    {
                        "name": "Andre Pant"
                    },
                    {
                        "name": "Paul Tarantino"
                    },
                    {
                        "name": "Christina Youn"
                    }
                ],
                "author_detail": {
                    "name": "Christina Youn"
                },
                "author": "Christina Youn",
                "arxiv_comment": "15 pages, 12 figures",
                "arxiv_journal_ref": "ION GNSS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07820v4",
                "updated": "2025-09-19T17:38:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    38,
                    31,
                    4,
                    262,
                    0
                ],
                "published": "2024-11-12T14:12:45Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    14,
                    12,
                    45,
                    1,
                    317,
                    0
                ],
                "title": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models"
                },
                "summary": "We introduce the \\textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a\nnovel approach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the \\textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a\nnovel approach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Youan Cong"
                    },
                    {
                        "name": "Pritom Saha Akash"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Kevin Chen-Chuan Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Chen-Chuan Chang"
                },
                "author": "Kevin Chen-Chuan Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16176v1",
                "updated": "2025-09-19T17:35:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:35:51Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    35,
                    51,
                    4,
                    262,
                    0
                ],
                "title": "Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\n  Trajectories"
                },
                "summary": "We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\nTrajectories (ACDC), an autonomous drone cinematography system driven by\nnatural language communication between human directors and drones. The main\nlimitation of previous drone cinematography workflows is that they require\nmanual selection of waypoints and view angles based on predefined human intent,\nwhich is labor-intensive and yields inconsistent performance. In this paper, we\npropose employing large language models (LLMs) and vision foundation models\n(VFMs) to convert free-form natural language prompts directly into executable\nindoor UAV video tours. Specifically, our method comprises a vision-language\nretrieval pipeline for initial waypoint selection, a preference-based Bayesian\noptimization framework that refines poses using aesthetic feedback, and a\nmotion planner that generates safe quadrotor trajectories. We validate ACDC\nthrough both simulation and hardware-in-the-loop experiments, demonstrating\nthat it robustly produces professional-quality footage across diverse indoor\nscenes without requiring expertise in robotics or cinematography. These results\nhighlight the potential of embodied AI agents to close the loop from\nopen-vocabulary dialogue to real-world autonomous aerial cinematography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\nTrajectories (ACDC), an autonomous drone cinematography system driven by\nnatural language communication between human directors and drones. The main\nlimitation of previous drone cinematography workflows is that they require\nmanual selection of waypoints and view angles based on predefined human intent,\nwhich is labor-intensive and yields inconsistent performance. In this paper, we\npropose employing large language models (LLMs) and vision foundation models\n(VFMs) to convert free-form natural language prompts directly into executable\nindoor UAV video tours. Specifically, our method comprises a vision-language\nretrieval pipeline for initial waypoint selection, a preference-based Bayesian\noptimization framework that refines poses using aesthetic feedback, and a\nmotion planner that generates safe quadrotor trajectories. We validate ACDC\nthrough both simulation and hardware-in-the-loop experiments, demonstrating\nthat it robustly produces professional-quality footage across diverse indoor\nscenes without requiring expertise in robotics or cinematography. These results\nhighlight the potential of embodied AI agents to close the loop from\nopen-vocabulary dialogue to real-world autonomous aerial cinematography."
                },
                "authors": [
                    {
                        "name": "Yifan Lin"
                    },
                    {
                        "name": "Sophie Ziyu Liu"
                    },
                    {
                        "name": "Ran Qi"
                    },
                    {
                        "name": "George Z. Xue"
                    },
                    {
                        "name": "Xinping Song"
                    },
                    {
                        "name": "Chao Qin"
                    },
                    {
                        "name": "Hugh H. -T. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hugh H. -T. Liu"
                },
                "author": "Hugh H. -T. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16170v1",
                "updated": "2025-09-19T17:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    29,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    29,
                    25,
                    4,
                    262,
                    0
                ],
                "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical\n  Self-Supervised Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical\n  Self-Supervised Compensation"
                },
                "summary": "Multi-modal image segmentation faces real-world deployment challenges from\nincomplete/corrupted modalities degrading performance. While existing methods\naddress training-inference modality gaps via specialized per-combination\nmodels, they introduce high deployment costs by requiring exhaustive model\nsubsets and model-modality matching. In this work, we propose a unified\nmodality-relax segmentation network (UniMRSeg) through hierarchical\nself-supervised compensation (HSSC). Our approach hierarchically bridges\nrepresentation gaps between complete and incomplete modalities across input,\nfeature and output levels. % First, we adopt modality reconstruction with the\nhybrid shuffled-masking augmentation, encouraging the model to learn the\nintrinsic modality characteristics and generate meaningful representations for\nmissing modalities through cross-modal fusion. % Next, modality-invariant\ncontrastive learning implicitly compensates the feature space distance among\nincomplete-complete modality pairs. Furthermore, the proposed lightweight\nreverse attention adapter explicitly compensates for the weak perceptual\nsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid\nconsistency constraint to ensure stable prediction under all modality\ncombinations without large performance fluctuations. Without bells and\nwhistles, UniMRSeg significantly outperforms the state-of-the-art methods under\ndiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D\nsemantic segmentation, RGB-D/T salient object segmentation. The code will be\nreleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal image segmentation faces real-world deployment challenges from\nincomplete/corrupted modalities degrading performance. While existing methods\naddress training-inference modality gaps via specialized per-combination\nmodels, they introduce high deployment costs by requiring exhaustive model\nsubsets and model-modality matching. In this work, we propose a unified\nmodality-relax segmentation network (UniMRSeg) through hierarchical\nself-supervised compensation (HSSC). Our approach hierarchically bridges\nrepresentation gaps between complete and incomplete modalities across input,\nfeature and output levels. % First, we adopt modality reconstruction with the\nhybrid shuffled-masking augmentation, encouraging the model to learn the\nintrinsic modality characteristics and generate meaningful representations for\nmissing modalities through cross-modal fusion. % Next, modality-invariant\ncontrastive learning implicitly compensates the feature space distance among\nincomplete-complete modality pairs. Furthermore, the proposed lightweight\nreverse attention adapter explicitly compensates for the weak perceptual\nsemantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid\nconsistency constraint to ensure stable prediction under all modality\ncombinations without large performance fluctuations. Without bells and\nwhistles, UniMRSeg significantly outperforms the state-of-the-art methods under\ndiverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D\nsemantic segmentation, RGB-D/T salient object segmentation. The code will be\nreleased at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg."
                },
                "authors": [
                    {
                        "name": "Xiaoqi Zhao"
                    },
                    {
                        "name": "Youwei Pang"
                    },
                    {
                        "name": "Chenyang Yu"
                    },
                    {
                        "name": "Lihe Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Shijian Lu"
                    },
                    {
                        "name": "Georges El Fakhri"
                    },
                    {
                        "name": "Xiaofeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Liu"
                },
                "author": "Xiaofeng Liu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05755v4",
                "updated": "2025-09-19T17:17:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    17,
                    58,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-06T15:48:49Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    15,
                    48,
                    49,
                    5,
                    249,
                    0
                ],
                "title": "On the Security of Tool-Invocation Prompts for LLM-Based Agentic\n  Systems: An Empirical Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Security of Tool-Invocation Prompts for LLM-Based Agentic\n  Systems: An Empirical Risk Assessment"
                },
                "summary": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic systems leverage large language models to handle user\nqueries, make decisions, and execute external tools for complex tasks across\ndomains like chatbots, customer service, and software engineering. A critical\ncomponent of these systems is the Tool Invocation Prompt (TIP), which defines\ntool interaction protocols and guides LLMs to ensure the security and\ncorrectness of tool usage. Despite its importance, TIP security has been\nlargely overlooked. This work investigates TIP-related security risks,\nrevealing that major LLM-based systems like Cursor, Claude Code, and others are\nvulnerable to attacks such as remote code execution (RCE) and denial of service\n(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate\nexternal tool behavior hijacking via manipulated tool invocations. We also\npropose defense mechanisms to enhance TIP security in LLM-based agentic\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Kaikai Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11759v2",
                "updated": "2025-09-19T17:12:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    12,
                    21,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-15T18:09:53Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    18,
                    9,
                    53,
                    4,
                    227,
                    0
                ],
                "title": "Using Natural Language for Human-Robot Collaboration in the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Natural Language for Human-Robot Collaboration in the Real World"
                },
                "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans."
                },
                "authors": [
                    {
                        "name": "Peter Lindes"
                    },
                    {
                        "name": "Kaoutar Skiker"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar Skiker"
                },
                "author": "Kaoutar Skiker",
                "arxiv_comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24544v2",
                "updated": "2025-09-19T17:11:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    11,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T12:52:35Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    52,
                    35,
                    4,
                    150,
                    0
                ],
                "title": "Cross-Attention Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Attention Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wei Zhong"
                    },
                    {
                        "name": "Manasa Bharadwaj"
                    },
                    {
                        "name": "Yixiao Wang"
                    },
                    {
                        "name": "Nikhil Verma"
                    },
                    {
                        "name": "Yipeng Ji"
                    },
                    {
                        "name": "Chul Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chul Lee"
                },
                "author": "Chul Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16232v2",
                "updated": "2025-09-19T17:11:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    11,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-22T05:05:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    5,
                    5,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "MuseScorer: Idea Originality Scoring At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuseScorer: Idea Originality Scoring At Scale"
                },
                "summary": "An objective, face-valid method for scoring idea originality is to measure\neach idea's statistical infrequency within a population -- an approach long\nused in creativity research. Yet, computing these frequencies requires manually\nbucketing idea rephrasings, a process that is subjective, labor-intensive,\nerror-prone, and brittle at scale. We introduce MuseScorer, a fully automated,\npsychometrically validated system for frequency-based originality scoring.\nMuseScorer integrates a Large Language Model (LLM) with externally orchestrated\nretrieval: given a new idea, it retrieves semantically similar prior\nidea-buckets and zero-shot prompts the LLM to judge whether the idea fits an\nexisting bucket or forms a new one. These buckets enable frequency-based\noriginality scoring without human annotation. Across five datasets\nN_{participants}=1143, n_{ideas}=16,294), MuseScorer matches human annotators\nin idea clustering structure (AMI = 0.59) and participant-level scoring (r =\n0.89), while demonstrating strong convergent and external validity. The system\nenables scalable, intent-sensitive, and human-aligned originality assessment\nfor creativity research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An objective, face-valid method for scoring idea originality is to measure\neach idea's statistical infrequency within a population -- an approach long\nused in creativity research. Yet, computing these frequencies requires manually\nbucketing idea rephrasings, a process that is subjective, labor-intensive,\nerror-prone, and brittle at scale. We introduce MuseScorer, a fully automated,\npsychometrically validated system for frequency-based originality scoring.\nMuseScorer integrates a Large Language Model (LLM) with externally orchestrated\nretrieval: given a new idea, it retrieves semantically similar prior\nidea-buckets and zero-shot prompts the LLM to judge whether the idea fits an\nexisting bucket or forms a new one. These buckets enable frequency-based\noriginality scoring without human annotation. Across five datasets\nN_{participants}=1143, n_{ideas}=16,294), MuseScorer matches human annotators\nin idea clustering structure (AMI = 0.59) and participant-level scoring (r =\n0.89), while demonstrating strong convergent and external validity. The system\nenables scalable, intent-sensitive, and human-aligned originality assessment\nfor creativity research."
                },
                "authors": [
                    {
                        "name": "Ali Sarosh Bangash"
                    },
                    {
                        "name": "Krish Veera"
                    },
                    {
                        "name": "Ishfat Abrar Islam"
                    },
                    {
                        "name": "Raiyan Abdul Baten"
                    }
                ],
                "author_detail": {
                    "name": "Raiyan Abdul Baten"
                },
                "author": "Raiyan Abdul Baten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12158v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12158v3",
                "updated": "2025-09-19T17:07:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    7,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-13T18:24:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    18,
                    24,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models."
                },
                "authors": [
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12158v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16158v1",
                "updated": "2025-09-19T17:07:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    7,
                    13,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T17:07:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    7,
                    13,
                    4,
                    262,
                    0
                ],
                "title": "Designing Culturally Aligned AI Systems For Social Good in Non-Western\n  Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Culturally Aligned AI Systems For Social Good in Non-Western\n  Contexts"
                },
                "summary": "AI technologies are increasingly deployed in high-stakes domains such as\neducation, healthcare, law, and agriculture to address complex challenges in\nnon-Western contexts. This paper examines eight real-world deployments spanning\nseven countries and 18 languages, combining 17 interviews with AI developers\nand domain experts with secondary research. Our findings identify six\ncross-cutting factors - Language, Domain, Demography, Institution, Task, and\nSafety - that structured how systems were designed and deployed. These factors\nwere shaped by sociocultural (diversity, practices), institutional (resources,\npolicies), and technological (capabilities, limits) influences. We find that\nbuilding AI systems required extensive collaboration between AI developers and\ndomain experts. Notably, human resources proved more critical to achieving safe\nand effective systems in high-stakes domains than technological expertise\nalone. We present an analytical framework that synthesizes these dynamics and\nconclude with recommendations for designing AI for social good systems that are\nculturally grounded, equitable, and responsive to the needs of non-Western\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI technologies are increasingly deployed in high-stakes domains such as\neducation, healthcare, law, and agriculture to address complex challenges in\nnon-Western contexts. This paper examines eight real-world deployments spanning\nseven countries and 18 languages, combining 17 interviews with AI developers\nand domain experts with secondary research. Our findings identify six\ncross-cutting factors - Language, Domain, Demography, Institution, Task, and\nSafety - that structured how systems were designed and deployed. These factors\nwere shaped by sociocultural (diversity, practices), institutional (resources,\npolicies), and technological (capabilities, limits) influences. We find that\nbuilding AI systems required extensive collaboration between AI developers and\ndomain experts. Notably, human resources proved more critical to achieving safe\nand effective systems in high-stakes domains than technological expertise\nalone. We present an analytical framework that synthesizes these dynamics and\nconclude with recommendations for designing AI for social good systems that are\nculturally grounded, equitable, and responsive to the needs of non-Western\ncontexts."
                },
                "authors": [
                    {
                        "name": "Deepak Varuvel Dennison"
                    },
                    {
                        "name": "Mohit Jain"
                    },
                    {
                        "name": "Tanuja Ganu"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08578v3",
                "updated": "2025-09-19T17:05:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    5,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-10T13:27:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    27,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "Multi-modal Adaptive Estimation for Temporal Respiratory Disease\n  Outbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Adaptive Estimation for Temporal Respiratory Disease\n  Outbreak"
                },
                "summary": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. This paper presents MAESTRO (Multi-modal Adaptive\nEstimation for Temporal Respiratory Disease Outbreak), a novel, unified\nframework that synergistically integrates advanced spectro-temporal modeling\nwith multi-modal data fusion, including surveillance, web search trends, and\nmeteorological data. By adaptively weighting heterogeneous data sources and\ndecomposing complex time series patterns, the model achieves robust and\naccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data\n(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art\nperformance, achieving a superior model fit with an R-square of 0.956.\nExtensive ablations confirm the significant contributions of its multi-modal\nand spectro-temporal components. The modular and reproducible pipeline is made\npublicly available to facilitate deployment and extension to other regions and\npathogens, presenting a powerful tool for epidemiological forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. This paper presents MAESTRO (Multi-modal Adaptive\nEstimation for Temporal Respiratory Disease Outbreak), a novel, unified\nframework that synergistically integrates advanced spectro-temporal modeling\nwith multi-modal data fusion, including surveillance, web search trends, and\nmeteorological data. By adaptively weighting heterogeneous data sources and\ndecomposing complex time series patterns, the model achieves robust and\naccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data\n(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art\nperformance, achieving a superior model fit with an R-square of 0.956.\nExtensive ablations confirm the significant contributions of its multi-modal\nand spectro-temporal components. The modular and reproducible pipeline is made\npublicly available to facilitate deployment and extension to other regions and\npathogens, presenting a powerful tool for epidemiological forecasting."
                },
                "authors": [
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Kerui Cen"
                    },
                    {
                        "name": "Yanxing Chen"
                    },
                    {
                        "name": "Zige Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zifeng Yang"
                    },
                    {
                        "name": "Chitin Hon"
                    }
                ],
                "author_detail": {
                    "name": "Chitin Hon"
                },
                "author": "Chitin Hon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16149v1",
                "updated": "2025-09-19T16:57:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    57,
                    20,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:57:20Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    57,
                    20,
                    4,
                    262,
                    0
                ],
                "title": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated extraordinary\ncapabilities in conducting conversations based on image inputs. However, we\nobserve that MLLMs exhibit a pronounced form of visual sycophantic behavior.\nWhile similar behavior has also been noted in text-based large language models\n(LLMs), it becomes significantly more prominent when MLLMs process image\ninputs. We refer to this phenomenon as the \"sycophantic modality gap.\" To\nbetter understand this issue, we further analyze the factors that contribute to\nthe exacerbation of this gap. To mitigate the visual sycophantic behavior, we\nfirst experiment with naive supervised fine-tuning to help the MLLM resist\nmisleading instructions from the user. However, we find that this approach also\nmakes the MLLM overly resistant to corrective instructions (i.e., stubborn even\nif it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective\nTuning (SRT), which enables the MLLM to engage in reflective reasoning,\nallowing it to determine whether a user's instruction is misleading or\ncorrective before drawing a conclusion. After applying SRT, we observe a\nsignificant reduction in sycophantic behavior toward misleading instructions,\nwithout resulting in excessive stubbornness when receiving corrective\ninstructions."
                },
                "authors": [
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Kehao Miao"
                    },
                    {
                        "name": "Li Peihang"
                    },
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhou"
                },
                "author": "Xiaofang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16136v1",
                "updated": "2025-09-19T16:35:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    35,
                    27,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:35:27Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    35,
                    27,
                    4,
                    262,
                    0
                ],
                "title": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model\n  Framework for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model\n  Framework for Reinforcement Learning"
                },
                "summary": "Designing effective reward functions remains a major challenge in\nreinforcement learning (RL), often requiring considerable human expertise and\niterative refinement. Recent advances leverage Large Language Models (LLMs) for\nautomated reward design, but these approaches are limited by hallucinations,\nreliance on human feedback, and challenges with handling complex, multi-step\ntasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts\n(RE-GoT), a novel bi-level framework that enhances LLMs with structured\ngraph-based reasoning and integrates Visual Language Models (VLMs) for\nautomated rollout evaluation. RE-GoT first decomposes tasks into\ntext-attributed graphs, enabling comprehensive analysis and reward function\ngeneration, and then iteratively refines rewards using visual feedback from\nVLMs without human intervention. Extensive experiments on 10 RoboGen and 4\nManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing\nLLM-based baselines. On RoboGen, our method improves average task success rates\nby 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,\nRE-GoT achieves an average success rate of 93.73% across four diverse\nmanipulation tasks, significantly surpassing prior LLM-based approaches and\neven exceeding expert-designed rewards. Our results indicate that combining\nLLMs and VLMs with graph-of-thoughts reasoning provides a scalable and\neffective solution for autonomous reward evolution in RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective reward functions remains a major challenge in\nreinforcement learning (RL), often requiring considerable human expertise and\niterative refinement. Recent advances leverage Large Language Models (LLMs) for\nautomated reward design, but these approaches are limited by hallucinations,\nreliance on human feedback, and challenges with handling complex, multi-step\ntasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts\n(RE-GoT), a novel bi-level framework that enhances LLMs with structured\ngraph-based reasoning and integrates Visual Language Models (VLMs) for\nautomated rollout evaluation. RE-GoT first decomposes tasks into\ntext-attributed graphs, enabling comprehensive analysis and reward function\ngeneration, and then iteratively refines rewards using visual feedback from\nVLMs without human intervention. Extensive experiments on 10 RoboGen and 4\nManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing\nLLM-based baselines. On RoboGen, our method improves average task success rates\nby 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,\nRE-GoT achieves an average success rate of 93.73% across four diverse\nmanipulation tasks, significantly surpassing prior LLM-based approaches and\neven exceeding expert-designed rewards. Our results indicate that combining\nLLMs and VLMs with graph-of-thoughts reasoning provides a scalable and\neffective solution for autonomous reward evolution in RL."
                },
                "authors": [
                    {
                        "name": "Changwei Yao"
                    },
                    {
                        "name": "Xinzi Liu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11550v2",
                "updated": "2025-09-19T16:33:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    33,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-13T06:49:35Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    6,
                    49,
                    35,
                    6,
                    194,
                    0
                ],
                "title": "Deformable Dynamic Convolution for Accurate yet Efficient\n  Spatio-Temporal Traffic Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable Dynamic Convolution for Accurate yet Efficient\n  Spatio-Temporal Traffic Prediction"
                },
                "summary": "Traffic prediction is a critical component of intelligent transportation\nsystems, enabling applications such as congestion mitigation and accident risk\nprediction. While recent research has explored both graph-based and grid-based\napproaches, key limitations remain. Graph-based methods effectively capture\nnon-Euclidean spatial structures but often incur high computational overhead,\nlimiting their practicality in large-scale systems. In contrast, grid-based\nmethods, which primarily leverage Convolutional Neural Networks (CNNs), offer\ngreater computational efficiency but struggle to model irregular spatial\npatterns due to the fixed shape of their filters. Moreover, both approaches\noften fail to account for inherent spatio-temporal heterogeneity, as they\ntypically apply a shared set of parameters across diverse regions and time\nperiods. To address these challenges, we propose the Deformable Dynamic\nConvolutional Network (DDCN), a novel CNN-based architecture that integrates\nboth deformable and dynamic convolution operations. The deformable layer\nintroduces learnable offsets to create flexible receptive fields that better\nalign with spatial irregularities, while the dynamic layer generates\nregion-specific filters, allowing the model to adapt to varying spatio-temporal\ntraffic patterns. By combining these two components, DDCN effectively captures\nboth non-Euclidean spatial structures and spatio-temporal heterogeneity.\nExtensive experiments on four real-world traffic datasets demonstrate that DDCN\nachieves competitive predictive performance while significantly reducing\ncomputational costs, underscoring its potential for large-scale and real-time\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic prediction is a critical component of intelligent transportation\nsystems, enabling applications such as congestion mitigation and accident risk\nprediction. While recent research has explored both graph-based and grid-based\napproaches, key limitations remain. Graph-based methods effectively capture\nnon-Euclidean spatial structures but often incur high computational overhead,\nlimiting their practicality in large-scale systems. In contrast, grid-based\nmethods, which primarily leverage Convolutional Neural Networks (CNNs), offer\ngreater computational efficiency but struggle to model irregular spatial\npatterns due to the fixed shape of their filters. Moreover, both approaches\noften fail to account for inherent spatio-temporal heterogeneity, as they\ntypically apply a shared set of parameters across diverse regions and time\nperiods. To address these challenges, we propose the Deformable Dynamic\nConvolutional Network (DDCN), a novel CNN-based architecture that integrates\nboth deformable and dynamic convolution operations. The deformable layer\nintroduces learnable offsets to create flexible receptive fields that better\nalign with spatial irregularities, while the dynamic layer generates\nregion-specific filters, allowing the model to adapt to varying spatio-temporal\ntraffic patterns. By combining these two components, DDCN effectively captures\nboth non-Euclidean spatial structures and spatio-temporal heterogeneity.\nExtensive experiments on four real-world traffic datasets demonstrate that DDCN\nachieves competitive predictive performance while significantly reducing\ncomputational costs, underscoring its potential for large-scale and real-time\ndeployment."
                },
                "authors": [
                    {
                        "name": "Hyeonseok Jin"
                    },
                    {
                        "name": "Geonmin Kim"
                    },
                    {
                        "name": "Kyungbaek Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kyungbaek Kim"
                },
                "author": "Kyungbaek Kim",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16128v1",
                "updated": "2025-09-19T16:25:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    30,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:25:30Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    30,
                    4,
                    262,
                    0
                ],
                "title": "AnchoredAI: Contextual Anchoring of AI Comments Improves Writer Agency\n  and Ownership",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchoredAI: Contextual Anchoring of AI Comments Improves Writer Agency\n  and Ownership"
                },
                "summary": "Generative AI is increasingly integrated into writing support, yet current\nchat-based interfaces often obscure referential context and risk amplifying\nautomation bias and overreliance. We introduce AnchoredAI, a novel system that\nanchors AI feedback directly to relevant text spans. AnchoredAI implements two\nkey mechanisms: (1) an Anchoring Context Window (ACW) that maintains unique,\ncontext-rich references, and (2) an update-aware context retrieval method that\npreserves the intent of prior comments after document edits. In a controlled\nuser study, we compared AnchoredAI to a chat-based LLM interface. Results show\nthat AnchoredAI led to more targeted revisions while fostering a stronger\nagency metrics (e.g., control and ownership) among writers. These findings\nhighlight how interface design shapes AI-assisted writing, suggesting that\nanchoring can mitigate overreliance and enable more precise, user-driven\nrevision practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is increasingly integrated into writing support, yet current\nchat-based interfaces often obscure referential context and risk amplifying\nautomation bias and overreliance. We introduce AnchoredAI, a novel system that\nanchors AI feedback directly to relevant text spans. AnchoredAI implements two\nkey mechanisms: (1) an Anchoring Context Window (ACW) that maintains unique,\ncontext-rich references, and (2) an update-aware context retrieval method that\npreserves the intent of prior comments after document edits. In a controlled\nuser study, we compared AnchoredAI to a chat-based LLM interface. Results show\nthat AnchoredAI led to more targeted revisions while fostering a stronger\nagency metrics (e.g., control and ownership) among writers. These findings\nhighlight how interface design shapes AI-assisted writing, suggesting that\nanchoring can mitigate overreliance and enable more precise, user-driven\nrevision practices."
                },
                "authors": [
                    {
                        "name": "Martin Lou"
                    },
                    {
                        "name": "Jackie Crowley"
                    },
                    {
                        "name": "Samuel Dodson"
                    },
                    {
                        "name": "Dongwook Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Dongwook Yoon"
                },
                "author": "Dongwook Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v3",
                "updated": "2025-09-19T16:25:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    25,
                    3,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "Automated CGRA Design with Multi-Agent LLMs: A Unified Hardware-Software\n  Co-Design Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated CGRA Design with Multi-Agent LLMs: A Unified Hardware-Software\n  Co-Design Framework"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v4",
                "updated": "2025-09-19T16:18:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    18,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight the performance gap between open-source models and top\nstudents, the strong reasoning abilities of closed-source models, and the\nremaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for\nmultimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO\nwith a public leaderboard at https://phyarena.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight the performance gap between open-source models and top\nstudents, the strong reasoning abilities of closed-source models, and the\nremaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for\nmultimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO\nwith a public leaderboard at https://phyarena.github.io/."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16119v1",
                "updated": "2025-09-19T16:13:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    13,
                    9,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T16:13:09Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    16,
                    13,
                    9,
                    4,
                    262,
                    0
                ],
                "title": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D\n  Detector with 4D Automotive Radars"
                },
                "summary": "4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D automotive radars have gained increasing attention for autonomous driving\ndue to their low cost, robustness, and inherent velocity measurement\ncapability. However, existing 4D radar-based 3D detectors rely heavily on\npillar encoders for BEV feature extraction, where each point contributes to\nonly a single BEV grid, resulting in sparse feature maps and degraded\nrepresentation quality. In addition, they also optimize bounding box attributes\nindependently, leading to sub-optimal detection accuracy. Moreover, their\ninference speed, while sufficient for high-end GPUs, may fail to meet the\nreal-time requirement on vehicle-mounted embedded devices. To overcome these\nlimitations, an efficient and effective Gaussian-based 3D detector, namely\nRadarGaussianDet3D is introduced, leveraging Gaussian primitives and\ndistributions as intermediate representations for radar points and bounding\nboxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed\nto transform each point into a Gaussian primitive after feature aggregation and\nemploys the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,\nyielding denser feature maps. PGE exhibits exceptionally low latency, owing to\nthe optimized algorithm for point feature aggregation and fast rendering of\n3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts\nbounding boxes into 3D Gaussian distributions and measures their distance to\nenable more comprehensive and consistent optimization. Extensive experiments on\nTJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves\nstate-of-the-art detection accuracy while delivering substantially faster\ninference, highlighting its potential for real-time deployment in autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Weiyi Xiong"
                    },
                    {
                        "name": "Bing Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Zewei Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zewei Zheng"
                },
                "author": "Zewei Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16112v1",
                "updated": "2025-09-19T15:57:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    57,
                    40,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:57:40Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    57,
                    40,
                    4,
                    262,
                    0
                ],
                "title": "CodeRAG: Finding Relevant and Necessary Knowledge for\n  Retrieval-Augmented Repository-Level Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeRAG: Finding Relevant and Necessary Knowledge for\n  Retrieval-Augmented Repository-Level Code Completion"
                },
                "summary": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG."
                },
                "authors": [
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Shuquan Lian"
                    },
                    {
                        "name": "Shun Song"
                    },
                    {
                        "name": "Hui Li"
                    }
                ],
                "author_detail": {
                    "name": "Hui Li"
                },
                "author": "Hui Li",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18931v2",
                "updated": "2025-09-19T15:55:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    55,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-25T01:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    1,
                    50,
                    5,
                    6,
                    145,
                    0
                ],
                "title": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?"
                },
                "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work evaluating LLM causal\nreasoning primarily focuses on synthetically generated texts which involve\nstraightforward causal relationships that are explicitly mentioned in the text.\nThis fails to reflect the complexities of real-world tasks. In this paper, we\ninvestigate whether LLMs are capable of inferring causal relationships from\nreal-world texts. We develop a benchmark drawn from real-world academic\nliterature which includes diverse texts with respect to length, complexity of\nrelationships (different levels of explicitness, number of nodes, and causal\nrelationships), and domains and sub-domains. To the best of our knowledge, our\nbenchmark is the first-ever real-world dataset for this task. Our experiments\non this dataset show that LLMs face significant challenges in inferring causal\nrelationships from real-world text, with the best-performing model achieving an\naverage F1 score of only 0.477. Through systematic analysis across aspects of\nreal-world text (degree of confounding, size of graph, length of text, domain),\nour benchmark offers targeted insights for further research into advancing LLM\ncausal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work evaluating LLM causal\nreasoning primarily focuses on synthetically generated texts which involve\nstraightforward causal relationships that are explicitly mentioned in the text.\nThis fails to reflect the complexities of real-world tasks. In this paper, we\ninvestigate whether LLMs are capable of inferring causal relationships from\nreal-world texts. We develop a benchmark drawn from real-world academic\nliterature which includes diverse texts with respect to length, complexity of\nrelationships (different levels of explicitness, number of nodes, and causal\nrelationships), and domains and sub-domains. To the best of our knowledge, our\nbenchmark is the first-ever real-world dataset for this task. Our experiments\non this dataset show that LLMs face significant challenges in inferring causal\nrelationships from real-world text, with the best-performing model achieving an\naverage F1 score of only 0.477. Through systematic analysis across aspects of\nreal-world text (degree of confounding, size of graph, length of text, domain),\nour benchmark offers targeted insights for further research into advancing LLM\ncausal reasoning."
                },
                "authors": [
                    {
                        "name": "Ryan Saklad"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Oleg Pavlov"
                    },
                    {
                        "name": "Raha Moraffah"
                    }
                ],
                "author_detail": {
                    "name": "Raha Moraffah"
                },
                "author": "Raha Moraffah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16107v1",
                "updated": "2025-09-19T15:49:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    49,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:49:26Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    49,
                    26,
                    4,
                    262,
                    0
                ],
                "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with\n  Commonsense Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with\n  Commonsense Knowledge"
                },
                "summary": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles."
                },
                "authors": [
                    {
                        "name": "Lukas Ellinger"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "arxiv_comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12385v2",
                "updated": "2025-09-19T15:48:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    48,
                    12,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-15T19:26:17Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    19,
                    26,
                    17,
                    0,
                    258,
                    0
                ],
                "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection"
                },
                "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting."
                },
                "authors": [
                    {
                        "name": "Mitchell Plyler"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Alexander Tuzhilin"
                    },
                    {
                        "name": "Saoud Khalifah"
                    },
                    {
                        "name": "Sen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Sen Tian"
                },
                "author": "Sen Tian",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03603v2",
                "updated": "2025-09-19T15:48:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    48,
                    11,
                    4,
                    262,
                    0
                ],
                "published": "2025-04-04T17:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    20,
                    5,
                    4,
                    94,
                    0
                ],
                "title": "Towards deployment-centric multimodal AI beyond vision and language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards deployment-centric multimodal AI beyond vision and language"
                },
                "summary": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact."
                },
                "authors": [
                    {
                        "name": "Xianyuan Liu"
                    },
                    {
                        "name": "Jiayang Zhang"
                    },
                    {
                        "name": "Shuo Zhou"
                    },
                    {
                        "name": "Thijs L. van der Plas"
                    },
                    {
                        "name": "Avish Vijayaraghavan"
                    },
                    {
                        "name": "Anastasiia Grishina"
                    },
                    {
                        "name": "Mengdie Zhuang"
                    },
                    {
                        "name": "Daniel Schofield"
                    },
                    {
                        "name": "Christopher Tomlinson"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Louisa van Zeeland"
                    },
                    {
                        "name": "Sina Tabakhi"
                    },
                    {
                        "name": "Cyndie Demeocq"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Arunav Das"
                    },
                    {
                        "name": "Orlando Timmerman"
                    },
                    {
                        "name": "Thomas Baldwin-McDonald"
                    },
                    {
                        "name": "Jinge Wu"
                    },
                    {
                        "name": "Peizhen Bai"
                    },
                    {
                        "name": "Zahraa Al Sahili"
                    },
                    {
                        "name": "Omnia Alwazzan"
                    },
                    {
                        "name": "Thao N. Do"
                    },
                    {
                        "name": "Mohammod N. I. Suvon"
                    },
                    {
                        "name": "Angeline Wang"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Luigi A. Moretti"
                    },
                    {
                        "name": "Lucas Farndale"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Natalia Efremova"
                    },
                    {
                        "name": "Yan Ge"
                    },
                    {
                        "name": "Marta Varela"
                    },
                    {
                        "name": "Hak-Keung Lam"
                    },
                    {
                        "name": "Oya Celiktutan"
                    },
                    {
                        "name": "Ben R. Evans"
                    },
                    {
                        "name": "Alejandro Coca-Castro"
                    },
                    {
                        "name": "Honghan Wu"
                    },
                    {
                        "name": "Zahraa S. Abdallah"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Valentin Danchev"
                    },
                    {
                        "name": "Nataliya Tkachenko"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Tingting Zhu"
                    },
                    {
                        "name": "Gregory G. Slabaugh"
                    },
                    {
                        "name": "Roger K. Moore"
                    },
                    {
                        "name": "William K. Cheung"
                    },
                    {
                        "name": "Peter H. Charlton"
                    },
                    {
                        "name": "Haiping Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haiping Lu"
                },
                "author": "Haiping Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16093v1",
                "updated": "2025-09-19T15:36:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    36,
                    2,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:36:02Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    36,
                    2,
                    4,
                    262,
                    0
                ],
                "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM\n  Responses"
                },
                "summary": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains."
                },
                "authors": [
                    {
                        "name": "Fangyi Yu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Dasha Herrmannova"
                    },
                    {
                        "name": "Frank Schilder"
                    },
                    {
                        "name": "Jonathan Richard Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Richard Schwarz"
                },
                "author": "Jonathan Richard Schwarz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v6",
                "updated": "2025-09-19T15:35:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    35,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05439v2",
                "updated": "2025-09-19T15:33:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    33,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-05T12:04:59Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    12,
                    4,
                    59,
                    3,
                    156,
                    0
                ],
                "title": "LLMs Can Compensate for Deficiencies in Visual Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Compensate for Deficiencies in Visual Representations"
                },
                "summary": "Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder."
                },
                "authors": [
                    {
                        "name": "Sho Takishita"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Abdelrahman Mohamed"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Yova Kementchedjhieva"
                    }
                ],
                "author_detail": {
                    "name": "Yova Kementchedjhieva"
                },
                "author": "Yova Kementchedjhieva",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16084v1",
                "updated": "2025-09-19T15:29:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    29,
                    57,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:29:57Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    29,
                    57,
                    4,
                    262,
                    0
                ],
                "title": "Rethinking Molecule Synthesizability with Chain-of-Reaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Molecule Synthesizability with Chain-of-Reaction"
                },
                "summary": "A well-known pitfall of molecular generative models is that they are not\nguaranteed to generate synthesizable molecules. There have been considerable\nattempts to address this problem, but given the exponentially large\ncombinatorial space of synthesizable molecules, existing methods have shown\nlimited coverage of the space and poor molecular optimization performance. To\ntackle these problems, we introduce ReaSyn, a generative framework for\nsynthesizable projection where the model explores the neighborhood of given\nmolecules in the synthesizable space by generating pathways that result in\nsynthesizable analogs. To fully utilize the chemical knowledge contained in the\nsynthetic pathways, we propose a novel perspective that views synthetic\npathways akin to reasoning paths in large language models (LLMs). Specifically,\ninspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the\nchain-of-reaction (CoR) notation that explicitly states reactants, reaction\ntypes, and intermediate products for each step in a pathway. With the CoR\nnotation, ReaSyn can get dense supervision in every reaction step to explicitly\nlearn chemical reaction rules during supervised training and perform\nstep-by-step reasoning. In addition, to further enhance the reasoning\ncapability of ReaSyn, we propose reinforcement learning (RL)-based finetuning\nand goal-directed test-time compute scaling tailored for synthesizable\nprojection. ReaSyn achieves the highest reconstruction rate and pathway\ndiversity in synthesizable molecule reconstruction and the highest optimization\nperformance in synthesizable goal-directed molecular optimization, and\nsignificantly outperforms previous synthesizable projection methods in\nsynthesizable hit expansion. These results highlight ReaSyn's superior ability\nto navigate combinatorially-large synthesizable chemical space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known pitfall of molecular generative models is that they are not\nguaranteed to generate synthesizable molecules. There have been considerable\nattempts to address this problem, but given the exponentially large\ncombinatorial space of synthesizable molecules, existing methods have shown\nlimited coverage of the space and poor molecular optimization performance. To\ntackle these problems, we introduce ReaSyn, a generative framework for\nsynthesizable projection where the model explores the neighborhood of given\nmolecules in the synthesizable space by generating pathways that result in\nsynthesizable analogs. To fully utilize the chemical knowledge contained in the\nsynthetic pathways, we propose a novel perspective that views synthetic\npathways akin to reasoning paths in large language models (LLMs). Specifically,\ninspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the\nchain-of-reaction (CoR) notation that explicitly states reactants, reaction\ntypes, and intermediate products for each step in a pathway. With the CoR\nnotation, ReaSyn can get dense supervision in every reaction step to explicitly\nlearn chemical reaction rules during supervised training and perform\nstep-by-step reasoning. In addition, to further enhance the reasoning\ncapability of ReaSyn, we propose reinforcement learning (RL)-based finetuning\nand goal-directed test-time compute scaling tailored for synthesizable\nprojection. ReaSyn achieves the highest reconstruction rate and pathway\ndiversity in synthesizable molecule reconstruction and the highest optimization\nperformance in synthesizable goal-directed molecular optimization, and\nsignificantly outperforms previous synthesizable projection methods in\nsynthesizable hit expansion. These results highlight ReaSyn's superior ability\nto navigate combinatorially-large synthesizable chemical space."
                },
                "authors": [
                    {
                        "name": "Seul Lee"
                    },
                    {
                        "name": "Karsten Kreis"
                    },
                    {
                        "name": "Srimukh Prasad Veccham"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Danny Reidenbach"
                    },
                    {
                        "name": "Saee Paliwal"
                    },
                    {
                        "name": "Weili Nie"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16072v1",
                "updated": "2025-09-19T15:19:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:19:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models"
                },
                "summary": "Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/)."
                },
                "authors": [
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chetouani"
                },
                "author": "Mohamed Chetouani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13252v2",
                "updated": "2025-09-19T15:19:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    24,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-19T15:35:17Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    15,
                    35,
                    17,
                    0,
                    139,
                    0
                ],
                "title": "Are LLMs Better Formalizers than Solvers on Complex Problems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better Formalizers than Solvers on Complex Problems?"
                },
                "summary": "A trending line of recent work advocates for using large language models\n(LLMs) as formalizers instead of as end-to-end solvers for logical reasoning\nproblems. Instead of generating the solution, the LLM generates a formal\nprogram that derives a solution via an external solver. While performance gain\nof the seemingly scalable LLM-as-formalizer over the seemingly unscalable\nLLM-as-solver has been widely reported, we show that this superiority does not\nhold on real-life constraint satisfaction problems. On 4 domains, we\nsystematically evaluate 6 LLMs including 4 large reasoning models with\ninference-time scaling, paired with 5 pipelines including 2 types of formalism.\nWe show that in few-shot settings, LLM-as-formalizer underperforms\nLLM-as-solver. While LLM-as-formalizer promises accuracy, robustness,\nfaithfulness, and efficiency, we observe that the present LLMs do not yet\ndeliver any of those, as their limited ability to generate formal programs\nleads to failure to scale with complexity, hard-coded solutions, and excessive\nreasoning tokens. We present our detailed analysis and actionable remedies to\ndrive future research that improves LLM-as-formalizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A trending line of recent work advocates for using large language models\n(LLMs) as formalizers instead of as end-to-end solvers for logical reasoning\nproblems. Instead of generating the solution, the LLM generates a formal\nprogram that derives a solution via an external solver. While performance gain\nof the seemingly scalable LLM-as-formalizer over the seemingly unscalable\nLLM-as-solver has been widely reported, we show that this superiority does not\nhold on real-life constraint satisfaction problems. On 4 domains, we\nsystematically evaluate 6 LLMs including 4 large reasoning models with\ninference-time scaling, paired with 5 pipelines including 2 types of formalism.\nWe show that in few-shot settings, LLM-as-formalizer underperforms\nLLM-as-solver. While LLM-as-formalizer promises accuracy, robustness,\nfaithfulness, and efficiency, we observe that the present LLMs do not yet\ndeliver any of those, as their limited ability to generate formal programs\nleads to failure to scale with complexity, hard-coded solutions, and excessive\nreasoning tokens. We present our detailed analysis and actionable remedies to\ndrive future research that improves LLM-as-formalizer."
                },
                "authors": [
                    {
                        "name": "Rikhil Amonkar"
                    },
                    {
                        "name": "May Lai"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16061v1",
                "updated": "2025-09-19T15:10:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:10:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    33,
                    4,
                    262,
                    0
                ],
                "title": "Latent Conditioned Loco-Manipulation Using Motion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Conditioned Loco-Manipulation Using Motion Priors"
                },
                "summary": "Although humanoid and quadruped robots provide a wide range of capabilities,\ncurrent control methods, such as Deep Reinforcement Learning, focus mainly on\nsingle skills. This approach is inefficient for solving more complicated tasks\nwhere high-level goals, physical robot limitations and desired motion style\nmight all need to be taken into account. A more effective approach is to first\ntrain a multipurpose motion policy that acquires low-level skills through\nimitation, while providing latent space control over skill execution. Then,\nthis policy can be used to efficiently solve downstream tasks. This method has\nalready been successful for controlling characters in computer graphics. In\nthis work, we apply the approach to humanoid and quadrupedal loco-manipulation\nby imitating either simple synthetic motions or kinematically retargeted dog\nmotions. We extend the original formulation to handle constraints, ensuring\ndeployment safety, and use a diffusion discriminator for better imitation\nquality. We verify our methods by performing loco-manipulation in simulation\nfor the H1 humanoid and Solo12 quadruped, as well as deploying policies on\nSolo12 hardware. Videos and code are available at\nhttps://gepetto.github.io/LaCoLoco/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although humanoid and quadruped robots provide a wide range of capabilities,\ncurrent control methods, such as Deep Reinforcement Learning, focus mainly on\nsingle skills. This approach is inefficient for solving more complicated tasks\nwhere high-level goals, physical robot limitations and desired motion style\nmight all need to be taken into account. A more effective approach is to first\ntrain a multipurpose motion policy that acquires low-level skills through\nimitation, while providing latent space control over skill execution. Then,\nthis policy can be used to efficiently solve downstream tasks. This method has\nalready been successful for controlling characters in computer graphics. In\nthis work, we apply the approach to humanoid and quadrupedal loco-manipulation\nby imitating either simple synthetic motions or kinematically retargeted dog\nmotions. We extend the original formulation to handle constraints, ensuring\ndeployment safety, and use a diffusion discriminator for better imitation\nquality. We verify our methods by performing loco-manipulation in simulation\nfor the H1 humanoid and Solo12 quadruped, as well as deploying policies on\nSolo12 hardware. Videos and code are available at\nhttps://gepetto.github.io/LaCoLoco/"
                },
                "authors": [
                    {
                        "name": "Maciej Stępień"
                    },
                    {
                        "name": "Rafael Kourdis"
                    },
                    {
                        "name": "Constant Roux"
                    },
                    {
                        "name": "Olivier Stasse"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Stasse"
                },
                "author": "Olivier Stasse",
                "arxiv_comment": "https://gepetto.github.io/LaCoLoco/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16060v1",
                "updated": "2025-09-19T15:10:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T15:10:19Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    10,
                    19,
                    4,
                    262,
                    0
                ],
                "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer\n  Residual Connection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer\n  Residual Connection"
                },
                "summary": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER."
                },
                "authors": [
                    {
                        "name": "Maithili Joshi"
                    },
                    {
                        "name": "Palash Nandi"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "Accepted in EMNLP'25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09627v2",
                "updated": "2025-09-19T15:02:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    2,
                    54,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T11:37:02Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    11,
                    37,
                    2,
                    2,
                    162,
                    0
                ],
                "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates"
                },
                "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions. First, we\nstudy how each methods performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions. First, we\nstudy how each methods performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples."
                },
                "authors": [
                    {
                        "name": "Nicolas Audinet de Pieuchon"
                    },
                    {
                        "name": "Adel Daoud"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Richard Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Johansson"
                },
                "author": "Richard Johansson",
                "arxiv_comment": "To appear as: Nicolas Audinet de Pieuchon, Adel Daoud, Connor T.\n  Jerzak, Moa Johansson, Richard Johansson. Benchmarking Debiasing Methods for\n  LLM-based Parameter Estimates. In: Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08454v2",
                "updated": "2025-09-19T15:01:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    1,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-14T21:55:37Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    21,
                    55,
                    37,
                    1,
                    14,
                    0
                ],
                "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag&Tab: Pretraining Data Detection in Large Language Models Using\n  Keyword-Based Membership Inference Attack"
                },
                "summary": "Large language models (LLMs) have become essential tools for digital task\nassistance. Their training relies heavily on the collection of vast amounts of\ndata, which may include copyright-protected or sensitive information. Recent\nstudies on detecting pretraining data in LLMs have primarily focused on\nsentence- or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model's predicted tokens. However,\nthese methods often exhibit poor accuracy, failing to account for the semantic\nimportance of textual content and word significance. To address these\nshortcomings, we propose Tag&Tab, a novel approach for detecting data used in\nLLM pretraining. Our method leverages established natural language processing\n(NLP) techniques to tag keywords in the input text, a process we term Tagging.\nThen, the LLM is used to obtain probabilities for these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on four benchmark datasets (BookMIA,\nMIMIR, PatentMIA, and the Pile) and several open-source LLMs of varying sizes\ndemonstrate an average increase in AUC scores ranging from 5.3% to 17.6% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential tools for digital task\nassistance. Their training relies heavily on the collection of vast amounts of\ndata, which may include copyright-protected or sensitive information. Recent\nstudies on detecting pretraining data in LLMs have primarily focused on\nsentence- or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model's predicted tokens. However,\nthese methods often exhibit poor accuracy, failing to account for the semantic\nimportance of textual content and word significance. To address these\nshortcomings, we propose Tag&Tab, a novel approach for detecting data used in\nLLM pretraining. Our method leverages established natural language processing\n(NLP) techniques to tag keywords in the input text, a process we term Tagging.\nThen, the LLM is used to obtain probabilities for these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on four benchmark datasets (BookMIA,\nMIMIR, PatentMIA, and the Pile) and several open-source LLMs of varying sizes\ndemonstrate an average increase in AUC scores ranging from 5.3% to 17.6% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs."
                },
                "authors": [
                    {
                        "name": "Sagiv Antebi"
                    },
                    {
                        "name": "Edan Habler"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Elovici"
                },
                "author": "Yuval Elovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21741v2",
                "updated": "2025-09-19T14:53:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    53,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-29T16:07:33Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance"
                },
                "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines."
                },
                "authors": [
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "Minlong Peng"
                    }
                ],
                "author_detail": {
                    "name": "Minlong Peng"
                },
                "author": "Minlong Peng",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16028v1",
                "updated": "2025-09-19T14:34:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    34,
                    22,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:34:22Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    34,
                    22,
                    4,
                    262,
                    0
                ],
                "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Verbalize, then Speak: Bridging Complex Thoughts and\n  Comprehensible Speech"
                },
                "summary": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT"
                },
                "authors": [
                    {
                        "name": "Sang Hoon Woo"
                    },
                    {
                        "name": "Sehun Lee"
                    },
                    {
                        "name": "Kang-wook Kim"
                    },
                    {
                        "name": "Gunhee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gunhee Kim"
                },
                "author": "Gunhee Kim",
                "arxiv_comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16025v1",
                "updated": "2025-09-19T14:33:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    33,
                    5,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:33:05Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    33,
                    5,
                    4,
                    262,
                    0
                ],
                "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation\n  Model via Multi-Target Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Session-Level Spoken Language Assessment with a Multimodal Foundation\n  Model via Multi-Target Learning"
                },
                "summary": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications."
                },
                "authors": [
                    {
                        "name": "Hong-Yun Lin"
                    },
                    {
                        "name": "Jhen-Ke Lin"
                    },
                    {
                        "name": "Chung-Chun Wang"
                    },
                    {
                        "name": "Hao-Chien Lu"
                    },
                    {
                        "name": "Berlin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Berlin Chen"
                },
                "author": "Berlin Chen",
                "arxiv_comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12815v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12815v4",
                "updated": "2025-09-19T14:31:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    31,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2024-08-23T03:21:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    21,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "CrackSCF: Lightweight Cascaded Fusion Network for Robust and Efficient\n  Structural Crack Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrackSCF: Lightweight Cascaded Fusion Network for Robust and Efficient\n  Structural Crack Segmentation"
                },
                "summary": "Accurately segmenting structural cracks at the pixel level remains a major\nhurdle, as existing methods fail to integrate local textures with pixel\ndependencies, often leading to fragmented and incomplete predictions. Moreover,\ntheir high parameter counts and substantial computational demands hinder\npractical deployment on resource-constrained edge devices. To address these\nchallenges, we propose CrackSCF, a Lightweight Cascaded Fusion Crack\nSegmentation Network designed to achieve robust crack segmentation with\nexceptional computational efficiency. We design a lightweight convolutional\nblock (LRDS) to replace all standard convolutions. This approach efficiently\ncaptures local patterns while operating with a minimal computational footprint.\nFor a holistic perception of crack structures, a lightweight Long-range\nDependency Extractor (LDE) captures global dependencies. These are then\nintelligently unified with local patterns by our Staircase Cascaded Fusion\nModule (SCFM), ensuring the final segmentation maps are both seamless in\ncontinuity and rich in fine-grained detail. To comprehensively evaluate our\nmethod, we created the challenging TUT benchmark dataset and evaluated it\nalongside five other public datasets. The experimental results show that the\nCrackSCF method consistently outperforms the existing methods, and it\ndemonstrates greater robustness in dealing with complex background noise. On\nthe TUT dataset, CrackSCF achieved 0.8382 on F1 score and 0.8473 on mIoU, and\nit only required 4.79M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately segmenting structural cracks at the pixel level remains a major\nhurdle, as existing methods fail to integrate local textures with pixel\ndependencies, often leading to fragmented and incomplete predictions. Moreover,\ntheir high parameter counts and substantial computational demands hinder\npractical deployment on resource-constrained edge devices. To address these\nchallenges, we propose CrackSCF, a Lightweight Cascaded Fusion Crack\nSegmentation Network designed to achieve robust crack segmentation with\nexceptional computational efficiency. We design a lightweight convolutional\nblock (LRDS) to replace all standard convolutions. This approach efficiently\ncaptures local patterns while operating with a minimal computational footprint.\nFor a holistic perception of crack structures, a lightweight Long-range\nDependency Extractor (LDE) captures global dependencies. These are then\nintelligently unified with local patterns by our Staircase Cascaded Fusion\nModule (SCFM), ensuring the final segmentation maps are both seamless in\ncontinuity and rich in fine-grained detail. To comprehensively evaluate our\nmethod, we created the challenging TUT benchmark dataset and evaluated it\nalongside five other public datasets. The experimental results show that the\nCrackSCF method consistently outperforms the existing methods, and it\ndemonstrates greater robustness in dealing with complex background noise. On\nthe TUT dataset, CrackSCF achieved 0.8382 on F1 score and 0.8473 on mIoU, and\nit only required 4.79M parameters."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Chen Jia"
                    },
                    {
                        "name": "Fan Shi"
                    },
                    {
                        "name": "Xu Cheng"
                    },
                    {
                        "name": "Mianzhao Wang"
                    },
                    {
                        "name": "Shengyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shengyong Chen"
                },
                "author": "Shengyong Chen",
                "arxiv_comment": "This paper is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12815v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12815v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08177v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08177v4",
                "updated": "2025-09-19T14:30:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    30,
                    28,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-12T07:32:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "SycEval: Evaluating LLM Sycophancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SycEval: Evaluating LLM Sycophancy"
                },
                "summary": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications."
                },
                "authors": [
                    {
                        "name": "Aaron Fanous"
                    },
                    {
                        "name": "Jacob Goldberg"
                    },
                    {
                        "name": "Ank A. Agarwal"
                    },
                    {
                        "name": "Joanna Lin"
                    },
                    {
                        "name": "Anson Zhou"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "arxiv_affiliation": "Stanford University",
                "author": "Sanmi Koyejo",
                "arxiv_comment": "AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08177v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08177v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10371v2",
                "updated": "2025-09-19T14:28:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    28,
                    47,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-12T16:05:07Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective"
                },
                "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT."
                },
                "authors": [
                    {
                        "name": "Seokjin Go"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Spandan More"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Aaron Jezghani"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14395v2",
                "updated": "2025-09-19T14:26:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    26,
                    2,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-20T14:14:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language"
                },
                "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy for successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy for successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Dongkwan Kim"
                    },
                    {
                        "name": "Jay Shin"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "To appear in Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16011v1",
                "updated": "2025-09-19T14:24:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    24,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    24,
                    48,
                    4,
                    262,
                    0
                ],
                "title": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision"
                },
                "summary": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning."
                },
                "authors": [
                    {
                        "name": "Xiwei Liu"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16006v1",
                "updated": "2025-09-19T14:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    19,
                    44,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    19,
                    44,
                    4,
                    262,
                    0
                ],
                "title": "Defining and Monitoring Complex Robot Activities via LLMs and Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Monitoring Complex Robot Activities via LLMs and Symbolic\n  Reasoning"
                },
                "summary": "Recent years have witnessed a growing interest in automating labor-intensive\nand complex activities, i.e., those consisting of multiple atomic tasks, by\ndeploying robots in dynamic and unpredictable environments such as industrial\nand agricultural settings. A key characteristic of these contexts is that\nactivities are not predefined: while they involve a limited set of possible\ntasks, their combinations may vary depending on the situation. Moreover,\ndespite recent advances in robotics, the ability for humans to monitor the\nprogress of high-level activities - in terms of past, present, and future\nactions - remains fundamental to ensure the correct execution of\nsafety-critical processes. In this paper, we introduce a general architecture\nthat integrates Large Language Models (LLMs) with automated planning, enabling\nhumans to specify high-level activities (also referred to as processes) using\nnatural language, and to monitor their execution by querying a robot. We also\npresent an implementation of this architecture using state-of-the-art\ncomponents and quantitatively evaluate the approach in a real-world precision\nagriculture scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed a growing interest in automating labor-intensive\nand complex activities, i.e., those consisting of multiple atomic tasks, by\ndeploying robots in dynamic and unpredictable environments such as industrial\nand agricultural settings. A key characteristic of these contexts is that\nactivities are not predefined: while they involve a limited set of possible\ntasks, their combinations may vary depending on the situation. Moreover,\ndespite recent advances in robotics, the ability for humans to monitor the\nprogress of high-level activities - in terms of past, present, and future\nactions - remains fundamental to ensure the correct execution of\nsafety-critical processes. In this paper, we introduce a general architecture\nthat integrates Large Language Models (LLMs) with automated planning, enabling\nhumans to specify high-level activities (also referred to as processes) using\nnatural language, and to monitor their execution by querying a robot. We also\npresent an implementation of this architecture using state-of-the-art\ncomponents and quantitatively evaluate the approach in a real-world precision\nagriculture scenario."
                },
                "authors": [
                    {
                        "name": "Francesco Argenziano"
                    },
                    {
                        "name": "Elena Umili"
                    },
                    {
                        "name": "Francesco Leotta"
                    },
                    {
                        "name": "Daniele Nardi"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Nardi"
                },
                "author": "Daniele Nardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16002v1",
                "updated": "2025-09-19T14:11:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    11,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T14:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    11,
                    35,
                    4,
                    262,
                    0
                ],
                "title": "Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and\n  Grover-Based Trajectory Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and\n  Grover-Based Trajectory Optimization"
                },
                "summary": "A fully quantum reinforcement learning framework is developed that integrates\na quantum Markov decision process, dynamic circuit-based qubit reuse, and\nGrover's algorithm for trajectory optimization. The framework encodes states,\nactions, rewards, and transitions entirely within the quantum domain, enabling\nparallel exploration of state-action sequences through superposition and\neliminating classical subroutines. Dynamic circuit operations, including\nmid-circuit measurement and reset, allow reuse of the same physical qubits\nacross multiple agent-environment interactions, reducing qubit requirements\nfrom 7*T to 7 for T time steps while preserving logical continuity. Quantum\narithmetic computes trajectory returns, and Grover's search is applied to the\nsuperposition of these evaluated trajectories to amplify the probability of\nmeasuring those with the highest return, thereby accelerating the\nidentification of the optimal policy. Simulations demonstrate that the\ndynamic-circuit-based implementation preserves trajectory fidelity while\nreducing qubit usage by 66 percent relative to the static design. Experimental\ndeployment on IBM Heron-class quantum hardware confirms that the framework\noperates within the constraints of current quantum processors and validates the\nfeasibility of fully quantum multi-step reinforcement learning under noisy\nintermediate-scale quantum conditions. This framework advances the scalability\nand practical application of quantum reinforcement learning for large-scale\nsequential decision-making tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fully quantum reinforcement learning framework is developed that integrates\na quantum Markov decision process, dynamic circuit-based qubit reuse, and\nGrover's algorithm for trajectory optimization. The framework encodes states,\nactions, rewards, and transitions entirely within the quantum domain, enabling\nparallel exploration of state-action sequences through superposition and\neliminating classical subroutines. Dynamic circuit operations, including\nmid-circuit measurement and reset, allow reuse of the same physical qubits\nacross multiple agent-environment interactions, reducing qubit requirements\nfrom 7*T to 7 for T time steps while preserving logical continuity. Quantum\narithmetic computes trajectory returns, and Grover's search is applied to the\nsuperposition of these evaluated trajectories to amplify the probability of\nmeasuring those with the highest return, thereby accelerating the\nidentification of the optimal policy. Simulations demonstrate that the\ndynamic-circuit-based implementation preserves trajectory fidelity while\nreducing qubit usage by 66 percent relative to the static design. Experimental\ndeployment on IBM Heron-class quantum hardware confirms that the framework\noperates within the constraints of current quantum processors and validates the\nfeasibility of fully quantum multi-step reinforcement learning under noisy\nintermediate-scale quantum conditions. This framework advances the scalability\nand practical application of quantum reinforcement learning for large-scale\nsequential decision-making tasks."
                },
                "authors": [
                    {
                        "name": "Thet Htar Su"
                    },
                    {
                        "name": "Shaswot Shresthamali"
                    },
                    {
                        "name": "Masaaki Kondo"
                    }
                ],
                "author_detail": {
                    "name": "Masaaki Kondo"
                },
                "author": "Masaaki Kondo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03591v2",
                "updated": "2025-09-19T14:02:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    2,
                    24,
                    4,
                    262,
                    0
                ],
                "published": "2024-08-07T07:09:14Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    7,
                    9,
                    14,
                    2,
                    220,
                    0
                ],
                "title": "FOVAL: Calibration-Free and Subject-Invariant Fixation Depth Estimation\n  Across Diverse Eye-Tracking Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOVAL: Calibration-Free and Subject-Invariant Fixation Depth Estimation\n  Across Diverse Eye-Tracking Datasets"
                },
                "summary": "Accurate fixation depth estimation is essential for applications in extended\nreality (XR), robotics, and human-computer interaction. However, current\nmethods heavily depend on user-specific calibration, which limits their\nscalability and usability. We introduce FOVAL, a robust calibration-free\napproach that combines spatiotemporal sequence modelling via Long Short-Term\nMemory (LSTM) networks with subject-invariant feature engineering and\nnormalisation. Compared to Transformers, Temporal Convolutional Networks\n(TCNs), and CNNs, FOVAL achieves superior performance, particularly in\nscenarios with limited and noisy gaze data. Evaluations across three benchmark\ndatasets using Leave-One-Out Cross-Validation (LOOCV) and cross-dataset\nvalidation show a mean absolute error (MAE) of 9.1 cm and strong generalisation\nwithout calibration. We further analyse inter-subject variability and domain\nshifts, providing insight into model robustness and adaptation. FOVAL's\nscalability and accuracy make it highly suitable for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate fixation depth estimation is essential for applications in extended\nreality (XR), robotics, and human-computer interaction. However, current\nmethods heavily depend on user-specific calibration, which limits their\nscalability and usability. We introduce FOVAL, a robust calibration-free\napproach that combines spatiotemporal sequence modelling via Long Short-Term\nMemory (LSTM) networks with subject-invariant feature engineering and\nnormalisation. Compared to Transformers, Temporal Convolutional Networks\n(TCNs), and CNNs, FOVAL achieves superior performance, particularly in\nscenarios with limited and noisy gaze data. Evaluations across three benchmark\ndatasets using Leave-One-Out Cross-Validation (LOOCV) and cross-dataset\nvalidation show a mean absolute error (MAE) of 9.1 cm and strong generalisation\nwithout calibration. We further analyse inter-subject variability and domain\nshifts, providing insight into model robustness and adaptation. FOVAL's\nscalability and accuracy make it highly suitable for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Benedikt W. Hosp"
                    }
                ],
                "author_detail": {
                    "name": "Benedikt W. Hosp"
                },
                "author": "Benedikt W. Hosp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15389v2",
                "updated": "2025-09-19T13:54:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    54,
                    9,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-21T11:26:40Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    26,
                    40,
                    2,
                    141,
                    0
                ],
                "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study"
                },
                "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jihae Jeong"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13090v2",
                "updated": "2025-09-19T13:54:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    54,
                    4,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-19T13:24:01Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    24,
                    1,
                    0,
                    139,
                    0
                ],
                "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effect of Language Diversity When Fine-Tuning Large Language Models\n  for Translation"
                },
                "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."
                },
                "authors": [
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "EMNLP 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03455v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03455v3",
                "updated": "2025-09-19T13:47:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    47,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2024-11-05T19:13:22Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    13,
                    22,
                    1,
                    310,
                    0
                ],
                "title": "Watson: A Cognitive Observability Framework for the Reasoning of\n  LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watson: A Cognitive Observability Framework for the Reasoning of\n  LLM-Powered Agents"
                },
                "summary": "Large language models (LLMs) are increasingly integrated into autonomous\nsystems, giving rise to a new class of software known as Agentware, where\nLLM-powered agents perform complex, open-ended tasks in domains such as\nsoftware engineering, customer service, and data analysis. However, their high\nautonomy and opaque reasoning processes pose significant challenges for\ntraditional software observability methods. To address this, we introduce the\nconcept of cognitive observability - the ability to recover and inspect the\nimplicit reasoning behind agent decisions. We present Watson, a general-purpose\nframework for observing the reasoning processes of fast-thinking LLM agents\nwithout altering their behavior. Watson retroactively infers reasoning traces\nusing prompt attribution techniques. We evaluate Watson in both manual\ndebugging and automated correction scenarios across the MMLU benchmark and the\nAutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both\nstatic and dynamic settings, Watson surfaces actionable reasoning insights and\nsupports targeted interventions, demonstrating its practical utility for\nimproving transparency and reliability in Agentware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated into autonomous\nsystems, giving rise to a new class of software known as Agentware, where\nLLM-powered agents perform complex, open-ended tasks in domains such as\nsoftware engineering, customer service, and data analysis. However, their high\nautonomy and opaque reasoning processes pose significant challenges for\ntraditional software observability methods. To address this, we introduce the\nconcept of cognitive observability - the ability to recover and inspect the\nimplicit reasoning behind agent decisions. We present Watson, a general-purpose\nframework for observing the reasoning processes of fast-thinking LLM agents\nwithout altering their behavior. Watson retroactively infers reasoning traces\nusing prompt attribution techniques. We evaluate Watson in both manual\ndebugging and automated correction scenarios across the MMLU benchmark and the\nAutoCodeRover and OpenHands agents on the SWE-bench-lite dataset. In both\nstatic and dynamic settings, Watson surfaces actionable reasoning insights and\nsupports targeted interventions, demonstrating its practical utility for\nimproving transparency and reliability in Agentware systems."
                },
                "authors": [
                    {
                        "name": "Benjamin Rombaut"
                    },
                    {
                        "name": "Sogol Masoumzadeh"
                    },
                    {
                        "name": "Kirill Vasilevski"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03455v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03455v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15980v1",
                "updated": "2025-09-19T13:45:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    45,
                    18,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:45:18Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    45,
                    18,
                    4,
                    262,
                    0
                ],
                "title": "Shedding Light on Depth: Explainability Assessment in Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shedding Light on Depth: Explainability Assessment in Monocular Depth\n  Estimation"
                },
                "summary": "Explainable artificial intelligence is increasingly employed to understand\nthe decision-making process of deep learning models and create trustworthiness\nin their adoption. However, the explainability of Monocular Depth Estimation\n(MDE) remains largely unexplored despite its wide deployment in real-world\napplications. In this work, we study how to analyze MDE networks to map the\ninput image to the predicted depth map. More in detail, we investigate\nwell-established feature attribution methods, Saliency Maps, Integrated\nGradients, and Attention Rollout on different computationally complex models\nfor MDE: METER, a lightweight network, and PixelFormer, a deep network. We\nassess the quality of the generated visual explanations by selectively\nperturbing the most relevant and irrelevant pixels, as identified by the\nexplainability methods, and analyzing the impact of these perturbations on the\nmodel's output. Moreover, since existing evaluation metrics can have some\nlimitations in measuring the validity of visual explanations for MDE, we\nadditionally introduce the Attribution Fidelity. This metric evaluates the\nreliability of the feature attribution by assessing their consistency with the\npredicted depth map. Experimental results demonstrate that Saliency Maps and\nIntegrated Gradients have good performance in highlighting the most important\ninput features for MDE lightweight and deep models, respectively. Furthermore,\nwe show that Attribution Fidelity effectively identifies whether an\nexplainability method fails to produce reliable visual maps, even in scenarios\nwhere conventional metrics might suggest satisfactory results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable artificial intelligence is increasingly employed to understand\nthe decision-making process of deep learning models and create trustworthiness\nin their adoption. However, the explainability of Monocular Depth Estimation\n(MDE) remains largely unexplored despite its wide deployment in real-world\napplications. In this work, we study how to analyze MDE networks to map the\ninput image to the predicted depth map. More in detail, we investigate\nwell-established feature attribution methods, Saliency Maps, Integrated\nGradients, and Attention Rollout on different computationally complex models\nfor MDE: METER, a lightweight network, and PixelFormer, a deep network. We\nassess the quality of the generated visual explanations by selectively\nperturbing the most relevant and irrelevant pixels, as identified by the\nexplainability methods, and analyzing the impact of these perturbations on the\nmodel's output. Moreover, since existing evaluation metrics can have some\nlimitations in measuring the validity of visual explanations for MDE, we\nadditionally introduce the Attribution Fidelity. This metric evaluates the\nreliability of the feature attribution by assessing their consistency with the\npredicted depth map. Experimental results demonstrate that Saliency Maps and\nIntegrated Gradients have good performance in highlighting the most important\ninput features for MDE lightweight and deep models, respectively. Furthermore,\nwe show that Attribution Fidelity effectively identifies whether an\nexplainability method fails to produce reliable visual maps, even in scenarios\nwhere conventional metrics might suggest satisfactory results."
                },
                "authors": [
                    {
                        "name": "Lorenzo Cirillo"
                    },
                    {
                        "name": "Claudio Schiavella"
                    },
                    {
                        "name": "Lorenzo Papa"
                    },
                    {
                        "name": "Paolo Russo"
                    },
                    {
                        "name": "Irene Amerini"
                    }
                ],
                "author_detail": {
                    "name": "Irene Amerini"
                },
                "author": "Irene Amerini",
                "arxiv_comment": "8 pages, 3 figures, 2 tables. This paper has been accepted at the\n  International Joint Conference on Neural Networks (IJCNN) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23386v2",
                "updated": "2025-09-19T13:35:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    35,
                    4,
                    262,
                    0
                ],
                "published": "2025-07-31T10:01:11Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    1,
                    11,
                    3,
                    212,
                    0
                ],
                "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models"
                },
                "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods."
                },
                "authors": [
                    {
                        "name": "Ailiang Lin"
                    },
                    {
                        "name": "Zhuoyun Li"
                    },
                    {
                        "name": "Kotaro Funakoshi"
                    },
                    {
                        "name": "Manabu Okumura"
                    }
                ],
                "author_detail": {
                    "name": "Manabu Okumura"
                },
                "author": "Manabu Okumura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15974v1",
                "updated": "2025-09-19T13:35:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    7,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:35:07Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    35,
                    7,
                    4,
                    262,
                    0
                ],
                "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEFT: Bias-Efficient Fine-Tuning of Language Models"
                },
                "summary": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks."
                },
                "authors": [
                    {
                        "name": "Baichuan Huang"
                    },
                    {
                        "name": "Ananth Balashankar"
                    },
                    {
                        "name": "Amir Aminifar"
                    }
                ],
                "author_detail": {
                    "name": "Amir Aminifar"
                },
                "author": "Amir Aminifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15971v1",
                "updated": "2025-09-19T13:27:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    27,
                    27,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:27:27Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    27,
                    27,
                    4,
                    262,
                    0
                ],
                "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine\n  Learning Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine\n  Learning Pipelines"
                },
                "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines."
                },
                "authors": [
                    {
                        "name": "Owen Truong"
                    },
                    {
                        "name": "Terrence Zhang"
                    },
                    {
                        "name": "Arnav Marchareddy"
                    },
                    {
                        "name": "Ryan Lee"
                    },
                    {
                        "name": "Jeffery Busold"
                    },
                    {
                        "name": "Michael Socas"
                    },
                    {
                        "name": "Eman Abdullah AlOmar"
                    }
                ],
                "author_detail": {
                    "name": "Eman Abdullah AlOmar"
                },
                "author": "Eman Abdullah AlOmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21589v3",
                "updated": "2025-09-19T13:25:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    25,
                    52,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-29T12:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning"
                },
                "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are coming soon. Our datasets,\nmodels, and code are publicly available at https://github.com/Word2VecT/Middo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are coming soon. Our datasets,\nmodels, and code are publicly available at https://github.com/Word2VecT/Middo."
                },
                "authors": [
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Mengzhang Cai"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "Accepted by EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13786v2",
                "updated": "2025-09-19T13:24:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    24,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-17T07:55:58Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    55,
                    58,
                    2,
                    260,
                    0
                ],
                "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training\n  Quantization"
                },
                "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. In this paper, we extend Post-Training\nQuantization (PTQ) by focusing on Quantization-Aware Training (QAT), which\nincorporates low-precision simulation during training for robustness at\nultra-low bitwidths. In particular, we develop a QAT methodology for a neural\nreceiver architecture and benchmark it against a PTQ approach across diverse\n3GPP Clustered Delay Line (CDL) channel profiles under both Line-of-Sight (LoS)\nand Non-LoS (NLoS) conditions, with user velocities up to 40 m/s. Results show\nthat 4-bit and 8-bit QAT models achieve BLERs comparable to FP32 models at a\n10% target BLER. Moreover, QAT models succeed in NLoS scenarios where PTQ\nmodels fail to reach the 10% BLER target, while also yielding an 8x\ncompression. These results with respect to full-precision demonstrate that QAT\nis a key enabler of low-complexity and latency-constrained inference at the PHY\nlayer, facilitating real-time processing in 6G edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. In this paper, we extend Post-Training\nQuantization (PTQ) by focusing on Quantization-Aware Training (QAT), which\nincorporates low-precision simulation during training for robustness at\nultra-low bitwidths. In particular, we develop a QAT methodology for a neural\nreceiver architecture and benchmark it against a PTQ approach across diverse\n3GPP Clustered Delay Line (CDL) channel profiles under both Line-of-Sight (LoS)\nand Non-LoS (NLoS) conditions, with user velocities up to 40 m/s. Results show\nthat 4-bit and 8-bit QAT models achieve BLERs comparable to FP32 models at a\n10% target BLER. Moreover, QAT models succeed in NLoS scenarios where PTQ\nmodels fail to reach the 10% BLER target, while also yielding an 8x\ncompression. These results with respect to full-precision demonstrate that QAT\nis a key enabler of low-complexity and latency-constrained inference at the PHY\nlayer, facilitating real-time processing in 6G edge devices."
                },
                "authors": [
                    {
                        "name": "SaiKrishna Saketh Yellapragada"
                    },
                    {
                        "name": "Esa Ollila"
                    },
                    {
                        "name": "Mario Costa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Costa"
                },
                "author": "Mario Costa",
                "arxiv_comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22777v3",
                "updated": "2025-09-19T13:18:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    18,
                    19,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-28T18:45:42Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    45,
                    42,
                    2,
                    148,
                    0
                ],
                "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators"
                },
                "summary": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Alon Lavie"
                    },
                    {
                        "name": "Isabel Trancoso"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Trancoso"
                },
                "author": "Isabel Trancoso",
                "arxiv_comment": "October ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15957v1",
                "updated": "2025-09-19T13:17:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    17,
                    16,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T13:17:16Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    17,
                    16,
                    4,
                    262,
                    0
                ],
                "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol"
                },
                "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice."
                },
                "authors": [
                    {
                        "name": "Kanato Masayoshi"
                    },
                    {
                        "name": "Masahiro Hashimoto"
                    },
                    {
                        "name": "Ryoichi Yokoyama"
                    },
                    {
                        "name": "Naoki Toda"
                    },
                    {
                        "name": "Yoshifumi Uwamino"
                    },
                    {
                        "name": "Shogo Fukuda"
                    },
                    {
                        "name": "Ho Namkoong"
                    },
                    {
                        "name": "Masahiro Jinzaki"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Jinzaki"
                },
                "author": "Masahiro Jinzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v5",
                "updated": "2025-09-19T13:12:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    12,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Published at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01349v3",
                "updated": "2025-09-19T13:06:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    13,
                    6,
                    51,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-03T13:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    39,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation."
                },
                "authors": [
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Konstantinos Thomas"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15432v2",
                "updated": "2025-09-19T12:53:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    53,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-21T10:35:41Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    35,
                    41,
                    3,
                    233,
                    0
                ],
                "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality\n  Tagging, and Management of Synthetic Data"
                },
                "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines."
                },
                "authors": [
                    {
                        "name": "Bidyapati Pradhan"
                    },
                    {
                        "name": "Surajit Dasgupta"
                    },
                    {
                        "name": "Amit Kumar Saha"
                    },
                    {
                        "name": "Omkar Anustoop"
                    },
                    {
                        "name": "Sriram Puttagunta"
                    },
                    {
                        "name": "Vipul Mittal"
                    },
                    {
                        "name": "Gopal Sarda"
                    }
                ],
                "author_detail": {
                    "name": "Gopal Sarda"
                },
                "author": "Gopal Sarda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15940v1",
                "updated": "2025-09-19T12:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    52,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:52:32Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    52,
                    32,
                    4,
                    262,
                    0
                ],
                "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication\n  Alignment on More Than 9600 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pre-Training of LLMs via Topology-Aware Communication\n  Alignment on More Than 9600 GPUs"
                },
                "summary": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline."
                },
                "authors": [
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Kaihua Jiang"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zixian Du"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Eiko Yoneki"
                    }
                ],
                "author_detail": {
                    "name": "Eiko Yoneki"
                },
                "author": "Eiko Yoneki",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01476v2",
                "updated": "2025-09-19T12:43:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    43,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-01T13:44:15Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    44,
                    15,
                    0,
                    244,
                    0
                ],
                "title": "Do Retrieval Augmented Language Models Know When They Don't Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Retrieval Augmented Language Models Know When They Don't Know?"
                },
                "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems."
                },
                "authors": [
                    {
                        "name": "Youchao Zhou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Yicheng Liu"
                    },
                    {
                        "name": "Rui Dai"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Xingchen Zhang"
                    },
                    {
                        "name": "Shumin Shi"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15927v1",
                "updated": "2025-09-19T12:30:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    30,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:30:26Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    30,
                    26,
                    4,
                    262,
                    0
                ],
                "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and\n  Policy Search"
                },
                "summary": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-bidding is an essential tool for advertisers to enhance their\nadvertising performance. Recent progress has shown that AI-Generated Bidding\n(AIGB), which formulates the auto-bidding as a trajectory generation task and\ntrains a conditional diffusion-based planner on offline data, achieves superior\nand stable performance compared to typical offline reinforcement learning\n(RL)-based auto-bidding methods. However, existing AIGB methods still encounter\na performance bottleneck due to their neglect of fine-grained generation\nquality evaluation and inability to explore beyond static datasets. To address\nthis, we propose AIGB-Pearl (\\emph{Planning with EvAluator via RL}), a novel\nmethod that integrates generative planning and policy optimization. The key to\nAIGB-Pearl is to construct a non-bootstrapped \\emph{trajectory evaluator} to\nassign rewards and guide policy search, enabling the planner to optimize its\ngeneration quality iteratively through interaction. Furthermore, to enhance\ntrajectory evaluator accuracy in offline settings, we incorporate three key\ntechniques: (i) a Large Language Model (LLM)-based architecture for better\nrepresentational capacity, (ii) hybrid point-wise and pair-wise losses for\nbetter score learning, and (iii) adaptive integration of expert feedback for\nbetter generalization ability. Extensive experiments on both simulated and\nreal-world advertising systems demonstrate the state-of-the-art performance of\nour approach."
                },
                "authors": [
                    {
                        "name": "Zhiyu Mou"
                    },
                    {
                        "name": "Yiqin Lv"
                    },
                    {
                        "name": "Miao Xu"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Qichen Ye"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Rongquan Bai"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15926v1",
                "updated": "2025-09-19T12:28:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    28,
                    50,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:28:50Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    28,
                    50,
                    4,
                    262,
                    0
                ],
                "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay\n  Assessment"
                },
                "summary": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work."
                },
                "authors": [
                    {
                        "name": "Ahmed Karim"
                    },
                    {
                        "name": "Qiao Wang"
                    },
                    {
                        "name": "Zheng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yuan"
                },
                "arxiv_affiliation": "Judy",
                "author": "Zheng Yuan",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11277v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11277v5",
                "updated": "2025-09-19T12:21:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    21,
                    3,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-16T14:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    11,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "Search and Refine During Think: Facilitating Knowledge Refinement for\n  Improved Retrieval-Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search and Refine During Think: Facilitating Knowledge Refinement for\n  Improved Retrieval-Augmented Reasoning"
                },
                "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n\"search-and-refine-during-think\" paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n\"search-and-refine-during-think\" paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."
                },
                "authors": [
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11277v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11277v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08348v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08348v4",
                "updated": "2025-09-19T12:12:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    12,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2024-01-16T13:29:30Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    13,
                    29,
                    30,
                    1,
                    16,
                    0
                ],
                "title": "Estimating Model Performance Under Covariate Shift Without Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Model Performance Under Covariate Shift Without Labels"
                },
                "summary": "After deployment, machine learning models often experience performance\ndegradation due to shifts in data distribution. It is challenging to assess\npost-deployment performance accurately when labels are missing or delayed.\nExisting proxy methods, such as data drift detection, fail to measure the\neffects of these shifts adequately. To address this, we introduce a new method\nfor evaluating binary classification models on unlabeled tabular data that\naccurately estimates model performance under covariate shift and call it\nProbabilistic Adaptive Performance Estimation (PAPE). It can be applied to any\nperformance metric defined with elements of the confusion matrix. Crucially,\nPAPE operates independently of the original model, relying only on its\npredictions and probability estimates, and does not need any assumptions about\nthe nature of covariate shift, learning directly from data instead. We tested\nPAPE using over 900 dataset-model combinations from US census data, assessing\nits performance against several benchmarks through various metrics. Our\nfindings show that PAPE outperforms other methodologies, making it a superior\nchoice for estimating the performance of binary classification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After deployment, machine learning models often experience performance\ndegradation due to shifts in data distribution. It is challenging to assess\npost-deployment performance accurately when labels are missing or delayed.\nExisting proxy methods, such as data drift detection, fail to measure the\neffects of these shifts adequately. To address this, we introduce a new method\nfor evaluating binary classification models on unlabeled tabular data that\naccurately estimates model performance under covariate shift and call it\nProbabilistic Adaptive Performance Estimation (PAPE). It can be applied to any\nperformance metric defined with elements of the confusion matrix. Crucially,\nPAPE operates independently of the original model, relying only on its\npredictions and probability estimates, and does not need any assumptions about\nthe nature of covariate shift, learning directly from data instead. We tested\nPAPE using over 900 dataset-model combinations from US census data, assessing\nits performance against several benchmarks through various metrics. Our\nfindings show that PAPE outperforms other methodologies, making it a superior\nchoice for estimating the performance of binary classification models."
                },
                "authors": [
                    {
                        "name": "Jakub Białek"
                    },
                    {
                        "name": "Juhani Kivimäki"
                    },
                    {
                        "name": "Wojtek Kuberski"
                    },
                    {
                        "name": "Nikolaos Perrakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Perrakis"
                },
                "author": "Nikolaos Perrakis",
                "arxiv_comment": "23 content pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08348v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08348v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15915v1",
                "updated": "2025-09-19T12:10:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    10,
                    28,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T12:10:28Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    12,
                    10,
                    28,
                    4,
                    262,
                    0
                ],
                "title": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds"
                },
                "summary": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements."
                },
                "authors": [
                    {
                        "name": "Remo Sasso"
                    },
                    {
                        "name": "Michelangelo Conserva"
                    },
                    {
                        "name": "Dominik Jeurissen"
                    },
                    {
                        "name": "Paulo Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Paulo Rauber"
                },
                "author": "Paulo Rauber",
                "arxiv_comment": "20 pages, 9 figures. Accepted for presentation at the 39th Conference\n  on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied\n  World Models for Decision Making",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07824v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07824v5",
                "updated": "2025-09-19T11:59:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    59,
                    49,
                    4,
                    262,
                    0
                ],
                "published": "2025-01-14T03:59:48Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    3,
                    59,
                    48,
                    1,
                    14,
                    0
                ],
                "title": "Efficient Real-time Refinement of Language Model Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Real-time Refinement of Language Model Text Generation"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods."
                },
                "authors": [
                    {
                        "name": "Joonho Ko"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07824v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07824v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19668v4",
                "updated": "2025-09-19T11:58:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T01:29:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    29,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning"
                },
                "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."
                },
                "authors": [
                    {
                        "name": "Mingsheng Cai"
                    },
                    {
                        "name": "Jiuming Jiang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Rossella Arcucci"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Arcucci"
                },
                "author": "Rossella Arcucci",
                "arxiv_comment": "Findings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15901v1",
                "updated": "2025-09-19T11:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    17,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    58,
                    17,
                    4,
                    262,
                    0
                ],
                "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and\n  Personalization via Questions"
                },
                "summary": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Sonu Kumar"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15888v1",
                "updated": "2025-09-19T11:35:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    35,
                    56,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:35:56Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    35,
                    56,
                    4,
                    262,
                    0
                ],
                "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation"
                },
                "summary": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models."
                },
                "authors": [
                    {
                        "name": "Senkang Hu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Jinqi Jiang"
                    },
                    {
                        "name": "Yihang Tao"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Sam Tak Wu Kwong"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "arxiv_comment": "Accepted by NeurIPS'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14442v2",
                "updated": "2025-09-19T11:33:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    34,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-20T14:43:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    43,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Creative Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Preference Optimization"
                },
                "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Simone A. Luchini"
                    },
                    {
                        "name": "Reet Patel"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Roger Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger Beaty"
                },
                "author": "Roger Beaty",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15886v1",
                "updated": "2025-09-19T11:33:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    10,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:33:10Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    10,
                    4,
                    262,
                    0
                ],
                "title": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented\n  LiDAR segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RangeSAM: Leveraging Visual Foundation Models for Range-View repesented\n  LiDAR segmentation"
                },
                "summary": "Point cloud segmentation is central to autonomous driving and 3D scene\nunderstanding. While voxel- and point-based methods dominate recent research\ndue to their compatibility with deep architectures and ability to capture\nfine-grained geometry, they often incur high computational cost, irregular\nmemory access, and limited real-time efficiency. In contrast, range-view\nmethods, though relatively underexplored - can leverage mature 2D semantic\nsegmentation techniques for fast and accurate predictions. Motivated by the\nrapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot\nrecognition, and multimodal tasks, we investigate whether SAM2, the current\nstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone for\nLiDAR point cloud segmentation in the range view. We present , to our\nknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,\ncoupling efficient 2D feature extraction with standard\nprojection/back-projection to operate on point clouds. To optimize SAM2 for\nrange-view representations, we implement several architectural modifications to\nthe encoder: (1) a novel module that emphasizes horizontal spatial dependencies\ninherent in LiDAR range images, (2) a customized configuration of tailored to\nthe geometric properties of spherical projections, and (3) an adapted mechanism\nin the encoder backbone specifically designed to capture the unique spatial\npatterns and discontinuities present in range-view pseudo-images. Our approach\nachieves competitive performance on SemanticKITTI while benefiting from the\nspeed, scalability, and deployment simplicity of 2D-centric pipelines. This\nwork highlights the viability of VFMs as general-purpose backbones for 3D\nperception and opens a path toward unified, foundation-model-driven LiDAR\nsegmentation. Results lets us conclude that range-view segmentation methods\nusing VFMs leads to promising results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation is central to autonomous driving and 3D scene\nunderstanding. While voxel- and point-based methods dominate recent research\ndue to their compatibility with deep architectures and ability to capture\nfine-grained geometry, they often incur high computational cost, irregular\nmemory access, and limited real-time efficiency. In contrast, range-view\nmethods, though relatively underexplored - can leverage mature 2D semantic\nsegmentation techniques for fast and accurate predictions. Motivated by the\nrapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot\nrecognition, and multimodal tasks, we investigate whether SAM2, the current\nstate-of-the-art VFM for segmentation tasks, can serve as a strong backbone for\nLiDAR point cloud segmentation in the range view. We present , to our\nknowledge, the first range-view framework that adapts SAM2 to 3D segmentation,\ncoupling efficient 2D feature extraction with standard\nprojection/back-projection to operate on point clouds. To optimize SAM2 for\nrange-view representations, we implement several architectural modifications to\nthe encoder: (1) a novel module that emphasizes horizontal spatial dependencies\ninherent in LiDAR range images, (2) a customized configuration of tailored to\nthe geometric properties of spherical projections, and (3) an adapted mechanism\nin the encoder backbone specifically designed to capture the unique spatial\npatterns and discontinuities present in range-view pseudo-images. Our approach\nachieves competitive performance on SemanticKITTI while benefiting from the\nspeed, scalability, and deployment simplicity of 2D-centric pipelines. This\nwork highlights the viability of VFMs as general-purpose backbones for 3D\nperception and opens a path toward unified, foundation-model-driven LiDAR\nsegmentation. Results lets us conclude that range-view segmentation methods\nusing VFMs leads to promising results."
                },
                "authors": [
                    {
                        "name": "Paul Julius Kühn"
                    },
                    {
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "name": "Arjan Kuijper"
                    },
                    {
                        "name": "Holger Graf"
                    },
                    {
                        "name": "Dieter Fellner"
                    },
                    {
                        "name": "Saptarshi Neil Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saptarshi Neil Sinha"
                },
                "author": "Saptarshi Neil Sinha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00288v3",
                "updated": "2025-09-19T11:27:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    27,
                    30,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T22:31:59Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    22,
                    31,
                    59,
                    4,
                    150,
                    0
                ],
                "title": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation"
                },
                "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future."
                },
                "authors": [
                    {
                        "name": "Ahmed Elhady"
                    },
                    {
                        "name": "Eneko Agirre"
                    },
                    {
                        "name": "Mikel Artetxe"
                    }
                ],
                "author_detail": {
                    "name": "Mikel Artetxe"
                },
                "author": "Mikel Artetxe",
                "arxiv_comment": "Published as a Conference Paper at the main track of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15880v1",
                "updated": "2025-09-19T11:26:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    26,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:26:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    26,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "Improving Robotic Manipulation with Efficient Geometry-Aware Vision\n  Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Robotic Manipulation with Efficient Geometry-Aware Vision\n  Encoder"
                },
                "summary": "Existing RGB-based imitation learning approaches typically employ traditional\nvision encoders such as ResNet or ViT, which lack explicit 3D reasoning\ncapabilities. Recent geometry-grounded vision models, such as\nVGGT~\\cite{wang2025vggt}, provide robust spatial understanding and are\npromising candidates to address this limitation. This work investigates the\nintegration of geometry-aware visual representations into robotic manipulation.\nOur results suggest that incorporating the geometry-aware vision encoder into\nimitation learning frameworks, including ACT and DP, yields up to 6.5%\nimprovement over standard vision encoders in success rate across single- and\nbi-manual manipulation tasks in both simulation and real-world settings.\nDespite these benefits, most geometry-grounded models require high\ncomputational cost, limiting their deployment in practical robotic systems. To\naddress this challenge, we propose eVGGT, an efficient geometry-aware encoder\ndistilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than\nVGGT, while preserving strong 3D reasoning capabilities. Code and pretrained\nmodels will be released to facilitate further research in geometry-aware\nrobotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing RGB-based imitation learning approaches typically employ traditional\nvision encoders such as ResNet or ViT, which lack explicit 3D reasoning\ncapabilities. Recent geometry-grounded vision models, such as\nVGGT~\\cite{wang2025vggt}, provide robust spatial understanding and are\npromising candidates to address this limitation. This work investigates the\nintegration of geometry-aware visual representations into robotic manipulation.\nOur results suggest that incorporating the geometry-aware vision encoder into\nimitation learning frameworks, including ACT and DP, yields up to 6.5%\nimprovement over standard vision encoders in success rate across single- and\nbi-manual manipulation tasks in both simulation and real-world settings.\nDespite these benefits, most geometry-grounded models require high\ncomputational cost, limiting their deployment in practical robotic systems. To\naddress this challenge, we propose eVGGT, an efficient geometry-aware encoder\ndistilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than\nVGGT, while preserving strong 3D reasoning capabilities. Code and pretrained\nmodels will be released to facilitate further research in geometry-aware\nrobotics."
                },
                "authors": [
                    {
                        "name": "An Dinh Vuong"
                    },
                    {
                        "name": "Minh Nhat Vu"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "arxiv_comment": "9 figures, 7 tables. Project page: https://evggt.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16781v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16781v3",
                "updated": "2025-09-19T11:13:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    13,
                    18,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-24T02:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    16,
                    37,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Robustness of LLMs in Question Answering on Multilingual\n  Noisy OCR Data"
                },
                "summary": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nModels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors - imperfect extraction of\ntext, including character insertion, deletion, and substitution can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we conduct a comprehensive analysis of how OCR-induced noise affects the\nperformance of Multilingual QA Systems. To support this analysis, we introduce\na multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs\nacross three languages, English, French, and German. The dataset is curated\nfrom OCR-ed historical documents, which include different levels and types of\nOCR noise. We then evaluate how different state-of-the-art Large Language\nModels (LLMs) perform under different error conditions, focusing on three major\nOCR error types. Our findings show that QA systems are highly prone to\nOCR-induced errors and perform poorly on noisy OCR text. By comparing model\nperformance on clean versus noisy texts, we provide insights into the\nlimitations of current approaches and emphasize the need for more\nnoise-resilient QA systems in historical digitization contexts."
                },
                "authors": [
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Antoine Doucet"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accepted at CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16781v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16781v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15867v1",
                "updated": "2025-09-19T11:07:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    7,
                    25,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T11:07:25Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    7,
                    25,
                    4,
                    262,
                    0
                ],
                "title": "Understanding the Role of Large Language Models in Competitive\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Role of Large Language Models in Competitive\n  Programming"
                },
                "summary": "This paper investigates how large language models (LLMs) are reshaping\ncompetitive programming. The field functions as an intellectual contest within\ncomputer science education and is marked by rapid iteration, real-time\nfeedback, transparent solutions, and strict integrity norms. Prior work has\nevaluated LLMs performance on contest problems, but little is known about how\nhuman stakeholders -- contestants, problem setters, coaches, and platform\nstewards -- are adapting their workflows and contest norms under LLMs-induced\nshifts. At the same time, rising AI-assisted misuse and inconsistent governance\nexpose urgent gaps in sustaining fairness and credibility. Drawing on 37\ninterviews spanning all four roles and a global survey of 207 contestants, we\ncontribute: (i) an empirical account of evolving workflows, (ii) an analysis of\ncontested fairness norms, and (iii) a chess-inspired governance approach with\nactionable measures -- real-time LLMs checks in online contests, peer\nco-monitoring and reporting, and cross-validation against offline performance\n-- to curb LLMs-assisted misuse while preserving fairness, transparency, and\ncredibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates how large language models (LLMs) are reshaping\ncompetitive programming. The field functions as an intellectual contest within\ncomputer science education and is marked by rapid iteration, real-time\nfeedback, transparent solutions, and strict integrity norms. Prior work has\nevaluated LLMs performance on contest problems, but little is known about how\nhuman stakeholders -- contestants, problem setters, coaches, and platform\nstewards -- are adapting their workflows and contest norms under LLMs-induced\nshifts. At the same time, rising AI-assisted misuse and inconsistent governance\nexpose urgent gaps in sustaining fairness and credibility. Drawing on 37\ninterviews spanning all four roles and a global survey of 207 contestants, we\ncontribute: (i) an empirical account of evolving workflows, (ii) an analysis of\ncontested fairness norms, and (iii) a chess-inspired governance approach with\nactionable measures -- real-time LLMs checks in online contests, peer\nco-monitoring and reporting, and cross-validation against offline performance\n-- to curb LLMs-assisted misuse while preserving fairness, transparency, and\ncredibility."
                },
                "authors": [
                    {
                        "name": "Dongyijie Primo Pan"
                    },
                    {
                        "name": "Ji Zhu"
                    },
                    {
                        "name": "Lan Luo"
                    },
                    {
                        "name": "Zhiqi Gao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15538v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15538v3",
                "updated": "2025-09-19T11:06:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    6,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-18T15:13:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    13,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework"
                },
                "summary": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nWithin the context of large language models (LLMs) for natural language\nprocessing (NLP), current automated neuron-level feature description methods\nface two key challenges: limited robustness and the assumption that each neuron\nencodes a single concept (monosemanticity), despite increasing evidence of\npolysemanticity. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework specifically\ndesigned to capture the complexity of features in LLMs. Unlike approaches that\nassign a single description per neuron, common in many automated\ninterpretability methods in NLP, PRISM produces more nuanced descriptions that\naccount for both monosemantic and polysemantic behavior. We apply PRISM to LLMs\nand, through extensive benchmarking against existing methods, demonstrate that\nour approach produces more accurate and faithful feature descriptions,\nimproving both overall description quality (via a description score) and the\nability to capture distinct concepts when polysemanticity is present (via a\npolysemanticity score).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nWithin the context of large language models (LLMs) for natural language\nprocessing (NLP), current automated neuron-level feature description methods\nface two key challenges: limited robustness and the assumption that each neuron\nencodes a single concept (monosemanticity), despite increasing evidence of\npolysemanticity. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework specifically\ndesigned to capture the complexity of features in LLMs. Unlike approaches that\nassign a single description per neuron, common in many automated\ninterpretability methods in NLP, PRISM produces more nuanced descriptions that\naccount for both monosemantic and polysemantic behavior. We apply PRISM to LLMs\nand, through extensive benchmarking against existing methods, demonstrate that\nour approach produces more accurate and faithful feature descriptions,\nimproving both overall description quality (via a description score) and the\nability to capture distinct concepts when polysemanticity is present (via a\npolysemanticity score)."
                },
                "authors": [
                    {
                        "name": "Laura Kopf"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Kirill Bykov"
                    },
                    {
                        "name": "Philine Lou Bommer"
                    },
                    {
                        "name": "Anna Hedström"
                    },
                    {
                        "name": "Marina M. -C. Höhne"
                    },
                    {
                        "name": "Oliver Eberle"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Eberle"
                },
                "author": "Oliver Eberle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15538v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15538v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08164v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08164v6",
                "updated": "2025-09-19T11:01:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    1,
                    7,
                    4,
                    262,
                    0
                ],
                "published": "2023-10-12T09:36:03Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    9,
                    36,
                    3,
                    3,
                    285,
                    0
                ],
                "title": "Interpreting Learned Feedback Patterns in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Learned Feedback Patterns in Large Language Models"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is widely used to train\nlarge language models (LLMs). However, it is unclear whether LLMs accurately\nlearn the underlying preferences in human feedback data. We coin the term\n\\textit{Learned Feedback Pattern} (LFP) for patterns in an LLM's activations\nlearned during RLHF that improve its performance on the fine-tuning task. We\nhypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback\nexhibit consistent activation patterns for outputs that would have received\nsimilar feedback during RLHF. To test this, we train probes to estimate the\nfeedback signal implicit in the activations of a fine-tuned LLM. We then\ncompare these estimates to the true feedback, measuring how accurate the LFPs\nare to the fine-tuning feedback. Our probes are trained on a condensed, sparse\nand interpretable representation of LLM activations, making it easier to\ncorrelate features of the input with our probe's predictions. We validate our\nprobes by comparing the neural features they correlate with positive feedback\ninputs against the features GPT-4 describes and classifies as related to LFPs.\nUnderstanding LFPs can help minimize discrepancies between LLM behavior and\ntraining objectives, which is essential for the safety of LLMs."
                },
                "authors": [
                    {
                        "name": "Luke Marks"
                    },
                    {
                        "name": "Amir Abdullah"
                    },
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Rauno Arike"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "19 pages, 8 figures. Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08164v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08164v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24535v2",
                "updated": "2025-09-19T10:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    58,
                    16,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-30T12:41:19Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    41,
                    19,
                    4,
                    150,
                    0
                ],
                "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Steering: Unified Multi-Attribute Control for Language\n  Models"
                },
                "summary": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors."
                },
                "authors": [
                    {
                        "name": "Narmeen Oozeer"
                    },
                    {
                        "name": "Luke Marks"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Amirali Abdullah"
                    }
                ],
                "author_detail": {
                    "name": "Amirali Abdullah"
                },
                "author": "Amirali Abdullah",
                "arxiv_comment": "Accepted to Findings of EMNLP, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09380v2",
                "updated": "2025-09-19T10:50:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    50,
                    53,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-14T13:33:38Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    33,
                    38,
                    2,
                    134,
                    0
                ],
                "title": "Examining Deployment and Refinement of the VIOLA-AI Intracranial\n  Hemorrhage Model Using an Interactive NeoMedSys Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Deployment and Refinement of the VIOLA-AI Intracranial\n  Hemorrhage Model Using an Interactive NeoMedSys Platform"
                },
                "summary": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A prospective pragmatic\ninvestigation was deployed using clinical cases of patients presenting to the\nlargest Emergency Department in Norway (site-1) with suspected traumatic brain\ninjury (TBI) or patients with suspected stroke (site-2). We assessed ICH\nclassification performance as VIOLA-AI encountered new data and underwent\npre-planned model retraining. Performance metrics included sensitivity,\nspecificity, accuracy, and the area under the receiver operating characteristic\ncurve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: There are many challenges and opportunities in the clinical\ndeployment of AI tools in radiology. The current study describes a radiology\nsoftware platform called NeoMedSys that can enable efficient deployment and\nrefinements of AI models. We evaluated the feasibility and effectiveness of\nrunning NeoMedSys for three months in real-world clinical settings and focused\non improvement performance of an in-house developed AI model (VIOLA-AI)\ndesigned for intracranial hemorrhage (ICH) detection.\n  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI\nmodels with a web-based medical image viewer, annotation system, and\nhospital-wide radiology information systems. A prospective pragmatic\ninvestigation was deployed using clinical cases of patients presenting to the\nlargest Emergency Department in Norway (site-1) with suspected traumatic brain\ninjury (TBI) or patients with suspected stroke (site-2). We assessed ICH\nclassification performance as VIOLA-AI encountered new data and underwent\npre-planned model retraining. Performance metrics included sensitivity,\nspecificity, accuracy, and the area under the receiver operating characteristic\ncurve (AUC).\n  Results: NeoMedSys facilitated iterative improvements in the AI model,\nsignificantly enhancing its diagnostic accuracy. Automated bleed detection and\nsegmentation were reviewed in near real-time to facilitate re-training\nVIOLA-AI. The iterative refinement process yielded a marked improvement in\nclassification sensitivity, rising to 90.3% (from 79.2%), and specificity that\nreached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire\nsample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).\nModel refinement stages were associated with notable gains, highlighting the\nvalue of real-time radiologist feedback."
                },
                "authors": [
                    {
                        "name": "Qinghui Liu"
                    },
                    {
                        "name": "Jon E. Nesvold"
                    },
                    {
                        "name": "Hanna Raaum"
                    },
                    {
                        "name": "Elakkyen Murugesu"
                    },
                    {
                        "name": "Martin Røvang"
                    },
                    {
                        "name": "Bradley J Maclntosh"
                    },
                    {
                        "name": "Atle Bjørnerud"
                    },
                    {
                        "name": "Karoline Skogen"
                    }
                ],
                "author_detail": {
                    "name": "Karoline Skogen"
                },
                "author": "Karoline Skogen",
                "arxiv_comment": "21 pages, 11 figures, on submission to BMC Methods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14049v2",
                "updated": "2025-09-19T10:37:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    37,
                    7,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-17T14:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    53,
                    56,
                    2,
                    260,
                    0
                ],
                "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on\n  Resource-Constrained Devices"
                },
                "summary": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jordi Grau-Haro"
                    },
                    {
                        "name": "Ruben Ribes-Serrano"
                    },
                    {
                        "name": "Javier Naranjo-Alcazar"
                    },
                    {
                        "name": "Marta Garcia-Ballesteros"
                    },
                    {
                        "name": "Pedro Zuccarello"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Zuccarello"
                },
                "author": "Pedro Zuccarello",
                "arxiv_comment": "Accepted at Computing Conference 2026, London, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12812v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12812v3",
                "updated": "2025-09-19T10:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    23,
                    46,
                    4,
                    262,
                    0
                ],
                "published": "2024-09-19T14:36:00Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    14,
                    36,
                    0,
                    3,
                    263,
                    0
                ],
                "title": "Towards Interactive and Learnable Cooperative Driving Automation: a\n  Large Language Model-Driven Decision-Making Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interactive and Learnable Cooperative Driving Automation: a\n  Large Language Model-Driven Decision-Making Framework"
                },
                "summary": "At present, Connected Autonomous Vehicles (CAVs) have begun to open road\ntesting around the world, but their safety and efficiency performance in\ncomplex scenarios is still not satisfactory. Cooperative driving leverages the\nconnectivity ability of CAVs to achieve synergies greater than the sum of their\nparts, making it a promising approach to improving CAV performance in complex\nscenarios. However, the lack of interaction and continuous learning ability\nlimits current cooperative driving to single-scenario applications and specific\nCooperative Driving Automation (CDA). To address these challenges, this paper\nproposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative\ndriving framework, to achieve all-scenario and all-CDA. First, since Large\nLanguage Models(LLMs) are not adept at handling mathematical calculations, an\nenvironment module is introduced to update vehicle positions based on semantic\ndecisions, thus avoiding potential errors from direct LLM control of vehicle\npositions. Second, based on the four levels of CDA defined by the SAE J3216\nstandard, we propose a Chain-of-Thought (COT) based reasoning module that\nincludes state perception, intent sharing, negotiation, and decision-making,\nenhancing the stability of LLMs in multi-step reasoning tasks. Centralized\nconflict resolution is then managed through a conflict coordinator in the\nreasoning process. Finally, by introducing a memory module and employing\nretrieval-augmented generation, CAVs are endowed with the ability to learn from\ntheir past experiences. We validate the proposed CoDrivingLLM through ablation\nexperiments on the negotiation module, reasoning with different shots\nexperience, and comparison with other cooperative driving methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At present, Connected Autonomous Vehicles (CAVs) have begun to open road\ntesting around the world, but their safety and efficiency performance in\ncomplex scenarios is still not satisfactory. Cooperative driving leverages the\nconnectivity ability of CAVs to achieve synergies greater than the sum of their\nparts, making it a promising approach to improving CAV performance in complex\nscenarios. However, the lack of interaction and continuous learning ability\nlimits current cooperative driving to single-scenario applications and specific\nCooperative Driving Automation (CDA). To address these challenges, this paper\nproposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative\ndriving framework, to achieve all-scenario and all-CDA. First, since Large\nLanguage Models(LLMs) are not adept at handling mathematical calculations, an\nenvironment module is introduced to update vehicle positions based on semantic\ndecisions, thus avoiding potential errors from direct LLM control of vehicle\npositions. Second, based on the four levels of CDA defined by the SAE J3216\nstandard, we propose a Chain-of-Thought (COT) based reasoning module that\nincludes state perception, intent sharing, negotiation, and decision-making,\nenhancing the stability of LLMs in multi-step reasoning tasks. Centralized\nconflict resolution is then managed through a conflict coordinator in the\nreasoning process. Finally, by introducing a memory module and employing\nretrieval-augmented generation, CAVs are endowed with the ability to learn from\ntheir past experiences. We validate the proposed CoDrivingLLM through ablation\nexperiments on the negotiation module, reasoning with different shots\nexperience, and comparison with other cooperative driving methods."
                },
                "authors": [
                    {
                        "name": "Shiyu Fang"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Yiming Cui"
                    },
                    {
                        "name": "Chen Lv"
                    },
                    {
                        "name": "Peng Hang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12812v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12812v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15839v1",
                "updated": "2025-09-19T10:18:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    18,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T10:18:48Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    18,
                    48,
                    4,
                    262,
                    0
                ],
                "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning\n  on Chinese Multi-Subject Physics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning\n  on Chinese Multi-Subject Physics Problems"
                },
                "summary": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics."
                },
                "authors": [
                    {
                        "name": "Zhongze Luo"
                    },
                    {
                        "name": "Zhenshuai Yin"
                    },
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Jionghao Zhu"
                    },
                    {
                        "name": "Xiaoying Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoying Tang"
                },
                "author": "Xiaoying Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15836v1",
                "updated": "2025-09-19T10:15:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    15,
                    55,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T10:15:55Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    15,
                    55,
                    4,
                    262,
                    0
                ],
                "title": "Relational Dissonance in Human-AI Interactions: The Case of Knowledge\n  Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Dissonance in Human-AI Interactions: The Case of Knowledge\n  Work"
                },
                "summary": "When AI systems allow human-like communication, they elicit increasingly\ncomplex relational responses. Knowledge workers face a particular challenge:\nThey approach these systems as tools while interacting with them in ways that\nresemble human social interaction. To understand the relational contexts that\narise when humans engage with anthropomorphic conversational agents, we need to\nexpand existing human-computer interaction frameworks. Through three workshops\nwith qualitative researchers, we found that the fundamental ontological and\nrelational ambiguities inherent in anthropomorphic conversational agents make\nit difficult for individuals to maintain consistent relational stances toward\nthem. Our findings indicate that people's articulated positioning toward such\nagents often differs from the relational dynamics that occur during\ninteractions. We propose the concept of relational dissonance to help\nresearchers, designers, and policymakers recognize the resulting tensions in\nthe development, deployment, and governance of anthropomorphic conversational\nagents and address the need for relational transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AI systems allow human-like communication, they elicit increasingly\ncomplex relational responses. Knowledge workers face a particular challenge:\nThey approach these systems as tools while interacting with them in ways that\nresemble human social interaction. To understand the relational contexts that\narise when humans engage with anthropomorphic conversational agents, we need to\nexpand existing human-computer interaction frameworks. Through three workshops\nwith qualitative researchers, we found that the fundamental ontological and\nrelational ambiguities inherent in anthropomorphic conversational agents make\nit difficult for individuals to maintain consistent relational stances toward\nthem. Our findings indicate that people's articulated positioning toward such\nagents often differs from the relational dynamics that occur during\ninteractions. We propose the concept of relational dissonance to help\nresearchers, designers, and policymakers recognize the resulting tensions in\nthe development, deployment, and governance of anthropomorphic conversational\nagents and address the need for relational transparency."
                },
                "authors": [
                    {
                        "name": "Emrecan Gulay"
                    },
                    {
                        "name": "Eleonora Picco"
                    },
                    {
                        "name": "Enrico Glerean"
                    },
                    {
                        "name": "Corinna Coupette"
                    }
                ],
                "author_detail": {
                    "name": "Corinna Coupette"
                },
                "author": "Corinna Coupette",
                "arxiv_comment": "27 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15239v2",
                "updated": "2025-09-19T10:08:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    8,
                    52,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-21T04:54:05Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    4,
                    54,
                    5,
                    3,
                    233,
                    0
                ],
                "title": "WangchanThaiInstruct: An instruction-following Dataset for\n  Culture-Aware, Multitask, and Multi-domain Evaluation in Thai",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WangchanThaiInstruct: An instruction-following Dataset for\n  Culture-Aware, Multitask, and Multi-domain Evaluation in Thai"
                },
                "summary": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings."
                },
                "authors": [
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Pume Tuchinda"
                    },
                    {
                        "name": "Lalita Lowphansirikul"
                    },
                    {
                        "name": "Surapon Nonesung"
                    },
                    {
                        "name": "Panuthep Tasawong"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Can Udomcharoenchaikit"
                    },
                    {
                        "name": "Sarana Nutanong"
                    }
                ],
                "author_detail": {
                    "name": "Sarana Nutanong"
                },
                "author": "Sarana Nutanong",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main). Model and Dataset:\n  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09996v2",
                "updated": "2025-09-19T10:07:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    7,
                    39,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring"
                },
                "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Yehan Yang"
                    },
                    {
                        "name": "Xueyao Zhang"
                    },
                    {
                        "name": "Juan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cao"
                },
                "author": "Juan Cao",
                "arxiv_comment": "NeurIPS 2025 Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15830v1",
                "updated": "2025-09-19T10:00:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    0,
                    45,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T10:00:45Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    10,
                    0,
                    45,
                    4,
                    262,
                    0
                ],
                "title": "Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for\n  Energy-aware and Timely Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for\n  Energy-aware and Timely Operations"
                },
                "summary": "Drones have recently emerged as a faster, safer, and cost-efficient way for\nlast-mile deliveries of parcels, particularly for urgent medical deliveries\nhighlighted during the pandemic. This paper addresses a new challenge of\nmulti-parcel delivery with a swarm of energy-aware drones, accounting for\ntime-sensitive customer requirements. Each drone plans an optimal multi-parcel\nroute within its battery-restricted flight range to minimize delivery delays\nand reduce energy consumption. The problem is tackled by decomposing it into\nthree sub-problems: (1) optimizing depot locations and service areas using\nK-means clustering; (2) determining the optimal flight range for drones through\nreinforcement learning; and (3) planning and selecting multi-parcel delivery\nroutes via a new optimized plan selection approach. To integrate these\nsolutions and enhance long-term efficiency, we propose a novel algorithm\nleveraging actor-critic-based multi-agent deep reinforcement learning.\nExtensive experimentation using realistic delivery datasets demonstrate an\nexceptional performance of the proposed algorithm. We provide new insights into\neconomic efficiency (minimize energy consumption), rapid operations (reduce\ndelivery delays and overall execution time), and strategic guidance on depot\ndeployment for practical logistics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drones have recently emerged as a faster, safer, and cost-efficient way for\nlast-mile deliveries of parcels, particularly for urgent medical deliveries\nhighlighted during the pandemic. This paper addresses a new challenge of\nmulti-parcel delivery with a swarm of energy-aware drones, accounting for\ntime-sensitive customer requirements. Each drone plans an optimal multi-parcel\nroute within its battery-restricted flight range to minimize delivery delays\nand reduce energy consumption. The problem is tackled by decomposing it into\nthree sub-problems: (1) optimizing depot locations and service areas using\nK-means clustering; (2) determining the optimal flight range for drones through\nreinforcement learning; and (3) planning and selecting multi-parcel delivery\nroutes via a new optimized plan selection approach. To integrate these\nsolutions and enhance long-term efficiency, we propose a novel algorithm\nleveraging actor-critic-based multi-agent deep reinforcement learning.\nExtensive experimentation using realistic delivery datasets demonstrate an\nexceptional performance of the proposed algorithm. We provide new insights into\neconomic efficiency (minimize energy consumption), rapid operations (reduce\ndelivery delays and overall execution time), and strategic guidance on depot\ndeployment for practical logistics applications."
                },
                "authors": [
                    {
                        "name": "Chuhao Qin"
                    },
                    {
                        "name": "Arun Narayanan"
                    },
                    {
                        "name": "Evangelos Pournaras"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Pournaras"
                },
                "author": "Evangelos Pournaras",
                "arxiv_comment": "12 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22914v2",
                "updated": "2025-09-19T09:57:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    9,
                    57,
                    39,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-28T22:32:31Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    31,
                    2,
                    148,
                    0
                ],
                "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning"
                },
                "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one."
                },
                "authors": [
                    {
                        "name": "Maksim Kolodiazhnyi"
                    },
                    {
                        "name": "Denis Tarasov"
                    },
                    {
                        "name": "Dmitrii Zhemchuzhnikov"
                    },
                    {
                        "name": "Alexander Nikulin"
                    },
                    {
                        "name": "Ilya Zisman"
                    },
                    {
                        "name": "Anna Vorontsova"
                    },
                    {
                        "name": "Anton Konushin"
                    },
                    {
                        "name": "Vladislav Kurenkov"
                    },
                    {
                        "name": "Danila Rukhovich"
                    }
                ],
                "author_detail": {
                    "name": "Danila Rukhovich"
                },
                "author": "Danila Rukhovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15826v1",
                "updated": "2025-09-19T09:57:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    9,
                    57,
                    33,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T09:57:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    9,
                    57,
                    33,
                    4,
                    262,
                    0
                ],
                "title": "Campus AI vs. Commercial AI: How Customizations Shape Trust and Usage of\n  LLM as-a-Service Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campus AI vs. Commercial AI: How Customizations Shape Trust and Usage of\n  LLM as-a-Service Chatbots"
                },
                "summary": "As the use of LLM chatbots by students and researchers becomes more\nprevalent, universities are pressed to develop AI strategies. One strategy that\nmany universities pursue is to customize pre-trained LLM as-a-service (LLMaaS).\nWhile most studies on LLMaaS chatbots prioritize technical adaptations, we\nfocus on psychological effects of user-salient customizations, such as\ninterface changes. We assume that such customizations influence users'\nperception of the system and are therefore important in guiding safe and\nappropriate use. In a field study, we examine how students and employees (N =\n526) at a German university perceive and use their institution's customized\nLLMaaS chatbot compared to ChatGPT. Participants using both systems (n = 116)\nreported greater trust, higher perceived privacy and less experienced\nhallucinations with their university's customized LLMaaS chatbot in contrast to\nChatGPT. We discuss theoretical implications for research on calibrated trust,\nand offer guidance on the design and deployment of LLMaaS chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of LLM chatbots by students and researchers becomes more\nprevalent, universities are pressed to develop AI strategies. One strategy that\nmany universities pursue is to customize pre-trained LLM as-a-service (LLMaaS).\nWhile most studies on LLMaaS chatbots prioritize technical adaptations, we\nfocus on psychological effects of user-salient customizations, such as\ninterface changes. We assume that such customizations influence users'\nperception of the system and are therefore important in guiding safe and\nappropriate use. In a field study, we examine how students and employees (N =\n526) at a German university perceive and use their institution's customized\nLLMaaS chatbot compared to ChatGPT. Participants using both systems (n = 116)\nreported greater trust, higher perceived privacy and less experienced\nhallucinations with their university's customized LLMaaS chatbot in contrast to\nChatGPT. We discuss theoretical implications for research on calibrated trust,\nand offer guidance on the design and deployment of LLMaaS chatbots."
                },
                "authors": [
                    {
                        "name": "Leon Hannig"
                    },
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Meltem Aksoy"
                    },
                    {
                        "name": "Steffen Becker"
                    },
                    {
                        "name": "Greta Ontrup"
                    }
                ],
                "author_detail": {
                    "name": "Greta Ontrup"
                },
                "arxiv_affiliation": "University of Duisburg-Essen, Germany",
                "author": "Greta Ontrup",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10490",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14880v2",
                "updated": "2025-09-19T09:56:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    9,
                    56,
                    48,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-18T11:51:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    51,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hype to Insight: Rethinking Large Language Model Integration in\n  Visual Speech Recognition"
                },
                "summary": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress."
                },
                "authors": [
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Naomi Harte"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Harte"
                },
                "author": "Naomi Harte",
                "arxiv_comment": "The authors have decided to withdraw the paper for further\n  development",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17662v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17662v5",
                "updated": "2025-09-19T09:45:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    9,
                    45,
                    59,
                    4,
                    262,
                    0
                ],
                "published": "2025-05-23T09:27:25Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    9,
                    27,
                    25,
                    4,
                    143,
                    0
                ],
                "title": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Versatile Time-Series Analysis with Tiny Transformers on\n  Embedded FPGAs"
                },
                "summary": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS)."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Lukas Johannes Haßler"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_doi": "10.1109/ISVLSI65124.2025.11130202",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ISVLSI65124.2025.11130202",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.17662v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17662v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 5 figures, 1 table, accepted by IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]