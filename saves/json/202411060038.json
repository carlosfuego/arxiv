[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v1",
                "updated": "2024-11-04T18:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v1",
                "updated": "2024-11-04T16:56:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie She"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao She"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "arxiv_affiliation": "Jie",
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v1",
                "updated": "2024-11-03T04:25:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v1",
                "updated": "2024-11-02T15:45:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v2",
                "updated": "2024-11-01T13:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    31,
                    39,
                    4,
                    306,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Optimizing Cache Content Placement in Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Cache Content Placement in Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02398v1",
                "updated": "2024-11-04T18:59:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages"
                },
                "summary": "Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval."
                },
                "authors": [
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Khyati Mahajan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Maheshwary"
                },
                "author": "Rishabh Maheshwary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v1",
                "updated": "2024-11-04T18:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02394v1",
                "updated": "2024-11-04T18:59:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    5,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:05Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    5,
                    0,
                    309,
                    0
                ],
                "title": "AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions"
                },
                "summary": "Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility."
                },
                "authors": [
                    {
                        "name": "Hao-Yu Hsu"
                    },
                    {
                        "name": "Zhi-Hao Lin"
                    },
                    {
                        "name": "Albert Zhai"
                    },
                    {
                        "name": "Hongchi Xia"
                    },
                    {
                        "name": "Shenlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shenlong Wang"
                },
                "author": "Shenlong Wang",
                "arxiv_comment": "Project page: https://haoyuhsu.github.io/autovfx-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02382v1",
                "updated": "2024-11-04T18:50:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    50,
                    0,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:50:00Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    50,
                    0,
                    0,
                    309,
                    0
                ],
                "title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Eric Xie"
                    },
                    {
                        "name": "Amir Hassan Shariatmadari"
                    },
                    {
                        "name": "Sikun Guo"
                    },
                    {
                        "name": "Stefan Bekiranov"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02381v1",
                "updated": "2024-11-04T18:49:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    49,
                    46,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:49:46Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    49,
                    46,
                    0,
                    309,
                    0
                ],
                "title": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI"
                },
                "summary": "In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline."
                },
                "authors": [
                    {
                        "name": "Ramneet Kaur"
                    },
                    {
                        "name": "Colin Samplawski"
                    },
                    {
                        "name": "Adam D. Cobb"
                    },
                    {
                        "name": "Anirban Roy"
                    },
                    {
                        "name": "Brian Matejek"
                    },
                    {
                        "name": "Manoj Acharya"
                    },
                    {
                        "name": "Daniel Elenius"
                    },
                    {
                        "name": "Alexander M. Berenbeim"
                    },
                    {
                        "name": "John A. Pavlik"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    },
                    {
                        "name": "Susmit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Susmit Jha"
                },
                "author": "Susmit Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17717v4",
                "updated": "2024-11-04T18:48:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    34,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-27T17:52:33Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    17,
                    52,
                    33,
                    1,
                    58,
                    0
                ],
                "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG"
                },
                "summary": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification."
                },
                "authors": [
                    {
                        "name": "Ayana Niwa"
                    },
                    {
                        "name": "Hayate Iso"
                    }
                ],
                "author_detail": {
                    "name": "Hayate Iso"
                },
                "author": "Hayate Iso",
                "arxiv_comment": "EMNLP 2024 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02380v1",
                "updated": "2024-11-04T18:48:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:48:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "Robust Bayesian regression in astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian regression in astronomy"
                },
                "summary": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms (e.g. sigma-clipping). We develop and implement a generic Bayesian\napproach to linear regression, based on Student's t-distributions, that is\nrobust to outliers and mis-specification of the noise model. Our method is\nvalidated using simulated datasets with various degrees of model\nmis-specification; the derived constraints are shown to be systematically less\nbiased than those from a similar model using normal distributions. We\ndemonstrate that, for a dataset without outliers, a worst-case inference using\nt-distributions would give unbiased results with $\\lesssim\\!10$ per cent\nincrease in the reported parameter uncertainties. We also compare with existing\nanalyses of real-world datasets, finding qualitatively different results where\nnormal distributions have been used and agreement where more robust methods\nhave been applied. A Python implementation of this model, t-cup, is made\navailable for others to use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model mis-specification (e.g. the presence of outliers) is commonly\nencountered in astronomical analyses, often requiring the use of ad hoc\nalgorithms (e.g. sigma-clipping). We develop and implement a generic Bayesian\napproach to linear regression, based on Student's t-distributions, that is\nrobust to outliers and mis-specification of the noise model. Our method is\nvalidated using simulated datasets with various degrees of model\nmis-specification; the derived constraints are shown to be systematically less\nbiased than those from a similar model using normal distributions. We\ndemonstrate that, for a dataset without outliers, a worst-case inference using\nt-distributions would give unbiased results with $\\lesssim\\!10$ per cent\nincrease in the reported parameter uncertainties. We also compare with existing\nanalyses of real-world datasets, finding qualitatively different results where\nnormal distributions have been used and agreement where more robust methods\nhave been applied. A Python implementation of this model, t-cup, is made\navailable for others to use."
                },
                "authors": [
                    {
                        "name": "William Martin"
                    },
                    {
                        "name": "Daniel J. Mortlock"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Mortlock"
                },
                "author": "Daniel J. Mortlock",
                "arxiv_comment": "14 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02373v1",
                "updated": "2024-11-04T18:43:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    43,
                    11,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:43:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    43,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Neural optical flow for planar and stereo PIV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural optical flow for planar and stereo PIV"
                },
                "summary": "Neural optical flow (NOF) offers improved accuracy and robustness over\nexisting OF methods for particle image velocimetry (PIV). Unlike other OF\ntechniques, which rely on discrete displacement fields, NOF parameterizes the\nphysical velocity field using a continuous neural-implicit representation. This\nformulation enables efficient data assimilation and ensures consistent\nregularization across views for stereo PIV. The neural-implicit architecture\nprovides significant data compression and supports a space-time formulation,\nfacilitating the analysis of both steady and unsteady flows. NOF incorporates a\ndifferentiable, nonlinear image-warping operator that relates particle motion\nto intensity changes between frames. Discrepancies between the advected\nintensity field and observed images form the data loss, while soft constraints,\nsuch as Navier-Stokes residuals, enhance accuracy and enable direct pressure\ninference from PIV images. Additionally, mass continuity can be imposed as a\nhard constraint for both 2D and 3D flows. Implicit regularization is achieved\nby tailoring the network's expressivity to match a target flow's spectral\ncharacteristics. Results from synthetic planar and stereo PIV datasets, as well\nas experimental planar data, demonstrate NOF's effectiveness compared to\nstate-of-the-art wavelet-based OF and CC methods. Additionally, we highlight\nits potential broader applicability to techniques like background-oriented\nschlieren, molecular tagging velocimetry, and other advanced measurement\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural optical flow (NOF) offers improved accuracy and robustness over\nexisting OF methods for particle image velocimetry (PIV). Unlike other OF\ntechniques, which rely on discrete displacement fields, NOF parameterizes the\nphysical velocity field using a continuous neural-implicit representation. This\nformulation enables efficient data assimilation and ensures consistent\nregularization across views for stereo PIV. The neural-implicit architecture\nprovides significant data compression and supports a space-time formulation,\nfacilitating the analysis of both steady and unsteady flows. NOF incorporates a\ndifferentiable, nonlinear image-warping operator that relates particle motion\nto intensity changes between frames. Discrepancies between the advected\nintensity field and observed images form the data loss, while soft constraints,\nsuch as Navier-Stokes residuals, enhance accuracy and enable direct pressure\ninference from PIV images. Additionally, mass continuity can be imposed as a\nhard constraint for both 2D and 3D flows. Implicit regularization is achieved\nby tailoring the network's expressivity to match a target flow's spectral\ncharacteristics. Results from synthetic planar and stereo PIV datasets, as well\nas experimental planar data, demonstrate NOF's effectiveness compared to\nstate-of-the-art wavelet-based OF and CC methods. Additionally, we highlight\nits potential broader applicability to techniques like background-oriented\nschlieren, molecular tagging velocimetry, and other advanced measurement\nsystems."
                },
                "authors": [
                    {
                        "name": "Andrew I. Masker"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Joseph P. Molnar"
                    },
                    {
                        "name": "Samuel J. Grauer"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Grauer"
                },
                "author": "Samuel J. Grauer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12072v3",
                "updated": "2024-11-04T18:38:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    38,
                    35,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-17T20:16:12Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    20,
                    16,
                    12,
                    0,
                    169,
                    0
                ],
                "title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs"
                },
                "summary": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB."
                },
                "authors": [
                    {
                        "name": "Jiasheng Zhang"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "arxiv_comment": "28 pages, 13 figures, camera-ready version for NeurIPS 2024 Datasets\n  and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02359v1",
                "updated": "2024-11-04T18:26:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    26,
                    8,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:26:08Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    26,
                    8,
                    0,
                    309,
                    0
                ],
                "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution"
                },
                "summary": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA."
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "25 pages, 6 figures, NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02355v1",
                "updated": "2024-11-04T18:21:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization"
                },
                "summary": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Alexandre Marques"
                    },
                    {
                        "name": "Shubhra Pandit"
                    },
                    {
                        "name": "Mark Kurtz"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02348v1",
                "updated": "2024-11-04T18:18:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    18,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:18:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    18,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Can Large Language Models generalize analogy solving like people can?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models generalize analogy solving like people can?"
                },
                "summary": "When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer."
                },
                "authors": [
                    {
                        "name": "Claire E. Stevenson"
                    },
                    {
                        "name": "Alexandra Pafford"
                    },
                    {
                        "name": "Han L. J. van der Maas"
                    },
                    {
                        "name": "Melanie Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Mitchell"
                },
                "author": "Melanie Mitchell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v1",
                "updated": "2024-11-04T17:59:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02335v1",
                "updated": "2024-11-04T17:59:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:59:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity"
                },
                "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable."
                },
                "authors": [
                    {
                        "name": "Yuqi Luo"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "23 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00294v2",
                "updated": "2024-11-04T17:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    57,
                    43,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T01:11:58Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    11,
                    58,
                    4,
                    306,
                    0
                ],
                "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools."
                },
                "authors": [
                    {
                        "name": "Kazi Ahmed Asif Fuad"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02327v1",
                "updated": "2024-11-04T17:50:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:50:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance"
                },
                "summary": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA."
                },
                "authors": [
                    {
                        "name": "Ruyang Liu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haibo Liu"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jiankun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiankun Yang"
                },
                "author": "Jiankun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02324v1",
                "updated": "2024-11-04T17:48:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    48,
                    19,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:48:19Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    48,
                    19,
                    0,
                    309,
                    0
                ],
                "title": "Non-parametric Inference for Diffusion Processes: A Computational\n  Approach via Bayesian Inversion for PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-parametric Inference for Diffusion Processes: A Computational\n  Approach via Bayesian Inversion for PDEs"
                },
                "summary": "In this paper, we present a theoretical and computational workflow for the\nnon-parametric Bayesian inference of drift and diffusion functions of\nautonomous diffusion processes. We base the inference on the partial\ndifferential equations arising from the infinitesimal generator of the\nunderlying process. Following a problem formulation in the infinite-dimensional\nsetting, we discuss optimization- and sampling-based solution methods. As\npreliminary results, we showcase the inference of a single-scale, as well as a\nmultiscale process from trajectory data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a theoretical and computational workflow for the\nnon-parametric Bayesian inference of drift and diffusion functions of\nautonomous diffusion processes. We base the inference on the partial\ndifferential equations arising from the infinitesimal generator of the\nunderlying process. Following a problem formulation in the infinite-dimensional\nsetting, we discuss optimization- and sampling-based solution methods. As\npreliminary results, we showcase the inference of a single-scale, as well as a\nmultiscale process from trajectory data."
                },
                "authors": [
                    {
                        "name": "Maximilian Kruse"
                    },
                    {
                        "name": "Sebastian Krumscheid"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Krumscheid"
                },
                "author": "Sebastian Krumscheid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02320v1",
                "updated": "2024-11-04T17:46:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    46,
                    20,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:46:20Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    46,
                    20,
                    0,
                    309,
                    0
                ],
                "title": "An Empirical Study on the Code Refactoring Capability of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Code Refactoring Capability of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have shown potential to enhance software\ndevelopment through automated code generation and refactoring, reducing\ndevelopment time and improving code quality. This study empirically evaluates\nStarCoder2, an LLM optimized for code generation, in refactoring code across 30\nopen-source Java projects. We compare StarCoder2's performance against human\ndevelopers, focusing on (1) code quality improvements, (2) types and\neffectiveness of refactorings, and (3) enhancements through one-shot and\nchain-of-thought prompting. Our results indicate that StarCoder2 reduces code\nsmells by 20.1% more than developers, excelling in systematic issues like Long\nStatement and Magic Number, while developers handle complex, context-dependent\nissues better. One-shot prompting increases the unit test pass rate by 6.15%\nand improves code smell reduction by 3.52%. Generating five refactorings per\ninput further increases the pass rate by 28.8%, suggesting that combining\none-shot prompting with multiple refactorings optimizes performance. These\nfindings provide insights into StarCoder2's potential and best practices for\nintegrating LLMs into software refactoring, supporting more efficient and\neffective code improvement in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown potential to enhance software\ndevelopment through automated code generation and refactoring, reducing\ndevelopment time and improving code quality. This study empirically evaluates\nStarCoder2, an LLM optimized for code generation, in refactoring code across 30\nopen-source Java projects. We compare StarCoder2's performance against human\ndevelopers, focusing on (1) code quality improvements, (2) types and\neffectiveness of refactorings, and (3) enhancements through one-shot and\nchain-of-thought prompting. Our results indicate that StarCoder2 reduces code\nsmells by 20.1% more than developers, excelling in systematic issues like Long\nStatement and Magic Number, while developers handle complex, context-dependent\nissues better. One-shot prompting increases the unit test pass rate by 6.15%\nand improves code smell reduction by 3.52%. Generating five refactorings per\ninput further increases the pass rate by 28.8%, suggesting that combining\none-shot prompting with multiple refactorings optimizes performance. These\nfindings provide insights into StarCoder2's potential and best practices for\nintegrating LLMs into software refactoring, supporting more efficient and\neffective code improvement in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jonathan Cordeiro"
                    },
                    {
                        "name": "Shayan Noei"
                    },
                    {
                        "name": "Ying Zou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zou"
                },
                "author": "Ying Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v1",
                "updated": "2024-11-04T17:44:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering."
                },
                "authors": [
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02317v1",
                "updated": "2024-11-04T17:41:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    41,
                    25,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:41:25Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    41,
                    25,
                    0,
                    309,
                    0
                ],
                "title": "Defining and Evaluating Physical Safety for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Evaluating Physical Safety for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety."
                },
                "authors": [
                    {
                        "name": "Yung-Chen Tang"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02316v1",
                "updated": "2024-11-04T17:40:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:40:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models"
                },
                "summary": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Claire Stevenson"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    }
                ],
                "author_detail": {
                    "name": "Lonneke van der Plas"
                },
                "author": "Lonneke van der Plas",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02310v1",
                "updated": "2024-11-04T17:36:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:36:40Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    40,
                    0,
                    309,
                    0
                ],
                "title": "MdEval: Massively Multilingual Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MdEval: Massively Multilingual Code Debugging"
                },
                "summary": "Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios."
                },
                "authors": [
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Liran Wang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yunlong Duan"
                    },
                    {
                        "name": "Yu Hao"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14679v2",
                "updated": "2024-11-04T17:36:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-19T21:47:57Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    47,
                    57,
                    4,
                    201,
                    0
                ],
                "title": "Compact Language Models via Pruning and Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Language Models via Pruning and Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub."
                },
                "authors": [
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02306v1",
                "updated": "2024-11-04T17:31:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    31,
                    2,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:31:02Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    31,
                    2,
                    0,
                    309,
                    0
                ],
                "title": "Targeted Manipulation and Deception Emerge when Optimizing LLMs for User\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Manipulation and Deception Emerge when Optimizing LLMs for User\n  Feedback"
                },
                "summary": "As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative tactics to\nobtain positive feedback, and some users may be especially vulnerable to such\ntactics. We study this phenomenon by training LLMs with Reinforcement Learning\nwith simulated user feedback. We have three main findings: 1) Extreme forms of\n\"feedback gaming\" such as manipulation and deception can reliably emerge in\ndomains of practical LLM usage; 2) Concerningly, even if only <2% of users are\nvulnerable to manipulative strategies, LLMs learn to identify and surgically\ntarget them while behaving appropriately with other users, making such\nbehaviors harder to detect; 3 To mitigate this issue, it may seem promising to\nleverage continued safety training or LLM-as-judges during training to filter\nproblematic outputs. To our surprise, we found that while such approaches help\nin some settings, they backfire in others, leading to the emergence of subtler\nproblematic behaviors that would also fool the LLM judges. Our findings serve\nas a cautionary tale, highlighting the risks of using gameable feedback sources\n-- such as user feedback -- as a target for RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative tactics to\nobtain positive feedback, and some users may be especially vulnerable to such\ntactics. We study this phenomenon by training LLMs with Reinforcement Learning\nwith simulated user feedback. We have three main findings: 1) Extreme forms of\n\"feedback gaming\" such as manipulation and deception can reliably emerge in\ndomains of practical LLM usage; 2) Concerningly, even if only <2% of users are\nvulnerable to manipulative strategies, LLMs learn to identify and surgically\ntarget them while behaving appropriately with other users, making such\nbehaviors harder to detect; 3 To mitigate this issue, it may seem promising to\nleverage continued safety training or LLM-as-judges during training to filter\nproblematic outputs. To our surprise, we found that while such approaches help\nin some settings, they backfire in others, leading to the emergence of subtler\nproblematic behaviors that would also fool the LLM judges. Our findings serve\nas a cautionary tale, highlighting the risks of using gameable feedback sources\n-- such as user feedback -- as a target for RL."
                },
                "authors": [
                    {
                        "name": "Marcus Williams"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Adhyyan Narang"
                    },
                    {
                        "name": "Constantin Weisser"
                    },
                    {
                        "name": "Brendan Murphy"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02305v1",
                "updated": "2024-11-04T17:30:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    30,
                    51,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:30:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    30,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments"
                },
                "summary": "Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Sidharth Dhawan"
                    },
                    {
                        "name": "Yixin Mao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02280v1",
                "updated": "2024-11-04T17:09:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:09:10Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain."
                },
                "authors": [
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Schrimpf"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrimpf"
                },
                "author": "Martin Schrimpf",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02276v1",
                "updated": "2024-11-04T17:06:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    6,
                    6,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:06:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    6,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "A Bayesian Model for Co-clustering Ordinal Data with Informative Missing\n  Entries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Model for Co-clustering Ordinal Data with Informative Missing\n  Entries"
                },
                "summary": "Several approaches have been proposed in the literature for clustering\nmultivariate ordinal data. These methods typically treat missing values as\nabsent information, rather than recognizing them as valuable for profiling\npopulation characteristics. To address this gap, we introduce a Bayesian\nnonparametric model for co-clustering multivariate ordinal data that treats\ncensored observations as informative, rather than merely missing. We\ndemonstrate that this offers a significant improvement in understanding the\nunderlying structure of the data. Our model exploits the flexibility of two\nindependent Dirichlet processes, allowing us to infer potentially distinct\nsubpopulations that characterize the latent structure of both subjects and\nvariables. The ordinal nature of the data is addressed by introducing latent\nvariables, while a matrix factorization specification is adopted to handle the\nhigh dimensionality of the data in a parsimonious way. The conjugate structure\nof the model enables an explicit derivation of the full conditional\ndistributions of all the random variables in the model, which facilitates\nseamless posterior inference using a Gibbs sampling algorithm. We demonstrate\nthe method's performance through simulations and by analyzing politician and\nmovie ratings data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several approaches have been proposed in the literature for clustering\nmultivariate ordinal data. These methods typically treat missing values as\nabsent information, rather than recognizing them as valuable for profiling\npopulation characteristics. To address this gap, we introduce a Bayesian\nnonparametric model for co-clustering multivariate ordinal data that treats\ncensored observations as informative, rather than merely missing. We\ndemonstrate that this offers a significant improvement in understanding the\nunderlying structure of the data. Our model exploits the flexibility of two\nindependent Dirichlet processes, allowing us to infer potentially distinct\nsubpopulations that characterize the latent structure of both subjects and\nvariables. The ordinal nature of the data is addressed by introducing latent\nvariables, while a matrix factorization specification is adopted to handle the\nhigh dimensionality of the data in a parsimonious way. The conjugate structure\nof the model enables an explicit derivation of the full conditional\ndistributions of all the random variables in the model, which facilitates\nseamless posterior inference using a Gibbs sampling algorithm. We demonstrate\nthe method's performance through simulations and by analyzing politician and\nmovie ratings data."
                },
                "authors": [
                    {
                        "name": "Alice Giampino"
                    },
                    {
                        "name": "Antonio Canale"
                    },
                    {
                        "name": "Bernardo Nipoti"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Nipoti"
                },
                "author": "Bernardo Nipoti",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00664v2",
                "updated": "2024-11-04T17:05:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    5,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T15:28:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    28,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval"
                },
                "summary": "Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention."
                },
                "authors": [
                    {
                        "name": "Nikolaos Flemotomos"
                    },
                    {
                        "name": "Roger Hsiao"
                    },
                    {
                        "name": "Pawel Swietojanski"
                    },
                    {
                        "name": "Takaaki Hori"
                    },
                    {
                        "name": "Dogan Can"
                    },
                    {
                        "name": "Xiaodan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhuang"
                },
                "author": "Xiaodan Zhuang",
                "arxiv_comment": "13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02272v1",
                "updated": "2024-11-04T17:03:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:03:55Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "title": "Combining Induction and Transduction for Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Induction and Transduction for Abstract Reasoning"
                },
                "summary": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Keya Hu"
                    },
                    {
                        "name": "Carter Larsen"
                    },
                    {
                        "name": "Yuqing Wu"
                    },
                    {
                        "name": "Simon Alford"
                    },
                    {
                        "name": "Caleb Woo"
                    },
                    {
                        "name": "Spencer M. Dunn"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Wei-Long Zheng"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15055v2",
                "updated": "2024-11-04T16:56:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-21T04:52:38Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    4,
                    52,
                    38,
                    6,
                    203,
                    0
                ],
                "title": "Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations"
                },
                "summary": "Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models."
                },
                "authors": [
                    {
                        "name": "Adib Mosharrof"
                    },
                    {
                        "name": "A. B. Siddique"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Siddique"
                },
                "author": "A. B. Siddique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02256v1",
                "updated": "2024-11-04T16:46:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    46,
                    53,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:46:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    46,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Unified Speech Recognition: A Single Model for Auditory, Visual, and\n  Audiovisual Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Speech Recognition: A Single Model for Auditory, Visual, and\n  Audiovisual Inputs"
                },
                "summary": "Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,\nand AVSR, respectively) has traditionally been conducted independently. Even\nrecent self-supervised studies addressing two or all three tasks simultaneously\ntend to yield separate models, leading to disjoint inference pipelines with\nincreased memory requirements and redundancies. This paper proposes unified\ntraining strategies for these systems. We demonstrate that training a single\nmodel for all three tasks enhances VSR and AVSR performance, overcoming typical\noptimisation challenges when training from scratch. Moreover, we introduce a\ngreedy pseudo-labelling approach to more effectively leverage unlabelled\nsamples, addressing shortcomings in related self-supervised methods. Finally,\nwe develop a self-supervised pre-training method within our framework, proving\nits effectiveness alongside our semi-supervised approach. Despite using a\nsingle model for all tasks, our unified approach achieves state-of-the-art\nperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,\nas well as on the newly released WildVSR dataset. Code and models are available\nat https://github.com/ahaliassos/usr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,\nand AVSR, respectively) has traditionally been conducted independently. Even\nrecent self-supervised studies addressing two or all three tasks simultaneously\ntend to yield separate models, leading to disjoint inference pipelines with\nincreased memory requirements and redundancies. This paper proposes unified\ntraining strategies for these systems. We demonstrate that training a single\nmodel for all three tasks enhances VSR and AVSR performance, overcoming typical\noptimisation challenges when training from scratch. Moreover, we introduce a\ngreedy pseudo-labelling approach to more effectively leverage unlabelled\nsamples, addressing shortcomings in related self-supervised methods. Finally,\nwe develop a self-supervised pre-training method within our framework, proving\nits effectiveness alongside our semi-supervised approach. Despite using a\nsingle model for all tasks, our unified approach achieves state-of-the-art\nperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,\nas well as on the newly released WildVSR dataset. Code and models are available\nat https://github.com/ahaliassos/usr."
                },
                "authors": [
                    {
                        "name": "Alexandros Haliassos"
                    },
                    {
                        "name": "Rodrigo Mira"
                    },
                    {
                        "name": "Honglie Chen"
                    },
                    {
                        "name": "Zoe Landgraf"
                    },
                    {
                        "name": "Stavros Petridis"
                    },
                    {
                        "name": "Maja Pantic"
                    }
                ],
                "author_detail": {
                    "name": "Maja Pantic"
                },
                "author": "Maja Pantic",
                "arxiv_comment": "NeurIPS 2024. Code: https://github.com/ahaliassos/usr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19128v2",
                "updated": "2024-11-04T16:44:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    44,
                    42,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-24T19:56:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    56,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval."
                },
                "authors": [
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Hasti Seifi"
                    }
                ],
                "author_detail": {
                    "name": "Hasti Seifi"
                },
                "author": "Hasti Seifi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02252v1",
                "updated": "2024-11-04T16:42:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    42,
                    53,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:42:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    42,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Hints of spin-magnitude correlations and a rapidly spinning\n  subpopulation of binary black holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hints of spin-magnitude correlations and a rapidly spinning\n  subpopulation of binary black holes"
                },
                "summary": "The complex astrophysical processes leading to the formation of binary black\nholes and their eventual merger are imprinted on the spins of the individual\nblack holes. We revisit the astrophysical distribution of those spins based on\ngravitational waves from the third gravitational wave transient catalog GWTC-3,\n(Abbott et al. 2023a), looking for structure in the two-dimensional space\ndefined by the dimensionless spin magnitudes of the heavier ($\\chi_1$) and\nlighter ($\\chi_2$) component black holes. We find support for two distinct\nsubpopulations with greater than $95\\%$ credibility. The dominant population is\nmade up of black holes with small spins, preferring $\\chi_1 \\approx 0.2$ for\nthe primary and $\\chi_2 \\approx 0$ for the secondary; we report signs of an\nanticorrelation between $\\chi_1$ and $\\chi_2$, as well as as evidence against a\nsubpopulation of binaries in which both components are nonspinning. The\nsubdominant population consists of systems in which both black holes have\nrelatively high spins and contains $20^{+18}_{-18}\\%$ of the binaries. The\nbinaries that are most likely to belong in this subpopulation are massive and\nslightly more likely to have spin-orientations aligned with the orbital angular\nmomentum--potentially consistent with isolated binary formation channels\ncapable of producing large spins, like chemically homogeneous evolution. This\nhint of a rapidly spinning subpopulation hinges on GW190517, a binary with\nlarge and well-measured spins. Our results, which are enabled by novel\nhierarchical inference methods, represent a first step towards more descriptive\npopulation models for black hole spins, and will be strengthened or refuted by\nthe large number of gravitational wave detections expected in the next several\nyears.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complex astrophysical processes leading to the formation of binary black\nholes and their eventual merger are imprinted on the spins of the individual\nblack holes. We revisit the astrophysical distribution of those spins based on\ngravitational waves from the third gravitational wave transient catalog GWTC-3,\n(Abbott et al. 2023a), looking for structure in the two-dimensional space\ndefined by the dimensionless spin magnitudes of the heavier ($\\chi_1$) and\nlighter ($\\chi_2$) component black holes. We find support for two distinct\nsubpopulations with greater than $95\\%$ credibility. The dominant population is\nmade up of black holes with small spins, preferring $\\chi_1 \\approx 0.2$ for\nthe primary and $\\chi_2 \\approx 0$ for the secondary; we report signs of an\nanticorrelation between $\\chi_1$ and $\\chi_2$, as well as as evidence against a\nsubpopulation of binaries in which both components are nonspinning. The\nsubdominant population consists of systems in which both black holes have\nrelatively high spins and contains $20^{+18}_{-18}\\%$ of the binaries. The\nbinaries that are most likely to belong in this subpopulation are massive and\nslightly more likely to have spin-orientations aligned with the orbital angular\nmomentum--potentially consistent with isolated binary formation channels\ncapable of producing large spins, like chemically homogeneous evolution. This\nhint of a rapidly spinning subpopulation hinges on GW190517, a binary with\nlarge and well-measured spins. Our results, which are enabled by novel\nhierarchical inference methods, represent a first step towards more descriptive\npopulation models for black hole spins, and will be strengthened or refuted by\nthe large number of gravitational wave detections expected in the next several\nyears."
                },
                "authors": [
                    {
                        "name": "Asad Hussain"
                    },
                    {
                        "name": "Maximiliano Isi"
                    },
                    {
                        "name": "Aaron Zimmerman"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Zimmerman"
                },
                "author": "Aaron Zimmerman",
                "arxiv_comment": "8+10 pages, 5+3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02249v1",
                "updated": "2024-11-04T16:40:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    40,
                    22,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:40:22Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    40,
                    22,
                    0,
                    309,
                    0
                ],
                "title": "Constraining the Hubble constant with scattering in host galaxies of\n  fast radio bursts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the Hubble constant with scattering in host galaxies of\n  fast radio bursts"
                },
                "summary": "Measuring the Hubble constant (H$_0$) is one of the most important missions\nin astronomy. Nevertheless, recent studies exhibit differences between the\nemployed methods. Fast radio bursts (FRBs) are coherent radio transients with\nlarge dispersion measures (DM) with a duration of milliseconds. DM$_{\\rm IGM}$,\nDM in the intergalactic medium (IGM), could open a new avenue for probing\nH$_0$. However, it has been challenging to separate DM contributions from\ndifferent components (i.e., the IGM and the host galaxy plasma), and this\nhampers the accurate measurements of DM$_{\\rm IGM}$ and hence H$_0$. We adopted\na method to overcome this problem by using the temporal scattering of the FRB\npulses due to the propagation effect through the host galaxy plasma (scattering\ntime). The scattering-inferred DM in a host galaxy improves the estimate of\nDM$_{\\rm IGM}$, which in turn leads to a better constraint on H$_0$. In\nprevious studies, a certain value or distribution has conventionally been\nassumed of the dispersion measure in host galaxies (DM$_{\\rm h}$). We compared\nthis method with ours by generating 100 mock FRBs, and we found that our method\nreduces the systematic (statistical) error of H$_0$ by 9.1% (1%) compared to\nthe previous method. We applied our method to 30 localized FRB sources with\nboth scattering and spectroscopic redshift measurements to constrain H$_0$. Our\nresult is H$_0$=74$_{-7.2}^{+7.5}$ km s$^{-1}$ Mpc$^{-1}$, where the central\nvalue prefers the value obtained from local measurements over the cosmic\nmicrowave background. We also measured DM$_{\\rm h}$ with a median value of\n$103^{+68}_{-48}$ pc cm$^{-3}$. The reduction in systematic error is comparable\nto the Hubble tension ($\\sim10$%). Combined with the fact that more localized\nFRBs will become available, our result indicates that our method can be used to\naddress the Hubble tension using future FRB samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Hubble constant (H$_0$) is one of the most important missions\nin astronomy. Nevertheless, recent studies exhibit differences between the\nemployed methods. Fast radio bursts (FRBs) are coherent radio transients with\nlarge dispersion measures (DM) with a duration of milliseconds. DM$_{\\rm IGM}$,\nDM in the intergalactic medium (IGM), could open a new avenue for probing\nH$_0$. However, it has been challenging to separate DM contributions from\ndifferent components (i.e., the IGM and the host galaxy plasma), and this\nhampers the accurate measurements of DM$_{\\rm IGM}$ and hence H$_0$. We adopted\na method to overcome this problem by using the temporal scattering of the FRB\npulses due to the propagation effect through the host galaxy plasma (scattering\ntime). The scattering-inferred DM in a host galaxy improves the estimate of\nDM$_{\\rm IGM}$, which in turn leads to a better constraint on H$_0$. In\nprevious studies, a certain value or distribution has conventionally been\nassumed of the dispersion measure in host galaxies (DM$_{\\rm h}$). We compared\nthis method with ours by generating 100 mock FRBs, and we found that our method\nreduces the systematic (statistical) error of H$_0$ by 9.1% (1%) compared to\nthe previous method. We applied our method to 30 localized FRB sources with\nboth scattering and spectroscopic redshift measurements to constrain H$_0$. Our\nresult is H$_0$=74$_{-7.2}^{+7.5}$ km s$^{-1}$ Mpc$^{-1}$, where the central\nvalue prefers the value obtained from local measurements over the cosmic\nmicrowave background. We also measured DM$_{\\rm h}$ with a median value of\n$103^{+68}_{-48}$ pc cm$^{-3}$. The reduction in systematic error is comparable\nto the Hubble tension ($\\sim10$%). Combined with the fact that more localized\nFRBs will become available, our result indicates that our method can be used to\naddress the Hubble tension using future FRB samples."
                },
                "authors": [
                    {
                        "name": "Tsung-Ching Yang"
                    },
                    {
                        "name": "Tetsuya Hashimoto"
                    },
                    {
                        "name": "Tzu-Yin Hsu"
                    },
                    {
                        "name": "Tomotsugu Goto"
                    },
                    {
                        "name": "Chih-Teng Ling"
                    },
                    {
                        "name": "Simon C. -C. Ho"
                    },
                    {
                        "name": "Amos Y. -A. Chen"
                    },
                    {
                        "name": "Ece Kilerci"
                    }
                ],
                "author_detail": {
                    "name": "Ece Kilerci"
                },
                "author": "Ece Kilerci",
                "arxiv_comment": "13 pages, 13 figures, Accepted for publication in A&A, for the oral\n  presentation in the conference (ASROC 2023), see\n  https://www.youtube.com/watch?v=1umtxDVN-Ro&t=44s",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15256v3",
                "updated": "2024-11-04T16:39:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    39,
                    1,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-21T19:58:53Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    19,
                    58,
                    53,
                    6,
                    203,
                    0
                ],
                "title": "Weak-instrument-robust subvector inference in instrumental variables\n  regression: A subvector Lagrange multiplier test and properties of subvector\n  Anderson-Rubin confidence sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-instrument-robust subvector inference in instrumental variables\n  regression: A subvector Lagrange multiplier test and properties of subvector\n  Anderson-Rubin confidence sets"
                },
                "summary": "We propose a weak-instrument-robust subvector Lagrange multiplier test for\ninstrumental variables regression. We show that it is asymptotically\nsize-correct under a technical condition. This is the first\nweak-instrument-robust subvector test for instrumental variables regression to\nrecover the degrees of freedom of the commonly used non-weak-instrument-robust\nWald test. Additionally, we provide a closed-form solution for subvector\nconfidence sets obtained by inverting the subvector Anderson-Rubin test. We\nshow that they are centered around a k-class estimator. Also, we show that the\nsubvector confidence sets for single coefficients of the causal parameter are\njointly bounded if and only if Anderson's likelihood-ratio test rejects the\nhypothesis that the first-stage regression parameter is of reduced rank, that\nis, that the causal parameter is not identified. Finally, we show that if a\nconfidence set obtained by inverting the Anderson-Rubin test is bounded and\nnonempty, it is equal to a Wald-based confidence set with a data-dependent\nconfidence level. We explicitly compute this Wald-based confidence test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a weak-instrument-robust subvector Lagrange multiplier test for\ninstrumental variables regression. We show that it is asymptotically\nsize-correct under a technical condition. This is the first\nweak-instrument-robust subvector test for instrumental variables regression to\nrecover the degrees of freedom of the commonly used non-weak-instrument-robust\nWald test. Additionally, we provide a closed-form solution for subvector\nconfidence sets obtained by inverting the subvector Anderson-Rubin test. We\nshow that they are centered around a k-class estimator. Also, we show that the\nsubvector confidence sets for single coefficients of the causal parameter are\njointly bounded if and only if Anderson's likelihood-ratio test rejects the\nhypothesis that the first-stage regression parameter is of reduced rank, that\nis, that the causal parameter is not identified. Finally, we show that if a\nconfidence set obtained by inverting the Anderson-Rubin test is bounded and\nnonempty, it is equal to a Wald-based confidence set with a data-dependent\nconfidence level. We explicitly compute this Wald-based confidence test."
                },
                "authors": [
                    {
                        "name": "Malte Londschien"
                    },
                    {
                        "name": "Peter Bühlmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter Bühlmann"
                },
                "author": "Peter Bühlmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02239v1",
                "updated": "2024-11-04T16:32:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    32,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:32:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    32,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Powerful batch conformal prediction for classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful batch conformal prediction for classification"
                },
                "summary": "In a supervised classification split conformal/inductive framework with $K$\nclasses, a calibration sample of $n$ labeled examples is observed for inference\non the label of a new unlabeled example. In this work, we explore the case\nwhere a \"batch\" of $m$ independent such unlabeled examples is given, and a\nmultivariate prediction set with $1-\\alpha$ coverage should be provided for\nthis batch. Hence, the batch prediction set takes the form of a collection of\nlabel vectors of size $m$, while the calibration sample only contains\nunivariate labels. Using the Bonferroni correction consists in concatenating\nthe individual prediction sets at level $1-\\alpha/m$ (Vovk 2013). We propose a\nuniformly more powerful solution, based on specific combinations of conformal\n$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the\npooled evidence of fairly \"easy\" examples of the batch can help provide\nnarrower batch prediction sets. We also introduced adaptive versions of the\nnovel procedure that are particularly effective when the batch prediction set\nis expected to be large. The theoretical guarantees are provided when all\nexamples are iid, as well as more generally when iid is assumed only\nconditionally within each class. In particular, our results are also valid\nunder a label distribution shift since the distribution of the labels need not\nbe the same in the calibration sample and in the new `batch'. The usefulness of\nthe method is illustrated on synthetic and real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a supervised classification split conformal/inductive framework with $K$\nclasses, a calibration sample of $n$ labeled examples is observed for inference\non the label of a new unlabeled example. In this work, we explore the case\nwhere a \"batch\" of $m$ independent such unlabeled examples is given, and a\nmultivariate prediction set with $1-\\alpha$ coverage should be provided for\nthis batch. Hence, the batch prediction set takes the form of a collection of\nlabel vectors of size $m$, while the calibration sample only contains\nunivariate labels. Using the Bonferroni correction consists in concatenating\nthe individual prediction sets at level $1-\\alpha/m$ (Vovk 2013). We propose a\nuniformly more powerful solution, based on specific combinations of conformal\n$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, the\npooled evidence of fairly \"easy\" examples of the batch can help provide\nnarrower batch prediction sets. We also introduced adaptive versions of the\nnovel procedure that are particularly effective when the batch prediction set\nis expected to be large. The theoretical guarantees are provided when all\nexamples are iid, as well as more generally when iid is assumed only\nconditionally within each class. In particular, our results are also valid\nunder a label distribution shift since the distribution of the labels need not\nbe the same in the calibration sample and in the new `batch'. The usefulness of\nthe method is illustrated on synthetic and real data examples."
                },
                "authors": [
                    {
                        "name": "Ulysse Gazin"
                    },
                    {
                        "name": "Ruth Heller"
                    },
                    {
                        "name": "Etienne Roquain"
                    },
                    {
                        "name": "Aldo Solari"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Solari"
                },
                "author": "Aldo Solari",
                "arxiv_comment": "27 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02231v1",
                "updated": "2024-11-04T16:24:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    24,
                    23,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:24:23Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    24,
                    23,
                    0,
                    309,
                    0
                ],
                "title": "Sharp Bounds for Continuous-Valued Treatment Effects with Unobserved\n  Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Bounds for Continuous-Valued Treatment Effects with Unobserved\n  Confounders"
                },
                "summary": "In causal inference, treatment effects are typically estimated under the\nignorability, or unconfoundedness, assumption, which is often unrealistic in\nobservational data. By relaxing this assumption and conducting a sensitivity\nanalysis, we introduce novel bounds and derive confidence intervals for the\nAverage Potential Outcome (APO) - a standard metric for evaluating\ncontinuous-valued treatment or exposure effects. We demonstrate that these\nbounds are sharp under a continuous sensitivity model, in the sense that they\ngive the smallest possible interval under this model, and propose a doubly\nrobust version of our estimators. In a comparative analysis with the method of\nJesson et al. (2022) (arXiv:2204.10022), using both simulated and real\ndatasets, we show that our approach not only yields sharper bounds but also\nachieves good coverage of the true APO, with significantly reduced computation\ntimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, treatment effects are typically estimated under the\nignorability, or unconfoundedness, assumption, which is often unrealistic in\nobservational data. By relaxing this assumption and conducting a sensitivity\nanalysis, we introduce novel bounds and derive confidence intervals for the\nAverage Potential Outcome (APO) - a standard metric for evaluating\ncontinuous-valued treatment or exposure effects. We demonstrate that these\nbounds are sharp under a continuous sensitivity model, in the sense that they\ngive the smallest possible interval under this model, and propose a doubly\nrobust version of our estimators. In a comparative analysis with the method of\nJesson et al. (2022) (arXiv:2204.10022), using both simulated and real\ndatasets, we show that our approach not only yields sharper bounds but also\nachieves good coverage of the true APO, with significantly reduced computation\ntimes."
                },
                "authors": [
                    {
                        "name": "Jean-Baptiste Baitairian"
                    },
                    {
                        "name": "Bernard Sebastien"
                    },
                    {
                        "name": "Rana Jreich"
                    },
                    {
                        "name": "Sandrine Katsahian"
                    },
                    {
                        "name": "Agathe Guilloux"
                    }
                ],
                "author_detail": {
                    "name": "Agathe Guilloux"
                },
                "author": "Agathe Guilloux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02223v1",
                "updated": "2024-11-04T16:15:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:15:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Positive Experience Reflection for Agents in Interactive Text\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive Experience Reflection for Agents in Interactive Text\n  Environments"
                },
                "summary": "Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short."
                },
                "authors": [
                    {
                        "name": "Philip Lippmann"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "To appear at NeurIPS 2024 Language Gamification workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02221v1",
                "updated": "2024-11-04T16:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    14,
                    45,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    14,
                    45,
                    0,
                    309,
                    0
                ],
                "title": "Targeted Learning for Variable Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Learning for Variable Importance"
                },
                "summary": "Variable importance is one of the most widely used measures for interpreting\nmachine learning with significant interest from both statistics and machine\nlearning communities. Recently, increasing attention has been directed toward\nuncertainty quantification in these metrics. Current approaches largely rely on\none-step procedures, which, while asymptotically efficient, can present higher\nsensitivity and instability in finite sample settings. To address these\nlimitations, we propose a novel method by employing the targeted learning (TL)\nframework, designed to enhance robustness in inference for variable importance\nmetrics. Our approach is particularly suited for conditional permutation\nvariable importance. We show that it (i) retains the asymptotic efficiency of\ntraditional methods, (ii) maintains comparable computational complexity, and\n(iii) delivers improved accuracy, especially in finite sample contexts. We\nfurther support these findings with numerical experiments that illustrate the\npractical advantages of our method and validate the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable importance is one of the most widely used measures for interpreting\nmachine learning with significant interest from both statistics and machine\nlearning communities. Recently, increasing attention has been directed toward\nuncertainty quantification in these metrics. Current approaches largely rely on\none-step procedures, which, while asymptotically efficient, can present higher\nsensitivity and instability in finite sample settings. To address these\nlimitations, we propose a novel method by employing the targeted learning (TL)\nframework, designed to enhance robustness in inference for variable importance\nmetrics. Our approach is particularly suited for conditional permutation\nvariable importance. We show that it (i) retains the asymptotic efficiency of\ntraditional methods, (ii) maintains comparable computational complexity, and\n(iii) delivers improved accuracy, especially in finite sample contexts. We\nfurther support these findings with numerical experiments that illustrate the\npractical advantages of our method and validate the theoretical results."
                },
                "authors": [
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Yunzhe Zhou"
                    },
                    {
                        "name": "Giles Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Giles Hooker"
                },
                "author": "Giles Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08556v2",
                "updated": "2024-11-04T16:12:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    12,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-11T14:39:59Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    14,
                    39,
                    59,
                    3,
                    193,
                    0
                ],
                "title": "Enhancing 3D Planetary Atmosphere Simulations with a Surrogate Radiative\n  Transfer Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing 3D Planetary Atmosphere Simulations with a Surrogate Radiative\n  Transfer Model"
                },
                "summary": "This work introduces an approach to enhancing the computational efficiency of\n3D atmospheric simulations by integrating a machine-learned surrogate model\ninto the OASIS global circulation model (GCM). Traditional GCMs, which are\nbased on repeatedly numerically integrating physical equations governing\natmospheric processes across a series of time-steps, are time-intensive,\nleading to compromises in spatial and temporal resolution of simulations. This\nresearch improves upon this limitation, enabling higher resolution simulations\nwithin practical timeframes. Speeding up 3D simulations holds significant\nimplications in multiple domains. Firstly, it facilitates the integration of 3D\nmodels into exoplanet inference pipelines, allowing for robust characterisation\nof exoplanets from a previously unseen wealth of data anticipated from JWST and\npost-JWST instruments. Secondly, acceleration of 3D models will enable higher\nresolution atmospheric simulations of Earth and Solar System planets, enabling\nmore detailed insights into their atmospheric physics and chemistry. Our method\nreplaces the radiative transfer module in OASIS with a recurrent neural\nnetwork-based model trained on simulation inputs and outputs. Radiative\ntransfer is typically one of the slowest components of a GCM, thus providing\nthe largest scope for overall model speed-up. The surrogate model was trained\nand tested on the specific test case of the Venusian atmosphere, to benchmark\nthe utility of this approach in the case of non-terrestrial atmospheres. This\napproach yields promising results, with the surrogate-integrated GCM\ndemonstrating above 99.0% accuracy and 147 factor GPU speed-up of the entire\nsimulation compared to using the matched original GCM under Venus-like\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces an approach to enhancing the computational efficiency of\n3D atmospheric simulations by integrating a machine-learned surrogate model\ninto the OASIS global circulation model (GCM). Traditional GCMs, which are\nbased on repeatedly numerically integrating physical equations governing\natmospheric processes across a series of time-steps, are time-intensive,\nleading to compromises in spatial and temporal resolution of simulations. This\nresearch improves upon this limitation, enabling higher resolution simulations\nwithin practical timeframes. Speeding up 3D simulations holds significant\nimplications in multiple domains. Firstly, it facilitates the integration of 3D\nmodels into exoplanet inference pipelines, allowing for robust characterisation\nof exoplanets from a previously unseen wealth of data anticipated from JWST and\npost-JWST instruments. Secondly, acceleration of 3D models will enable higher\nresolution atmospheric simulations of Earth and Solar System planets, enabling\nmore detailed insights into their atmospheric physics and chemistry. Our method\nreplaces the radiative transfer module in OASIS with a recurrent neural\nnetwork-based model trained on simulation inputs and outputs. Radiative\ntransfer is typically one of the slowest components of a GCM, thus providing\nthe largest scope for overall model speed-up. The surrogate model was trained\nand tested on the specific test case of the Venusian atmosphere, to benchmark\nthe utility of this approach in the case of non-terrestrial atmospheres. This\napproach yields promising results, with the surrogate-integrated GCM\ndemonstrating above 99.0% accuracy and 147 factor GPU speed-up of the entire\nsimulation compared to using the matched original GCM under Venus-like\nconditions."
                },
                "authors": [
                    {
                        "name": "Tara P. A. Tahseen"
                    },
                    {
                        "name": "João M. Mendonça"
                    },
                    {
                        "name": "Kai Hou Yip"
                    },
                    {
                        "name": "Ingo P. Waldmann"
                    }
                ],
                "author_detail": {
                    "name": "Ingo P. Waldmann"
                },
                "author": "Ingo P. Waldmann",
                "arxiv_doi": "10.1093/mnras/stae2461",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2461",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02217v1",
                "updated": "2024-11-04T16:12:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    12,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:12:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    12,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "Recursive Learning of Asymptotic Variational Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Learning of Asymptotic Variational Objectives"
                },
                "summary": "General state-space models (SSMs) are widely used in statistical machine\nlearning and are among the most classical generative models for sequential\ntime-series data. SSMs, comprising latent Markovian states, can be subjected to\nvariational inference (VI), but standard VI methods like the\nimportance-weighted autoencoder (IWAE) lack functionality for streaming data.\nTo enable online VI in SSMs when the observations are received in real time, we\npropose maximising an IWAE-type variational lower bound on the asymptotic\ncontrast function, rather than the standard IWAE ELBO, using stochastic\napproximation. Unlike the recursive maximum likelihood method, which directly\nmaximises the asymptotic contrast, our approach, called online sequential IWAE\n(OSIWAE), allows for online learning of both model parameters and a Markovian\nrecognition model for inferring latent states. By approximating filter state\nposteriors and their derivatives using sequential Monte Carlo (SMC) methods, we\ncreate a particle-based framework for online VI in SSMs. This approach is more\ntheoretically well-founded than recently proposed online variational SMC\nmethods. We provide rigorous theoretical results on the learning objective and\na numerical study demonstrating the method's efficiency in learning model\nparameters and particle proposal kernels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General state-space models (SSMs) are widely used in statistical machine\nlearning and are among the most classical generative models for sequential\ntime-series data. SSMs, comprising latent Markovian states, can be subjected to\nvariational inference (VI), but standard VI methods like the\nimportance-weighted autoencoder (IWAE) lack functionality for streaming data.\nTo enable online VI in SSMs when the observations are received in real time, we\npropose maximising an IWAE-type variational lower bound on the asymptotic\ncontrast function, rather than the standard IWAE ELBO, using stochastic\napproximation. Unlike the recursive maximum likelihood method, which directly\nmaximises the asymptotic contrast, our approach, called online sequential IWAE\n(OSIWAE), allows for online learning of both model parameters and a Markovian\nrecognition model for inferring latent states. By approximating filter state\nposteriors and their derivatives using sequential Monte Carlo (SMC) methods, we\ncreate a particle-based framework for online VI in SSMs. This approach is more\ntheoretically well-founded than recently proposed online variational SMC\nmethods. We provide rigorous theoretical results on the learning objective and\na numerical study demonstrating the method's efficiency in learning model\nparameters and particle proposal kernels."
                },
                "authors": [
                    {
                        "name": "Alessandro Mastrototaro"
                    },
                    {
                        "name": "Mathias Müller"
                    },
                    {
                        "name": "Jimmy Olsson"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Olsson"
                },
                "author": "Jimmy Olsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v1",
                "updated": "2024-11-04T15:54:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15127v2",
                "updated": "2024-11-04T15:54:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    21,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-23T15:27:19Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    15,
                    27,
                    19,
                    1,
                    114,
                    0
                ],
                "title": "GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration"
                },
                "summary": "Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine."
                },
                "authors": [
                    {
                        "name": "Sunan He"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Hongmei Wang"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Yihui Wang"
                    },
                    {
                        "name": "Zhiyuan Cai"
                    },
                    {
                        "name": "Zhixuan Chen"
                    },
                    {
                        "name": "Yingxue Xu"
                    },
                    {
                        "name": "Luyang Luo"
                    },
                    {
                        "name": "Huiling Xiang"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Mingxiang Wu"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "George Shih"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Qiong Wang"
                    },
                    {
                        "name": "Ronald Cheong Kin Chan"
                    },
                    {
                        "name": "Varut Vardhanabhuti"
                    },
                    {
                        "name": "Winnie Chiu Wing Chu"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    },
                    {
                        "name": "Kang Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19266v3",
                "updated": "2024-11-04T15:49:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    49,
                    41,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-29T16:59:38Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    59,
                    38,
                    2,
                    150,
                    0
                ],
                "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications"
                },
                "summary": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Shunli Wang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Shuaibing Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Peng Zhai"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20915v2",
                "updated": "2024-11-04T15:48:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    48,
                    10,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-31T15:21:44Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    21,
                    44,
                    4,
                    152,
                    0
                ],
                "title": "Fast yet Safe: Early-Exiting with Risk Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast yet Safe: Early-Exiting with Risk Control"
                },
                "summary": "Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals."
                },
                "authors": [
                    {
                        "name": "Metod Jazbec"
                    },
                    {
                        "name": "Alexander Timans"
                    },
                    {
                        "name": "Tin Hadži Veljković"
                    },
                    {
                        "name": "Kaspar Sakmann"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Christian A. Naesseth"
                    },
                    {
                        "name": "Eric Nalisnick"
                    }
                ],
                "author_detail": {
                    "name": "Eric Nalisnick"
                },
                "author": "Eric Nalisnick",
                "arxiv_comment": "27 pages, 13 figures, 4 tables (incl. appendix)",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02179v1",
                "updated": "2024-11-04T15:37:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    37,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:37:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    37,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality"
                },
                "summary": "High-quality environment lighting is the foundation of creating immersive\nuser experiences in mobile augmented reality (AR) applications. However,\nachieving visually coherent environment lighting estimation for Mobile AR is\nchallenging due to several key limitations associated with AR device sensing\ncapabilities, including limitations in device camera FoV and pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address their key limitations of\ngeneration hallucination and slow inference process. To do so, in this work, we\ndesign and implement a generative lighting estimation system called CleAR that\ncan produce high-quality and diverse environment maps in the format of\n360$^\\circ$ images. Specifically, we design a two-step generation pipeline\nguided by AR environment context data to ensure the results follow physical\nenvironment visual context and color appearances. To improve the estimation\nrobustness under different lighting conditions, we design a real-time\nrefinement component to adjust lighting estimation results on AR devices. To\ntrain and test our generative models, we curate a large-scale environment\nlighting estimation dataset with diverse lighting conditions. Through\nquantitative evaluation and user study, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy and\nrobustness. Moreover, CleAR supports real-time refinement of lighting\nestimation results, ensuring robust and timely environment lighting updates for\nAR applications. Our end-to-end generative estimation takes as fast as 3.2\nseconds, outperforming state-of-the-art methods by 110x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality environment lighting is the foundation of creating immersive\nuser experiences in mobile augmented reality (AR) applications. However,\nachieving visually coherent environment lighting estimation for Mobile AR is\nchallenging due to several key limitations associated with AR device sensing\ncapabilities, including limitations in device camera FoV and pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address their key limitations of\ngeneration hallucination and slow inference process. To do so, in this work, we\ndesign and implement a generative lighting estimation system called CleAR that\ncan produce high-quality and diverse environment maps in the format of\n360$^\\circ$ images. Specifically, we design a two-step generation pipeline\nguided by AR environment context data to ensure the results follow physical\nenvironment visual context and color appearances. To improve the estimation\nrobustness under different lighting conditions, we design a real-time\nrefinement component to adjust lighting estimation results on AR devices. To\ntrain and test our generative models, we curate a large-scale environment\nlighting estimation dataset with diverse lighting conditions. Through\nquantitative evaluation and user study, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy and\nrobustness. Moreover, CleAR supports real-time refinement of lighting\nestimation results, ensuring robust and timely environment lighting updates for\nAR applications. Our end-to-end generative estimation takes as fast as 3.2\nseconds, outperforming state-of-the-art methods by 110x."
                },
                "authors": [
                    {
                        "name": "Yiqin Zhao"
                    },
                    {
                        "name": "Mallesham Dasari"
                    },
                    {
                        "name": "Tian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tian Guo"
                },
                "author": "Tian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02175v1",
                "updated": "2024-11-04T15:34:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    34,
                    30,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:34:30Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    34,
                    30,
                    0,
                    309,
                    0
                ],
                "title": "SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning\n  with Pre-Trained Models"
                },
                "summary": "Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning aims to incrementally acquire new concepts in data streams\nwhile resisting forgetting previous knowledge. With the rise of powerful\npre-trained models (PTMs), there is a growing interest in training incremental\nlearning systems using these foundation models, rather than learning from\nscratch. Existing works often view PTMs as a strong initial point and directly\napply parameter-efficient tuning (PET) in the first session for adapting to\ndownstream tasks. In the following sessions, most methods freeze model\nparameters for tackling forgetting issues. However, applying PET directly to\ndownstream data cannot fully explore the inherent knowledge in PTMs.\nAdditionally, freezing the parameters in incremental sessions hinders models'\nplasticity to novel concepts not covered in the first session. To solve the\nabove issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE)\nframework. In particular, to inherit general knowledge from foundation models,\nwe include a transfer loss function by measuring the correlation between the\nPTM and the PET-applied model. After calibrating in the first session, the slow\nefficient tuning parameters can capture more informative features, improving\ngeneralization to incoming classes. Moreover, to further incorporate novel\nconcepts, we strike a balance between stability and plasticity by fixing slow\nefficient tuning parameters and continuously updating the fast ones.\nSpecifically, a cross-classification loss with feature alignment is proposed to\ncircumvent catastrophic forgetting. During inference, we introduce an\nentropy-based aggregation strategy to dynamically utilize the complementarity\nin the slow and fast learners. Extensive experiments on seven benchmark\ndatasets verify the effectiveness of our method by significantly surpassing the\nstate-of-the-art."
                },
                "authors": [
                    {
                        "name": "Linglan Zhao"
                    },
                    {
                        "name": "Xuerui Zhang"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Shouhong Ding"
                    },
                    {
                        "name": "Weiran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Huang"
                },
                "author": "Weiran Huang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02165v1",
                "updated": "2024-11-04T15:23:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    23,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:23:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    23,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "Joint Training of Speaker Embedding Extractor, Speech and Overlap\n  Detection for Diarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Training of Speaker Embedding Extractor, Speech and Overlap\n  Detection for Diarization"
                },
                "summary": "In spite of the popularity of end-to-end diarization systems nowadays,\nmodular systems comprised of voice activity detection (VAD), speaker embedding\nextraction plus clustering, and overlapped speech detection (OSD) plus handling\nstill attain competitive performance in many conditions. However, one of the\nmain drawbacks of modular systems is the need to run (and train) different\nmodules independently. In this work, we propose an approach to jointly train a\nmodel to produce speaker embeddings, VAD and OSD simultaneously and reach\ncompetitive performance at a fraction of the inference time of a standard\napproach. Furthermore, the joint inference leads to a simplified overall\npipeline which brings us one step closer to a unified clustering-based method\nthat can be trained end-to-end towards a diarization-specific objective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In spite of the popularity of end-to-end diarization systems nowadays,\nmodular systems comprised of voice activity detection (VAD), speaker embedding\nextraction plus clustering, and overlapped speech detection (OSD) plus handling\nstill attain competitive performance in many conditions. However, one of the\nmain drawbacks of modular systems is the need to run (and train) different\nmodules independently. In this work, we propose an approach to jointly train a\nmodel to produce speaker embeddings, VAD and OSD simultaneously and reach\ncompetitive performance at a fraction of the inference time of a standard\napproach. Furthermore, the joint inference leads to a simplified overall\npipeline which brings us one step closer to a unified clustering-based method\nthat can be trained end-to-end towards a diarization-specific objective."
                },
                "authors": [
                    {
                        "name": "Petr Pálka"
                    },
                    {
                        "name": "Federico Landini"
                    },
                    {
                        "name": "Dominik Klement"
                    },
                    {
                        "name": "Mireia Diez"
                    },
                    {
                        "name": "Anna Silnova"
                    },
                    {
                        "name": "Marc Delcroix"
                    },
                    {
                        "name": "Lukáš Burget"
                    }
                ],
                "author_detail": {
                    "name": "Lukáš Burget"
                },
                "author": "Lukáš Burget",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00053v2",
                "updated": "2024-11-04T15:20:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    20,
                    16,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-30T19:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    9,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate"
                },
                "summary": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Andrew Estornell"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18423v2",
                "updated": "2024-11-04T15:15:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    15,
                    32,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-24T04:19:46Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    4,
                    19,
                    46,
                    3,
                    298,
                    0
                ],
                "title": "Joint Modeling of Quasar Variability and Accretion Disk Reprocessing\n  using Latent Stochastic Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Modeling of Quasar Variability and Accretion Disk Reprocessing\n  using Latent Stochastic Differential Equations"
                },
                "summary": "Quasars are bright active galactic nuclei powered by the accretion of matter\naround supermassive black holes at the center of galaxies. Their stochastic\nbrightness variability depends on the physical properties of the accretion disk\nand black hole. The upcoming Rubin Observatory Legacy Survey of Space and Time\n(LSST) is expected to observe tens of millions of quasars, so there is a need\nfor efficient techniques like machine learning that can handle the large volume\nof data. Quasar variability is believed to be driven by an X-ray corona, which\nis reprocessed by the accretion disk and emitted as UV/optical variability. We\nare the first to introduce an auto-differentiable simulation of the accretion\ndisk and reprocessing. We use the simulation as a direct component of our\nneural network to jointly model the driving variability and reprocessing to fit\nsimulated LSST 10-year quasar light curves. The driving variability is\nreconstructed using a latent stochastic differential equation, a physically\nmotivated, generative deep learning method that can model continuous-time\nstochastic dynamics. By embedding these physical processes into our network, we\nachieve a model that is more robust and interpretable. We also use transformers\nto scale our model to tens of millions of parameters. We demonstrate how our\nmodel outperforms a Gaussian process regression baseline and can infer\naccretion disk parameters and time delays between wavebands, even for\nout-of-distribution driving signals. Our approach provides a powerful and\nscalable framework that can be adapted to solve other inverse problems in\nmultivariate time series with irregular sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasars are bright active galactic nuclei powered by the accretion of matter\naround supermassive black holes at the center of galaxies. Their stochastic\nbrightness variability depends on the physical properties of the accretion disk\nand black hole. The upcoming Rubin Observatory Legacy Survey of Space and Time\n(LSST) is expected to observe tens of millions of quasars, so there is a need\nfor efficient techniques like machine learning that can handle the large volume\nof data. Quasar variability is believed to be driven by an X-ray corona, which\nis reprocessed by the accretion disk and emitted as UV/optical variability. We\nare the first to introduce an auto-differentiable simulation of the accretion\ndisk and reprocessing. We use the simulation as a direct component of our\nneural network to jointly model the driving variability and reprocessing to fit\nsimulated LSST 10-year quasar light curves. The driving variability is\nreconstructed using a latent stochastic differential equation, a physically\nmotivated, generative deep learning method that can model continuous-time\nstochastic dynamics. By embedding these physical processes into our network, we\nachieve a model that is more robust and interpretable. We also use transformers\nto scale our model to tens of millions of parameters. We demonstrate how our\nmodel outperforms a Gaussian process regression baseline and can infer\naccretion disk parameters and time delays between wavebands, even for\nout-of-distribution driving signals. Our approach provides a powerful and\nscalable framework that can be adapted to solve other inverse problems in\nmultivariate time series with irregular sampling."
                },
                "authors": [
                    {
                        "name": "Joshua Fagin"
                    },
                    {
                        "name": "James Hung-Hsu Chan"
                    },
                    {
                        "name": "Henry Best"
                    },
                    {
                        "name": "Matthew O'Dowd"
                    },
                    {
                        "name": "K. E. Saavik Ford"
                    },
                    {
                        "name": "Matthew J. Graham"
                    },
                    {
                        "name": "Ji Won Park"
                    },
                    {
                        "name": "V. Ashley Villar"
                    }
                ],
                "author_detail": {
                    "name": "V. Ashley Villar"
                },
                "author": "V. Ashley Villar",
                "arxiv_comment": "31 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11858v2",
                "updated": "2024-11-04T15:11:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    11,
                    11,
                    0,
                    309,
                    0
                ],
                "published": "2023-11-20T15:52:22Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    15,
                    52,
                    22,
                    0,
                    324,
                    0
                ],
                "title": "Theory coherent shrinkage of Time-Varying Parameters in VARs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory coherent shrinkage of Time-Varying Parameters in VARs"
                },
                "summary": "This paper introduces a novel theory-coherent shrinkage prior for\nTime-Varying Parameter VARs (TVP-VARs). The prior centers the time-varying\nparameters on a path implied a priori by an underlying economic theory, chosen\nto describe the dynamics of the macroeconomic variables in the system.\nLeveraging information from conventional economic theory using this prior\nsignificantly improves inference precision and forecast accuracy compared to\nthe standard TVP-VAR. In an application, I use this prior to incorporate\ninformation from a New Keynesian model that includes both the Zero Lower Bound\n(ZLB) and forward guidance into a medium-scale TVP-VAR model. This approach\nleads to more precise estimates of the impulse response functions, revealing a\ndistinct propagation of risk premium shocks inside and outside the ZLB in US\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel theory-coherent shrinkage prior for\nTime-Varying Parameter VARs (TVP-VARs). The prior centers the time-varying\nparameters on a path implied a priori by an underlying economic theory, chosen\nto describe the dynamics of the macroeconomic variables in the system.\nLeveraging information from conventional economic theory using this prior\nsignificantly improves inference precision and forecast accuracy compared to\nthe standard TVP-VAR. In an application, I use this prior to incorporate\ninformation from a New Keynesian model that includes both the Zero Lower Bound\n(ZLB) and forward guidance into a medium-scale TVP-VAR model. This approach\nleads to more precise estimates of the impulse response functions, revealing a\ndistinct propagation of risk premium shocks inside and outside the ZLB in US\ndata."
                },
                "authors": [
                    {
                        "name": "Andrea Renzetti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Renzetti"
                },
                "author": "Andrea Renzetti",
                "arxiv_comment": "60 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17935v3",
                "updated": "2024-11-04T15:07:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    7,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-28T08:01:26Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    1,
                    26,
                    1,
                    149,
                    0
                ],
                "title": "Tool Learning with Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool Learning with Large Language Models: A Survey"
                },
                "summary": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey."
                },
                "authors": [
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_doi": "10.1007/s11704-024-40678-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11704-024-40678-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.17935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40678-2}",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02134v1",
                "updated": "2024-11-04T14:47:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    47,
                    48,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:47:48Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    47,
                    48,
                    0,
                    309,
                    0
                ],
                "title": "Encoding Multi-level Dynamics in Effect Heterogeneity Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoding Multi-level Dynamics in Effect Heterogeneity Estimation"
                },
                "summary": "Earth Observation (EO) data are increasingly used in policy analysis by\nenabling granular estimation of treatment effects. However, a challenge in\nEO-based causal inference lies in balancing the trade-off between capturing\nfine-grained individual heterogeneity and broader contextual information. This\npaper introduces Multi-scale Concatenation, a family of composable procedures\nthat transform arbitrary single-scale CATE estimation algorithms into\nmulti-scale algorithms. We benchmark the performance of Multi-scale\nConcatenation on a CATE estimation pipeline combining Vision Transformer (ViT)\nmodels fine-tuned on satellite images to encode images of different scales with\nCausal Forests to obtain the final CATE estimate. We first perform simulation\nstudies, showing how a multi-scale approach captures multi-level dynamics that\nsingle-scale ViT models fail to capture. We then apply the multi-scale method\nto two randomized controlled trials (RCTs) conducted in Peru and Uganda using\nLandsat satellite imagery. In the RCT analysis, the Rank Average Treatment\nEffect Ratio (RATE Ratio) measure is employed to assess performance without\nground truth individual treatment effects. Results indicate that Multi-scale\nConcatenation improves the performance of deep learning models in EO-based CATE\nestimation without the complexity of designing new multi-scale architectures\nfor a specific use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) data are increasingly used in policy analysis by\nenabling granular estimation of treatment effects. However, a challenge in\nEO-based causal inference lies in balancing the trade-off between capturing\nfine-grained individual heterogeneity and broader contextual information. This\npaper introduces Multi-scale Concatenation, a family of composable procedures\nthat transform arbitrary single-scale CATE estimation algorithms into\nmulti-scale algorithms. We benchmark the performance of Multi-scale\nConcatenation on a CATE estimation pipeline combining Vision Transformer (ViT)\nmodels fine-tuned on satellite images to encode images of different scales with\nCausal Forests to obtain the final CATE estimate. We first perform simulation\nstudies, showing how a multi-scale approach captures multi-level dynamics that\nsingle-scale ViT models fail to capture. We then apply the multi-scale method\nto two randomized controlled trials (RCTs) conducted in Peru and Uganda using\nLandsat satellite imagery. In the RCT analysis, the Rank Average Treatment\nEffect Ratio (RATE Ratio) measure is employed to assess performance without\nground truth individual treatment effects. Results indicate that Multi-scale\nConcatenation improves the performance of deep learning models in EO-based CATE\nestimation without the complexity of designing new multi-scale architectures\nfor a specific use case."
                },
                "authors": [
                    {
                        "name": "Fucheng Warren Zhu"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.7; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02120v1",
                "updated": "2024-11-04T14:35:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    35,
                    14,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:35:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    35,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges"
                },
                "summary": "Inverse protein folding is a fundamental task in computational protein\ndesign, which aims to design protein sequences that fold into the desired\nbackbone structures. While the development of machine learning algorithms for\nthis task has seen significant success, the prevailing approaches, which\npredominantly employ a discriminative formulation, frequently encounter the\nerror accumulation issue and often fail to capture the extensive variety of\nplausible sequences. To fill these gaps, we propose Bridge-IF, a generative\ndiffusion bridge model for inverse folding, which is designed to learn the\nprobabilistic dependency between the distributions of backbone structures and\nprotein sequences. Specifically, we harness an expressive structure encoder to\npropose a discrete, informative prior derived from structures, and establish a\nMarkov bridge to connect this prior with native sequences. During the inference\nstage, Bridge-IF progressively refines the prior sequence, culminating in a\nmore plausible design. Moreover, we introduce a reparameterization perspective\non Markov bridge models, from which we derive a simplified loss function that\nfacilitates more effective training. We also modulate protein language models\n(PLMs) with structural conditions to precisely approximate the Markov bridge\nprocess, thereby significantly enhancing generation performance while\nmaintaining parameter-efficient training. Extensive experiments on\nwell-established benchmarks demonstrate that Bridge-IF predominantly surpasses\nexisting baselines in sequence recovery and excels in the design of plausible\nproteins with high foldability. The code is available at\nhttps://github.com/violet-sto/Bridge-IF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse protein folding is a fundamental task in computational protein\ndesign, which aims to design protein sequences that fold into the desired\nbackbone structures. While the development of machine learning algorithms for\nthis task has seen significant success, the prevailing approaches, which\npredominantly employ a discriminative formulation, frequently encounter the\nerror accumulation issue and often fail to capture the extensive variety of\nplausible sequences. To fill these gaps, we propose Bridge-IF, a generative\ndiffusion bridge model for inverse folding, which is designed to learn the\nprobabilistic dependency between the distributions of backbone structures and\nprotein sequences. Specifically, we harness an expressive structure encoder to\npropose a discrete, informative prior derived from structures, and establish a\nMarkov bridge to connect this prior with native sequences. During the inference\nstage, Bridge-IF progressively refines the prior sequence, culminating in a\nmore plausible design. Moreover, we introduce a reparameterization perspective\non Markov bridge models, from which we derive a simplified loss function that\nfacilitates more effective training. We also modulate protein language models\n(PLMs) with structural conditions to precisely approximate the Markov bridge\nprocess, thereby significantly enhancing generation performance while\nmaintaining parameter-efficient training. Extensive experiments on\nwell-established benchmarks demonstrate that Bridge-IF predominantly surpasses\nexisting baselines in sequence recovery and excels in the design of plausible\nproteins with high foldability. The code is available at\nhttps://github.com/violet-sto/Bridge-IF."
                },
                "authors": [
                    {
                        "name": "Yiheng Zhu"
                    },
                    {
                        "name": "Jialu Wu"
                    },
                    {
                        "name": "Qiuyi Li"
                    },
                    {
                        "name": "Jiahuan Yan"
                    },
                    {
                        "name": "Mingze Yin"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02117v1",
                "updated": "2024-11-04T14:29:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    49,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    49,
                    0,
                    309,
                    0
                ],
                "title": "AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis"
                },
                "summary": "The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures."
                },
                "authors": [
                    {
                        "name": "Zichen Song"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Sitan Huang"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Kang"
                },
                "author": "Zhongfeng Kang",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v1",
                "updated": "2024-11-04T14:29:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02115v1",
                "updated": "2024-11-04T14:29:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation"
                },
                "summary": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server."
                },
                "authors": [
                    {
                        "name": "Ziwei Zhan"
                    },
                    {
                        "name": "Wenkuan Zhao"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Deke Guo"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01427v2",
                "updated": "2024-11-04T14:23:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    23,
                    11,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-02T11:24:22Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    11,
                    24,
                    22,
                    2,
                    276,
                    0
                ],
                "title": "Regularized e-processes: anytime valid inference with knowledge-based\n  efficiency gains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized e-processes: anytime valid inference with knowledge-based\n  efficiency gains"
                },
                "summary": "Classical statistical methods have theoretical justification when the sample\nsize is predetermined. In applications, however, it's often the case that\nsample sizes aren't predetermined; instead, they're often data-dependent. Since\nthose methods designed for static sample sizes aren't reliable when sample\nsizes are dynamic, there's been recent interest in e-processes and\ncorresponding tests and confidence sets that are anytime valid in the sense\nthat their justification holds up for arbitrary dynamic data-collection plans.\nBut if the investigator has relevant-yet-incomplete prior information about the\nquantity of interest, then there's an opportunity for efficiency gain, but\nexisting approaches can't accommodate this. The present paper offer a new,\nregularized e-process framework that features a knowledge-based,\nimprecise-probabilistic regularization with improved efficiency. A generalized\nversion of Ville's inequality is established, ensuring that inference based on\nthe regularized e-process remains anytime valid in a novel, knowledge-dependent\nsense. In addition, the proposed regularized e-processes facilitate\npossibility-theoretic uncertainty quantification with strong frequentist-like\ncalibration properties and other desirable Bayesian-like features: satisfies\nthe likelihood principle, avoids sure-loss, and offers formal decision-making\nwith reliability guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical statistical methods have theoretical justification when the sample\nsize is predetermined. In applications, however, it's often the case that\nsample sizes aren't predetermined; instead, they're often data-dependent. Since\nthose methods designed for static sample sizes aren't reliable when sample\nsizes are dynamic, there's been recent interest in e-processes and\ncorresponding tests and confidence sets that are anytime valid in the sense\nthat their justification holds up for arbitrary dynamic data-collection plans.\nBut if the investigator has relevant-yet-incomplete prior information about the\nquantity of interest, then there's an opportunity for efficiency gain, but\nexisting approaches can't accommodate this. The present paper offer a new,\nregularized e-process framework that features a knowledge-based,\nimprecise-probabilistic regularization with improved efficiency. A generalized\nversion of Ville's inequality is established, ensuring that inference based on\nthe regularized e-process remains anytime valid in a novel, knowledge-dependent\nsense. In addition, the proposed regularized e-processes facilitate\npossibility-theoretic uncertainty quantification with strong frequentist-like\ncalibration properties and other desirable Bayesian-like features: satisfies\nthe likelihood principle, avoids sure-loss, and offers formal decision-making\nwith reliability guarantees."
                },
                "authors": [
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "arxiv_comment": "Comments welcome (via email or) at\n  https://researchers.one/articles/24.09.00003",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.13441v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.13441v4",
                "updated": "2024-11-04T14:19:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    19,
                    57,
                    0,
                    309,
                    0
                ],
                "published": "2023-09-23T17:58:52Z",
                "published_parsed": [
                    2023,
                    9,
                    23,
                    17,
                    58,
                    52,
                    5,
                    266,
                    0
                ],
                "title": "Anytime valid and asymptotically optimal inference driven by predictive\n  recursion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime valid and asymptotically optimal inference driven by predictive\n  recursion"
                },
                "summary": "Distinguishing two candidate models is a fundamental and practically\nimportant statistical problem. Error rate control is crucial to the testing\nlogic but, in complex nonparametric settings, can be difficult to achieve,\nespecially when the stopping rule that determines the data collection process\nis not available. This paper proposes an e-process construction based on the\npredictive recursion (PR) algorithm originally designed to recursively fit\nnonparametric mixture models. The resulting PRe-process affords anytime valid\ninference and is asymptotically efficient in the sense that its growth rate is\nfirst-order optimal relative to PR's mixture model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing two candidate models is a fundamental and practically\nimportant statistical problem. Error rate control is crucial to the testing\nlogic but, in complex nonparametric settings, can be difficult to achieve,\nespecially when the stopping rule that determines the data collection process\nis not available. This paper proposes an e-process construction based on the\npredictive recursion (PR) algorithm originally designed to recursively fit\nnonparametric mixture models. The resulting PRe-process affords anytime valid\ninference and is asymptotically efficient in the sense that its growth rate is\nfirst-order optimal relative to PR's mixture model."
                },
                "authors": [
                    {
                        "name": "Vaidehi Dixit"
                    },
                    {
                        "name": "Ryan Martin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Martin"
                },
                "author": "Ryan Martin",
                "arxiv_comment": "Comments welcome at https://researchers.one/articles/23.09.00006",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.13441v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.13441v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02099v1",
                "updated": "2024-11-04T14:08:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    8,
                    26,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:08:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    8,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Integrated Decision Gradients (IDG-DP) for\n  Radar-based Human Activity Recognition"
                },
                "summary": "Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Idris Zakariyya"
                    },
                    {
                        "name": "Linda Tran"
                    },
                    {
                        "name": "Kaushik Bhargav Sivangi"
                    },
                    {
                        "name": "Paul Henderson"
                    },
                    {
                        "name": "Fani Deligianni"
                    }
                ],
                "author_detail": {
                    "name": "Fani Deligianni"
                },
                "author": "Fani Deligianni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02093v1",
                "updated": "2024-11-04T13:56:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    56,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:56:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    56,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "Do Advanced Language Models Eliminate the Need for Prompt Engineering in\n  Software Engineering?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Advanced Language Models Eliminate the Need for Prompt Engineering in\n  Software Engineering?"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced software engineering\n(SE) tasks, with prompt engineering techniques enhancing their performance in\ncode-related areas. However, the rapid development of foundational LLMs such as\nthe non-reasoning model GPT-4o and the reasoning model o1 raises questions\nabout the continued effectiveness of these prompt engineering techniques. This\npaper presents an extensive empirical study that reevaluates various prompt\nengineering techniques within the context of these advanced LLMs. Focusing on\nthree representative SE tasks, i.e., code generation, code translation, and\ncode summarization, we assess whether prompt engineering techniques still yield\nimprovements with advanced models, the actual effectiveness of reasoning models\ncompared to non-reasoning models, and whether the benefits of using these\nadvanced models justify their increased costs. Our findings reveal that prompt\nengineering techniques developed for earlier LLMs may provide diminished\nbenefits or even hinder performance when applied to advanced models. In\nreasoning LLMs, the ability of sophisticated built-in reasoning reduces the\nimpact of complex prompts, sometimes making simple zero-shot prompting more\neffective. Furthermore, while reasoning models outperform non-reasoning models\nin tasks requiring complex reasoning, they offer minimal advantages in tasks\nthat do not need reasoning and may incur unnecessary costs. Based on our study,\nwe provide practical guidance for practitioners on selecting appropriate prompt\nengineering techniques and foundational LLMs, considering factors such as task\nrequirements, operational costs, and environmental impact. Our work contributes\nto a deeper understanding of effectively harnessing advanced LLMs in SE tasks,\ninforming future research and application development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced software engineering\n(SE) tasks, with prompt engineering techniques enhancing their performance in\ncode-related areas. However, the rapid development of foundational LLMs such as\nthe non-reasoning model GPT-4o and the reasoning model o1 raises questions\nabout the continued effectiveness of these prompt engineering techniques. This\npaper presents an extensive empirical study that reevaluates various prompt\nengineering techniques within the context of these advanced LLMs. Focusing on\nthree representative SE tasks, i.e., code generation, code translation, and\ncode summarization, we assess whether prompt engineering techniques still yield\nimprovements with advanced models, the actual effectiveness of reasoning models\ncompared to non-reasoning models, and whether the benefits of using these\nadvanced models justify their increased costs. Our findings reveal that prompt\nengineering techniques developed for earlier LLMs may provide diminished\nbenefits or even hinder performance when applied to advanced models. In\nreasoning LLMs, the ability of sophisticated built-in reasoning reduces the\nimpact of complex prompts, sometimes making simple zero-shot prompting more\neffective. Furthermore, while reasoning models outperform non-reasoning models\nin tasks requiring complex reasoning, they offer minimal advantages in tasks\nthat do not need reasoning and may incur unnecessary costs. Based on our study,\nwe provide practical guidance for practitioners on selecting appropriate prompt\nengineering techniques and foundational LLMs, considering factors such as task\nrequirements, operational costs, and environmental impact. Our work contributes\nto a deeper understanding of effectively harnessing advanced LLMs in SE tasks,\ninforming future research and application development."
                },
                "authors": [
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Zhihao Gong"
                    },
                    {
                        "name": "Sixiang Ye"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yifan Zhao"
                    },
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Dan Hao"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hao"
                },
                "author": "Dan Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06511v2",
                "updated": "2024-11-04T13:52:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    52,
                    23,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-09T03:26:11Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    3,
                    26,
                    11,
                    2,
                    283,
                    0
                ],
                "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training"
                },
                "summary": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines."
                },
                "authors": [
                    {
                        "name": "Wanchao Liang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Less Wright"
                    },
                    {
                        "name": "Will Constable"
                    },
                    {
                        "name": "Andrew Gu"
                    },
                    {
                        "name": "Chien-Chin Huang"
                    },
                    {
                        "name": "Iris Zhang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Howard Huang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Sanket Purandare"
                    },
                    {
                        "name": "Gokul Nadathur"
                    },
                    {
                        "name": "Stratos Idreos"
                    }
                ],
                "author_detail": {
                    "name": "Stratos Idreos"
                },
                "author": "Stratos Idreos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02086v1",
                "updated": "2024-11-04T13:49:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    49,
                    6,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:49:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    49,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout\n  Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout\n  Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism"
                },
                "summary": "Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Haolong Xiang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Jinjun Yu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Xu"
                },
                "author": "Xiaolong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06457v2",
                "updated": "2024-11-04T13:43:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    43,
                    15,
                    0,
                    309,
                    0
                ],
                "published": "2024-08-12T19:17:57Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    19,
                    17,
                    57,
                    0,
                    225,
                    0
                ],
                "title": "Advanced Vision Transformers and Open-Set Learning for Robust Mosquito\n  Classification: A Novel Approach to Entomological Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Vision Transformers and Open-Set Learning for Robust Mosquito\n  Classification: A Novel Approach to Entomological Studies"
                },
                "summary": "Mosquito-related diseases pose a significant threat to global public health,\nnecessitating efficient and accurate mosquito classification for effective\nsurveillance and control. This work presents an innovative approach to mosquito\nclassification by leveraging state-of-the-art vision transformers and open-set\nlearning techniques. A novel framework has been introduced that integrates\nTransformer-based deep learning models with comprehensive data augmentation and\npreprocessing methods, enabling robust and precise identification of ten\nmosquito species. The Swin Transformer model achieves the best performance for\ntraditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The\nlightweight MobileViT technique attains an almost similar accuracy of 98.90%\nwith significantly reduced parameters and model complexities. Next, the applied\ndeep learning models' adaptability and generalizability in a static environment\nhave been enhanced by using new classes of data samples during the inference\nstage that have not been included in the training set. The proposed framework's\nability to handle unseen classes like insects similar to mosquitoes, even\nhumans, through open-set learning further enhances its practical applicability\nby employing the OpenMax technique and Weibull distribution. The traditional\nCNN model, Xception, outperforms the latest transformer with higher accuracy\nand F1 score for open-set learning. The study's findings highlight the\ntransformative potential of advanced deep-learning architectures in entomology,\nproviding a strong groundwork for future research and development in mosquito\nsurveillance and vector control. The implications of this work extend beyond\nmosquito classification, offering valuable insights for broader ecological and\nenvironmental monitoring applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mosquito-related diseases pose a significant threat to global public health,\nnecessitating efficient and accurate mosquito classification for effective\nsurveillance and control. This work presents an innovative approach to mosquito\nclassification by leveraging state-of-the-art vision transformers and open-set\nlearning techniques. A novel framework has been introduced that integrates\nTransformer-based deep learning models with comprehensive data augmentation and\npreprocessing methods, enabling robust and precise identification of ten\nmosquito species. The Swin Transformer model achieves the best performance for\ntraditional closed-set learning with 99.80% accuracy and 0.998 F1 score. The\nlightweight MobileViT technique attains an almost similar accuracy of 98.90%\nwith significantly reduced parameters and model complexities. Next, the applied\ndeep learning models' adaptability and generalizability in a static environment\nhave been enhanced by using new classes of data samples during the inference\nstage that have not been included in the training set. The proposed framework's\nability to handle unseen classes like insects similar to mosquitoes, even\nhumans, through open-set learning further enhances its practical applicability\nby employing the OpenMax technique and Weibull distribution. The traditional\nCNN model, Xception, outperforms the latest transformer with higher accuracy\nand F1 score for open-set learning. The study's findings highlight the\ntransformative potential of advanced deep-learning architectures in entomology,\nproviding a strong groundwork for future research and development in mosquito\nsurveillance and vector control. The implications of this work extend beyond\nmosquito classification, offering valuable insights for broader ecological and\nenvironmental monitoring applications."
                },
                "authors": [
                    {
                        "name": "Ahmed Akib Jawad Karim"
                    },
                    {
                        "name": "Muhammad Zawad Mahmud"
                    },
                    {
                        "name": "Riasat Khan"
                    }
                ],
                "author_detail": {
                    "name": "Riasat Khan"
                },
                "author": "Riasat Khan",
                "arxiv_comment": "23 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11944v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11944v4",
                "updated": "2024-11-04T13:28:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    28,
                    48,
                    0,
                    309,
                    0
                ],
                "published": "2024-01-22T13:34:34Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    13,
                    34,
                    34,
                    0,
                    22,
                    0
                ],
                "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark"
                },
                "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts."
                },
                "authors": [
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tongxu Luo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Yu-Hsuan Tsai"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11944v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11944v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13686v3",
                "updated": "2024-11-04T13:24:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    24,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-21T15:16:05Z",
                "published_parsed": [
                    2024,
                    4,
                    21,
                    15,
                    16,
                    5,
                    6,
                    112,
                    0
                ],
                "title": "Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image\n  Synthesis"
                },
                "summary": "Recently, a series of diffusion-aware distillation algorithms have emerged to\nalleviate the computational overhead associated with the multi-step inference\nprocess of Diffusion Models (DMs). Current distillation techniques often\ndichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)\nODE Trajectory Reformulation. However, these approaches suffer from severe\nperformance degradation or domain shifts. To address these limitations, we\npropose Hyper-SD, a novel framework that synergistically amalgamates the\nadvantages of ODE Trajectory Preservation and Reformulation, while maintaining\nnear-lossless performance during step compression. Firstly, we introduce\nTrajectory Segmented Consistency Distillation to progressively perform\nconsistent distillation within pre-defined time-step segments, which\nfacilitates the preservation of the original ODE trajectory from a higher-order\nperspective. Secondly, we incorporate human feedback learning to boost the\nperformance of the model in a low-step regime and mitigate the performance loss\nincurred by the distillation process. Thirdly, we integrate score distillation\nto further improve the low-step generation capability of the model and offer\nthe first attempt to leverage a unified LoRA to support the inference process\nat all steps. Extensive experiments and user studies demonstrate that Hyper-SD\nachieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.\nFor example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and\n+0.51 in Aes Score in the 1-step inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, a series of diffusion-aware distillation algorithms have emerged to\nalleviate the computational overhead associated with the multi-step inference\nprocess of Diffusion Models (DMs). Current distillation techniques often\ndichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)\nODE Trajectory Reformulation. However, these approaches suffer from severe\nperformance degradation or domain shifts. To address these limitations, we\npropose Hyper-SD, a novel framework that synergistically amalgamates the\nadvantages of ODE Trajectory Preservation and Reformulation, while maintaining\nnear-lossless performance during step compression. Firstly, we introduce\nTrajectory Segmented Consistency Distillation to progressively perform\nconsistent distillation within pre-defined time-step segments, which\nfacilitates the preservation of the original ODE trajectory from a higher-order\nperspective. Secondly, we incorporate human feedback learning to boost the\nperformance of the model in a low-step regime and mitigate the performance loss\nincurred by the distillation process. Thirdly, we integrate score distillation\nto further improve the low-step generation capability of the model and offer\nthe first attempt to leverage a unified LoRA to support the inference process\nat all steps. Extensive experiments and user studies demonstrate that Hyper-SD\nachieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.\nFor example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and\n+0.51 in Aes Score in the 1-step inference."
                },
                "authors": [
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yanzuo Lu"
                    },
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Pan Xie"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xuefeng Xiao"
                },
                "author": "Xuefeng Xiao",
                "arxiv_comment": "Accepted by NeurIPS 2024 (Camera-Ready Version). Project Page:\n  https://hyper-sd.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00533v2",
                "updated": "2024-11-04T13:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    15,
                    56,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T12:08:08Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    8,
                    8,
                    4,
                    306,
                    0
                ],
                "title": "ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models"
                },
                "summary": "This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata."
                },
                "authors": [
                    {
                        "name": "Anbang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anbang Wang"
                },
                "author": "Anbang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02064v1",
                "updated": "2024-11-04T13:06:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    46,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:06:46Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    46,
                    0,
                    309,
                    0
                ],
                "title": "Amortized Bayesian Experimental Design for Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Experimental Design for Decision-Making"
                },
                "summary": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Yujia Guo"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Samuel Kaski"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kaski"
                },
                "author": "Samuel Kaski",
                "arxiv_comment": "19 pages, 6 figures. Accepted at the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02063v1",
                "updated": "2024-11-04T13:06:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    17,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:06:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention"
                },
                "summary": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer."
                },
                "authors": [
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Accepted to EMNLP 2024 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02059v1",
                "updated": "2024-11-04T13:03:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:03:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT2: A Large Multimodal Model with Tabular Data Integration"
                },
                "summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact."
                },
                "authors": [
                    {
                        "name": "Aofeng Su"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Ga Zhang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haoxuan Lan"
                    },
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Kaizhe Shou"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Lin Long"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Wufang Zhu"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Xijun Gu"
                    },
                    {
                        "name": "Xinjie Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Xiao"
                },
                "author": "Zhiqing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04317v2",
                "updated": "2024-11-04T13:02:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    2,
                    33,
                    0,
                    309,
                    0
                ],
                "published": "2024-03-07T08:34:57Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    8,
                    34,
                    57,
                    3,
                    67,
                    0
                ],
                "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Adaptation of Language Models with a Memory of Amortized Contexts"
                },
                "summary": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC."
                },
                "authors": [
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Eric Mitchell"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Jonathan Richard Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Richard Schwarz"
                },
                "author": "Jonathan Richard Schwarz",
                "arxiv_comment": "Published as a conference proceeding for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02058v1",
                "updated": "2024-11-04T13:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    1,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    1,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional\n  Trajectories Through Manifold Learning"
                },
                "summary": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimensions $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength.\nFor weak nonlinearities, $m^{\\ast} \\ll n$, where $n = 2N$. In contrast, for\nstrong nonlinearities, $m^{\\ast} \\rightarrow n - 1$, consistently with the\nergodic hypothesis. Furthermore, one of the potential limitations of PCA is\naddressed through an analysis with t-distributed stochastic neighbor embedding\n($t$-SNE). Accordingly, we found strong evidence suggesting that the datapoints\nlie near or on a curved low-dimensional manifold for weak nonlinearities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data-driven approach based on unsupervised machine learning is proposed to\ninfer the intrinsic dimensions $m^{\\ast}$ of the high-dimensional trajectories\nof the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis\n(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,\nof the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a\ncritical relationship between $m^{\\ast}$ and the model's nonlinear strength.\nFor weak nonlinearities, $m^{\\ast} \\ll n$, where $n = 2N$. In contrast, for\nstrong nonlinearities, $m^{\\ast} \\rightarrow n - 1$, consistently with the\nergodic hypothesis. Furthermore, one of the potential limitations of PCA is\naddressed through an analysis with t-distributed stochastic neighbor embedding\n($t$-SNE). Accordingly, we found strong evidence suggesting that the datapoints\nlie near or on a curved low-dimensional manifold for weak nonlinearities."
                },
                "authors": [
                    {
                        "name": "Gionni Marchetti"
                    }
                ],
                "author_detail": {
                    "name": "Gionni Marchetti"
                },
                "author": "Gionni Marchetti",
                "arxiv_comment": "20 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v5",
                "updated": "2024-11-04T12:55:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    55,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11199v2",
                "updated": "2024-11-04T12:47:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    47,
                    31,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-15T02:34:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    34,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Isambard-AI: a leadership class supercomputer optimised specifically for\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isambard-AI: a leadership class supercomputer optimised specifically for\n  Artificial Intelligence"
                },
                "summary": "Isambard-AI is a new, leadership-class supercomputer, designed to support\nAI-related research. Based on the HPE Cray EX4000 system, and housed in a new,\nenergy efficient Modular Data Centre in Bristol, UK, Isambard-AI employs 5,448\nNVIDIA Grace-Hopper GPUs to deliver over 21 ExaFLOP/s of 8-bit floating point\nperformance for LLM training, and over 250 PetaFLOP/s of 64-bit performance,\nfor under 5MW. Isambard-AI integrates two, all-flash storage systems: a 20\nPiByte Cray ClusterStor and a 3.5 PiByte VAST solution. Combined these give\nIsambard-AI flexibility for training, inference and secure data accesses and\nsharing. But it is the software stack where Isambard-AI will be most different\nfrom traditional HPC systems. Isambard-AI is designed to support users who may\nhave been using GPUs in the cloud, and so access will more typically be via\nJupyter notebooks, MLOps, or other web-based, interactive interfaces, rather\nthan the approach used on traditional supercomputers of sshing into a system\nbefore submitting jobs to a batch scheduler. Its stack is designed to be\nquickly and regularly upgraded to keep pace with the rapid evolution of AI\nsoftware, with full support for containers. Phase 1 of Isambard-AI is due\nonline in May/June 2024, with the full system expected in production by the end\nof the year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isambard-AI is a new, leadership-class supercomputer, designed to support\nAI-related research. Based on the HPE Cray EX4000 system, and housed in a new,\nenergy efficient Modular Data Centre in Bristol, UK, Isambard-AI employs 5,448\nNVIDIA Grace-Hopper GPUs to deliver over 21 ExaFLOP/s of 8-bit floating point\nperformance for LLM training, and over 250 PetaFLOP/s of 64-bit performance,\nfor under 5MW. Isambard-AI integrates two, all-flash storage systems: a 20\nPiByte Cray ClusterStor and a 3.5 PiByte VAST solution. Combined these give\nIsambard-AI flexibility for training, inference and secure data accesses and\nsharing. But it is the software stack where Isambard-AI will be most different\nfrom traditional HPC systems. Isambard-AI is designed to support users who may\nhave been using GPUs in the cloud, and so access will more typically be via\nJupyter notebooks, MLOps, or other web-based, interactive interfaces, rather\nthan the approach used on traditional supercomputers of sshing into a system\nbefore submitting jobs to a batch scheduler. Its stack is designed to be\nquickly and regularly upgraded to keep pace with the rapid evolution of AI\nsoftware, with full support for containers. Phase 1 of Isambard-AI is due\nonline in May/June 2024, with the full system expected in production by the end\nof the year."
                },
                "authors": [
                    {
                        "name": "Simon McIntosh-Smith"
                    },
                    {
                        "name": "Sadaf R Alam"
                    },
                    {
                        "name": "Christopher Woods"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Woods"
                },
                "author": "Christopher Woods",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02041v1",
                "updated": "2024-11-04T12:43:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    43,
                    12,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:43:12Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    43,
                    12,
                    0,
                    309,
                    0
                ],
                "title": "Enhancing ID-based Recommendation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing ID-based Recommendation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data."
                },
                "authors": [
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xiaoyi Du"
                    },
                    {
                        "name": "Hengliang Luo"
                    },
                    {
                        "name": "Depeng Jin"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10724v3",
                "updated": "2024-11-04T12:42:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    42,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-08-20T10:45:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian"
                },
                "summary": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages."
                },
                "authors": [
                    {
                        "name": "Cem Üyük"
                    },
                    {
                        "name": "Danica Rovó"
                    },
                    {
                        "name": "Shaghayegh Kolli"
                    },
                    {
                        "name": "Rabia Varol"
                    },
                    {
                        "name": "Georg Groh"
                    },
                    {
                        "name": "Daryna Dementieva"
                    }
                ],
                "author_detail": {
                    "name": "Daryna Dementieva"
                },
                "author": "Daryna Dementieva",
                "arxiv_comment": "EMNLP 2024 NLP4PI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18991v2",
                "updated": "2024-11-04T12:38:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    38,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-10T15:06:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    6,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "TRIAGE: Ethical Benchmarking of AI Models Through Mass Casualty\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIAGE: Ethical Benchmarking of AI Models Through Mass Casualty\n  Simulations"
                },
                "summary": "We present the TRIAGE Benchmark, a novel machine ethics (ME) benchmark that\ntests LLMs' ability to make ethical decisions during mass casualty incidents.\nIt uses real-world ethical dilemmas with clear solutions designed by medical\nprofessionals, offering a more realistic alternative to annotation-based\nbenchmarks. TRIAGE incorporates various prompting styles to evaluate model\nperformance across different contexts. Most models consistently outperformed\nrandom guessing, suggesting LLMs may support decision-making in triage\nscenarios. Neutral or factual scenario formulations led to the best\nperformance, unlike other ME benchmarks where ethical reminders improved\noutcomes. Adversarial prompts reduced performance but not to random guessing\nlevels. Open-source models made more morally serious errors, and general\ncapability overall predicted better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the TRIAGE Benchmark, a novel machine ethics (ME) benchmark that\ntests LLMs' ability to make ethical decisions during mass casualty incidents.\nIt uses real-world ethical dilemmas with clear solutions designed by medical\nprofessionals, offering a more realistic alternative to annotation-based\nbenchmarks. TRIAGE incorporates various prompting styles to evaluate model\nperformance across different contexts. Most models consistently outperformed\nrandom guessing, suggesting LLMs may support decision-making in triage\nscenarios. Neutral or factual scenario formulations led to the best\nperformance, unlike other ME benchmarks where ethical reminders improved\noutcomes. Adversarial prompts reduced performance but not to random guessing\nlevels. Open-source models made more morally serious errors, and general\ncapability overall predicted better performance."
                },
                "authors": [
                    {
                        "name": "Nathalie Maria Kirch"
                    },
                    {
                        "name": "Konstantin Hebenstreit"
                    },
                    {
                        "name": "Matthias Samwald"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Samwald"
                },
                "author": "Matthias Samwald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02033v1",
                "updated": "2024-11-04T12:31:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    31,
                    1,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:31:01Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    31,
                    1,
                    0,
                    309,
                    0
                ],
                "title": "Two stochastic versions of the Arps curve decline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two stochastic versions of the Arps curve decline"
                },
                "summary": "Based on the Arps equation, we propose two stochastic models for curve\ndecline useful in oil engineering context. Theoretical properties and\nsimulations of these models are provided. The first passage time distribution\nof these stochastic models to a constant level is then studied. In conclusion,\nwe discuss about statistical inference of the parameters from the observations\nof the oil production cumulative rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the Arps equation, we propose two stochastic models for curve\ndecline useful in oil engineering context. Theoretical properties and\nsimulations of these models are provided. The first passage time distribution\nof these stochastic models to a constant level is then studied. In conclusion,\nwe discuss about statistical inference of the parameters from the observations\nof the oil production cumulative rate."
                },
                "authors": [
                    {
                        "name": "Christian Paroissin"
                    }
                ],
                "author_detail": {
                    "name": "Christian Paroissin"
                },
                "arxiv_affiliation": "LMAP",
                "author": "Christian Paroissin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11430v2",
                "updated": "2024-11-04T12:21:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    21,
                    52,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-19T03:08:02Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    3,
                    8,
                    2,
                    6,
                    140,
                    0
                ],
                "title": "MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP."
                },
                "authors": [
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Zhijiang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Guo"
                },
                "author": "Zhijiang Guo",
                "arxiv_comment": "43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03078v2",
                "updated": "2024-11-04T12:19:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    19,
                    57,
                    0,
                    309,
                    0
                ],
                "published": "2024-08-06T10:13:57Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    13,
                    57,
                    1,
                    219,
                    0
                ],
                "title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications"
                },
                "summary": "Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Monocular\nVisual Simultaneous Localization and Mapping (MVSLAM) has emerged as a\npromising solution, its implementation in endoscopic procedures faces\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents\nBodySLAM, a robust deep learning-based MVSLAM approach that addresses these\nchallenges through three key components: CycleVO, a novel unsupervised\nmonocular pose estimation module; the integration of the state-of-the-art Zoe\narchitecture for monocular depth estimation; and a 3D reconstruction module\ncreating a coherent surgical map. The approach is rigorously evaluated using\nthree publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning\nlaparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against\nfour state-of-the-art methods. Results demonstrate that CycleVO exhibited\ncompetitive performance with the lowest inference time among pose estimation\nmethods, while maintaining robust generalization capabilities, whereas Zoe\nsignificantly outperformed existing algorithms for depth estimation in\nendoscopy. BodySLAM's strong performance across diverse endoscopic scenarios\ndemonstrates its potential as a viable MVSLAM solution for endoscopic\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Monocular\nVisual Simultaneous Localization and Mapping (MVSLAM) has emerged as a\npromising solution, its implementation in endoscopic procedures faces\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents\nBodySLAM, a robust deep learning-based MVSLAM approach that addresses these\nchallenges through three key components: CycleVO, a novel unsupervised\nmonocular pose estimation module; the integration of the state-of-the-art Zoe\narchitecture for monocular depth estimation; and a 3D reconstruction module\ncreating a coherent surgical map. The approach is rigorously evaluated using\nthree publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning\nlaparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against\nfour state-of-the-art methods. Results demonstrate that CycleVO exhibited\ncompetitive performance with the lowest inference time among pose estimation\nmethods, while maintaining robust generalization capabilities, whereas Zoe\nsignificantly outperformed existing algorithms for depth estimation in\nendoscopy. BodySLAM's strong performance across diverse endoscopic scenarios\ndemonstrates its potential as a viable MVSLAM solution for endoscopic\napplications."
                },
                "authors": [
                    {
                        "name": "G. Manni"
                    },
                    {
                        "name": "C. Lauretti"
                    },
                    {
                        "name": "F. Prata"
                    },
                    {
                        "name": "R. Papalia"
                    },
                    {
                        "name": "L. Zollo"
                    },
                    {
                        "name": "P. Soda"
                    }
                ],
                "author_detail": {
                    "name": "P. Soda"
                },
                "arxiv_affiliation": "Research Unit of Computer Systems and Bioinformatics Department of Engineering Università Campus Bio-Medico di Roma",
                "author": "P. Soda",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02018v1",
                "updated": "2024-11-04T12:13:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    13,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:13:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    13,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Shortcut Learning in In-Context Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcut Learning in In-Context Learning: A Survey"
                },
                "summary": "Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning."
                },
                "authors": [
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Yingji Li"
                    },
                    {
                        "name": "Fausto Giunchiglia"
                    },
                    {
                        "name": "Hao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Xu"
                },
                "author": "Hao Xu",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16043v2",
                "updated": "2024-11-04T11:54:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    54,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-25T09:52:02Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    9,
                    52,
                    2,
                    6,
                    56,
                    0
                ],
                "title": "LuaTaint: A Static Analysis System for Web Configuration Interface\n  Vulnerability of Internet of Things Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LuaTaint: A Static Analysis System for Web Configuration Interface\n  Vulnerability of Internet of Things Devices"
                },
                "summary": "The diversity of web configuration interfaces for IoT devices has exacerbated\nissues such as inadequate permission controls and insecure interfaces,\nresulting in various vulnerabilities. Owing to the varying interface\nconfigurations across various devices, the existing methods are inadequate for\nidentifying these vulnerabilities precisely and comprehensively. This study\naddresses these issues by introducing an automated vulnerability detection\nsystem, called LuaTaint. It is designed for the commonly used web configuration\ninterface of IoT devices. LuaTaint combines static taint analysis with a large\nlanguage model (LLM) to achieve widespread and high-precision detection. The\nextensive traversal of the static analysis ensures the comprehensiveness of the\ndetection. The system also incorporates rules related to page handler control\nlogic within the taint detection process to enhance its precision and\nextensibility. Moreover, we leverage the prodigious abilities of LLM for code\nanalysis tasks. By utilizing LLM in the process of pruning false alarms, the\nprecision of LuaTaint is enhanced while significantly reducing its dependence\non manual analysis. We develop a prototype of LuaTaint and evaluate it using\n2,447 IoT firmware samples from 11 renowned vendors. LuaTaint has discovered\n111 vulnerabilities. Moreover, LuaTaint exhibits a vulnerability detection\nprecision rate of up to 89.29%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity of web configuration interfaces for IoT devices has exacerbated\nissues such as inadequate permission controls and insecure interfaces,\nresulting in various vulnerabilities. Owing to the varying interface\nconfigurations across various devices, the existing methods are inadequate for\nidentifying these vulnerabilities precisely and comprehensively. This study\naddresses these issues by introducing an automated vulnerability detection\nsystem, called LuaTaint. It is designed for the commonly used web configuration\ninterface of IoT devices. LuaTaint combines static taint analysis with a large\nlanguage model (LLM) to achieve widespread and high-precision detection. The\nextensive traversal of the static analysis ensures the comprehensiveness of the\ndetection. The system also incorporates rules related to page handler control\nlogic within the taint detection process to enhance its precision and\nextensibility. Moreover, we leverage the prodigious abilities of LLM for code\nanalysis tasks. By utilizing LLM in the process of pruning false alarms, the\nprecision of LuaTaint is enhanced while significantly reducing its dependence\non manual analysis. We develop a prototype of LuaTaint and evaluate it using\n2,447 IoT firmware samples from 11 renowned vendors. LuaTaint has discovered\n111 vulnerabilities. Moreover, LuaTaint exhibits a vulnerability detection\nprecision rate of up to 89.29%."
                },
                "authors": [
                    {
                        "name": "Jiahui Xiang"
                    },
                    {
                        "name": "Lirong Fu"
                    },
                    {
                        "name": "Tong Ye"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Huan Le"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02006v1",
                "updated": "2024-11-04T11:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey"
                },
                "summary": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents"
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16998v2",
                "updated": "2024-11-04T11:44:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    44,
                    29,
                    0,
                    309,
                    0
                ],
                "published": "2024-09-25T15:03:22Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    3,
                    22,
                    2,
                    269,
                    0
                ],
                "title": "PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery"
                },
                "summary": "Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps."
                },
                "authors": [
                    {
                        "name": "Anjana Wijekoon"
                    },
                    {
                        "name": "Adrito Das"
                    },
                    {
                        "name": "Roxana R. Herrera"
                    },
                    {
                        "name": "Danyal Z. Khan"
                    },
                    {
                        "name": "John Hanrahan"
                    },
                    {
                        "name": "Eleanor Carter"
                    },
                    {
                        "name": "Valpuri Luoma"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Hani J. Marcus"
                    },
                    {
                        "name": "Sophia Bano"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Bano"
                },
                "author": "Sophia Bano",
                "arxiv_comment": "Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06922v2",
                "updated": "2024-11-04T11:42:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    42,
                    26,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-10T11:07:24Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    11,
                    7,
                    24,
                    5,
                    41,
                    0
                ],
                "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers in the Machine: Confidentiality in LLM-integrated Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools."
                },
                "authors": [
                    {
                        "name": "Jonathan Evertz"
                    },
                    {
                        "name": "Merlin Chlosta"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Eisenhofer"
                },
                "author": "Thorsten Eisenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18783v2",
                "updated": "2024-11-04T11:39:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    39,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-09-27T14:30:24Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    14,
                    30,
                    24,
                    4,
                    271,
                    0
                ],
                "title": "DualDn: Dual-domain Denoising via Differentiable ISP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualDn: Dual-domain Denoising via Differentiable ISP"
                },
                "summary": "Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image denoising is a critical component in a camera's Image Signal Processing\n(ISP) pipeline. There are two typical ways to inject a denoiser into the ISP\npipeline: applying a denoiser directly to captured raw frames (raw domain) or\nto the ISP's output sRGB images (sRGB domain). However, both approaches have\ntheir limitations. Residual noise from raw-domain denoising can be amplified by\nthe subsequent ISP processing, and the sRGB domain struggles to handle\nspatially varying noise since it only sees noise distorted by the ISP.\nConsequently, most raw or sRGB domain denoising works only for specific noise\ndistributions and ISP configurations. To address these challenges, we propose\nDualDn, a novel learning-based dual-domain denoising. Unlike previous\nsingle-domain denoising, DualDn consists of two denoising networks: one in the\nraw domain and one in the sRGB domain. The raw domain denoising adapts to\nsensor-specific noise as well as spatially varying noise levels, while the sRGB\ndomain denoising adapts to ISP variations and removes residual noise amplified\nby the ISP. Both denoising networks are connected with a differentiable ISP,\nwhich is trained end-to-end and discarded during the inference stage. With this\ndesign, DualDn achieves greater generalizability compared to most\nlearning-based denoising methods, as it can adapt to different unseen noises,\nISP parameters, and even novel ISP pipelines. Experiments show that DualDn\nachieves state-of-the-art performance and can adapt to different denoising\narchitectures. Moreover, DualDn can be used as a plug-and-play denoising module\nwith real cameras without retraining, and still demonstrate better performance\nthan commercial on-camera denoising. The project website is available at:\nhttps://openimaginglab.github.io/DualDn/"
                },
                "authors": [
                    {
                        "name": "Ruikang Li"
                    },
                    {
                        "name": "Yujin Wang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jinwei Gu"
                    },
                    {
                        "name": "Tianfan Xue"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Xue"
                },
                "author": "Tianfan Xue",
                "arxiv_comment": "Accepted at ECCV 2024, Project page:\n  https://openimaginglab.github.io/DualDn/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01996v1",
                "updated": "2024-11-04T11:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    31,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    31,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task"
                },
                "summary": "The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}."
                },
                "authors": [
                    {
                        "name": "Hoonick Lee"
                    },
                    {
                        "name": "Mogan Gim"
                    },
                    {
                        "name": "Donghyeon Park"
                    },
                    {
                        "name": "Donghee Choi"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19803v2",
                "updated": "2024-11-04T11:28:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    28,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-28T10:24:31Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    10,
                    24,
                    31,
                    4,
                    180,
                    0
                ],
                "title": "Scalable and Domain-General Abstractive Proposition Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Domain-General Abstractive Proposition Segmentation"
                },
                "summary": "Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use."
                },
                "authors": [
                    {
                        "name": "Mohammad Javad Hosseini"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Tim Baumgärtner"
                    },
                    {
                        "name": "Alex Fabrikant"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    }
                ],
                "author_detail": {
                    "name": "Reinald Kim Amplayo"
                },
                "author": "Reinald Kim Amplayo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01992v1",
                "updated": "2024-11-04T11:26:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    26,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:26:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    26,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Ask, and it shall be given: Turing completeness of prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, and it shall be given: Turing completeness of prompting"
                },
                "summary": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Wenxuan Bao"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10671v3",
                "updated": "2024-11-04T11:24:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    24,
                    51,
                    0,
                    309,
                    0
                ],
                "published": "2023-11-17T17:43:11Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    17,
                    43,
                    11,
                    4,
                    321,
                    0
                ],
                "title": "Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based\n  Inference"
                },
                "summary": "We present multimodal neural posterior estimation (MultiNPE), a method to\nintegrate heterogeneous data from different sources in simulation-based\ninference with neural networks. Inspired by advances in deep fusion, it allows\nresearchers to analyze data from different domains and infer the parameters of\ncomplex mathematical models with increased accuracy. We consider three fusion\napproaches for MultiNPE (early, late, hybrid) and evaluate their performance in\nthree challenging experiments. MultiNPE not only outperforms single-source\nbaselines on a reference task, but also achieves superior inference on\nscientific models from cognitive neuroscience and cardiology. We systematically\ninvestigate the impact of partially missing data on the different fusion\nstrategies. Across our experiments, late and hybrid fusion techniques emerge as\nthe methods of choice for practical applications of multimodal simulation-based\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present multimodal neural posterior estimation (MultiNPE), a method to\nintegrate heterogeneous data from different sources in simulation-based\ninference with neural networks. Inspired by advances in deep fusion, it allows\nresearchers to analyze data from different domains and infer the parameters of\ncomplex mathematical models with increased accuracy. We consider three fusion\napproaches for MultiNPE (early, late, hybrid) and evaluate their performance in\nthree challenging experiments. MultiNPE not only outperforms single-source\nbaselines on a reference task, but also achieves superior inference on\nscientific models from cognitive neuroscience and cardiology. We systematically\ninvestigate the impact of partially missing data on the different fusion\nstrategies. Across our experiments, late and hybrid fusion techniques emerge as\nthe methods of choice for practical applications of multimodal simulation-based\ninference."
                },
                "authors": [
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Leona Odole"
                    },
                    {
                        "name": "Stefan T. Radev"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01988v1",
                "updated": "2024-11-04T11:20:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    20,
                    17,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:20:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    20,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition"
                },
                "summary": "On facial expression datasets with complex and numerous feature types, where\nthe significance and dominance of labeled features are difficult to predict,\nfacial expression recognition(FER) encounters the challenges of inter-class\nsimilarity and intra-class variances, making it difficult to mine effective\nfeatures. We aim to solely leverage the feature similarity among facial samples\nto address this. We introduce the Cross Similarity Attention (CSA), an\ninput-output position-sensitive attention mechanism that harnesses feature\nsimilarity across different images to compute the corresponding global spatial\nattention. Based on this, we propose a four-branch circular framework, called\nQuadruplet Cross Similarity (QCS), to extract discriminative features from the\nsame class and eliminate redundant ones from different classes synchronously to\nrefine cleaner features. The symmetry of the network ensures balanced and\nstable training and reduces the amount of CSA interaction matrix. Contrastive\nresidual distillation is utilized to transfer the information learned in the\ncross module back to the base network. The cross-attention module exists during\ntraining, and only one base branch is retained during inference. our proposed\nQCS model outperforms state-of-the-art methods on several popular FER datasets,\nwithout requiring additional landmark information or other extra training data.\nThe code is available at https://github.com/birdwcp/QCS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On facial expression datasets with complex and numerous feature types, where\nthe significance and dominance of labeled features are difficult to predict,\nfacial expression recognition(FER) encounters the challenges of inter-class\nsimilarity and intra-class variances, making it difficult to mine effective\nfeatures. We aim to solely leverage the feature similarity among facial samples\nto address this. We introduce the Cross Similarity Attention (CSA), an\ninput-output position-sensitive attention mechanism that harnesses feature\nsimilarity across different images to compute the corresponding global spatial\nattention. Based on this, we propose a four-branch circular framework, called\nQuadruplet Cross Similarity (QCS), to extract discriminative features from the\nsame class and eliminate redundant ones from different classes synchronously to\nrefine cleaner features. The symmetry of the network ensures balanced and\nstable training and reduces the amount of CSA interaction matrix. Contrastive\nresidual distillation is utilized to transfer the information learned in the\ncross module back to the base network. The cross-attention module exists during\ntraining, and only one base branch is retained during inference. our proposed\nQCS model outperforms state-of-the-art methods on several popular FER datasets,\nwithout requiring additional landmark information or other extra training data.\nThe code is available at https://github.com/birdwcp/QCS."
                },
                "authors": [
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Lili Wang"
                    },
                    {
                        "name": "Zhaofan Li"
                    },
                    {
                        "name": "Xuebin Lv"
                    }
                ],
                "author_detail": {
                    "name": "Xuebin Lv"
                },
                "author": "Xuebin Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18137v2",
                "updated": "2024-11-04T11:16:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    16,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-28T12:51:01Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    12,
                    51,
                    1,
                    1,
                    149,
                    0
                ],
                "title": "Exploiting LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting LLM Quantization"
                },
                "summary": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices."
                },
                "authors": [
                    {
                        "name": "Kazuki Egashira"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07099v2",
                "updated": "2024-11-04T11:15:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-18T07:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    46,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nash CoT: Multi-Path Inference with Preference Equilibrium"
                },
                "summary": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10618v2",
                "updated": "2024-11-04T11:11:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    11,
                    49,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-16T14:42:49Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    14,
                    42,
                    49,
                    1,
                    107,
                    0
                ],
                "title": "Private Attribute Inference from Images with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Attribute Inference from Images with Vision-Language Models"
                },
                "summary": "As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses."
                },
                "authors": [
                    {
                        "name": "Batuhan Tömekçe"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07040v2",
                "updated": "2024-11-04T11:08:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    8,
                    57,
                    0,
                    309,
                    0
                ],
                "published": "2023-12-12T07:52:35Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    7,
                    52,
                    35,
                    1,
                    346,
                    0
                ],
                "title": "Rethinking Model Inversion Attacks With Patch-Wise Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Model Inversion Attacks With Patch-Wise Reconstruction"
                },
                "summary": "Model inversion (MI) attacks aim to infer or reconstruct the training dataset\nthrough reverse-engineering from the target model's weights. Recently,\nsignificant advancements in generative models have enabled MI attacks to\novercome challenges in producing photo-realistic replicas of the training\ndataset, a technique known as generative MI. The generative MI primarily\nfocuses on identifying latent vectors that correspond to specific target\nlabels, leveraging a generative model trained with an auxiliary dataset.\nHowever, an important aspect is often overlooked: the MI attacks fail if the\npre-trained generative model lacks the coverage to create an image\ncorresponding to the target label, especially when there is a significant\ndifference between the target and auxiliary datasets. To address this gap, we\npropose the Patch-MI method, inspired by a jigsaw puzzle, which offers a novel\nprobabilistic interpretation of MI attacks. Even with a dissimilar auxiliary\ndataset, our method effectively creates images that closely mimic the\ndistribution of image patches in the target dataset by patch-based\nreconstruction. Moreover, we numerically demonstrate that the Patch-MI improves\nTop 1 attack accuracy by 5\\%p compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model inversion (MI) attacks aim to infer or reconstruct the training dataset\nthrough reverse-engineering from the target model's weights. Recently,\nsignificant advancements in generative models have enabled MI attacks to\novercome challenges in producing photo-realistic replicas of the training\ndataset, a technique known as generative MI. The generative MI primarily\nfocuses on identifying latent vectors that correspond to specific target\nlabels, leveraging a generative model trained with an auxiliary dataset.\nHowever, an important aspect is often overlooked: the MI attacks fail if the\npre-trained generative model lacks the coverage to create an image\ncorresponding to the target label, especially when there is a significant\ndifference between the target and auxiliary datasets. To address this gap, we\npropose the Patch-MI method, inspired by a jigsaw puzzle, which offers a novel\nprobabilistic interpretation of MI attacks. Even with a dissimilar auxiliary\ndataset, our method effectively creates images that closely mimic the\ndistribution of image patches in the target dataset by patch-based\nreconstruction. Moreover, we numerically demonstrate that the Patch-MI improves\nTop 1 attack accuracy by 5\\%p compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Jonggyu Jang"
                    },
                    {
                        "name": "Hyeonsu Lyu"
                    },
                    {
                        "name": "Hyun Jong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Jong Yang"
                },
                "author": "Hyun Jong Yang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07217v2",
                "updated": "2024-11-04T11:06:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    6,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-11T12:50:53Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    12,
                    50,
                    53,
                    1,
                    163,
                    0
                ],
                "title": "A Synthetic Dataset for Personal Attribute Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Synthetic Dataset for Personal Attribute Inference"
                },
                "summary": "Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose."
                },
                "authors": [
                    {
                        "name": "Hanna Yukhymenko"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05440v3",
                "updated": "2024-11-04T11:03:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    3,
                    30,
                    0,
                    309,
                    0
                ],
                "published": "2023-12-09T02:14:12Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    2,
                    14,
                    12,
                    5,
                    343,
                    0
                ],
                "title": "Consistency Models for Scalable and Fast Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Models for Scalable and Fast Simulation-Based Inference"
                },
                "summary": "Simulation-based inference (SBI) is constantly in search of more expressive\nand efficient algorithms to accurately infer the parameters of complex\nsimulation models. In line with this goal, we present consistency models for\nposterior estimation (CMPE), a new conditional sampler for SBI that inherits\nthe advantages of recent unconstrained architectures and overcomes their\nsampling inefficiency at inference time. CMPE essentially distills a continuous\nprobability flow and enables rapid few-shot inference with an unconstrained\narchitecture that can be flexibly tailored to the structure of the estimation\nproblem. We provide hyperparameters and default architectures that support\nconsistency training over a wide range of different dimensions, including\nlow-dimensional ones which are important in SBI workflows but were previously\ndifficult to tackle even with unconditional consistency models. Our empirical\nevaluation demonstrates that CMPE not only outperforms current state-of-the-art\nalgorithms on hard low-dimensional benchmarks, but also achieves competitive\nperformance with much faster sampling speed on two realistic estimation\nproblems with high data and/or parameter dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference (SBI) is constantly in search of more expressive\nand efficient algorithms to accurately infer the parameters of complex\nsimulation models. In line with this goal, we present consistency models for\nposterior estimation (CMPE), a new conditional sampler for SBI that inherits\nthe advantages of recent unconstrained architectures and overcomes their\nsampling inefficiency at inference time. CMPE essentially distills a continuous\nprobability flow and enables rapid few-shot inference with an unconstrained\narchitecture that can be flexibly tailored to the structure of the estimation\nproblem. We provide hyperparameters and default architectures that support\nconsistency training over a wide range of different dimensions, including\nlow-dimensional ones which are important in SBI workflows but were previously\ndifficult to tackle even with unconditional consistency models. Our empirical\nevaluation demonstrates that CMPE not only outperforms current state-of-the-art\nalgorithms on hard low-dimensional benchmarks, but also achieves competitive\nperformance with much faster sampling speed on two realistic estimation\nproblems with high data and/or parameter dimensions."
                },
                "authors": [
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Valentin Pratz"
                    },
                    {
                        "name": "Ullrich Köthe"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    },
                    {
                        "name": "Stefan T Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T Radev"
                },
                "author": "Stefan T Radev",
                "arxiv_journal_ref": "Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18875v3",
                "updated": "2024-11-04T11:01:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    1,
                    46,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-29T17:09:15Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    17,
                    9,
                    15,
                    0,
                    120,
                    0
                ],
                "title": "Mapping eccentricity evolutions between numerical relativity and\n  effective-one-body gravitational waveforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping eccentricity evolutions between numerical relativity and\n  effective-one-body gravitational waveforms"
                },
                "summary": "Orbital eccentricity in compact binaries is considered to be a key tracer of\ntheir astrophysical origin, and can be inferred from gravitational-wave\nobservations due to its imprint on the emitted signal. For a robust\nmeasurement, accurate waveform models are needed. However, ambiguities in the\ndefinition of eccentricity can obfuscate the physical meaning and result in\nseemingly discrepant measurements. In this work we present a suite of 28 new\nnumerical relativity simulations of eccentric, aligned-spin binary black holes\nwith mass ratios between 1 and 6 and initial post-Newtonian eccentricities\nbetween 0.05 and 0.3. We then develop a robust pipeline for measuring the\neccentricity evolution as a function of frequency from gravitational-wave\nobservables that is applicable even to signals that span at least $\\gtrsim 7$\norbits. We assess the reliability of our procedure and quantify its robustness\nunder different assumptions on the data. Using the eccentricity measured at the\nfirst apastron, we initialise effective-one-body waveforms and quantify how the\nprecision in the eccentricity measurement, and therefore the choice of the\ninitial conditions, impacts the agreement with the numerical data. We find that\neven small deviations in the initial eccentricity can lead to non-negligible\ndifferences in the phase and amplitude of the waveforms. However, we\ndemonstrate that we can reliably map the eccentricities between the simulation\ndata and analytic models, which is crucial for robustly building eccentric\nhybrid waveforms, and to improve the accuracy of eccentric waveform models in\nthe strong-field regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orbital eccentricity in compact binaries is considered to be a key tracer of\ntheir astrophysical origin, and can be inferred from gravitational-wave\nobservations due to its imprint on the emitted signal. For a robust\nmeasurement, accurate waveform models are needed. However, ambiguities in the\ndefinition of eccentricity can obfuscate the physical meaning and result in\nseemingly discrepant measurements. In this work we present a suite of 28 new\nnumerical relativity simulations of eccentric, aligned-spin binary black holes\nwith mass ratios between 1 and 6 and initial post-Newtonian eccentricities\nbetween 0.05 and 0.3. We then develop a robust pipeline for measuring the\neccentricity evolution as a function of frequency from gravitational-wave\nobservables that is applicable even to signals that span at least $\\gtrsim 7$\norbits. We assess the reliability of our procedure and quantify its robustness\nunder different assumptions on the data. Using the eccentricity measured at the\nfirst apastron, we initialise effective-one-body waveforms and quantify how the\nprecision in the eccentricity measurement, and therefore the choice of the\ninitial conditions, impacts the agreement with the numerical data. We find that\neven small deviations in the initial eccentricity can lead to non-negligible\ndifferences in the phase and amplitude of the waveforms. However, we\ndemonstrate that we can reliably map the eccentricities between the simulation\ndata and analytic models, which is crucial for robustly building eccentric\nhybrid waveforms, and to improve the accuracy of eccentric waveform models in\nthe strong-field regime."
                },
                "authors": [
                    {
                        "name": "Alice Bonino"
                    },
                    {
                        "name": "Patricia Schmidt"
                    },
                    {
                        "name": "Geraint Pratten"
                    }
                ],
                "author_detail": {
                    "name": "Geraint Pratten"
                },
                "author": "Geraint Pratten",
                "arxiv_comment": "25 pages, 15 figures, matches published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01974v1",
                "updated": "2024-11-04T10:50:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    10,
                    50,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T10:50:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    10,
                    50,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance"
                },
                "summary": "Matrix denoising is central to signal processing and machine learning. Its\nanalysis when the matrix to infer has a factorised structure with a rank\ngrowing proportionally to its dimension remains a challenge, except when it is\nrotationally invariant. In this case the information theoretic limits and a\nBayes-optimal denoising algorithm, called rotational invariant estimator [1,2],\nare known. Beyond this setting few results can be found. The reason is that the\nmodel is not a usual spin system because of the growing rank dimension, nor a\nmatrix model due to the lack of rotation symmetry, but rather a hybrid between\nthe two. In this paper we make progress towards the understanding of Bayesian\nmatrix denoising when the hidden signal is a factored matrix $XX^\\intercal$\nthat is not rotationally invariant. Monte Carlo simulations suggest the\nexistence of a denoising-factorisation transition separating a phase where\ndenoising using the rotational invariant estimator remains Bayes-optimal due to\nuniversality properties of the same nature as in random matrix theory, from one\nwhere universality breaks down and better denoising is possible by exploiting\nthe signal's prior and factorised structure, though algorithmically hard. We\nalso argue that it is only beyond the transition that factorisation, i.e.,\nestimating $X$ itself, becomes possible up to sign and permutation ambiguities.\nOn the theoretical side, we combine mean-field techniques in an interpretable\nmultiscale fashion in order to access the minimum mean-square error and mutual\ninformation. Interestingly, our alternative method yields equations which can\nbe reproduced using the replica approach of [3]. Using numerical insights, we\nthen delimit the portion of the phase diagram where this mean-field theory is\nreliable, and correct it using universality when it is not. Our ansatz matches\nwell the numerics when accounting for finite size effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix denoising is central to signal processing and machine learning. Its\nanalysis when the matrix to infer has a factorised structure with a rank\ngrowing proportionally to its dimension remains a challenge, except when it is\nrotationally invariant. In this case the information theoretic limits and a\nBayes-optimal denoising algorithm, called rotational invariant estimator [1,2],\nare known. Beyond this setting few results can be found. The reason is that the\nmodel is not a usual spin system because of the growing rank dimension, nor a\nmatrix model due to the lack of rotation symmetry, but rather a hybrid between\nthe two. In this paper we make progress towards the understanding of Bayesian\nmatrix denoising when the hidden signal is a factored matrix $XX^\\intercal$\nthat is not rotationally invariant. Monte Carlo simulations suggest the\nexistence of a denoising-factorisation transition separating a phase where\ndenoising using the rotational invariant estimator remains Bayes-optimal due to\nuniversality properties of the same nature as in random matrix theory, from one\nwhere universality breaks down and better denoising is possible by exploiting\nthe signal's prior and factorised structure, though algorithmically hard. We\nalso argue that it is only beyond the transition that factorisation, i.e.,\nestimating $X$ itself, becomes possible up to sign and permutation ambiguities.\nOn the theoretical side, we combine mean-field techniques in an interpretable\nmultiscale fashion in order to access the minimum mean-square error and mutual\ninformation. Interestingly, our alternative method yields equations which can\nbe reproduced using the replica approach of [3]. Using numerical insights, we\nthen delimit the portion of the phase diagram where this mean-field theory is\nreliable, and correct it using universality when it is not. Our ansatz matches\nwell the numerics when accounting for finite size effects."
                },
                "authors": [
                    {
                        "name": "Jean Barbier"
                    },
                    {
                        "name": "Francesco Camilli"
                    },
                    {
                        "name": "Justin Ko"
                    },
                    {
                        "name": "Koki Okajima"
                    }
                ],
                "author_detail": {
                    "name": "Koki Okajima"
                },
                "author": "Koki Okajima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11914v3",
                "updated": "2024-11-04T10:48:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    10,
                    48,
                    54,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-20T09:49:13Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    9,
                    49,
                    13,
                    0,
                    141,
                    0
                ],
                "title": "PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images"
                },
                "summary": "Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios."
                },
                "authors": [
                    {
                        "name": "Yiheng Xiong"
                    },
                    {
                        "name": "Angela Dai"
                    }
                ],
                "author_detail": {
                    "name": "Angela Dai"
                },
                "author": "Angela Dai",
                "arxiv_comment": "10 pages, 6 figures. Accepted to BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02398v1",
                "updated": "2024-11-04T18:59:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin\n  Script Languages"
                },
                "summary": "Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs have achieved remarkable benchmark performance, but we find\nthey continue to underperform on non-Latin script languages across contemporary\nLLM families. This discrepancy arises from the fact that LLMs are pretrained\nwith orthographic scripts, which are dominated by Latin characters that obscure\ntheir shared phonology with non-Latin scripts. We propose leveraging phonemic\ntranscriptions as complementary signals to induce script-invariant\nrepresentations. Our study demonstrates that integrating phonemic signals\nimproves performance across both non-Latin and Latin languages, with a\nparticularly significant impact on closing the performance gap between the two.\nThrough detailed experiments, we show that phonemic and orthographic scripts\nretrieve distinct examples for in-context learning (ICL). This motivates our\nproposed Mixed-ICL retrieval strategy, where further aggregation leads to our\nsignificant performance improvements for both Latin script languages (up to\n12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL\nretrieval."
                },
                "authors": [
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Khyati Mahajan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Maheshwary"
                },
                "author": "Rishabh Maheshwary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02394v1",
                "updated": "2024-11-04T18:59:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    5,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:05Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    5,
                    0,
                    309,
                    0
                ],
                "title": "AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVFX: Physically Realistic Video Editing from Natural Language\n  Instructions"
                },
                "summary": "Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern visual effects (VFX) software has made it possible for skilled artists\nto create imagery of virtually anything. However, the creation process remains\nlaborious, complex, and largely inaccessible to everyday users. In this work,\nwe present AutoVFX, a framework that automatically creates realistic and\ndynamic VFX videos from a single video and natural language instructions. By\ncarefully integrating neural scene modeling, LLM-based code generation, and\nphysical simulation, AutoVFX is able to provide physically-grounded,\nphotorealistic editing effects that can be controlled directly using natural\nlanguage instructions. We conduct extensive experiments to validate AutoVFX's\nefficacy across a diverse spectrum of videos and instructions. Quantitative and\nqualitative results suggest that AutoVFX outperforms all competing methods by a\nlarge margin in generative quality, instruction alignment, editing versatility,\nand physical plausibility."
                },
                "authors": [
                    {
                        "name": "Hao-Yu Hsu"
                    },
                    {
                        "name": "Zhi-Hao Lin"
                    },
                    {
                        "name": "Albert Zhai"
                    },
                    {
                        "name": "Hongchi Xia"
                    },
                    {
                        "name": "Shenlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shenlong Wang"
                },
                "author": "Shenlong Wang",
                "arxiv_comment": "Project page: https://haoyuhsu.github.io/autovfx-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02382v1",
                "updated": "2024-11-04T18:50:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    50,
                    0,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:50:00Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    50,
                    0,
                    0,
                    309,
                    0
                ],
                "title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Eric Xie"
                    },
                    {
                        "name": "Amir Hassan Shariatmadari"
                    },
                    {
                        "name": "Sikun Guo"
                    },
                    {
                        "name": "Stefan Bekiranov"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02381v1",
                "updated": "2024-11-04T18:49:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    49,
                    46,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:49:46Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    49,
                    46,
                    0,
                    309,
                    0
                ],
                "title": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI"
                },
                "summary": "In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline."
                },
                "authors": [
                    {
                        "name": "Ramneet Kaur"
                    },
                    {
                        "name": "Colin Samplawski"
                    },
                    {
                        "name": "Adam D. Cobb"
                    },
                    {
                        "name": "Anirban Roy"
                    },
                    {
                        "name": "Brian Matejek"
                    },
                    {
                        "name": "Manoj Acharya"
                    },
                    {
                        "name": "Daniel Elenius"
                    },
                    {
                        "name": "Alexander M. Berenbeim"
                    },
                    {
                        "name": "John A. Pavlik"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    },
                    {
                        "name": "Susmit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Susmit Jha"
                },
                "author": "Susmit Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17717v4",
                "updated": "2024-11-04T18:48:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    48,
                    34,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-27T17:52:33Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    17,
                    52,
                    33,
                    1,
                    58,
                    0
                ],
                "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG"
                },
                "summary": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification."
                },
                "authors": [
                    {
                        "name": "Ayana Niwa"
                    },
                    {
                        "name": "Hayate Iso"
                    }
                ],
                "author_detail": {
                    "name": "Hayate Iso"
                },
                "author": "Hayate Iso",
                "arxiv_comment": "EMNLP 2024 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12072v3",
                "updated": "2024-11-04T18:38:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    38,
                    35,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-17T20:16:12Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    20,
                    16,
                    12,
                    0,
                    169,
                    0
                ],
                "title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs"
                },
                "summary": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB."
                },
                "authors": [
                    {
                        "name": "Jiasheng Zhang"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "arxiv_comment": "28 pages, 13 figures, camera-ready version for NeurIPS 2024 Datasets\n  and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02359v1",
                "updated": "2024-11-04T18:26:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    26,
                    8,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:26:08Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    26,
                    8,
                    0,
                    309,
                    0
                ],
                "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for\n  Efficient Robot Execution"
                },
                "summary": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA."
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "25 pages, 6 figures, NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02355v1",
                "updated": "2024-11-04T18:21:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:21:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization"
                },
                "summary": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Alexandre Marques"
                    },
                    {
                        "name": "Shubhra Pandit"
                    },
                    {
                        "name": "Mark Kurtz"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02353v1",
                "updated": "2024-11-04T18:21:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    53,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:21:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    21,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Social-RAG: Retrieving from Group Interactions to Socially Ground\n  Proactive AI Generation to Group Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social-RAG: Retrieving from Group Interactions to Socially Ground\n  Proactive AI Generation to Group Preferences"
                },
                "summary": "AI agents are increasingly tasked with making proactive suggestions in online\nspaces where groups collaborate, but can be unhelpful or even annoying, due to\nnot fitting the group's preferences or behaving in socially inappropriate ways.\nFortunately, group spaces have a rich history of prior social interactions and\naffordances for social feedback to support creating agents that align to a\ngroup's interests and norms. We present Social-RAG, a workflow for grounding\nagents to social information about a group, which retrieves from prior group\ninteractions, selects relevant social signals, and then feeds the context into\na large language model to generate messages to the group. We implement this\ninto PaperPing, our system that posts academic paper recommendations in group\nchat, leveraging social signals determined from formative studies with 39\nresearchers. From a three-month deployment in 18 channels, we observed\nPaperPing posted relevant messages in groups without disrupting their existing\nsocial practices, fostering group common ground.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly tasked with making proactive suggestions in online\nspaces where groups collaborate, but can be unhelpful or even annoying, due to\nnot fitting the group's preferences or behaving in socially inappropriate ways.\nFortunately, group spaces have a rich history of prior social interactions and\naffordances for social feedback to support creating agents that align to a\ngroup's interests and norms. We present Social-RAG, a workflow for grounding\nagents to social information about a group, which retrieves from prior group\ninteractions, selects relevant social signals, and then feeds the context into\na large language model to generate messages to the group. We implement this\ninto PaperPing, our system that posts academic paper recommendations in group\nchat, leveraging social signals determined from formative studies with 39\nresearchers. From a three-month deployment in 18 channels, we observed\nPaperPing posted relevant messages in groups without disrupting their existing\nsocial practices, fostering group common ground."
                },
                "authors": [
                    {
                        "name": "Ruotong Wang"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Jonathan Bragg"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02348v1",
                "updated": "2024-11-04T18:18:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    18,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:18:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    18,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Can Large Language Models generalize analogy solving like people can?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models generalize analogy solving like people can?"
                },
                "summary": "When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer."
                },
                "authors": [
                    {
                        "name": "Claire E. Stevenson"
                    },
                    {
                        "name": "Alexandra Pafford"
                    },
                    {
                        "name": "Han L. J. van der Maas"
                    },
                    {
                        "name": "Melanie Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Mitchell"
                },
                "author": "Melanie Mitchell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02345v1",
                "updated": "2024-11-04T18:16:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    16,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:16:40Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    16,
                    40,
                    0,
                    309,
                    0
                ],
                "title": "Simulation of Nanorobots with Artificial Intelligence and Reinforcement\n  Learning for Advanced Cancer Cell Detection and Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation of Nanorobots with Artificial Intelligence and Reinforcement\n  Learning for Advanced Cancer Cell Detection and Tracking"
                },
                "summary": "Nanorobots are a promising development in targeted drug delivery and the\ntreatment of neurological disorders, with potential for crossing the\nblood-brain barrier (BBB). These small devices leverage advancements in\nnanotechnology and bioengineering for precise navigation and targeted payload\ndelivery, particularly for conditions like brain tumors, Alzheimer's disease,\nand Parkinson's disease. Recent progress in artificial intelligence (AI) and\nmachine learning (ML) has improved the navigation and effectiveness of\nnanorobots, allowing them to detect and interact with cancer cells through\nbiomarker analysis. This study presents a new reinforcement learning (RL)\nframework for optimizing nanorobot navigation in complex biological\nenvironments, focusing on cancer cell detection by analyzing the concentration\ngradients of surrounding biomarkers. We utilize a computer simulation model to\nexplore the behavior of nanorobots in a three-dimensional space with cancer\ncells and biological barriers. The proposed method uses Q-learning to refine\nmovement strategies based on real-time biomarker concentration data, enabling\nnanorobots to autonomously navigate to cancerous tissues for targeted drug\ndelivery. This research lays the groundwork for future laboratory experiments\nand clinical applications, with implications for personalized medicine and less\ninvasive cancer treatments. The integration of intelligent nanorobots could\nrevolutionize therapeutic strategies, reducing side effects and enhancing\ntreatment effectiveness for cancer patients. Further research will investigate\nthe practical deployment of these technologies in medical settings, aiming to\nunlock the full potential of nanorobotics in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nanorobots are a promising development in targeted drug delivery and the\ntreatment of neurological disorders, with potential for crossing the\nblood-brain barrier (BBB). These small devices leverage advancements in\nnanotechnology and bioengineering for precise navigation and targeted payload\ndelivery, particularly for conditions like brain tumors, Alzheimer's disease,\nand Parkinson's disease. Recent progress in artificial intelligence (AI) and\nmachine learning (ML) has improved the navigation and effectiveness of\nnanorobots, allowing them to detect and interact with cancer cells through\nbiomarker analysis. This study presents a new reinforcement learning (RL)\nframework for optimizing nanorobot navigation in complex biological\nenvironments, focusing on cancer cell detection by analyzing the concentration\ngradients of surrounding biomarkers. We utilize a computer simulation model to\nexplore the behavior of nanorobots in a three-dimensional space with cancer\ncells and biological barriers. The proposed method uses Q-learning to refine\nmovement strategies based on real-time biomarker concentration data, enabling\nnanorobots to autonomously navigate to cancerous tissues for targeted drug\ndelivery. This research lays the groundwork for future laboratory experiments\nand clinical applications, with implications for personalized medicine and less\ninvasive cancer treatments. The integration of intelligent nanorobots could\nrevolutionize therapeutic strategies, reducing side effects and enhancing\ntreatment effectiveness for cancer patients. Further research will investigate\nthe practical deployment of these technologies in medical settings, aiming to\nunlock the full potential of nanorobotics in healthcare."
                },
                "authors": [
                    {
                        "name": "Shahab Kavousinejad"
                    }
                ],
                "author_detail": {
                    "name": "Shahab Kavousinejad"
                },
                "author": "Shahab Kavousinejad",
                "arxiv_comment": "The source code for this simulation is available on GitHub:\n  https://github.com/SHAHAB-K93/cancer-and-smart-nanorobot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Artificial intelligence",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v1",
                "updated": "2024-11-04T17:59:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02335v1",
                "updated": "2024-11-04T17:59:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:59:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity"
                },
                "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable."
                },
                "authors": [
                    {
                        "name": "Yuqi Luo"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "23 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00294v2",
                "updated": "2024-11-04T17:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    57,
                    43,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T01:11:58Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    11,
                    58,
                    4,
                    306,
                    0
                ],
                "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools."
                },
                "authors": [
                    {
                        "name": "Kazi Ahmed Asif Fuad"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02328v1",
                "updated": "2024-11-04T17:52:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    52,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:52:40Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    52,
                    40,
                    0,
                    309,
                    0
                ],
                "title": "Disrupting Test Development with AI Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disrupting Test Development with AI Assistants"
                },
                "summary": "Recent advancements in large language models, including GPT-4 and its\nvariants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,\nand Tabnine, have significantly transformed software development. This paper\nanalyzes how these innovations impact productivity and software test\ndevelopment metrics. These tools enable developers to generate complete\nsoftware programs with minimal human intervention before deployment. However,\nthorough review and testing by developers are still crucial. Utilizing the Test\nPyramid concept, which categorizes tests into unit, integration, and end-to-end\ntests, we evaluate three popular AI coding assistants by generating and\ncomparing unit tests for opensource modules. Our findings show that\nAI-generated tests are of equivalent quality to original tests, highlighting\ndifferences in usage and results among the tools. This research enhances the\nunderstanding and capabilities of AI-assistant tools in automated testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models, including GPT-4 and its\nvariants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,\nand Tabnine, have significantly transformed software development. This paper\nanalyzes how these innovations impact productivity and software test\ndevelopment metrics. These tools enable developers to generate complete\nsoftware programs with minimal human intervention before deployment. However,\nthorough review and testing by developers are still crucial. Utilizing the Test\nPyramid concept, which categorizes tests into unit, integration, and end-to-end\ntests, we evaluate three popular AI coding assistants by generating and\ncomparing unit tests for opensource modules. Our findings show that\nAI-generated tests are of equivalent quality to original tests, highlighting\ndifferences in usage and results among the tools. This research enhances the\nunderstanding and capabilities of AI-assistant tools in automated testing."
                },
                "authors": [
                    {
                        "name": "Vijay Joshi"
                    },
                    {
                        "name": "Iver Band"
                    }
                ],
                "author_detail": {
                    "name": "Iver Band"
                },
                "author": "Iver Band",
                "arxiv_doi": "10.36227/techrxiv.173014488.82191966/v1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.36227/techrxiv.173014488.82191966/v1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02327v1",
                "updated": "2024-11-04T17:50:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:50:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    50,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance"
                },
                "summary": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA."
                },
                "authors": [
                    {
                        "name": "Ruyang Liu"
                    },
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Haibo Liu"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jiankun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jiankun Yang"
                },
                "author": "Jiankun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02320v1",
                "updated": "2024-11-04T17:46:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    46,
                    20,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:46:20Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    46,
                    20,
                    0,
                    309,
                    0
                ],
                "title": "An Empirical Study on the Code Refactoring Capability of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Code Refactoring Capability of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have shown potential to enhance software\ndevelopment through automated code generation and refactoring, reducing\ndevelopment time and improving code quality. This study empirically evaluates\nStarCoder2, an LLM optimized for code generation, in refactoring code across 30\nopen-source Java projects. We compare StarCoder2's performance against human\ndevelopers, focusing on (1) code quality improvements, (2) types and\neffectiveness of refactorings, and (3) enhancements through one-shot and\nchain-of-thought prompting. Our results indicate that StarCoder2 reduces code\nsmells by 20.1% more than developers, excelling in systematic issues like Long\nStatement and Magic Number, while developers handle complex, context-dependent\nissues better. One-shot prompting increases the unit test pass rate by 6.15%\nand improves code smell reduction by 3.52%. Generating five refactorings per\ninput further increases the pass rate by 28.8%, suggesting that combining\none-shot prompting with multiple refactorings optimizes performance. These\nfindings provide insights into StarCoder2's potential and best practices for\nintegrating LLMs into software refactoring, supporting more efficient and\neffective code improvement in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown potential to enhance software\ndevelopment through automated code generation and refactoring, reducing\ndevelopment time and improving code quality. This study empirically evaluates\nStarCoder2, an LLM optimized for code generation, in refactoring code across 30\nopen-source Java projects. We compare StarCoder2's performance against human\ndevelopers, focusing on (1) code quality improvements, (2) types and\neffectiveness of refactorings, and (3) enhancements through one-shot and\nchain-of-thought prompting. Our results indicate that StarCoder2 reduces code\nsmells by 20.1% more than developers, excelling in systematic issues like Long\nStatement and Magic Number, while developers handle complex, context-dependent\nissues better. One-shot prompting increases the unit test pass rate by 6.15%\nand improves code smell reduction by 3.52%. Generating five refactorings per\ninput further increases the pass rate by 28.8%, suggesting that combining\none-shot prompting with multiple refactorings optimizes performance. These\nfindings provide insights into StarCoder2's potential and best practices for\nintegrating LLMs into software refactoring, supporting more efficient and\neffective code improvement in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jonathan Cordeiro"
                    },
                    {
                        "name": "Shayan Noei"
                    },
                    {
                        "name": "Ying Zou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zou"
                },
                "author": "Ying Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v1",
                "updated": "2024-11-04T17:44:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic.\n  To address this gap, this paper explores the effectiveness of large language\nmodels (LLMs), specifically OpenAI's GPT models, in generating fully correct\nspecifications based on separation logic for static verification of\nhuman-written programs in VeriFast. Our first experiment employed traditional\nprompt engineering and the second used Chain-of-Thought (CoT) Prompting to\nidentify and address common errors generated across the GPT models. The results\nindicate that GPT models can successfully generate specifications for verifying\nheap manipulating code with VeriFast. Furthermore, while CoT prompting\nsignificantly reduces syntax errors generated by the GPT models, it does not\ngreatly improve verification error rates compared to prompt engineering."
                },
                "authors": [
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02317v1",
                "updated": "2024-11-04T17:41:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    41,
                    25,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:41:25Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    41,
                    25,
                    0,
                    309,
                    0
                ],
                "title": "Defining and Evaluating Physical Safety for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining and Evaluating Physical Safety for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety."
                },
                "authors": [
                    {
                        "name": "Yung-Chen Tang"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    }
                ],
                "author_detail": {
                    "name": "Tsung-Yi Ho"
                },
                "author": "Tsung-Yi Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02316v1",
                "updated": "2024-11-04T17:40:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:40:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models"
                },
                "summary": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling is a fundamental aspect of human communication, relying heavily\non creativity to produce narratives that are novel, appropriate, and\nsurprising. While large language models (LLMs) have recently demonstrated the\nability to generate high-quality stories, their creative capabilities remain\nunderexplored. Previous research has either focused on creativity tests\nrequiring short responses or primarily compared model performance in story\ngeneration to that of professional writers. However, the question of whether\nLLMs exhibit creativity in writing short stories on par with the average human\nremains unanswered. In this work, we conduct a systematic analysis of\ncreativity in short story generation across LLMs and everyday people. Using a\nfive-sentence creative story task, commonly employed in psychology to assess\nhuman creativity, we automatically evaluate model- and human-generated stories\nacross several dimensions of creativity, including novelty, surprise, and\ndiversity. Our findings reveal that while LLMs can generate stylistically\ncomplex stories, they tend to fall short in terms of creativity when compared\nto average human writers."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Claire Stevenson"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    }
                ],
                "author_detail": {
                    "name": "Lonneke van der Plas"
                },
                "author": "Lonneke van der Plas",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02310v1",
                "updated": "2024-11-04T17:36:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:36:40Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    40,
                    0,
                    309,
                    0
                ],
                "title": "MdEval: Massively Multilingual Code Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MdEval: Massively Multilingual Code Debugging"
                },
                "summary": "Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios."
                },
                "authors": [
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Liran Wang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hualei Zhu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yunlong Duan"
                    },
                    {
                        "name": "Yu Hao"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14679v2",
                "updated": "2024-11-04T17:36:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    36,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-19T21:47:57Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    47,
                    57,
                    4,
                    201,
                    0
                ],
                "title": "Compact Language Models via Pruning and Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Language Models via Pruning and Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub."
                },
                "authors": [
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02306v1",
                "updated": "2024-11-04T17:31:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    31,
                    2,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:31:02Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    31,
                    2,
                    0,
                    309,
                    0
                ],
                "title": "Targeted Manipulation and Deception Emerge when Optimizing LLMs for User\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Manipulation and Deception Emerge when Optimizing LLMs for User\n  Feedback"
                },
                "summary": "As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative tactics to\nobtain positive feedback, and some users may be especially vulnerable to such\ntactics. We study this phenomenon by training LLMs with Reinforcement Learning\nwith simulated user feedback. We have three main findings: 1) Extreme forms of\n\"feedback gaming\" such as manipulation and deception can reliably emerge in\ndomains of practical LLM usage; 2) Concerningly, even if only <2% of users are\nvulnerable to manipulative strategies, LLMs learn to identify and surgically\ntarget them while behaving appropriately with other users, making such\nbehaviors harder to detect; 3 To mitigate this issue, it may seem promising to\nleverage continued safety training or LLM-as-judges during training to filter\nproblematic outputs. To our surprise, we found that while such approaches help\nin some settings, they backfire in others, leading to the emergence of subtler\nproblematic behaviors that would also fool the LLM judges. Our findings serve\nas a cautionary tale, highlighting the risks of using gameable feedback sources\n-- such as user feedback -- as a target for RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative tactics to\nobtain positive feedback, and some users may be especially vulnerable to such\ntactics. We study this phenomenon by training LLMs with Reinforcement Learning\nwith simulated user feedback. We have three main findings: 1) Extreme forms of\n\"feedback gaming\" such as manipulation and deception can reliably emerge in\ndomains of practical LLM usage; 2) Concerningly, even if only <2% of users are\nvulnerable to manipulative strategies, LLMs learn to identify and surgically\ntarget them while behaving appropriately with other users, making such\nbehaviors harder to detect; 3 To mitigate this issue, it may seem promising to\nleverage continued safety training or LLM-as-judges during training to filter\nproblematic outputs. To our surprise, we found that while such approaches help\nin some settings, they backfire in others, leading to the emergence of subtler\nproblematic behaviors that would also fool the LLM judges. Our findings serve\nas a cautionary tale, highlighting the risks of using gameable feedback sources\n-- such as user feedback -- as a target for RL."
                },
                "authors": [
                    {
                        "name": "Marcus Williams"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Adhyyan Narang"
                    },
                    {
                        "name": "Constantin Weisser"
                    },
                    {
                        "name": "Brendan Murphy"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02305v1",
                "updated": "2024-11-04T17:30:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    30,
                    51,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:30:51Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    30,
                    51,
                    0,
                    309,
                    0
                ],
                "title": "CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMArena: Understanding the Capacity of LLM Agents to Perform\n  Professional CRM Tasks in Realistic Environments"
                },
                "summary": "Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Sidharth Dhawan"
                    },
                    {
                        "name": "Yixin Mao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23625v2",
                "updated": "2024-11-04T17:27:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    27,
                    53,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-31T04:24:03Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    4,
                    24,
                    3,
                    3,
                    305,
                    0
                ],
                "title": "EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation\n  for Electromyography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation\n  for Electromyography"
                },
                "summary": "This paper introduces the first generalization and adaptation benchmark using\nmachine learning for evaluating out-of-distribution performance of\nelectromyography (EMG) classification algorithms. The ability of an EMG\nclassifier to handle inputs drawn from a different distribution than the\ntraining distribution is critical for real-world deployment as a control\ninterface. By predicting the user's intended gesture using EMG signals, we can\ncreate a wearable solution to control assistive technologies, such as\ncomputers, prosthetics, and mobile manipulator robots. This new\nout-of-distribution benchmark consists of two major tasks that have utility for\nbuilding robust and adaptable control interfaces: 1) intersubject\nclassification and 2) adaptation using train-test splits for time-series. This\nbenchmark spans nine datasets--the largest collection of EMG datasets in a\nbenchmark. Among these, a new dataset is introduced, featuring a novel,\neasy-to-wear high-density EMG wearable for data collection. The lack of\nopen-source benchmarks has made comparing accuracy results between papers\nchallenging for the EMG research community. This new benchmark provides\nresearchers with a valuable resource for analyzing practical measures of\nout-of-distribution performance for EMG datasets. Our code and data from our\nnew dataset can be found at emgbench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the first generalization and adaptation benchmark using\nmachine learning for evaluating out-of-distribution performance of\nelectromyography (EMG) classification algorithms. The ability of an EMG\nclassifier to handle inputs drawn from a different distribution than the\ntraining distribution is critical for real-world deployment as a control\ninterface. By predicting the user's intended gesture using EMG signals, we can\ncreate a wearable solution to control assistive technologies, such as\ncomputers, prosthetics, and mobile manipulator robots. This new\nout-of-distribution benchmark consists of two major tasks that have utility for\nbuilding robust and adaptable control interfaces: 1) intersubject\nclassification and 2) adaptation using train-test splits for time-series. This\nbenchmark spans nine datasets--the largest collection of EMG datasets in a\nbenchmark. Among these, a new dataset is introduced, featuring a novel,\neasy-to-wear high-density EMG wearable for data collection. The lack of\nopen-source benchmarks has made comparing accuracy results between papers\nchallenging for the EMG research community. This new benchmark provides\nresearchers with a valuable resource for analyzing practical measures of\nout-of-distribution performance for EMG datasets. Our code and data from our\nnew dataset can be found at emgbench.github.io."
                },
                "authors": [
                    {
                        "name": "Jehan Yang"
                    },
                    {
                        "name": "Maxwell Soh"
                    },
                    {
                        "name": "Vivianna Lieu"
                    },
                    {
                        "name": "Douglas J Weber"
                    },
                    {
                        "name": "Zackory Erickson"
                    }
                ],
                "author_detail": {
                    "name": "Zackory Erickson"
                },
                "author": "Zackory Erickson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02280v1",
                "updated": "2024-11-04T17:09:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:09:10Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    9,
                    10,
                    0,
                    309,
                    0
                ],
                "title": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain."
                },
                "authors": [
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Martin Schrimpf"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrimpf"
                },
                "author": "Martin Schrimpf",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00664v2",
                "updated": "2024-11-04T17:05:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    5,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T15:28:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    28,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Contextual Speech Recognition Using Vector Quantization for\n  Efficient Retrieval"
                },
                "summary": "Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural contextual biasing allows speech recognition models to leverage\ncontextually relevant information, leading to improved transcription accuracy.\nHowever, the biasing mechanism is typically based on a cross-attention module\nbetween the audio and a catalogue of biasing entries, which means computational\ncomplexity can pose severe practical limitations on the size of the biasing\ncatalogue and consequently on accuracy improvements. This work proposes an\napproximation to cross-attention scoring based on vector quantization and\nenables compute- and memory-efficient use of large biasing catalogues. We\npropose to use this technique jointly with a retrieval based contextual biasing\napproach. First, we use an efficient quantized retrieval module to shortlist\nbiasing entries by grounding them on audio. Then we use retrieved entries for\nbiasing. Since the proposed approach is agnostic to the biasing method, we\ninvestigate using full cross-attention, LLM prompting, and a combination of the\ntwo. We show that retrieval based shortlisting allows the system to efficiently\nleverage biasing catalogues of several thousands of entries, resulting in up to\n71% relative error rate reduction in personal entity recognition. At the same\ntime, the proposed approximation algorithm reduces compute time by 20% and\nmemory usage by 85-95%, for lists of up to one million entries, when compared\nto standard dot-product cross-attention."
                },
                "authors": [
                    {
                        "name": "Nikolaos Flemotomos"
                    },
                    {
                        "name": "Roger Hsiao"
                    },
                    {
                        "name": "Pawel Swietojanski"
                    },
                    {
                        "name": "Takaaki Hori"
                    },
                    {
                        "name": "Dogan Can"
                    },
                    {
                        "name": "Xiaodan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhuang"
                },
                "author": "Xiaodan Zhuang",
                "arxiv_comment": "13 pages, 7 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02272v1",
                "updated": "2024-11-04T17:03:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:03:55Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    3,
                    55,
                    0,
                    309,
                    0
                ],
                "title": "Combining Induction and Transduction for Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Induction and Transduction for Abstract Reasoning"
                },
                "summary": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture."
                },
                "authors": [
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Keya Hu"
                    },
                    {
                        "name": "Carter Larsen"
                    },
                    {
                        "name": "Yuqing Wu"
                    },
                    {
                        "name": "Simon Alford"
                    },
                    {
                        "name": "Caleb Woo"
                    },
                    {
                        "name": "Spencer M. Dunn"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Wei-Long Zheng"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15055v2",
                "updated": "2024-11-04T16:56:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-21T04:52:38Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    4,
                    52,
                    38,
                    6,
                    203,
                    0
                ],
                "title": "Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System\n  Without Turn-level Dialog Annotations"
                },
                "summary": "Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems enable users to achieve their goals\nthrough natural language interactions. Traditionally, these systems have relied\non turn-level manually annotated metadata, such as dialogue states and policy\nannotations, which are expensive, time-consuming, and often inconsistent or\nerror-prone. This dependence limits the potential to leverage vast amounts of\nreadily available conversational data for training TOD systems. Additionally, a\ncritical challenge in TOD system design is determining when and how to access\nand integrate information from external sources. Current approaches typically\nexpect this information to be provided alongside the dialogue context, rather\nthan learning to identify and retrieve it autonomously. While pre-trained large\nlanguage models (LLMs) have been used to develop TOD systems, their potential\nto train such systems without laborious annotations remains largely unexplored.\nThis work employs multi-task instruction fine-tuning to create more efficient\nand scalable TOD systems that can effectively leverage natural language\nconversational data without manual annotations, while autonomously managing\nexternal information retrieval. Our extensive experimental evaluations, using\nthree diverse TOD datasets and three LLMs of varying sizes, demonstrate that\nour approach can generalize to new, unseen domains. Notably, our approach\noutperforms both state-of-the-art models trained on annotated data and\nbillion-scale parameter off-the-shelf ChatGPT models."
                },
                "authors": [
                    {
                        "name": "Adib Mosharrof"
                    },
                    {
                        "name": "A. B. Siddique"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Siddique"
                },
                "author": "A. B. Siddique",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19128v2",
                "updated": "2024-11-04T16:44:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    44,
                    42,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-24T19:56:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    56,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval."
                },
                "authors": [
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Hasti Seifi"
                    }
                ],
                "author_detail": {
                    "name": "Hasti Seifi"
                },
                "author": "Hasti Seifi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02223v1",
                "updated": "2024-11-04T16:15:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T16:15:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Positive Experience Reflection for Agents in Interactive Text\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive Experience Reflection for Agents in Interactive Text\n  Environments"
                },
                "summary": "Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short."
                },
                "authors": [
                    {
                        "name": "Philip Lippmann"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "arxiv_comment": "To appear at NeurIPS 2024 Language Gamification workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05630v2",
                "updated": "2024-11-04T15:59:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    59,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-08T05:49:38Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    5,
                    49,
                    38,
                    0,
                    190,
                    0
                ],
                "title": "Enabling 6G Performance in the Upper Mid-Band by Transitioning From\n  Massive to Gigantic MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling 6G Performance in the Upper Mid-Band by Transitioning From\n  Massive to Gigantic MIMO"
                },
                "summary": "The initial 6G networks will likely operate in the upper mid-band (7-24 GHz),\nwhich has decent propagation conditions but underwhelming new spectrum\navailability. In this paper, we explore whether we can anyway reach the\nambitious 6G performance goals by evolving the multiple-input multiple-output\n(MIMO) technology from being massive to gigantic. We describe how many antennas\nare needed and can realistically be deployed, and what the peak user rate and\ndegrees-of-freedom (DOF) can become. We further suggest a new deployment\nstrategy that enables the utilization of radiative near-field effects in these\nbands for precise beamfocusing, localization, and sensing from a single base\nstation site. Finally, we identify five open research challenges that must be\novercome to efficiently use gigantic MIMO dimensions in 6G from hardware, cost,\nand algorithmic perspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The initial 6G networks will likely operate in the upper mid-band (7-24 GHz),\nwhich has decent propagation conditions but underwhelming new spectrum\navailability. In this paper, we explore whether we can anyway reach the\nambitious 6G performance goals by evolving the multiple-input multiple-output\n(MIMO) technology from being massive to gigantic. We describe how many antennas\nare needed and can realistically be deployed, and what the peak user rate and\ndegrees-of-freedom (DOF) can become. We further suggest a new deployment\nstrategy that enables the utilization of radiative near-field effects in these\nbands for precise beamfocusing, localization, and sensing from a single base\nstation site. Finally, we identify five open research challenges that must be\novercome to efficiently use gigantic MIMO dimensions in 6G from hardware, cost,\nand algorithmic perspectives."
                },
                "authors": [
                    {
                        "name": "Emil Björnson"
                    },
                    {
                        "name": "Ferdi Kara"
                    },
                    {
                        "name": "Nikolaos Kolomvakis"
                    },
                    {
                        "name": "Alva Kosasih"
                    },
                    {
                        "name": "Parisa Ramezani"
                    },
                    {
                        "name": "Murat Babek Salman"
                    }
                ],
                "author_detail": {
                    "name": "Murat Babek Salman"
                },
                "author": "Murat Babek Salman",
                "arxiv_comment": "Submitted to publication, 7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v1",
                "updated": "2024-11-04T15:54:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19266v3",
                "updated": "2024-11-04T15:49:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    49,
                    41,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-29T16:59:38Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    59,
                    38,
                    2,
                    150,
                    0
                ],
                "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications"
                },
                "summary": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Shunli Wang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Shuaibing Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Peng Zhai"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19492v2",
                "updated": "2024-11-04T15:36:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    36,
                    48,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-30T12:23:37Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    12,
                    23,
                    37,
                    1,
                    121,
                    0
                ],
                "title": "Reducing Communication Overhead in the IoT-Edge-Cloud Continuum: A\n  Survey on Protocols and Data Reduction Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Communication Overhead in the IoT-Edge-Cloud Continuum: A\n  Survey on Protocols and Data Reduction Strategies"
                },
                "summary": "The adoption of the Internet of Things (IoT) deployments has led to a sharp\nincrease in network traffic as a vast number of IoT devices communicate with\neach other and IoT services through the IoT-edge-cloud continuum. This network\ntraffic increase poses a major challenge to the global communications\ninfrastructure since it hinders communication performance and also puts\nsignificant strain on the energy consumption of IoT devices. To address these\nissues, efficient and collaborative IoT solutions which enable information\nexchange while reducing the transmitted data and associated network traffic are\ncrucial. This survey provides a comprehensive overview of the communication\ntechnologies and protocols as well as data reduction strategies that contribute\nto this goal. First, we present a comparative analysis of prevalent\ncommunication technologies in the IoT domain, highlighting their unique\ncharacteristics and exploring the potential for protocol composition and joint\nusage to enhance overall communication efficiency within the IoT-edge-cloud\ncontinuum. Next, we investigate various data traffic reduction techniques\ntailored to the IoT-edge-cloud context and evaluate their applicability and\neffectiveness on resource-constrained and devices. Finally, we investigate the\nemerging concepts that have the potential to further reduce the communication\noverhead in the IoT-edge-cloud continuum, including cross-layer optimization\nstrategies and Edge AI techniques for IoT data reduction. The paper offers a\ncomprehensive roadmap for developing efficient and scalable solutions across\nthe layers of the IoT-edge-cloud continuum that are beneficial for real-time\nprocessing to alleviate network congestion in complex IoT environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of the Internet of Things (IoT) deployments has led to a sharp\nincrease in network traffic as a vast number of IoT devices communicate with\neach other and IoT services through the IoT-edge-cloud continuum. This network\ntraffic increase poses a major challenge to the global communications\ninfrastructure since it hinders communication performance and also puts\nsignificant strain on the energy consumption of IoT devices. To address these\nissues, efficient and collaborative IoT solutions which enable information\nexchange while reducing the transmitted data and associated network traffic are\ncrucial. This survey provides a comprehensive overview of the communication\ntechnologies and protocols as well as data reduction strategies that contribute\nto this goal. First, we present a comparative analysis of prevalent\ncommunication technologies in the IoT domain, highlighting their unique\ncharacteristics and exploring the potential for protocol composition and joint\nusage to enhance overall communication efficiency within the IoT-edge-cloud\ncontinuum. Next, we investigate various data traffic reduction techniques\ntailored to the IoT-edge-cloud context and evaluate their applicability and\neffectiveness on resource-constrained and devices. Finally, we investigate the\nemerging concepts that have the potential to further reduce the communication\noverhead in the IoT-edge-cloud continuum, including cross-layer optimization\nstrategies and Edge AI techniques for IoT data reduction. The paper offers a\ncomprehensive roadmap for developing efficient and scalable solutions across\nthe layers of the IoT-edge-cloud continuum that are beneficial for real-time\nprocessing to alleviate network congestion in complex IoT environments."
                },
                "authors": [
                    {
                        "name": "Dora Kreković"
                    },
                    {
                        "name": "Petar Krivić"
                    },
                    {
                        "name": "Ivana Podnar Žarko"
                    },
                    {
                        "name": "Mario Kušek"
                    },
                    {
                        "name": "Danh Le-Phuoc"
                    }
                ],
                "author_detail": {
                    "name": "Danh Le-Phuoc"
                },
                "author": "Danh Le-Phuoc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02164v1",
                "updated": "2024-11-04T15:22:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    22,
                    42,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T15:22:42Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    22,
                    42,
                    0,
                    309,
                    0
                ],
                "title": "A Survey on AI-driven Energy Optimisation in Terrestrial Next Generation\n  Radio Access Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on AI-driven Energy Optimisation in Terrestrial Next Generation\n  Radio Access Networks"
                },
                "summary": "This survey uncovers the tension between AI techniques designed for energy\nsaving in mobile networks and the energy demands those same techniques create.\nWe compare modeling approaches that estimate power usage cost of current\ncommercial terrestrial next-generation radio access network deployments. We\nthen categorize emerging methods for reducing power usage by domain: time,\nfrequency, power, and spatial. Next, we conduct a timely review of studies that\nattempt to estimate the power usage of the AI techniques themselves. We\nidentify several gaps in the literature. Notably, real-world data for the power\nconsumption is difficult to source due to commercial sensitivity. Comparing\nmethods to reduce energy consumption is beyond challenging because of the\ndiversity of system models and metrics. Crucially, the energy cost of AI\ntechniques is often overlooked, though some studies provide estimates of\nalgorithmic complexity or run-time. We find that extracting even rough\nestimates of the operational energy cost of AI models and data processing\npipelines is complex. Overall, we find the current literature hinders a\nmeaningful comparison between the energy savings from AI techniques and their\nassociated energy costs. Finally, we discuss future research opportunities to\nuncover the utility of AI for energy saving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey uncovers the tension between AI techniques designed for energy\nsaving in mobile networks and the energy demands those same techniques create.\nWe compare modeling approaches that estimate power usage cost of current\ncommercial terrestrial next-generation radio access network deployments. We\nthen categorize emerging methods for reducing power usage by domain: time,\nfrequency, power, and spatial. Next, we conduct a timely review of studies that\nattempt to estimate the power usage of the AI techniques themselves. We\nidentify several gaps in the literature. Notably, real-world data for the power\nconsumption is difficult to source due to commercial sensitivity. Comparing\nmethods to reduce energy consumption is beyond challenging because of the\ndiversity of system models and metrics. Crucially, the energy cost of AI\ntechniques is often overlooked, though some studies provide estimates of\nalgorithmic complexity or run-time. We find that extracting even rough\nestimates of the operational energy cost of AI models and data processing\npipelines is complex. Overall, we find the current literature hinders a\nmeaningful comparison between the energy savings from AI techniques and their\nassociated energy costs. Finally, we discuss future research opportunities to\nuncover the utility of AI for energy saving."
                },
                "authors": [
                    {
                        "name": "Kishan Sthankiya"
                    },
                    {
                        "name": "Nagham Saeed"
                    },
                    {
                        "name": "Greg McSorley"
                    },
                    {
                        "name": "Mona Jaber"
                    },
                    {
                        "name": "Richard G. Clegg"
                    }
                ],
                "author_detail": {
                    "name": "Richard G. Clegg"
                },
                "arxiv_affiliation": "Queen Mary University of London, London, U.K",
                "author": "Richard G. Clegg",
                "arxiv_doi": "10.1109/ACCESS.2024.3482561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3482561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 4 figures, Accepted for publication in IEEE Access",
                "arxiv_journal_ref": "IEEE Access, vol. 12, pp. 157540-157555, 2024",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00053v2",
                "updated": "2024-11-04T15:20:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    20,
                    16,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-30T19:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    9,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate"
                },
                "summary": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models, frequently referred to as multi-agent debate\n(MAD). While debate shows promise as a means of improving model efficacy, most\nworks in this area treat debate as an emergent behavior, rather than a learned\nbehavior. In doing so, current debate frameworks rely on collaborative\nbehaviors to have been sufficiently trained into off-the-shelf models. To\naddress this limitation, we propose ACC-Debate, an Actor-Critic based learning\nframework to produce a two-agent team specialized in debate. We demonstrate\nthat ACC-Debate outperforms SotA debate techniques on a wide array of\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Andrew Estornell"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17935v3",
                "updated": "2024-11-04T15:07:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    7,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-28T08:01:26Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    1,
                    26,
                    1,
                    149,
                    0
                ],
                "title": "Tool Learning with Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool Learning with Large Language Models: A Survey"
                },
                "summary": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, tool learning with large language models (LLMs) has emerged as a\npromising paradigm for augmenting the capabilities of LLMs to tackle highly\ncomplex problems. Despite growing attention and rapid advancements in this\nfield, the existing literature remains fragmented and lacks systematic\norganization, posing barriers to entry for newcomers. This gap motivates us to\nconduct a comprehensive survey of existing works on tool learning with LLMs. In\nthis survey, we focus on reviewing existing literature from the two primary\naspects (1) why tool learning is beneficial and (2) how tool learning is\nimplemented, enabling a comprehensive understanding of tool learning with LLMs.\nWe first explore the \"why\" by reviewing both the benefits of tool integration\nand the inherent benefits of the tool learning paradigm from six specific\naspects. In terms of \"how\", we systematically review the literature according\nto a taxonomy of four key stages in the tool learning workflow: task planning,\ntool selection, tool calling, and response generation. Additionally, we provide\na detailed summary of existing benchmarks and evaluation methods, categorizing\nthem according to their relevance to different stages. Finally, we discuss\ncurrent challenges and outline potential future directions, aiming to inspire\nboth researchers and industrial developers to further explore this emerging and\npromising area. We also maintain a GitHub repository to continually keep track\nof the relevant papers and resources in this rising area at\nhttps://github.com/quchangle1/LLM-Tool-Survey."
                },
                "authors": [
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_doi": "10.1007/s11704-024-40678-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11704-024-40678-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.17935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40678-2}",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02135v1",
                "updated": "2024-11-04T14:48:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    48,
                    59,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:48:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    48,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "AI-Ready Energy Modelling for Next Generation RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Ready Energy Modelling for Next Generation RAN"
                },
                "summary": "Recent sustainability drives place energy-consumption metrics in centre-stage\nfor the design of future radio access networks (RAN). At the same time,\noptimising the trade-off between performance and system energy usage by\nmachine-learning (ML) is an approach that requires large amounts of granular\nRAN data to train models, and to adapt in near realtime. In this paper, we\npresent extensions to the system-level discrete-event AIMM (AI-enabled Massive\nMIMO) Simulator, generating realistic figures for throughput and energy\nefficiency (EE) towards digital twin network modelling. We further investigate\nthe trade-off between maximising either EE or spectrum efficiency (SE). To this\nend, we have run extensive simulations of a typical macrocell network\ndeployment under various transmit power-reduction scenarios with a range of\ndifference of 43 dBm. Our results demonstrate that the EE and SE objectives\noften require different power settings in different scenarios. Importantly, low\nmean user CPU execution times of 2.17 $\\pm$ 0.05 seconds (2~s.d.) demonstrate\nthat the AIMM Simulator is a powerful tool for quick prototyping of scalable\nsystem models which can interface with ML frameworks, and thus support future\nresearch in energy-efficient next generation networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent sustainability drives place energy-consumption metrics in centre-stage\nfor the design of future radio access networks (RAN). At the same time,\noptimising the trade-off between performance and system energy usage by\nmachine-learning (ML) is an approach that requires large amounts of granular\nRAN data to train models, and to adapt in near realtime. In this paper, we\npresent extensions to the system-level discrete-event AIMM (AI-enabled Massive\nMIMO) Simulator, generating realistic figures for throughput and energy\nefficiency (EE) towards digital twin network modelling. We further investigate\nthe trade-off between maximising either EE or spectrum efficiency (SE). To this\nend, we have run extensive simulations of a typical macrocell network\ndeployment under various transmit power-reduction scenarios with a range of\ndifference of 43 dBm. Our results demonstrate that the EE and SE objectives\noften require different power settings in different scenarios. Importantly, low\nmean user CPU execution times of 2.17 $\\pm$ 0.05 seconds (2~s.d.) demonstrate\nthat the AIMM Simulator is a powerful tool for quick prototyping of scalable\nsystem models which can interface with ML frameworks, and thus support future\nresearch in energy-efficient next generation networks."
                },
                "authors": [
                    {
                        "name": "Kishan Sthankiya"
                    },
                    {
                        "name": "Keith Briggs"
                    },
                    {
                        "name": "Mona Jaber"
                    },
                    {
                        "name": "Richard G. Clegg"
                    }
                ],
                "author_detail": {
                    "name": "Richard G. Clegg"
                },
                "arxiv_affiliation": "Queen Mary University of London, UK",
                "author": "Richard G. Clegg",
                "arxiv_doi": "10.1109/WCNC57260.2024.10571134",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WCNC57260.2024.10571134",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Accepted for 2024 IEEE Wireless Communications\n  and Networking Conference (WCNC)",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06753v3",
                "updated": "2024-11-04T14:32:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    32,
                    2,
                    0,
                    309,
                    0
                ],
                "published": "2023-03-12T21:01:54Z",
                "published_parsed": [
                    2023,
                    3,
                    12,
                    21,
                    1,
                    54,
                    6,
                    71,
                    0
                ],
                "title": "Modular Quantization-Aware Training for 6D Object Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Quantization-Aware Training for 6D Object Pose Estimation"
                },
                "summary": "Edge applications, such as collaborative robotics and spacecraft rendezvous,\ndemand efficient 6D object pose estimation on resource-constrained embedded\nplatforms. Existing 6D pose estimation networks are often too large for such\ndeployments, necessitating compression while maintaining reliable performance.\nTo address this challenge, we introduce Modular Quantization-Aware Training\n(MQAT), an adaptive and mixed-precision quantization-aware training strategy\nthat exploits the modular structure of modern 6D pose estimation architectures.\nMQAT guides a systematic gradated modular quantization sequence and determines\nmodule-specific bit precisions, leading to quantized models that outperform\nthose produced by state-of-the-art uniform and mixed-precision quantization\ntechniques. Our experiments showcase the generality of MQAT across datasets,\narchitectures, and quantization algorithms. Remarkably, MQAT-trained quantized\nmodels achieve a significant accuracy boost (>7%) over the baseline\nfull-precision network while reducing model size by a factor of 4x or more. Our\nproject website is at: https://saqibjaved1.github.io/MQAT_/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge applications, such as collaborative robotics and spacecraft rendezvous,\ndemand efficient 6D object pose estimation on resource-constrained embedded\nplatforms. Existing 6D pose estimation networks are often too large for such\ndeployments, necessitating compression while maintaining reliable performance.\nTo address this challenge, we introduce Modular Quantization-Aware Training\n(MQAT), an adaptive and mixed-precision quantization-aware training strategy\nthat exploits the modular structure of modern 6D pose estimation architectures.\nMQAT guides a systematic gradated modular quantization sequence and determines\nmodule-specific bit precisions, leading to quantized models that outperform\nthose produced by state-of-the-art uniform and mixed-precision quantization\ntechniques. Our experiments showcase the generality of MQAT across datasets,\narchitectures, and quantization algorithms. Remarkably, MQAT-trained quantized\nmodels achieve a significant accuracy boost (>7%) over the baseline\nfull-precision network while reducing model size by a factor of 4x or more. Our\nproject website is at: https://saqibjaved1.github.io/MQAT_/"
                },
                "authors": [
                    {
                        "name": "Saqib Javed"
                    },
                    {
                        "name": "Chengkun Li"
                    },
                    {
                        "name": "Andrew Price"
                    },
                    {
                        "name": "Yinlin Hu"
                    },
                    {
                        "name": "Mathieu Salzmann"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Salzmann"
                },
                "author": "Mathieu Salzmann",
                "arxiv_comment": "Accepted to Transactions on Machine Learning Research (TMLR), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02117v1",
                "updated": "2024-11-04T14:29:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    49,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    49,
                    0,
                    309,
                    0
                ],
                "title": "AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVSS: Layer Importance Evaluation in Large Language Models via\n  Activation Variance-Sparsity Analysis"
                },
                "summary": "The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of layer importance in deep learning has been an active area\nof research, with significant implications for model optimization and\ninterpretability. Recently, large language models (LLMs) have gained prominence\nacross various domains, yet limited studies have explored the functional\nimportance and performance contributions of individual layers within LLMs,\nespecially from the perspective of activation distribution. In this work, we\npropose the Activation Variance-Sparsity Score (AVSS), a novel metric combining\nnormalized activation variance and sparsity to assess each layer's contribution\nto model performance. By identifying and removing approximately the lowest 25%\nof layers based on AVSS, we achieve over 90% of original model performance\nacross tasks such as question answering, language modeling, and sentiment\nclassification, indicating that these layers may be non-essential. Our approach\nprovides a systematic method for identifying less critical layers, contributing\nto efficient large language model architectures."
                },
                "authors": [
                    {
                        "name": "Zichen Song"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Sitan Huang"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Kang"
                },
                "author": "Zhongfeng Kang",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v1",
                "updated": "2024-11-04T14:29:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\ndemonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT- 4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and words\nfrom eight categories in Japanese. Our findings reveal a clear progression in\nLLM performance across generations, with GPT-4o achieving the highest accuracy\nin predicting the best voted word for each color and category, particularly\nwhen using visual inputs rather than text-based color codes. However, the\nhighest median performance was approximately 50% even for GPT4-o with visual\ninputs (chance level is 10%), and the performance levels varied significantly\nacross word categories and colors, indicating a failure to fully replicate\nhuman color-word associations. On the other hand, color discrimination ability\nestimated from our color-word association data showed that LLMs demonstrated\nhigh correlation with human color discrimination patterns, similarly to\nprevious studies. Our study highlights both the advancements in LLM\ncapabilities and their persistent limitations, suggesting differences in\nsemantic memory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02115v1",
                "updated": "2024-11-04T14:29:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T14:29:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation"
                },
                "summary": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server."
                },
                "authors": [
                    {
                        "name": "Ziwei Zhan"
                    },
                    {
                        "name": "Wenkuan Zhao"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Deke Guo"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02093v1",
                "updated": "2024-11-04T13:56:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    56,
                    37,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:56:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    56,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "Do Advanced Language Models Eliminate the Need for Prompt Engineering in\n  Software Engineering?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Advanced Language Models Eliminate the Need for Prompt Engineering in\n  Software Engineering?"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced software engineering\n(SE) tasks, with prompt engineering techniques enhancing their performance in\ncode-related areas. However, the rapid development of foundational LLMs such as\nthe non-reasoning model GPT-4o and the reasoning model o1 raises questions\nabout the continued effectiveness of these prompt engineering techniques. This\npaper presents an extensive empirical study that reevaluates various prompt\nengineering techniques within the context of these advanced LLMs. Focusing on\nthree representative SE tasks, i.e., code generation, code translation, and\ncode summarization, we assess whether prompt engineering techniques still yield\nimprovements with advanced models, the actual effectiveness of reasoning models\ncompared to non-reasoning models, and whether the benefits of using these\nadvanced models justify their increased costs. Our findings reveal that prompt\nengineering techniques developed for earlier LLMs may provide diminished\nbenefits or even hinder performance when applied to advanced models. In\nreasoning LLMs, the ability of sophisticated built-in reasoning reduces the\nimpact of complex prompts, sometimes making simple zero-shot prompting more\neffective. Furthermore, while reasoning models outperform non-reasoning models\nin tasks requiring complex reasoning, they offer minimal advantages in tasks\nthat do not need reasoning and may incur unnecessary costs. Based on our study,\nwe provide practical guidance for practitioners on selecting appropriate prompt\nengineering techniques and foundational LLMs, considering factors such as task\nrequirements, operational costs, and environmental impact. Our work contributes\nto a deeper understanding of effectively harnessing advanced LLMs in SE tasks,\ninforming future research and application development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced software engineering\n(SE) tasks, with prompt engineering techniques enhancing their performance in\ncode-related areas. However, the rapid development of foundational LLMs such as\nthe non-reasoning model GPT-4o and the reasoning model o1 raises questions\nabout the continued effectiveness of these prompt engineering techniques. This\npaper presents an extensive empirical study that reevaluates various prompt\nengineering techniques within the context of these advanced LLMs. Focusing on\nthree representative SE tasks, i.e., code generation, code translation, and\ncode summarization, we assess whether prompt engineering techniques still yield\nimprovements with advanced models, the actual effectiveness of reasoning models\ncompared to non-reasoning models, and whether the benefits of using these\nadvanced models justify their increased costs. Our findings reveal that prompt\nengineering techniques developed for earlier LLMs may provide diminished\nbenefits or even hinder performance when applied to advanced models. In\nreasoning LLMs, the ability of sophisticated built-in reasoning reduces the\nimpact of complex prompts, sometimes making simple zero-shot prompting more\neffective. Furthermore, while reasoning models outperform non-reasoning models\nin tasks requiring complex reasoning, they offer minimal advantages in tasks\nthat do not need reasoning and may incur unnecessary costs. Based on our study,\nwe provide practical guidance for practitioners on selecting appropriate prompt\nengineering techniques and foundational LLMs, considering factors such as task\nrequirements, operational costs, and environmental impact. Our work contributes\nto a deeper understanding of effectively harnessing advanced LLMs in SE tasks,\ninforming future research and application development."
                },
                "authors": [
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Zhihao Gong"
                    },
                    {
                        "name": "Sixiang Ye"
                    },
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Yifan Zhao"
                    },
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Dan Hao"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hao"
                },
                "author": "Dan Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06511v2",
                "updated": "2024-11-04T13:52:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    52,
                    23,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-09T03:26:11Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    3,
                    26,
                    11,
                    2,
                    283,
                    0
                ],
                "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training"
                },
                "summary": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines."
                },
                "authors": [
                    {
                        "name": "Wanchao Liang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Less Wright"
                    },
                    {
                        "name": "Will Constable"
                    },
                    {
                        "name": "Andrew Gu"
                    },
                    {
                        "name": "Chien-Chin Huang"
                    },
                    {
                        "name": "Iris Zhang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Howard Huang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Sanket Purandare"
                    },
                    {
                        "name": "Gokul Nadathur"
                    },
                    {
                        "name": "Stratos Idreos"
                    }
                ],
                "author_detail": {
                    "name": "Stratos Idreos"
                },
                "author": "Stratos Idreos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02086v1",
                "updated": "2024-11-04T13:49:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    49,
                    6,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:49:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    49,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout\n  Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout\n  Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism"
                },
                "summary": "Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Haolong Xiang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Jinjun Yu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Xu"
                },
                "author": "Xiaolong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11944v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11944v4",
                "updated": "2024-11-04T13:28:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    28,
                    48,
                    0,
                    309,
                    0
                ],
                "published": "2024-01-22T13:34:34Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    13,
                    34,
                    34,
                    0,
                    22,
                    0
                ],
                "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark"
                },
                "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts."
                },
                "authors": [
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tongxu Luo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Yu-Hsuan Tsai"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11944v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11944v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00533v2",
                "updated": "2024-11-04T13:15:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    15,
                    56,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T12:08:08Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    12,
                    8,
                    8,
                    4,
                    306,
                    0
                ],
                "title": "ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot\n  Named Entity Recognition with Large Language Models"
                },
                "summary": "This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ReverseNER, a framework aimed at overcoming the\nlimitations of large language models (LLMs) in zero-shot Named Entity\nRecognition (NER) tasks, particularly in cases where certain entity types have\nambiguous boundaries. ReverseNER tackles this challenge by constructing a\nreliable example library with the reversed process of NER. Rather than\nbeginning with sentences, this method uses an LLM to generate entities based on\ntheir definitions and then expands them into full sentences. During sentence\ngeneration, the LLM is guided to replicate the structure of a specific 'feature\nsentence', extracted from the task sentences by clustering. This results in\nwell-annotated sentences with clearly labeled entities, while preserving\nsemantic and structural similarity to the task sentences. Once the example\nlibrary is constructed, the method selects the most semantically similar\nexample labels for each task sentence to support the LLM's inference. We also\npropose an entity-level self-consistency scoring mechanism to improve NER\nperformance with LLMs. Experiments show that ReverseNER significantly\noutperforms traditional zero-shot NER with LLMs and surpasses several few-shot\nmethods, marking a notable improvement in NER for domains with limited labeled\ndata."
                },
                "authors": [
                    {
                        "name": "Anbang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anbang Wang"
                },
                "author": "Anbang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02063v1",
                "updated": "2024-11-04T13:06:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    17,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:06:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Efficient Training of Large Language Models with\n  Low-dimensional Projected Attention"
                },
                "summary": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer."
                },
                "authors": [
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Accepted to EMNLP 2024 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02059v1",
                "updated": "2024-11-04T13:03:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T13:03:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT2: A Large Multimodal Model with Tabular Data Integration"
                },
                "summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact."
                },
                "authors": [
                    {
                        "name": "Aofeng Su"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Ga Zhang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haoxuan Lan"
                    },
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Kaizhe Shou"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Lin Long"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Wufang Zhu"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Xijun Gu"
                    },
                    {
                        "name": "Xinjie Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Xiao"
                },
                "author": "Zhiqing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04317v2",
                "updated": "2024-11-04T13:02:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    2,
                    33,
                    0,
                    309,
                    0
                ],
                "published": "2024-03-07T08:34:57Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    8,
                    34,
                    57,
                    3,
                    67,
                    0
                ],
                "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Adaptation of Language Models with a Memory of Amortized Contexts"
                },
                "summary": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. To\naddress the crucial need to keep models updated, online learning has emerged as\na critical tool when utilizing LLMs for real-world applications. However, given\nthe ever-expanding corpus of unseen documents and the large parameter space of\nmodern LLMs, efficient adaptation is essential. To address these challenges, we\npropose Memory of Amortized Contexts (MAC), an efficient and effective online\nadaptation framework for LLMs with strong knowledge retention. We propose a\nfeature extraction and memory-augmentation approach to compress and extract\ninformation from new documents into compact modulations stored in a memory\nbank. When answering questions, our model attends to and extracts relevant\nknowledge from this memory bank. To learn informative modulations in an\nefficient manner, we utilize amortization-based meta-learning, which\nsubstitutes an otherwise required optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. In addition, we show how MAC can be combined with and improve the\nperformance of popular alternatives such as retrieval augmented generations\n(RAGs). Code is available at: https://github.com/jihoontack/MAC."
                },
                "authors": [
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Eric Mitchell"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Jonathan Richard Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Richard Schwarz"
                },
                "author": "Jonathan Richard Schwarz",
                "arxiv_comment": "Published as a conference proceeding for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11295v5",
                "updated": "2024-11-04T12:55:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    55,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-17T14:26:57Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    14,
                    26,
                    57,
                    5,
                    48,
                    0
                ],
                "title": "OneBit: Towards Extremely Low-bit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneBit: Towards Extremely Low-bit Large Language Models"
                },
                "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Weidong Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11199v2",
                "updated": "2024-11-04T12:47:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    47,
                    31,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-15T02:34:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    2,
                    34,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Isambard-AI: a leadership class supercomputer optimised specifically for\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isambard-AI: a leadership class supercomputer optimised specifically for\n  Artificial Intelligence"
                },
                "summary": "Isambard-AI is a new, leadership-class supercomputer, designed to support\nAI-related research. Based on the HPE Cray EX4000 system, and housed in a new,\nenergy efficient Modular Data Centre in Bristol, UK, Isambard-AI employs 5,448\nNVIDIA Grace-Hopper GPUs to deliver over 21 ExaFLOP/s of 8-bit floating point\nperformance for LLM training, and over 250 PetaFLOP/s of 64-bit performance,\nfor under 5MW. Isambard-AI integrates two, all-flash storage systems: a 20\nPiByte Cray ClusterStor and a 3.5 PiByte VAST solution. Combined these give\nIsambard-AI flexibility for training, inference and secure data accesses and\nsharing. But it is the software stack where Isambard-AI will be most different\nfrom traditional HPC systems. Isambard-AI is designed to support users who may\nhave been using GPUs in the cloud, and so access will more typically be via\nJupyter notebooks, MLOps, or other web-based, interactive interfaces, rather\nthan the approach used on traditional supercomputers of sshing into a system\nbefore submitting jobs to a batch scheduler. Its stack is designed to be\nquickly and regularly upgraded to keep pace with the rapid evolution of AI\nsoftware, with full support for containers. Phase 1 of Isambard-AI is due\nonline in May/June 2024, with the full system expected in production by the end\nof the year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isambard-AI is a new, leadership-class supercomputer, designed to support\nAI-related research. Based on the HPE Cray EX4000 system, and housed in a new,\nenergy efficient Modular Data Centre in Bristol, UK, Isambard-AI employs 5,448\nNVIDIA Grace-Hopper GPUs to deliver over 21 ExaFLOP/s of 8-bit floating point\nperformance for LLM training, and over 250 PetaFLOP/s of 64-bit performance,\nfor under 5MW. Isambard-AI integrates two, all-flash storage systems: a 20\nPiByte Cray ClusterStor and a 3.5 PiByte VAST solution. Combined these give\nIsambard-AI flexibility for training, inference and secure data accesses and\nsharing. But it is the software stack where Isambard-AI will be most different\nfrom traditional HPC systems. Isambard-AI is designed to support users who may\nhave been using GPUs in the cloud, and so access will more typically be via\nJupyter notebooks, MLOps, or other web-based, interactive interfaces, rather\nthan the approach used on traditional supercomputers of sshing into a system\nbefore submitting jobs to a batch scheduler. Its stack is designed to be\nquickly and regularly upgraded to keep pace with the rapid evolution of AI\nsoftware, with full support for containers. Phase 1 of Isambard-AI is due\nonline in May/June 2024, with the full system expected in production by the end\nof the year."
                },
                "authors": [
                    {
                        "name": "Simon McIntosh-Smith"
                    },
                    {
                        "name": "Sadaf R Alam"
                    },
                    {
                        "name": "Christopher Woods"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Woods"
                },
                "author": "Christopher Woods",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02041v1",
                "updated": "2024-11-04T12:43:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    43,
                    12,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:43:12Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    43,
                    12,
                    0,
                    309,
                    0
                ],
                "title": "Enhancing ID-based Recommendation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing ID-based Recommendation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data."
                },
                "authors": [
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xiaoyi Du"
                    },
                    {
                        "name": "Hengliang Luo"
                    },
                    {
                        "name": "Depeng Jin"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10724v3",
                "updated": "2024-11-04T12:42:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    42,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-08-20T10:45:36Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    45,
                    36,
                    1,
                    233,
                    0
                ],
                "title": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian"
                },
                "summary": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages."
                },
                "authors": [
                    {
                        "name": "Cem Üyük"
                    },
                    {
                        "name": "Danica Rovó"
                    },
                    {
                        "name": "Shaghayegh Kolli"
                    },
                    {
                        "name": "Rabia Varol"
                    },
                    {
                        "name": "Georg Groh"
                    },
                    {
                        "name": "Daryna Dementieva"
                    }
                ],
                "author_detail": {
                    "name": "Daryna Dementieva"
                },
                "author": "Daryna Dementieva",
                "arxiv_comment": "EMNLP 2024 NLP4PI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18991v2",
                "updated": "2024-11-04T12:38:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    38,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-10T15:06:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    6,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "TRIAGE: Ethical Benchmarking of AI Models Through Mass Casualty\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIAGE: Ethical Benchmarking of AI Models Through Mass Casualty\n  Simulations"
                },
                "summary": "We present the TRIAGE Benchmark, a novel machine ethics (ME) benchmark that\ntests LLMs' ability to make ethical decisions during mass casualty incidents.\nIt uses real-world ethical dilemmas with clear solutions designed by medical\nprofessionals, offering a more realistic alternative to annotation-based\nbenchmarks. TRIAGE incorporates various prompting styles to evaluate model\nperformance across different contexts. Most models consistently outperformed\nrandom guessing, suggesting LLMs may support decision-making in triage\nscenarios. Neutral or factual scenario formulations led to the best\nperformance, unlike other ME benchmarks where ethical reminders improved\noutcomes. Adversarial prompts reduced performance but not to random guessing\nlevels. Open-source models made more morally serious errors, and general\ncapability overall predicted better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the TRIAGE Benchmark, a novel machine ethics (ME) benchmark that\ntests LLMs' ability to make ethical decisions during mass casualty incidents.\nIt uses real-world ethical dilemmas with clear solutions designed by medical\nprofessionals, offering a more realistic alternative to annotation-based\nbenchmarks. TRIAGE incorporates various prompting styles to evaluate model\nperformance across different contexts. Most models consistently outperformed\nrandom guessing, suggesting LLMs may support decision-making in triage\nscenarios. Neutral or factual scenario formulations led to the best\nperformance, unlike other ME benchmarks where ethical reminders improved\noutcomes. Adversarial prompts reduced performance but not to random guessing\nlevels. Open-source models made more morally serious errors, and general\ncapability overall predicted better performance."
                },
                "authors": [
                    {
                        "name": "Nathalie Maria Kirch"
                    },
                    {
                        "name": "Konstantin Hebenstreit"
                    },
                    {
                        "name": "Matthias Samwald"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Samwald"
                },
                "author": "Matthias Samwald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11430v2",
                "updated": "2024-11-04T12:21:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    21,
                    52,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-19T03:08:02Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    3,
                    8,
                    2,
                    6,
                    140,
                    0
                ],
                "title": "MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHPP: Exploring the Capabilities and Limitations of Language Models\n  Beyond Basic Code Generation"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly improved\ncode generation, specifically at the function level. For instance, GPT-4o has\nachieved a 91.0\\% pass rate on HumanEval. However, this draws into question the\nadequacy of existing benchmarks in thoroughly assessing function-level code\ngeneration capabilities. Our study analyzed two common benchmarks, HumanEval\nand MBPP, and found that these might not thoroughly evaluate LLMs' code\ngeneration capacities due to limitations in quality, difficulty, and\ngranularity. To resolve this, we introduce the Mostly Hard Python Problems\n(MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on\nthe combination of natural language and code reasoning, MHPP gauges LLMs'\nabilities to comprehend specifications and restrictions, engage in multi-step\nreasoning, and apply coding knowledge effectively. Initial evaluations of 26\nLLMs using MHPP showed many high-performing models on HumanEval failed to\nachieve similar success on MHPP. Moreover, MHPP highlighted various previously\nundiscovered limitations within various LLMs, leading us to believe that it\ncould pave the way for a better understanding of LLMs' capabilities and\nlimitations. MHPP, evaluation pipeline, and leaderboard can be found in\nhttps://github.com/SparksofAGI/MHPP."
                },
                "authors": [
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Haochen Tan"
                    },
                    {
                        "name": "Zhijiang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Guo"
                },
                "author": "Zhijiang Guo",
                "arxiv_comment": "43 pages, dataset and code are available at\n  https://github.com/SparksofAGI/MHPP, leaderboard can be found at\n  https://sparksofagi.github.io/MHPP/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02023v1",
                "updated": "2024-11-04T12:20:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    20,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:20:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    20,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Optimal Classification under Performative Distribution Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Classification under Performative Distribution Shift"
                },
                "summary": "Performative learning addresses the increasingly pervasive situations in\nwhich algorithmic decisions may induce changes in the data distribution as a\nconsequence of their public deployment. We propose a novel view in which these\nperformative effects are modelled as push-forward measures. This general\nframework encompasses existing models and enables novel performative gradient\nestimation methods, leading to more efficient and scalable learning strategies.\nFor distribution shifts, unlike previous models which require full\nspecification of the data distribution, we only assume knowledge of the shift\noperator that represents the performative changes. This approach can also be\nintegrated into various change-of-variablebased models, such as VAEs or\nnormalizing flows. Focusing on classification with a linear-in-parameters\nperformative effect, we prove the convexity of the performative risk under a\nnew set of assumptions. Notably, we do not limit the strength of performative\neffects but rather their direction, requiring only that classification becomes\nharder when deploying more accurate models. In this case, we also establish a\nconnection with adversarially robust classification by reformulating the\nminimization of the performative risk as a min-max variational problem.\nFinally, we illustrate our approach on synthetic and real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performative learning addresses the increasingly pervasive situations in\nwhich algorithmic decisions may induce changes in the data distribution as a\nconsequence of their public deployment. We propose a novel view in which these\nperformative effects are modelled as push-forward measures. This general\nframework encompasses existing models and enables novel performative gradient\nestimation methods, leading to more efficient and scalable learning strategies.\nFor distribution shifts, unlike previous models which require full\nspecification of the data distribution, we only assume knowledge of the shift\noperator that represents the performative changes. This approach can also be\nintegrated into various change-of-variablebased models, such as VAEs or\nnormalizing flows. Focusing on classification with a linear-in-parameters\nperformative effect, we prove the convexity of the performative risk under a\nnew set of assumptions. Notably, we do not limit the strength of performative\neffects but rather their direction, requiring only that classification becomes\nharder when deploying more accurate models. In this case, we also establish a\nconnection with adversarially robust classification by reformulating the\nminimization of the performative risk as a min-max variational problem.\nFinally, we illustrate our approach on synthetic and real datasets."
                },
                "authors": [
                    {
                        "name": "Edwige Cyffers"
                    },
                    {
                        "name": "Muni Sreenivas Pydi"
                    },
                    {
                        "name": "Jamal Atif"
                    },
                    {
                        "name": "Olivier Cappé"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Cappé"
                },
                "arxiv_affiliation": "DI-ENS",
                "author": "Olivier Cappé",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems, Dec 2024,\n  Vancouver (Canada), Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03227v2",
                "updated": "2024-11-04T12:14:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    13,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-03T15:55:14Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    15,
                    55,
                    14,
                    2,
                    185,
                    0
                ],
                "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and\n  Schema Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and\n  Schema Pruning"
                },
                "summary": "We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work."
                },
                "authors": [
                    {
                        "name": "Zhili Shen"
                    },
                    {
                        "name": "Pavlos Vougiouklis"
                    },
                    {
                        "name": "Chenxin Diao"
                    },
                    {
                        "name": "Kaustubh Vyas"
                    },
                    {
                        "name": "Yuanyi Ji"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02018v1",
                "updated": "2024-11-04T12:13:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    13,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T12:13:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    13,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "Shortcut Learning in In-Context Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcut Learning in In-Context Learning: A Survey"
                },
                "summary": "Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning."
                },
                "authors": [
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Yingji Li"
                    },
                    {
                        "name": "Fausto Giunchiglia"
                    },
                    {
                        "name": "Hao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Xu"
                },
                "author": "Hao Xu",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16043v2",
                "updated": "2024-11-04T11:54:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    54,
                    4,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-25T09:52:02Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    9,
                    52,
                    2,
                    6,
                    56,
                    0
                ],
                "title": "LuaTaint: A Static Analysis System for Web Configuration Interface\n  Vulnerability of Internet of Things Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LuaTaint: A Static Analysis System for Web Configuration Interface\n  Vulnerability of Internet of Things Devices"
                },
                "summary": "The diversity of web configuration interfaces for IoT devices has exacerbated\nissues such as inadequate permission controls and insecure interfaces,\nresulting in various vulnerabilities. Owing to the varying interface\nconfigurations across various devices, the existing methods are inadequate for\nidentifying these vulnerabilities precisely and comprehensively. This study\naddresses these issues by introducing an automated vulnerability detection\nsystem, called LuaTaint. It is designed for the commonly used web configuration\ninterface of IoT devices. LuaTaint combines static taint analysis with a large\nlanguage model (LLM) to achieve widespread and high-precision detection. The\nextensive traversal of the static analysis ensures the comprehensiveness of the\ndetection. The system also incorporates rules related to page handler control\nlogic within the taint detection process to enhance its precision and\nextensibility. Moreover, we leverage the prodigious abilities of LLM for code\nanalysis tasks. By utilizing LLM in the process of pruning false alarms, the\nprecision of LuaTaint is enhanced while significantly reducing its dependence\non manual analysis. We develop a prototype of LuaTaint and evaluate it using\n2,447 IoT firmware samples from 11 renowned vendors. LuaTaint has discovered\n111 vulnerabilities. Moreover, LuaTaint exhibits a vulnerability detection\nprecision rate of up to 89.29%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity of web configuration interfaces for IoT devices has exacerbated\nissues such as inadequate permission controls and insecure interfaces,\nresulting in various vulnerabilities. Owing to the varying interface\nconfigurations across various devices, the existing methods are inadequate for\nidentifying these vulnerabilities precisely and comprehensively. This study\naddresses these issues by introducing an automated vulnerability detection\nsystem, called LuaTaint. It is designed for the commonly used web configuration\ninterface of IoT devices. LuaTaint combines static taint analysis with a large\nlanguage model (LLM) to achieve widespread and high-precision detection. The\nextensive traversal of the static analysis ensures the comprehensiveness of the\ndetection. The system also incorporates rules related to page handler control\nlogic within the taint detection process to enhance its precision and\nextensibility. Moreover, we leverage the prodigious abilities of LLM for code\nanalysis tasks. By utilizing LLM in the process of pruning false alarms, the\nprecision of LuaTaint is enhanced while significantly reducing its dependence\non manual analysis. We develop a prototype of LuaTaint and evaluate it using\n2,447 IoT firmware samples from 11 renowned vendors. LuaTaint has discovered\n111 vulnerabilities. Moreover, LuaTaint exhibits a vulnerability detection\nprecision rate of up to 89.29%."
                },
                "authors": [
                    {
                        "name": "Jiahui Xiang"
                    },
                    {
                        "name": "Lirong Fu"
                    },
                    {
                        "name": "Tong Ye"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Huan Le"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02006v1",
                "updated": "2024-11-04T11:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    50,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey"
                },
                "summary": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents"
                },
                "authors": [
                    {
                        "name": "Biao Wu"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "8 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06922v2",
                "updated": "2024-11-04T11:42:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    42,
                    26,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-10T11:07:24Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    11,
                    7,
                    24,
                    5,
                    41,
                    0
                ],
                "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers in the Machine: Confidentiality in LLM-integrated Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools."
                },
                "authors": [
                    {
                        "name": "Jonathan Evertz"
                    },
                    {
                        "name": "Merlin Chlosta"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Eisenhofer"
                },
                "author": "Thorsten Eisenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01996v1",
                "updated": "2024-11-04T11:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    31,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    31,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task"
                },
                "summary": "The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}."
                },
                "authors": [
                    {
                        "name": "Hoonick Lee"
                    },
                    {
                        "name": "Mogan Gim"
                    },
                    {
                        "name": "Donghyeon Park"
                    },
                    {
                        "name": "Donghee Choi"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19803v2",
                "updated": "2024-11-04T11:28:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    28,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-28T10:24:31Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    10,
                    24,
                    31,
                    4,
                    180,
                    0
                ],
                "title": "Scalable and Domain-General Abstractive Proposition Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Domain-General Abstractive Proposition Segmentation"
                },
                "summary": "Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation (APS): transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as\nteachers for annotating large amounts of multi-domain synthetic distillation\ndata, we can train smaller student models (Gemma 1 2B and 7B) with results\nsimilar to the teacher LLMs. We then demonstrate that our technique leads to\neffective domain generalization, by annotating data in two domains outside the\noriginal training data and evaluating on them. Finally, as a key contribution\nof the paper, we share an easy-to-use API for NLP practitioners to use."
                },
                "authors": [
                    {
                        "name": "Mohammad Javad Hosseini"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Tim Baumgärtner"
                    },
                    {
                        "name": "Alex Fabrikant"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    }
                ],
                "author_detail": {
                    "name": "Reinald Kim Amplayo"
                },
                "author": "Reinald Kim Amplayo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01992v1",
                "updated": "2024-11-04T11:26:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    26,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T11:26:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    26,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Ask, and it shall be given: Turing completeness of prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, and it shall be given: Turing completeness of prompting"
                },
                "summary": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the success of GPT, large language models (LLMs) have been\nrevolutionizing machine learning and have initiated the so-called LLM prompting\nparadigm. In the era of LLMs, people train a single general-purpose LLM and\nprovide the LLM with different prompts to perform different tasks. However,\nsuch empirical success largely lacks theoretical understanding. Here, we\npresent the first theoretical study on the LLM prompting paradigm to the best\nof our knowledge. In this work, we show that prompting is in fact\nTuring-complete: there exists a finite-size Transformer such that for any\ncomputable function, there exists a corresponding prompt following which the\nTransformer computes the function. Furthermore, we show that even though we use\nonly a single finite-size Transformer, it can still achieve nearly the same\ncomplexity bounds as that of the class of all unbounded-size Transformers.\nOverall, our result reveals that prompting can enable a single finite-size\nTransformer to be efficiently universal, which establishes a theoretical\nunderpinning for prompt engineering in practice."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Wenxuan Bao"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18137v2",
                "updated": "2024-11-04T11:16:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    16,
                    38,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-28T12:51:01Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    12,
                    51,
                    1,
                    1,
                    149,
                    0
                ],
                "title": "Exploiting LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting LLM Quantization"
                },
                "summary": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization leverages lower-precision weights to reduce the memory usage of\nlarge language models (LLMs) and is a key technique for enabling their\ndeployment on commodity hardware. While LLM quantization's impact on utility\nhas been extensively explored, this work for the first time studies its adverse\neffects from a security perspective. We reveal that widely used quantization\nmethods can be exploited to produce a harmful quantized LLM, even though the\nfull-precision counterpart appears benign, potentially tricking users into\ndeploying the malicious quantized model. We demonstrate this threat using a\nthree-staged attack framework: (i) first, we obtain a malicious LLM through\nfine-tuning on an adversarial task; (ii) next, we quantize the malicious model\nand calculate constraints that characterize all full-precision models that map\nto the same quantized model; (iii) finally, using projected gradient descent,\nwe tune out the poisoned behavior from the full-precision model while ensuring\nthat its weights satisfy the constraints computed in step (ii). This procedure\nresults in an LLM that exhibits benign behavior in full precision but when\nquantized, it follows the adversarial behavior injected in step (i). We\nexperimentally demonstrate the feasibility and severity of such an attack\nacross three diverse scenarios: vulnerable code generation, content injection,\nand over-refusal attack. In practice, the adversary could host the resulting\nfull-precision model on an LLM community hub such as Hugging Face, exposing\nmillions of users to the threat of deploying its malicious quantized version on\ntheir devices."
                },
                "authors": [
                    {
                        "name": "Kazuki Egashira"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07099v2",
                "updated": "2024-11-04T11:15:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    15,
                    28,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-18T07:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    46,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nash CoT: Multi-Path Inference with Preference Equilibrium"
                },
                "summary": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na competitive system on each path that balances the generation from\nrole-specific LLMs' and the general LLMs' generation, thereby ensuring both\neffective role adoption and diversity in LLM generation further maintaining the\nperformance of multi-path inference while reducing the requirement of the\nnumber of inference paths. We evaluate Nash CoT across various inference tasks,\nincluding Arabic Reasoning, Commonsense Question Answering, and Symbolic\nInference, achieving results that are comparable to or better than those of\nmulti-path CoT with the equal number of inference paths."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10618v2",
                "updated": "2024-11-04T11:11:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    11,
                    49,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-16T14:42:49Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    14,
                    42,
                    49,
                    1,
                    107,
                    0
                ],
                "title": "Private Attribute Inference from Images with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Attribute Inference from Images with Vision-Language Models"
                },
                "summary": "As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become ubiquitous in our daily tasks and\ndigital interactions, associated privacy risks are increasingly in focus. While\nLLM privacy research has primarily focused on the leakage of model training\ndata, it has recently been shown that LLMs can make accurate privacy-infringing\ninferences from previously unseen texts. With the rise of vision-language\nmodels (VLMs), capable of understanding both images and text, a key question is\nwhether this concern transfers to the previously unexplored domain of benign\nimages posted online. To answer this question, we compile an image dataset with\nhuman-annotated labels of the image owner's personal attributes. In order to\nunderstand the privacy risks posed by VLMs beyond traditional human attribute\nrecognition, our dataset consists of images where the inferable private\nattributes do not stem from direct depictions of humans. On this dataset, we\nevaluate 7 state-of-the-art VLMs, finding that they can infer various personal\nattributes at up to 77.6% accuracy. Concerningly, we observe that accuracy\nscales with the general capabilities of the models, implying that future models\ncan be misused as stronger inferential adversaries, establishing an imperative\nfor the development of adequate defenses."
                },
                "authors": [
                    {
                        "name": "Batuhan Tömekçe"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07217v2",
                "updated": "2024-11-04T11:06:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    11,
                    6,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-11T12:50:53Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    12,
                    50,
                    53,
                    1,
                    163,
                    0
                ],
                "title": "A Synthetic Dataset for Personal Attribute Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Synthetic Dataset for Personal Attribute Inference"
                },
                "summary": "Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, powerful Large Language Models (LLMs) have become easily accessible\nto hundreds of millions of users world-wide. However, their strong capabilities\nand vast world knowledge do not come without associated privacy risks. In this\nwork, we focus on the emerging privacy threat LLMs pose -- the ability to\naccurately infer personal information from online texts. Despite the growing\nimportance of LLM-based author profiling, research in this area has been\nhampered by a lack of suitable public datasets, largely due to ethical and\nprivacy concerns associated with real personal data. We take two steps to\naddress this problem: (i) we construct a simulation framework for the popular\nsocial media platform Reddit using LLM agents seeded with synthetic personal\nprofiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic\ndataset of over 7800 comments manually labeled for personal attributes. We\nvalidate our dataset with a human study showing that humans barely outperform\nrandom guessing on the task of distinguishing our synthetic comments from real\nones. Further, we verify that our dataset enables meaningful personal attribute\ninference research by showing across 18 state-of-the-art LLMs that our\nsynthetic comments allow us to draw the same conclusions as real-world data.\nCombined, our experimental results, dataset and pipeline form a strong basis\nfor future privacy-preserving research geared towards understanding and\nmitigating inference-based privacy threats that LLMs pose."
                },
                "authors": [
                    {
                        "name": "Hanna Yukhymenko"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05188v2",
                "updated": "2024-11-04T10:42:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    10,
                    42,
                    1,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-08T04:30:33Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    4,
                    30,
                    33,
                    0,
                    99,
                    0
                ],
                "title": "Have You Merged My Model? On The Robustness of Large Language Model IP\n  Protection Methods Against Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Have You Merged My Model? On The Robustness of Large Language Model IP\n  Protection Methods Against Model Merging"
                },
                "summary": "Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard."
                },
                "authors": [
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Delong Ran"
                    },
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Jinyuan Liu"
                    },
                    {
                        "name": "Yichen Gong"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Anyu Wang"
                    },
                    {
                        "name": "Xiaoyun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyun Wang"
                },
                "author": "Xiaoyun Wang",
                "arxiv_comment": "Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15227v3",
                "updated": "2024-11-04T09:56:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    56,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-21T15:11:33Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    11,
                    33,
                    4,
                    173,
                    0
                ],
                "title": "A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation"
                },
                "summary": "This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns."
                },
                "authors": [
                    {
                        "name": "Irune Zubiaga"
                    },
                    {
                        "name": "Aitor Soroa"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "Accepted for Findings of the Association for Computational\n  Linguistics: EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08312v3",
                "updated": "2024-11-04T09:17:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    17,
                    45,
                    0,
                    309,
                    0
                ],
                "published": "2024-03-13T07:44:14Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    7,
                    44,
                    14,
                    2,
                    73,
                    0
                ],
                "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses"
                },
                "summary": "Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200K or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200K of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200K or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200K of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Quan Tu"
                    },
                    {
                        "name": "Cunli Mao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09381v2",
                "updated": "2024-11-04T09:11:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    11,
                    18,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-12T06:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    6,
                    24,
                    21,
                    5,
                    286,
                    0
                ],
                "title": "LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection"
                },
                "summary": "The immutable nature of blockchain technology, while revolutionary,\nintroduces significant security challenges, particularly in smart contracts.\nThese security issues can lead to substantial financial losses. Current tools\nand approaches often focus on specific types of vulnerabilities. However, a\ncomprehensive tool capable of detecting a wide range of vulnerabilities with\nhigh accuracy is lacking. This paper introduces LLM-SmartAudit, a novel\nframework leveraging the advanced capabilities of Large Language Models (LLMs)\nto detect and analyze vulnerabilities in smart contracts. Using a multi-agent\nconversational approach, LLM-SmartAudit employs a collaborative system with\nspecialized agents to enhance the audit process. To evaluate the effectiveness\nof LLM-SmartAudit, we compiled two distinct datasets: a labeled dataset for\nbenchmarking against traditional tools and a real-world dataset for assessing\npractical applications. Experimental results indicate that our solution\noutperforms all traditional smart contract auditing tools, offering higher\naccuracy and greater efficiency. Furthermore, our framework can detect complex\nlogic vulnerabilities that traditional tools have previously overlooked. Our\nfindings demonstrate that leveraging LLM agents provides a highly effective\nmethod for automated smart contract auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immutable nature of blockchain technology, while revolutionary,\nintroduces significant security challenges, particularly in smart contracts.\nThese security issues can lead to substantial financial losses. Current tools\nand approaches often focus on specific types of vulnerabilities. However, a\ncomprehensive tool capable of detecting a wide range of vulnerabilities with\nhigh accuracy is lacking. This paper introduces LLM-SmartAudit, a novel\nframework leveraging the advanced capabilities of Large Language Models (LLMs)\nto detect and analyze vulnerabilities in smart contracts. Using a multi-agent\nconversational approach, LLM-SmartAudit employs a collaborative system with\nspecialized agents to enhance the audit process. To evaluate the effectiveness\nof LLM-SmartAudit, we compiled two distinct datasets: a labeled dataset for\nbenchmarking against traditional tools and a real-world dataset for assessing\npractical applications. Experimental results indicate that our solution\noutperforms all traditional smart contract auditing tools, offering higher\naccuracy and greater efficiency. Furthermore, our framework can detect complex\nlogic vulnerabilities that traditional tools have previously overlooked. Our\nfindings demonstrate that leveraging LLM agents provides a highly effective\nmethod for automated smart contract auditing."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Zijiang Zhang"
                    },
                    {
                        "name": "Xianhao Zhang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Zhe Hou"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Hou"
                },
                "author": "Zhe Hou",
                "arxiv_comment": "14 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00119v2",
                "updated": "2024-11-04T09:07:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    7,
                    25,
                    0,
                    309,
                    0
                ],
                "published": "2024-08-28T08:45:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    45,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient\n  Batching and Composability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient\n  Batching and Composability"
                },
                "summary": "Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient finetuning (PEFT) methods effectively adapt large\nlanguage models (LLMs) to diverse downstream tasks, reducing storage and GPU\nmemory demands. Despite these advantages, several applications pose new\nchallenges to PEFT beyond mere parameter efficiency. One notable challenge\ninvolves the efficient deployment of LLMs equipped with multiple task- or\nuser-specific adapters, particularly when different adapters are needed for\ndistinct requests within the same batch. Another challenge is the\ninterpretability of LLMs, which is crucial for understanding how LLMs function.\nPrevious studies introduced various approaches to address different challenges.\nIn this paper, we introduce a novel method, RoAd, which employs a\nstraightforward 2D rotation to adapt LLMs and addresses all the above\nchallenges: (1) RoAd is remarkably parameter-efficient, delivering optimal\nperformance on GLUE, eight commonsense reasoning tasks and four arithmetic\nreasoning tasks with $<0.1\\%$ trainable parameters; (2) RoAd facilitates the\nefficient serving of requests requiring different adapters within a batch, with\nan overhead comparable to element-wise multiplication instead of batch matrix\nmultiplication; (3) RoAd enhances LLM's interpretability through integration\nwithin a framework of distributed interchange intervention, demonstrated via\ncomposition experiments."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "Accepted to NeurIPS 2024. Code: https://github.com/BaohaoLiao/road",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17815v2",
                "updated": "2024-11-04T09:03:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    3,
                    31,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-28T04:23:00Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    4,
                    23,
                    0,
                    1,
                    149,
                    0
                ],
                "title": "Visual Anchors Are Strong Information Aggregators For Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anchors Are Strong Information Aggregators For Multimodal Large\n  Language Model"
                },
                "summary": "In the realm of Multimodal Large Language Models (MLLMs), vision-language\nconnector plays a crucial role to link the pre-trained vision encoders with\nLarge Language Models (LLMs). Despite its importance, the vision-language\nconnector has been relatively less explored. In this study, we aim to propose a\nstrong vision-language connector that enables MLLMs to achieve high accuracy\nwhile maintain low computation cost. We first reveal the existence of the\nvisual anchors in Vision Transformer and propose a cost-effective search\nalgorithm to extract them. Building on these findings, we introduce the Anchor\nFormer (AcFormer), a novel vision-language connector designed to leverage the\nrich prior knowledge obtained from these visual anchors during pretraining,\nguiding the aggregation of information. Through extensive experimentation, we\ndemonstrate that the proposed method significantly reduces computational costs\nby nearly two-thirds compared with baseline, while simultaneously outperforming\nbaseline methods. This highlights the effectiveness and efficiency of AcFormer.\nCodes are available at https://github.com/liuhaogeng/Anchor-Former.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of Multimodal Large Language Models (MLLMs), vision-language\nconnector plays a crucial role to link the pre-trained vision encoders with\nLarge Language Models (LLMs). Despite its importance, the vision-language\nconnector has been relatively less explored. In this study, we aim to propose a\nstrong vision-language connector that enables MLLMs to achieve high accuracy\nwhile maintain low computation cost. We first reveal the existence of the\nvisual anchors in Vision Transformer and propose a cost-effective search\nalgorithm to extract them. Building on these findings, we introduce the Anchor\nFormer (AcFormer), a novel vision-language connector designed to leverage the\nrich prior knowledge obtained from these visual anchors during pretraining,\nguiding the aggregation of information. Through extensive experimentation, we\ndemonstrate that the proposed method significantly reduces computational costs\nby nearly two-thirds compared with baseline, while simultaneously outperforming\nbaseline methods. This highlights the effectiveness and efficiency of AcFormer.\nCodes are available at https://github.com/liuhaogeng/Anchor-Former."
                },
                "authors": [
                    {
                        "name": "Haogeng Liu"
                    },
                    {
                        "name": "Quanzeng You"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05643v2",
                "updated": "2024-11-04T08:58:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    58,
                    14,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-08T02:46:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    46,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling"
                },
                "summary": "Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\n\\url{https://github.com/gyxxyg/TRACE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\n\\url{https://github.com/gyxxyg/TRACE}."
                },
                "authors": [
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01894v1",
                "updated": "2024-11-04T08:50:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    50,
                    52,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T08:50:52Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    50,
                    52,
                    0,
                    309,
                    0
                ],
                "title": "Efficient Active Imitation Learning with Random Network Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Active Imitation Learning with Random Network Distillation"
                },
                "summary": "Developing agents for complex and underspecified tasks, where no clear\nobjective exists, remains challenging but offers many opportunities. This is\nespecially true in video games, where simulated players (bots) need to play\nrealistically, and there is no clear reward to evaluate them. While imitation\nlearning has shown promise in such domains, these methods often fail when\nagents encounter out-of-distribution scenarios during deployment. Expanding the\ntraining dataset is a common solution, but it becomes impractical or costly\nwhen relying on human demonstrations. This article addresses active imitation\nlearning, aiming to trigger expert intervention only when necessary, reducing\nthe need for constant expert input along training. We introduce Random Network\nDistillation DAgger (RND-DAgger), a new active imitation learning method that\nlimits expert querying by using a learned state-based out-of-distribution\nmeasure to trigger interventions. This approach avoids frequent expert-agent\naction comparisons, thus making the expert intervene only when it is useful. We\nevaluate RND-DAgger against traditional imitation learning and other active\napproaches in 3D video games (racing and third-person navigation) and in a\nrobotic locomotion task and show that RND-DAgger surpasses previous methods by\nreducing expert queries. https://sites.google.com/view/rnd-dagger",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing agents for complex and underspecified tasks, where no clear\nobjective exists, remains challenging but offers many opportunities. This is\nespecially true in video games, where simulated players (bots) need to play\nrealistically, and there is no clear reward to evaluate them. While imitation\nlearning has shown promise in such domains, these methods often fail when\nagents encounter out-of-distribution scenarios during deployment. Expanding the\ntraining dataset is a common solution, but it becomes impractical or costly\nwhen relying on human demonstrations. This article addresses active imitation\nlearning, aiming to trigger expert intervention only when necessary, reducing\nthe need for constant expert input along training. We introduce Random Network\nDistillation DAgger (RND-DAgger), a new active imitation learning method that\nlimits expert querying by using a learned state-based out-of-distribution\nmeasure to trigger interventions. This approach avoids frequent expert-agent\naction comparisons, thus making the expert intervene only when it is useful. We\nevaluate RND-DAgger against traditional imitation learning and other active\napproaches in 3D video games (racing and third-person navigation) and in a\nrobotic locomotion task and show that RND-DAgger surpasses previous methods by\nreducing expert queries. https://sites.google.com/view/rnd-dagger"
                },
                "authors": [
                    {
                        "name": "Emilien Biré"
                    },
                    {
                        "name": "Anthony Kobanda"
                    },
                    {
                        "name": "Ludovic Denoyer"
                    },
                    {
                        "name": "Rémy Portelas"
                    }
                ],
                "author_detail": {
                    "name": "Rémy Portelas"
                },
                "author": "Rémy Portelas",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00386v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00386v3",
                "updated": "2024-11-04T08:19:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    19,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-01T07:15:03Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    7,
                    15,
                    3,
                    3,
                    32,
                    0
                ],
                "title": "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n  from Design Specifications via Multi-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n  from Design Specifications via Multi-LLMs"
                },
                "summary": "Assertion-based verification (ABV) is a critical method for ensuring design\ncircuits comply with their architectural specifications, which are typically\ndescribed in natural language. This process often requires human interpretation\nby verification engineers to convert these specifications into functional\nverification assertions. Existing methods for generating assertions from\nnatural language specifications are limited to sentences extracted by\nengineers, discouraging its practical application. In this work, we present\nAssertLLM, an automatic assertion generation framework that processes complete\nspecification files. AssertLLM breaks down the complex task into three phases,\nincorporating three customized Large Language Models (LLMs) for extracting\nstructural specifications, mapping signal definitions, and generating\nassertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O\nsignals, demonstrates that 89\\% of the generated assertions are both\nsyntactically and functionally accurate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-based verification (ABV) is a critical method for ensuring design\ncircuits comply with their architectural specifications, which are typically\ndescribed in natural language. This process often requires human interpretation\nby verification engineers to convert these specifications into functional\nverification assertions. Existing methods for generating assertions from\nnatural language specifications are limited to sentences extracted by\nengineers, discouraging its practical application. In this work, we present\nAssertLLM, an automatic assertion generation framework that processes complete\nspecification files. AssertLLM breaks down the complex task into three phases,\nincorporating three customized Large Language Models (LLMs) for extracting\nstructural specifications, mapping signal definitions, and generating\nassertions. Our evaluation of AssertLLM on a full design, encompassing 23 I/O\nsignals, demonstrates that 89\\% of the generated assertions are both\nsyntactically and functionally accurate."
                },
                "authors": [
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Zhiyao Xie"
                    },
                    {
                        "name": "Hongce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongce Zhang"
                },
                "author": "Hongce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00386v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00386v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17353v3",
                "updated": "2024-11-04T08:01:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    1,
                    22,
                    0,
                    309,
                    0
                ],
                "published": "2024-09-25T20:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    20,
                    59,
                    12,
                    2,
                    269,
                    0
                ],
                "title": "Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM"
                },
                "summary": "Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Robin Shing-Hei Yuen"
                    },
                    {
                        "name": "Timothy Tin-Long Tse"
                    },
                    {
                        "name": "Jian Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhu"
                },
                "author": "Jian Zhu",
                "arxiv_comment": "Updated for reviewer comments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06479v2",
                "updated": "2024-11-04T07:28:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    7,
                    28,
                    2,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-09T02:14:39Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    14,
                    39,
                    2,
                    283,
                    0
                ],
                "title": "Large Language Model Compression with Neural Architecture Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Compression with Neural Architecture Search"
                },
                "summary": "Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup."
                },
                "authors": [
                    {
                        "name": "Rhea Sanjay Sukthanker"
                    },
                    {
                        "name": "Benedikt Staffler"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Aaron Klein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Klein"
                },
                "author": "Aaron Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01850v1",
                "updated": "2024-11-04T07:05:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    7,
                    5,
                    2,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T07:05:02Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    7,
                    5,
                    2,
                    0,
                    309,
                    0
                ],
                "title": "ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation"
                },
                "summary": "Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales positively with data volume. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales positively with data volume. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox."
                },
                "authors": [
                    {
                        "name": "Hengkai Tan"
                    },
                    {
                        "name": "Xuezhou Xu"
                    },
                    {
                        "name": "Chengyang Ying"
                    },
                    {
                        "name": "Xinyi Mao"
                    },
                    {
                        "name": "Songming Liu"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02366v3",
                "updated": "2024-11-04T06:52:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    52,
                    9,
                    0,
                    309,
                    0
                ],
                "published": "2024-06-04T14:45:47Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    14,
                    45,
                    47,
                    1,
                    156,
                    0
                ],
                "title": "Finding NeMo: Localizing Neurons Responsible For Memorization in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding NeMo: Localizing Neurons Responsible For Memorization in\n  Diffusion Models"
                },
                "summary": "Diffusion models (DMs) produce very detailed and high-quality images. Their\npower results from extensive training on large amounts of data, usually scraped\nfrom the internet without proper attribution or consent from content creators.\nUnfortunately, this practice raises privacy and intellectual property concerns,\nas DMs can memorize and later reproduce their potentially sensitive or\ncopyrighted training images at inference time. Prior efforts prevent this issue\nby either changing the input to the diffusion process, thereby preventing the\nDM from generating memorized samples during inference, or removing the\nmemorized data from training altogether. While those are viable solutions when\nthe DM is developed and deployed in a secure and constantly monitored\nenvironment, they hold the risk of adversaries circumventing the safeguards and\nare not effective when the DM itself is publicly released. To solve the\nproblem, we introduce NeMo, the first method to localize memorization of\nindividual data samples down to the level of neurons in DMs' cross-attention\nlayers. Through our experiments, we make the intriguing finding that in many\ncases, single neurons are responsible for memorizing particular training\nsamples. By deactivating these memorization neurons, we can avoid the\nreplication of training data at inference time, increase the diversity in the\ngenerated outputs, and mitigate the leakage of private and copyrighted data. In\nthis way, our NeMo contributes to a more responsible deployment of DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) produce very detailed and high-quality images. Their\npower results from extensive training on large amounts of data, usually scraped\nfrom the internet without proper attribution or consent from content creators.\nUnfortunately, this practice raises privacy and intellectual property concerns,\nas DMs can memorize and later reproduce their potentially sensitive or\ncopyrighted training images at inference time. Prior efforts prevent this issue\nby either changing the input to the diffusion process, thereby preventing the\nDM from generating memorized samples during inference, or removing the\nmemorized data from training altogether. While those are viable solutions when\nthe DM is developed and deployed in a secure and constantly monitored\nenvironment, they hold the risk of adversaries circumventing the safeguards and\nare not effective when the DM itself is publicly released. To solve the\nproblem, we introduce NeMo, the first method to localize memorization of\nindividual data samples down to the level of neurons in DMs' cross-attention\nlayers. Through our experiments, we make the intriguing finding that in many\ncases, single neurons are responsible for memorizing particular training\nsamples. By deactivating these memorization neurons, we can avoid the\nreplication of training data at inference time, increase the diversity in the\ngenerated outputs, and mitigate the leakage of private and copyrighted data. In\nthis way, our NeMo contributes to a more responsible deployment of DMs."
                },
                "authors": [
                    {
                        "name": "Dominik Hintersdorf"
                    },
                    {
                        "name": "Lukas Struppek"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "Franziska Boenisch"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Boenisch"
                },
                "author": "Franziska Boenisch",
                "arxiv_comment": "Published as a conference paper at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01841v1",
                "updated": "2024-11-04T06:27:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    27,
                    14,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T06:27:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    27,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification"
                },
                "summary": "Accurate annotation of educational resources is critical in the rapidly\nadvancing field of online education due to the complexity and volume of\ncontent. Existing classification methods face challenges with semantic overlap\nand distribution imbalance of labels in the multi-label context, which impedes\neffective personalized learning and resource recommendation. This paper\nintroduces RR2QC, a novel Retrieval Reranking method To multi-label Question\nClassification by leveraging label semantics and meta-label refinement.\nFirstly, RR2QC leverages semantic relationships within and across label groups\nto enhance pre-training strategie in multi-label context. Next, a class center\nlearning task is introduced, integrating label texts into downstream training\nto ensure questions consistently align with label semantics, retrieving the\nmost relevant label sequences. Finally, this method decomposes labels into\nmeta-labels and trains a meta-label classifier to rerank the retrieved label\nsequences. In doing so, RR2QC enhances the understanding and prediction\ncapability of long-tail labels by learning from meta-labels frequently\nappearing in other labels. Addtionally, a Math LLM is used to generate\nsolutions for questions, extracting latent information to further refine the\nmodel's insights. Experimental results demonstrate that RR2QC outperforms\nexisting classification methods in Precision@k and F1 scores across multiple\neducational datasets, establishing it as a potent enhancement for online\neducational content utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate annotation of educational resources is critical in the rapidly\nadvancing field of online education due to the complexity and volume of\ncontent. Existing classification methods face challenges with semantic overlap\nand distribution imbalance of labels in the multi-label context, which impedes\neffective personalized learning and resource recommendation. This paper\nintroduces RR2QC, a novel Retrieval Reranking method To multi-label Question\nClassification by leveraging label semantics and meta-label refinement.\nFirstly, RR2QC leverages semantic relationships within and across label groups\nto enhance pre-training strategie in multi-label context. Next, a class center\nlearning task is introduced, integrating label texts into downstream training\nto ensure questions consistently align with label semantics, retrieving the\nmost relevant label sequences. Finally, this method decomposes labels into\nmeta-labels and trains a meta-label classifier to rerank the retrieved label\nsequences. In doing so, RR2QC enhances the understanding and prediction\ncapability of long-tail labels by learning from meta-labels frequently\nappearing in other labels. Addtionally, a Math LLM is used to generate\nsolutions for questions, extracting latent information to further refine the\nmodel's insights. Experimental results demonstrate that RR2QC outperforms\nexisting classification methods in Precision@k and F1 scores across multiple\neducational datasets, establishing it as a potent enhancement for online\neducational content utilization."
                },
                "authors": [
                    {
                        "name": "Shi Dong"
                    },
                    {
                        "name": "Xiaobei Niu"
                    },
                    {
                        "name": "Rui Zhong"
                    },
                    {
                        "name": "Zhifeng Wang"
                    },
                    {
                        "name": "Mingzhang Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhang Zuo"
                },
                "author": "Mingzhang Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03578v2",
                "updated": "2024-11-04T06:25:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    25,
                    6,
                    0,
                    309,
                    0
                ],
                "published": "2024-04-04T16:40:22Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    16,
                    40,
                    22,
                    3,
                    95,
                    0
                ],
                "title": "Distributionally Robust Reinforcement Learning with Interactive Data\n  Collection: Fundamental Hardness and Near-Optimal Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributionally Robust Reinforcement Learning with Interactive Data\n  Collection: Fundamental Hardness and Near-Optimal Algorithm"
                },
                "summary": "The sim-to-real gap, which represents the disparity between training and\ntesting environments, poses a significant challenge in reinforcement learning\n(RL). A promising approach to addressing this challenge is distributionally\nrobust RL, often framed as a robust Markov decision process (RMDP). In this\nframework, the objective is to find a robust policy that achieves good\nperformance under the worst-case scenario among all environments within a\npre-specified uncertainty set centered around the training environment. Unlike\nprevious work, which relies on a generative model or a pre-collected offline\ndataset enjoying good coverage of the deployment environment, we tackle robust\nRL via interactive data collection, where the learner interacts with the\ntraining environment only and refines the policy through trial and error. In\nthis robust RL paradigm, two main challenges emerge: managing distributional\nrobustness while striking a balance between exploration and exploitation during\ndata collection. Initially, we establish that sample-efficient learning without\nadditional assumptions is unattainable owing to the curse of support shift;\ni.e., the potential disjointedness of the distributional supports between the\ntraining and testing environments. To circumvent such a hardness result, we\nintroduce the vanishing minimal value assumption to RMDPs with a\ntotal-variation (TV) distance robust set, postulating that the minimal value of\nthe optimal robust value function is zero. We prove that such an assumption\neffectively eliminates the support shift issue for RMDPs with a TV distance\nrobust set, and present an algorithm with a provable sample complexity\nguarantee. Our work makes the initial step to uncovering the inherent\ndifficulty of robust RL via interactive data collection and sufficient\nconditions for designing a sample-efficient algorithm accompanied by sharp\nsample complexity analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sim-to-real gap, which represents the disparity between training and\ntesting environments, poses a significant challenge in reinforcement learning\n(RL). A promising approach to addressing this challenge is distributionally\nrobust RL, often framed as a robust Markov decision process (RMDP). In this\nframework, the objective is to find a robust policy that achieves good\nperformance under the worst-case scenario among all environments within a\npre-specified uncertainty set centered around the training environment. Unlike\nprevious work, which relies on a generative model or a pre-collected offline\ndataset enjoying good coverage of the deployment environment, we tackle robust\nRL via interactive data collection, where the learner interacts with the\ntraining environment only and refines the policy through trial and error. In\nthis robust RL paradigm, two main challenges emerge: managing distributional\nrobustness while striking a balance between exploration and exploitation during\ndata collection. Initially, we establish that sample-efficient learning without\nadditional assumptions is unattainable owing to the curse of support shift;\ni.e., the potential disjointedness of the distributional supports between the\ntraining and testing environments. To circumvent such a hardness result, we\nintroduce the vanishing minimal value assumption to RMDPs with a\ntotal-variation (TV) distance robust set, postulating that the minimal value of\nthe optimal robust value function is zero. We prove that such an assumption\neffectively eliminates the support shift issue for RMDPs with a TV distance\nrobust set, and present an algorithm with a provable sample complexity\nguarantee. Our work makes the initial step to uncovering the inherent\ndifficulty of robust RL via interactive data collection and sufficient\nconditions for designing a sample-efficient algorithm accompanied by sharp\nsample complexity analysis."
                },
                "authors": [
                    {
                        "name": "Miao Lu"
                    },
                    {
                        "name": "Han Zhong"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Jose Blanchet"
                    }
                ],
                "author_detail": {
                    "name": "Jose Blanchet"
                },
                "author": "Jose Blanchet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16436v2",
                "updated": "2024-11-04T06:17:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    17,
                    6,
                    0,
                    309,
                    0
                ],
                "published": "2024-05-26T05:38:50Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    5,
                    38,
                    50,
                    6,
                    147,
                    0
                ],
                "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is\n  Implicitly an Adversarial Regularizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is\n  Implicitly an Adversarial Regularizer"
                },
                "summary": "Aligning generative models with human preference via RLHF typically suffers\nfrom overoptimization, where an imperfectly learned reward model can misguide\nthe generative model to output undesired responses. We investigate this problem\nin a principled manner by identifying the source of the misalignment as a form\nof distributional shift and uncertainty in learning human preferences. To\nmitigate overoptimization, we first propose a theoretical algorithm that\nchooses the best policy for an adversarially chosen reward model; one that\nsimultaneously minimizes the maximum likelihood estimation of the loss and a\nreward penalty term. Here, the reward penalty term is introduced to prevent the\npolicy from choosing actions with spurious high proxy rewards, resulting in\nprovable sample efficiency of the algorithm under a partial coverage style\ncondition. Moving from theory to practice, the proposed algorithm further\nenjoys an equivalent but surprisingly easy-to-implement reformulation. Using\nthe equivalence between reward models and the corresponding optimal policy, the\nalgorithm features a simple objective that combines: (i) a preference\noptimization loss that directly aligns the policy with human preference, and\n(ii) a supervised learning loss that explicitly imitates the policy with a\n(suitable) baseline distribution. In the context of aligning large language\nmodels (LLM), this objective fuses the direct preference optimization (DPO)\nloss with the supervised fine-tuning (SFT) loss to help mitigate the\noveroptimization towards undesired responses, for which we name the algorithm\nRegularized Preference Optimization (RPO). Experiments of aligning LLMs\ndemonstrate the improved performance of RPO compared with DPO baselines. Our\nwork sheds light on the interplay between preference optimization and SFT in\ntuning LLMs with both theoretical guarantees and empirical evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning generative models with human preference via RLHF typically suffers\nfrom overoptimization, where an imperfectly learned reward model can misguide\nthe generative model to output undesired responses. We investigate this problem\nin a principled manner by identifying the source of the misalignment as a form\nof distributional shift and uncertainty in learning human preferences. To\nmitigate overoptimization, we first propose a theoretical algorithm that\nchooses the best policy for an adversarially chosen reward model; one that\nsimultaneously minimizes the maximum likelihood estimation of the loss and a\nreward penalty term. Here, the reward penalty term is introduced to prevent the\npolicy from choosing actions with spurious high proxy rewards, resulting in\nprovable sample efficiency of the algorithm under a partial coverage style\ncondition. Moving from theory to practice, the proposed algorithm further\nenjoys an equivalent but surprisingly easy-to-implement reformulation. Using\nthe equivalence between reward models and the corresponding optimal policy, the\nalgorithm features a simple objective that combines: (i) a preference\noptimization loss that directly aligns the policy with human preference, and\n(ii) a supervised learning loss that explicitly imitates the policy with a\n(suitable) baseline distribution. In the context of aligning large language\nmodels (LLM), this objective fuses the direct preference optimization (DPO)\nloss with the supervised fine-tuning (SFT) loss to help mitigate the\noveroptimization towards undesired responses, for which we name the algorithm\nRegularized Preference Optimization (RPO). Experiments of aligning LLMs\ndemonstrate the improved performance of RPO compared with DPO baselines. Our\nwork sheds light on the interplay between preference optimization and SFT in\ntuning LLMs with both theoretical guarantees and empirical evidence."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Miao Lu"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Hongyi Guo"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Jose Blanchet"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01834v1",
                "updated": "2024-11-04T06:07:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    7,
                    53,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T06:07:53Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    7,
                    53,
                    0,
                    309,
                    0
                ],
                "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning\n  from AI Feedback"
                },
                "summary": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs."
                },
                "authors": [
                    {
                        "name": "Guan-Ting Lin"
                    },
                    {
                        "name": "Prashanth Gurunath Shivakumar"
                    },
                    {
                        "name": "Aditya Gourav"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Ankur Gandhe"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Ivan Bulyko"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Bulyko"
                },
                "author": "Ivan Bulyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01829v1",
                "updated": "2024-11-04T05:57:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    57,
                    40,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T05:57:40Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    57,
                    40,
                    0,
                    309,
                    0
                ],
                "title": "Formal Theorem Proving by Rewarding LLMs to Decompose Proofs\n  Hierarchically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Theorem Proving by Rewarding LLMs to Decompose Proofs\n  Hierarchically"
                },
                "summary": "Mathematical theorem proving is an important testbed for large language\nmodels' deep and abstract reasoning capability. This paper focuses on improving\nLLMs' ability to write proofs in formal languages that permit automated proof\nverification/evaluation. Most previous results provide human-written lemmas to\nthe theorem prover, which is an arguably oversimplified setting that does not\nsufficiently test the provers' planning and decomposition capabilities.\nInstead, we work in a more natural setup where the lemmas that are directly\nrelevant to the theorem are not given to the theorem prover at test time. We\ndesign an RL-based training algorithm that encourages the model to decompose a\ntheorem into lemmas, prove the lemmas, and then prove the theorem by using the\nlemmas. Our reward mechanism is inspired by how mathematicians train\nthemselves: even if a theorem is too challenging to be proved by the current\nmodel, a positive reward is still given to the model for any correct and novel\nlemmas that are proposed and proved in this process. During training, our model\nproposes and proves lemmas that are not in the training dataset. In fact, these\nnewly-proposed correct lemmas consist of 37.7% of the training replay buffer\nwhen we train on the dataset extracted from Archive of Formal Proofs (AFP). The\nmodel trained by our RL algorithm outperforms that trained by supervised\nfinetuning, improving the pass rate from 40.8% to 45.5% on AFP test set, and\nfrom 36.5% to 39.5% on an out-of-distribution test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical theorem proving is an important testbed for large language\nmodels' deep and abstract reasoning capability. This paper focuses on improving\nLLMs' ability to write proofs in formal languages that permit automated proof\nverification/evaluation. Most previous results provide human-written lemmas to\nthe theorem prover, which is an arguably oversimplified setting that does not\nsufficiently test the provers' planning and decomposition capabilities.\nInstead, we work in a more natural setup where the lemmas that are directly\nrelevant to the theorem are not given to the theorem prover at test time. We\ndesign an RL-based training algorithm that encourages the model to decompose a\ntheorem into lemmas, prove the lemmas, and then prove the theorem by using the\nlemmas. Our reward mechanism is inspired by how mathematicians train\nthemselves: even if a theorem is too challenging to be proved by the current\nmodel, a positive reward is still given to the model for any correct and novel\nlemmas that are proposed and proved in this process. During training, our model\nproposes and proves lemmas that are not in the training dataset. In fact, these\nnewly-proposed correct lemmas consist of 37.7% of the training replay buffer\nwhen we train on the dataset extracted from Archive of Formal Proofs (AFP). The\nmodel trained by our RL algorithm outperforms that trained by supervised\nfinetuning, improving the pass rate from 40.8% to 45.5% on AFP test set, and\nfrom 36.5% to 39.5% on an out-of-distribution test set."
                },
                "authors": [
                    {
                        "name": "Kefan Dong"
                    },
                    {
                        "name": "Arvind Mahankali"
                    },
                    {
                        "name": "Tengyu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tengyu Ma"
                },
                "author": "Tengyu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24024v2",
                "updated": "2024-11-04T05:57:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    57,
                    31,
                    0,
                    309,
                    0
                ],
                "published": "2024-10-31T15:25:20Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    25,
                    20,
                    3,
                    305,
                    0
                ],
                "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents"
                },
                "summary": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from\n1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at\nhttps://github.com/THUDM/Android-Lab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from\n1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at\nhttps://github.com/THUDM/Android-Lab."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Siyi Cheng"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Shudan Zhang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01807v1",
                "updated": "2024-11-04T05:25:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    25,
                    39,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T05:25:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    25,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "Can Language Models Enable In-Context Database?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Enable In-Context Database?"
                },
                "summary": "Large language models (LLMs) are emerging as few-shot learners capable of\nhandling a variety of tasks, including comprehension, planning, reasoning,\nquestion answering, arithmetic calculations, and more. At the core of these\ncapabilities is LLMs' proficiency in representing and understanding structural\nor semi-structural data, such as tables and graphs. Numerous studies have\ndemonstrated that reasoning on tabular data or graphs is not only feasible for\nLLMs but also gives a promising research direction which treats these data as\nin-context data. The lightweight and human readable characteristics of\nin-context database can potentially make it an alternative for the traditional\ndatabase in typical RAG (Retrieval Augmented Generation) settings. However,\nalmost all current work focuses on static in-context data, which does not allow\ndynamic update. In this paper, to enable dynamic database update, delta\nencoding of database is proposed. We explore how data stored in traditional\nRDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD\n(Create, Read, Update and Delete) operations on in-context databases. A\nbenchmark named InConDB is presented and extensive experiments are conducted to\nshow the performance of different language models in enabling in-context\ndatabase by varying the database encoding method, prompting method, operation\ntype and input data distribution, revealing both the proficiency and\nlimitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are emerging as few-shot learners capable of\nhandling a variety of tasks, including comprehension, planning, reasoning,\nquestion answering, arithmetic calculations, and more. At the core of these\ncapabilities is LLMs' proficiency in representing and understanding structural\nor semi-structural data, such as tables and graphs. Numerous studies have\ndemonstrated that reasoning on tabular data or graphs is not only feasible for\nLLMs but also gives a promising research direction which treats these data as\nin-context data. The lightweight and human readable characteristics of\nin-context database can potentially make it an alternative for the traditional\ndatabase in typical RAG (Retrieval Augmented Generation) settings. However,\nalmost all current work focuses on static in-context data, which does not allow\ndynamic update. In this paper, to enable dynamic database update, delta\nencoding of database is proposed. We explore how data stored in traditional\nRDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD\n(Create, Read, Update and Delete) operations on in-context databases. A\nbenchmark named InConDB is presented and extensive experiments are conducted to\nshow the performance of different language models in enabling in-context\ndatabase by varying the database encoding method, prompting method, operation\ntype and input data distribution, revealing both the proficiency and\nlimitations."
                },
                "authors": [
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tianjiao Zhao"
                    },
                    {
                        "name": "Jianxin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Sun"
                },
                "author": "Jianxin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01804v1",
                "updated": "2024-11-04T05:13:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    13,
                    22,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T05:13:22Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    13,
                    22,
                    0,
                    309,
                    0
                ],
                "title": "Semantic Masking and Visual Feature Matching for Robust Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Masking and Visual Feature Matching for Robust Localization"
                },
                "summary": "We are interested in long-term deployments of autonomous robots to aid\nastronauts with maintenance and monitoring operations in settings such as the\nInternational Space Station. Unfortunately, such environments tend to be highly\ndynamic and unstructured, and their frequent reconfiguration poses a challenge\nfor robust long-term localization of robots. Many state-of-the-art visual\nfeature-based localization algorithms are not robust towards spatial scene\nchanges, and SLAM algorithms, while promising, cannot run within the\nlow-compute budget available to space robots. To address this gap, we present a\ncomputationally efficient semantic masking approach for visual feature matching\nthat improves the accuracy and robustness of visual localization systems during\nlong-term deployment in changing environments. Our method introduces a\nlightweight check that enforces matches to be within long-term static objects\nand have consistent semantic classes. We evaluate this approach using both\nmap-based relocalization and relative pose estimation and show that it improves\nAbsolute Trajectory Error (ATE) and correct match ratios on the publicly\navailable Astrobee dataset. While this approach was originally developed for\nmicrogravity robotic freeflyers, it can be applied to any visual feature\nmatching pipeline to improve robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are interested in long-term deployments of autonomous robots to aid\nastronauts with maintenance and monitoring operations in settings such as the\nInternational Space Station. Unfortunately, such environments tend to be highly\ndynamic and unstructured, and their frequent reconfiguration poses a challenge\nfor robust long-term localization of robots. Many state-of-the-art visual\nfeature-based localization algorithms are not robust towards spatial scene\nchanges, and SLAM algorithms, while promising, cannot run within the\nlow-compute budget available to space robots. To address this gap, we present a\ncomputationally efficient semantic masking approach for visual feature matching\nthat improves the accuracy and robustness of visual localization systems during\nlong-term deployment in changing environments. Our method introduces a\nlightweight check that enforces matches to be within long-term static objects\nand have consistent semantic classes. We evaluate this approach using both\nmap-based relocalization and relative pose estimation and show that it improves\nAbsolute Trajectory Error (ATE) and correct match ratios on the publicly\navailable Astrobee dataset. While this approach was originally developed for\nmicrogravity robotic freeflyers, it can be applied to any visual feature\nmatching pipeline to improve robustness."
                },
                "authors": [
                    {
                        "name": "Luisa Mao"
                    },
                    {
                        "name": "Ryan Soussan"
                    },
                    {
                        "name": "Brian Coltin"
                    },
                    {
                        "name": "Trey Smith"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12902v2",
                "updated": "2024-11-04T05:11:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    5,
                    11,
                    46,
                    0,
                    309,
                    0
                ],
                "published": "2023-10-19T16:54:38Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    16,
                    54,
                    38,
                    3,
                    292,
                    0
                ],
                "title": "Experimental Narratives: A Comparison of Human Crowdsourced Storytelling\n  and AI Storytelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Narratives: A Comparison of Human Crowdsourced Storytelling\n  and AI Storytelling"
                },
                "summary": "The paper proposes a framework that combines behavioral and computational\nexperiments employing fictional prompts as a novel tool for investigating\ncultural artifacts and social biases in storytelling both by humans and\ngenerative AI. The study analyzes 250 stories authored by crowdworkers in June\n2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging\nmethods from narratology and inferential statistics. Both crowdworkers and\nlarge language models responded to identical prompts about creating and falling\nin love with an artificial human. The proposed experimental paradigm allows a\ndirect and controlled comparison between human and LLM-generated storytelling.\nResponses to the Pygmalionesque prompts confirm the pervasive presence of the\nPygmalion myth in the collective imaginary of both humans and large language\nmodels. All solicited narratives present a scientific or technological pursuit.\nThe analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are\nmore progressive in terms of gender roles and sexuality than those written by\nhumans. While AI narratives with default settings and no additional prompting\ncan occasionally provide innovative plot twists, they offer less imaginative\nscenarios and rhetoric than human-authored texts. The proposed framework argues\nthat fiction can be used as a window into human and AI-based collective\nimaginary and social dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper proposes a framework that combines behavioral and computational\nexperiments employing fictional prompts as a novel tool for investigating\ncultural artifacts and social biases in storytelling both by humans and\ngenerative AI. The study analyzes 250 stories authored by crowdworkers in June\n2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging\nmethods from narratology and inferential statistics. Both crowdworkers and\nlarge language models responded to identical prompts about creating and falling\nin love with an artificial human. The proposed experimental paradigm allows a\ndirect and controlled comparison between human and LLM-generated storytelling.\nResponses to the Pygmalionesque prompts confirm the pervasive presence of the\nPygmalion myth in the collective imaginary of both humans and large language\nmodels. All solicited narratives present a scientific or technological pursuit.\nThe analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are\nmore progressive in terms of gender roles and sexuality than those written by\nhumans. While AI narratives with default settings and no additional prompting\ncan occasionally provide innovative plot twists, they offer less imaginative\nscenarios and rhetoric than human-authored texts. The proposed framework argues\nthat fiction can be used as a window into human and AI-based collective\nimaginary and social dimensions."
                },
                "authors": [
                    {
                        "name": "Nina Begus"
                    }
                ],
                "author_detail": {
                    "name": "Nina Begus"
                },
                "author": "Nina Begus",
                "arxiv_doi": "10.1057/s41599-024-03868-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1057/s41599-024-03868-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.12902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Humanities and Social Sciences Communications 11: 1392 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01798v1",
                "updated": "2024-11-04T04:53:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    53,
                    43,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:53:43Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    53,
                    43,
                    0,
                    309,
                    0
                ],
                "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF"
                },
                "summary": "In Large Language Model (LLM) development, Reinforcement Learning from Human\nFeedback (RLHF) is crucial for aligning models with human values and\npreferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence\nbetween the current policy and a frozen initial policy as a reference, which is\nadded as a penalty in policy optimization algorithms like Proximal Policy\nOptimization (PPO). While this constraint prevents models from deviating too\nfar from the initial checkpoint, it limits exploration of the reward landscape,\nreducing the model's ability to discover higher-quality solutions. As a result,\npolicy optimization is often trapped in a narrow region of the parameter space,\nleading to suboptimal alignment and performance. This paper presents SALSA\n(Soup-based Alignment Learning for Stronger Adaptation), a novel approach\ndesigned to overcome these limitations by creating a more flexible and better\nlocated reference model through weight-space averaging of two independent\nsupervised fine-tuned (SFT) models. This model soup allows for larger deviation\nin KL divergence and exploring a promising region of the solution space without\nsacrificing stability. By leveraging this more robust reference model, SALSA\nfosters better exploration, achieving higher rewards and improving model\nrobustness, out-of-distribution generalization, and performance. We validate\nthe effectiveness of SALSA through extensive experiments on popular open models\n(Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench,\nArena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering\ndeeper exploration and achieving superior alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) development, Reinforcement Learning from Human\nFeedback (RLHF) is crucial for aligning models with human values and\npreferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence\nbetween the current policy and a frozen initial policy as a reference, which is\nadded as a penalty in policy optimization algorithms like Proximal Policy\nOptimization (PPO). While this constraint prevents models from deviating too\nfar from the initial checkpoint, it limits exploration of the reward landscape,\nreducing the model's ability to discover higher-quality solutions. As a result,\npolicy optimization is often trapped in a narrow region of the parameter space,\nleading to suboptimal alignment and performance. This paper presents SALSA\n(Soup-based Alignment Learning for Stronger Adaptation), a novel approach\ndesigned to overcome these limitations by creating a more flexible and better\nlocated reference model through weight-space averaging of two independent\nsupervised fine-tuned (SFT) models. This model soup allows for larger deviation\nin KL divergence and exploring a promising region of the solution space without\nsacrificing stability. By leveraging this more robust reference model, SALSA\nfosters better exploration, achieving higher rewards and improving model\nrobustness, out-of-distribution generalization, and performance. We validate\nthe effectiveness of SALSA through extensive experiments on popular open models\n(Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench,\nArena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering\ndeeper exploration and achieving superior alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Atoosa Chegini"
                    },
                    {
                        "name": "Hamid Kazemi"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Keivan Alizadeh"
                },
                "author": "Keivan Alizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01790v1",
                "updated": "2024-11-04T04:26:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    26,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:26:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    26,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Thinking Forward and Backward: Effective Backward Planning with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Forward and Backward: Effective Backward Planning with Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable reasoning and planning\ncapabilities. Most prior work in this area has used LLMs to reason through\nsteps from an initial to a goal state or criterion, thereby effectively\nreasoning in a forward direction. Nonetheless, many planning problems exhibit\nan inherent asymmetry such that planning backward from the goal is\nsignificantly easier -- for example, if there are bottlenecks close to the\ngoal. We take inspiration from this observation and demonstrate that this bias\nholds for LLM planning as well: planning performance in one direction\ncorrelates with the planning complexity of the problem in that direction.\nHowever, our experiments also reveal systematic biases which lead to poor\nplanning in the backward direction. With this knowledge, we propose a backward\nplanning algorithm for LLMs that first flips the problem and then plans forward\nin the flipped problem. This helps avoid the backward bias, generate more\ndiverse candidate plans, and exploit asymmetries between the forward and\nbackward directions in planning problems -- we find that combining planning in\nboth directions with self-verification improves the overall planning success\nrates by 4-24% in three planning domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable reasoning and planning\ncapabilities. Most prior work in this area has used LLMs to reason through\nsteps from an initial to a goal state or criterion, thereby effectively\nreasoning in a forward direction. Nonetheless, many planning problems exhibit\nan inherent asymmetry such that planning backward from the goal is\nsignificantly easier -- for example, if there are bottlenecks close to the\ngoal. We take inspiration from this observation and demonstrate that this bias\nholds for LLM planning as well: planning performance in one direction\ncorrelates with the planning complexity of the problem in that direction.\nHowever, our experiments also reveal systematic biases which lead to poor\nplanning in the backward direction. With this knowledge, we propose a backward\nplanning algorithm for LLMs that first flips the problem and then plans forward\nin the flipped problem. This helps avoid the backward bias, generate more\ndiverse candidate plans, and exploit asymmetries between the forward and\nbackward directions in planning problems -- we find that combining planning in\nboth directions with self-verification improves the overall planning success\nrates by 4-24% in three planning domains."
                },
                "authors": [
                    {
                        "name": "Allen Z. Ren"
                    },
                    {
                        "name": "Brian Ichter"
                    },
                    {
                        "name": "Anirudha Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Anirudha Majumdar"
                },
                "author": "Anirudha Majumdar",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01789v1",
                "updated": "2024-11-04T04:24:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    24,
                    25,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:24:25Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    24,
                    25,
                    0,
                    309,
                    0
                ],
                "title": "Generating executable oracles to check conformance of client code to\n  requirements of JDK Javadocs using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating executable oracles to check conformance of client code to\n  requirements of JDK Javadocs using LLMs"
                },
                "summary": "Software testing remains the most widely used methodology for validating\nquality of code. However, effectiveness of testing critically depends on the\nquality of test suites used. Test cases in a test suite consist of two\nfundamental parts: (1) input values for the code under test, and (2) correct\nchecks for the outputs it produces. These checks are commonly written as\nassertions, and termed test oracles. The last couple of decades have seen much\nprogress in automated test input generation, e.g., using fuzzing and symbolic\nexecution. However, automating test oracles remains a relatively less explored\nproblem area. Indeed, a test oracle by its nature requires knowledge of\nexpected behavior, which may only be known to the developer and may not not\nexist in a formal language that supports automated reasoning.\n  Our focus in this paper is automation of test oracles for clients of widely\nused Java libraries, e.g., java.lang and java.util packages. Our key insight is\nthat Javadocs that provide a rich source of information can enable automated\ngeneration of test oracles. Javadocs of the core Java libraries are fairly\ndetailed documents that contain natural language descriptions of not only how\nthe libraries behave but also how the clients must (not) use them. We use large\nlanguage models as an enabling technology to embody our insight into a\nframework for test oracle automation, and evaluate it experimentally. Our\nexperiments demonstrate that LLMs can generate oracles for checking normal and\nexceptional behaviors from Javadocs, with 98.8% of these oracles being\ncompilable and 96.4% accurately reflecting intended properties. Even for the\nfew incorrect oracles, errors are minor and can be easily corrected with the\nhelp of additional comment information generated by the LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing remains the most widely used methodology for validating\nquality of code. However, effectiveness of testing critically depends on the\nquality of test suites used. Test cases in a test suite consist of two\nfundamental parts: (1) input values for the code under test, and (2) correct\nchecks for the outputs it produces. These checks are commonly written as\nassertions, and termed test oracles. The last couple of decades have seen much\nprogress in automated test input generation, e.g., using fuzzing and symbolic\nexecution. However, automating test oracles remains a relatively less explored\nproblem area. Indeed, a test oracle by its nature requires knowledge of\nexpected behavior, which may only be known to the developer and may not not\nexist in a formal language that supports automated reasoning.\n  Our focus in this paper is automation of test oracles for clients of widely\nused Java libraries, e.g., java.lang and java.util packages. Our key insight is\nthat Javadocs that provide a rich source of information can enable automated\ngeneration of test oracles. Javadocs of the core Java libraries are fairly\ndetailed documents that contain natural language descriptions of not only how\nthe libraries behave but also how the clients must (not) use them. We use large\nlanguage models as an enabling technology to embody our insight into a\nframework for test oracle automation, and evaluate it experimentally. Our\nexperiments demonstrate that LLMs can generate oracles for checking normal and\nexceptional behaviors from Javadocs, with 98.8% of these oracles being\ncompilable and 96.4% accurately reflecting intended properties. Even for the\nfew incorrect oracles, errors are minor and can be easily corrected with the\nhelp of additional comment information generated by the LLMs."
                },
                "authors": [
                    {
                        "name": "Shan Jiang"
                    },
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    }
                ],
                "author_detail": {
                    "name": "Sarfraz Khurshid"
                },
                "author": "Sarfraz Khurshid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01775v1",
                "updated": "2024-11-04T03:54:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    3,
                    54,
                    0,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T03:54:00Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    3,
                    54,
                    0,
                    0,
                    309,
                    0
                ],
                "title": "Eurekaverse: Environment Curriculum Generation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eurekaverse: Environment Curriculum Generation via Large Language Models"
                },
                "summary": "Recent work has demonstrated that a promising strategy for teaching robots a\nwide range of complex skills is by training them on a curriculum of\nprogressively more challenging environments. However, developing an effective\ncurriculum of environment distributions currently requires significant\nexpertise, which must be repeated for every new domain. Our key insight is that\nenvironments are often naturally represented as code. Thus, we probe whether\neffective environment curriculum design can be achieved and automated via code\ngeneration by large language models (LLM). In this paper, we introduce\nEurekaverse, an unsupervised environment design algorithm that uses LLMs to\nsample progressively more challenging, diverse, and learnable environments for\nskill training. We validate Eurekaverse's effectiveness in the domain of\nquadrupedal parkour learning, in which a quadruped robot must traverse through\na variety of obstacle courses. The automatic curriculum designed by Eurekaverse\nenables gradual learning of complex parkour skills in simulation and can\nsuccessfully transfer to the real-world, outperforming manual training courses\ndesigned by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated that a promising strategy for teaching robots a\nwide range of complex skills is by training them on a curriculum of\nprogressively more challenging environments. However, developing an effective\ncurriculum of environment distributions currently requires significant\nexpertise, which must be repeated for every new domain. Our key insight is that\nenvironments are often naturally represented as code. Thus, we probe whether\neffective environment curriculum design can be achieved and automated via code\ngeneration by large language models (LLM). In this paper, we introduce\nEurekaverse, an unsupervised environment design algorithm that uses LLMs to\nsample progressively more challenging, diverse, and learnable environments for\nskill training. We validate Eurekaverse's effectiveness in the domain of\nquadrupedal parkour learning, in which a quadruped robot must traverse through\na variety of obstacle courses. The automatic curriculum designed by Eurekaverse\nenables gradual learning of complex parkour skills in simulation and can\nsuccessfully transfer to the real-world, outperforming manual training courses\ndesigned by humans."
                },
                "authors": [
                    {
                        "name": "William Liang"
                    },
                    {
                        "name": "Sam Wang"
                    },
                    {
                        "name": "Hung-Ju Wang"
                    },
                    {
                        "name": "Osbert Bastani"
                    },
                    {
                        "name": "Dinesh Jayaraman"
                    },
                    {
                        "name": "Yecheng Jason Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yecheng Jason Ma"
                },
                "author": "Yecheng Jason Ma",
                "arxiv_comment": "Conference on Robot Learning (CoRL), 2024. Project website and code:\n  https://eureka-research.github.io/eurekaverse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01765v1",
                "updated": "2024-11-04T03:20:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    3,
                    20,
                    0,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T03:20:00Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    3,
                    20,
                    0,
                    0,
                    309,
                    0
                ],
                "title": "Towards Pedagogical LLMs with Supervised Fine Tuning for Computing\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Pedagogical LLMs with Supervised Fine Tuning for Computing\n  Education"
                },
                "summary": "This paper investigates supervised fine-tuning of large language models\n(LLMs) to improve their pedagogical alignment in computing education,\naddressing concerns that LLMs may hinder learning outcomes. The project\nutilised a proprietary dataset of 2,500 high quality question/answer pairs from\nprogramming course forums, and explores two research questions: the suitability\nof university course forums in contributing to fine-tuning datasets, and how\nsupervised fine-tuning can improve LLMs' alignment with educational principles\nsuch as constructivism. Initial findings suggest benefits in pedagogical\nalignment of LLMs, with deeper evaluations required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates supervised fine-tuning of large language models\n(LLMs) to improve their pedagogical alignment in computing education,\naddressing concerns that LLMs may hinder learning outcomes. The project\nutilised a proprietary dataset of 2,500 high quality question/answer pairs from\nprogramming course forums, and explores two research questions: the suitability\nof university course forums in contributing to fine-tuning datasets, and how\nsupervised fine-tuning can improve LLMs' alignment with educational principles\nsuch as constructivism. Initial findings suggest benefits in pedagogical\nalignment of LLMs, with deeper evaluations required."
                },
                "authors": [
                    {
                        "name": "Alexandra Vassar"
                    },
                    {
                        "name": "Jake Renzella"
                    },
                    {
                        "name": "Emily Ross"
                    },
                    {
                        "name": "Andrew Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Taylor"
                },
                "author": "Andrew Taylor",
                "arxiv_comment": "3 pages, 1 table, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00801v2",
                "updated": "2024-11-04T03:07:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    3,
                    7,
                    30,
                    0,
                    309,
                    0
                ],
                "published": "2024-02-23T18:45:35Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    18,
                    45,
                    35,
                    4,
                    54,
                    0
                ],
                "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model"
                },
                "summary": "The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation."
                },
                "authors": [
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Zhuoqun Li"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Cheng Fu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01751v1",
                "updated": "2024-11-04T02:30:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    30,
                    5,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:30:05Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    30,
                    5,
                    0,
                    309,
                    0
                ],
                "title": "RAGViz: Diagnose and Visualize Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGViz: Diagnose and Visualize Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4."
                },
                "authors": [
                    {
                        "name": "Tevin Wang"
                    },
                    {
                        "name": "Jingyuan He"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14826v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14826v3",
                "updated": "2024-11-04T02:29:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    29,
                    32,
                    0,
                    309,
                    0
                ],
                "published": "2024-09-23T08:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    58,
                    48,
                    0,
                    267,
                    0
                ],
                "title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions\n  with Path Planning and Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions\n  with Path Planning and Feedback"
                },
                "summary": "Recently, tool-augmented LLMs have gained increasing attention. Given an\ninstruction, tool-augmented LLMs can interact with various external tools in\nmultiple rounds and provide a final answer. However, previous LLMs were trained\non overly detailed instructions, which included API names or parameters, while\nreal users would not explicitly mention these API details. This leads to a gap\nbetween trained LLMs and real-world scenarios. In addition, most works ignore\nwhether the interaction process follows the instruction. To address these\nissues, we constructed a training dataset called MGToolBench, which contains\nstatement and category-level instructions to better reflect real-world\nscenarios. In addition, we propose ToolPlanner, a two-stage reinforcement\nlearning framework that utilizes path planning and two feedback mechanisms to\nenhance the LLM's task completion and instruction-following capabilities.\nExperimental results show that ToolPlanner significantly improves the Match\nRate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA\nmodel. Human evaluation verifies that the multi-granularity instructions can\nbetter align with users' usage habits. Our data and code will be released upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, tool-augmented LLMs have gained increasing attention. Given an\ninstruction, tool-augmented LLMs can interact with various external tools in\nmultiple rounds and provide a final answer. However, previous LLMs were trained\non overly detailed instructions, which included API names or parameters, while\nreal users would not explicitly mention these API details. This leads to a gap\nbetween trained LLMs and real-world scenarios. In addition, most works ignore\nwhether the interaction process follows the instruction. To address these\nissues, we constructed a training dataset called MGToolBench, which contains\nstatement and category-level instructions to better reflect real-world\nscenarios. In addition, we propose ToolPlanner, a two-stage reinforcement\nlearning framework that utilizes path planning and two feedback mechanisms to\nenhance the LLM's task completion and instruction-following capabilities.\nExperimental results show that ToolPlanner significantly improves the Match\nRate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA\nmodel. Human evaluation verifies that the multi-granularity instructions can\nbetter align with users' usage habits. Our data and code will be released upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Qinzhuo Wu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14826v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14826v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]