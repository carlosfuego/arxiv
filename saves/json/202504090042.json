[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v1",
                "updated": "2025-04-06T15:15:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v4",
                "updated": "2025-04-05T12:37:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    12,
                    37,
                    19,
                    5,
                    95,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Pérez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castaño"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castaño"
                },
                "author": "Manuel Gamero-Castaño",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03708v1",
                "updated": "2025-03-27T00:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T00:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "Solving AI Foundational Model Latency with Telco Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving AI Foundational Model Latency with Telco Infrastructure"
                },
                "summary": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams."
                },
                "authors": [
                    {
                        "name": "Sebastian Barros"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Barros"
                },
                "author": "Sebastian Barros",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2401.17377v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17377v4",
                "updated": "2025-04-07T17:59:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    50,
                    0,
                    97,
                    0
                ],
                "published": "2024-01-30T19:03:49Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    19,
                    3,
                    49,
                    1,
                    30,
                    0
                ],
                "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens"
                },
                "summary": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Published at COLM 2024, spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17377v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17377v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05304v1",
                "updated": "2025-04-07T17:59:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    42,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:59:42Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    42,
                    0,
                    97,
                    0
                ],
                "title": "Gaussian Mixture Flow Matching Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Mixture Flow Matching Models"
                },
                "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an $L_2$ denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256$\\times$256.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an $L_2$ denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256$\\times$256."
                },
                "authors": [
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Zexiang Xu"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Sai Bi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Bi"
                },
                "author": "Sai Bi",
                "arxiv_comment": "Code: https://github.com/Lakonik/GMFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05303v1",
                "updated": "2025-04-07T17:59:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:59:33Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    33,
                    0,
                    97,
                    0
                ],
                "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models"
                },
                "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de."
                },
                "authors": [
                    {
                        "name": "Sai Kumar Dwivedi"
                    },
                    {
                        "name": "Dimitrije Antić"
                    },
                    {
                        "name": "Shashank Tripathi"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Dimitrios Tzionas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tzionas"
                },
                "author": "Dimitrios Tzionas",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05299v1",
                "updated": "2025-04-07T17:58:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    58,
                    57,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:58:57Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    58,
                    57,
                    0,
                    97,
                    0
                ],
                "title": "SmolVLM: Redefining small and efficient multimodal models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmolVLM: Redefining small and efficient multimodal models"
                },
                "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales."
                },
                "authors": [
                    {
                        "name": "Andrés Marafioti"
                    },
                    {
                        "name": "Orr Zohar"
                    },
                    {
                        "name": "Miquel Farré"
                    },
                    {
                        "name": "Merve Noyan"
                    },
                    {
                        "name": "Elie Bakouch"
                    },
                    {
                        "name": "Pedro Cuenca"
                    },
                    {
                        "name": "Cyril Zakka"
                    },
                    {
                        "name": "Loubna Ben Allal"
                    },
                    {
                        "name": "Anton Lozhkov"
                    },
                    {
                        "name": "Nouamane Tazi"
                    },
                    {
                        "name": "Vaibhav Srivastav"
                    },
                    {
                        "name": "Joshua Lochner"
                    },
                    {
                        "name": "Hugo Larcher"
                    },
                    {
                        "name": "Mathieu Morlon"
                    },
                    {
                        "name": "Lewis Tunstall"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05294v1",
                "updated": "2025-04-07T17:49:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    23,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:49:23Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    23,
                    0,
                    97,
                    0
                ],
                "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations"
                },
                "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."
                },
                "authors": [
                    {
                        "name": "Pedro Ferreira"
                    },
                    {
                        "name": "Wilker Aziz"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "22 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11681v3",
                "updated": "2025-04-07T17:43:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    43,
                    1,
                    0,
                    97,
                    0
                ],
                "published": "2024-05-19T21:52:12Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    21,
                    52,
                    12,
                    6,
                    140,
                    0
                ],
                "title": "Distributed Tensor Principal Component Analysis with Data Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Tensor Principal Component Analysis with Data Heterogeneity"
                },
                "summary": "As tensors become widespread in modern data analysis, Tucker low-rank\nPrincipal Component Analysis (PCA) has become essential for dimensionality\nreduction and structural discovery in tensor datasets. Motivated by the common\nscenario where large-scale tensors are distributed across diverse geographic\nlocations, this paper investigates tensor PCA within a distributed framework\nwhere direct data pooling is impractical.\n  We offer a comprehensive analysis of three specific scenarios in distributed\nTensor PCA: a homogeneous setting in which tensors at various locations are\ngenerated from a single noise-affected model; a heterogeneous setting where\ntensors at different locations come from distinct models but share some\nprincipal components, aiming to improve estimation across all locations; and a\ntargeted heterogeneous setting, designed to boost estimation accuracy at a\nspecific location with limited samples by utilizing transferred knowledge from\nother sites with ample data.\n  We introduce novel estimation methods tailored to each scenario, establish\nstatistical guarantees, and develop distributed inference techniques to\nconstruct confidence regions. Our theoretical findings demonstrate that these\ndistributed methods achieve sharp rates of accuracy by efficiently aggregating\nshared information across different tensors, while maintaining reasonable\ncommunication costs. Empirical validation through simulations and real-world\ndata applications highlights the advantages of our approaches, particularly in\nmanaging heterogeneous tensor data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As tensors become widespread in modern data analysis, Tucker low-rank\nPrincipal Component Analysis (PCA) has become essential for dimensionality\nreduction and structural discovery in tensor datasets. Motivated by the common\nscenario where large-scale tensors are distributed across diverse geographic\nlocations, this paper investigates tensor PCA within a distributed framework\nwhere direct data pooling is impractical.\n  We offer a comprehensive analysis of three specific scenarios in distributed\nTensor PCA: a homogeneous setting in which tensors at various locations are\ngenerated from a single noise-affected model; a heterogeneous setting where\ntensors at different locations come from distinct models but share some\nprincipal components, aiming to improve estimation across all locations; and a\ntargeted heterogeneous setting, designed to boost estimation accuracy at a\nspecific location with limited samples by utilizing transferred knowledge from\nother sites with ample data.\n  We introduce novel estimation methods tailored to each scenario, establish\nstatistical guarantees, and develop distributed inference techniques to\nconstruct confidence regions. Our theoretical findings demonstrate that these\ndistributed methods achieve sharp rates of accuracy by efficiently aggregating\nshared information across different tensors, while maintaining reasonable\ncommunication costs. Empirical validation through simulations and real-world\ndata applications highlights the advantages of our approaches, particularly in\nmanaging heterogeneous tensor data."
                },
                "authors": [
                    {
                        "name": "Elynn Chen"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wenbo Jing"
                    },
                    {
                        "name": "Yichen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhang"
                },
                "author": "Yichen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19265v2",
                "updated": "2025-04-07T17:43:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    43,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-25T01:59:57Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    1,
                    59,
                    57,
                    1,
                    84,
                    0
                ],
                "title": "PHEONA: An Evaluation Framework for Large Language Model-based\n  Approaches to Computational Phenotyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHEONA: An Evaluation Framework for Large Language Model-based\n  Approaches to Computational Phenotyping"
                },
                "summary": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes."
                },
                "authors": [
                    {
                        "name": "Sarah Pungitore"
                    },
                    {
                        "name": "Shashank Yadav"
                    },
                    {
                        "name": "Vignesh Subbian"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh Subbian"
                },
                "author": "Vignesh Subbian",
                "arxiv_comment": "2 figures, 5 tables, submitted to 2025 AMIA Annual Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05278v1",
                "updated": "2025-04-07T17:24:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    24,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:24:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    24,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "The challenge of uncertainty quantification of large language models in\n  medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of uncertainty quantification of large language models in\n  medicine"
                },
                "summary": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge."
                },
                "authors": [
                    {
                        "name": "Zahra Atf"
                    },
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Peter R. Lewis"
                    },
                    {
                        "name": "Aref Mahjoubfar"
                    },
                    {
                        "name": "Nariman Naderi"
                    },
                    {
                        "name": "Thomas R. Savage"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05276v1",
                "updated": "2025-04-07T17:17:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    17,
                    41,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:17:41Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    17,
                    41,
                    0,
                    97,
                    0
                ],
                "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation"
                },
                "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."
                },
                "authors": [
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Haoyu Han"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yu Xue"
                    },
                    {
                        "name": "Tingting Li"
                    },
                    {
                        "name": "Joseph Krajcik"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02658v2",
                "updated": "2025-04-07T17:09:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    9,
                    26,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T14:54:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    54,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank\n  Compensators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank\n  Compensators"
                },
                "summary": "A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\nwith massive parameters is quantization. However, state-of-the-art MoE models\nsuffer from non-negligible accuracy loss with extreme quantization, such as\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\nhighly quantized MoEs with a mixture of low-rank compensators. These\ncompensators consume only a small amount of additional memory but significantly\nrecover accuracy loss from extreme quantization. MiLo also identifies that\nMoEmodels exhibit distinctive characteristics across weights due to their\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\nalong with iterative optimizations to close the accuracy gap. MiLo does not\nrely on calibration data, allowing it to generalize to different MoE models and\ndatasets without overfitting to a calibration set. To avoid the hardware\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\nmethods on SoTA MoE models across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\nwith massive parameters is quantization. However, state-of-the-art MoE models\nsuffer from non-negligible accuracy loss with extreme quantization, such as\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\nhighly quantized MoEs with a mixture of low-rank compensators. These\ncompensators consume only a small amount of additional memory but significantly\nrecover accuracy loss from extreme quantization. MiLo also identifies that\nMoEmodels exhibit distinctive characteristics across weights due to their\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\nalong with iterative optimizations to close the accuracy gap. MiLo does not\nrely on calibration data, allowing it to generalize to different MoE models and\ndatasets without overfitting to a calibration set. To avoid the hardware\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\nmethods on SoTA MoE models across various tasks."
                },
                "authors": [
                    {
                        "name": "Beichen Huang"
                    },
                    {
                        "name": "Yueming Yuan"
                    },
                    {
                        "name": "Zelei Shao"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05271v1",
                "updated": "2025-04-07T17:08:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    8,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:08:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    8,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point\n  Detection for Accurate Characterization of Anomalous Diffusion in Video Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point\n  Detection for Accurate Characterization of Anomalous Diffusion in Video Data"
                },
                "summary": "Anomalous diffusion occurs in a wide range of systems, including protein\ntransport within cells, animal movement in complex habitats, pollutant\ndispersion in groundwater, and nanoparticle motion in synthetic materials.\nAccurately estimating the anomalous diffusion exponent and the diffusion\ncoefficient from the particle trajectories is essential to distinguish between\nsub-diffusive, super-diffusive, or normal diffusion regimes. These estimates\nprovide a deeper insight into the underlying dynamics of the system,\nfacilitating the identification of particle behaviors and the detection of\nchanges in diffusion states. However, analyzing short and noisy video data,\nwhich often yield incomplete and heterogeneous trajectories, poses a\nsignificant challenge for traditional statistical approaches. We introduce a\ndata-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these\nissues. This approach not only infers the anomalous diffusion parameters with\nhigh accuracy but also identifies temporal transitions between different\nstates, even in the presence of noise and limited temporal resolution. Our\nmethodology demonstrated strong performance in the 2nd Anomalous Diffusion\n(AnDi) Challenge benchmark within the top submissions for video tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomalous diffusion occurs in a wide range of systems, including protein\ntransport within cells, animal movement in complex habitats, pollutant\ndispersion in groundwater, and nanoparticle motion in synthetic materials.\nAccurately estimating the anomalous diffusion exponent and the diffusion\ncoefficient from the particle trajectories is essential to distinguish between\nsub-diffusive, super-diffusive, or normal diffusion regimes. These estimates\nprovide a deeper insight into the underlying dynamics of the system,\nfacilitating the identification of particle behaviors and the detection of\nchanges in diffusion states. However, analyzing short and noisy video data,\nwhich often yield incomplete and heterogeneous trajectories, poses a\nsignificant challenge for traditional statistical approaches. We introduce a\ndata-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these\nissues. This approach not only infers the anomalous diffusion parameters with\nhigh accuracy but also identifies temporal transitions between different\nstates, even in the presence of noise and limited temporal resolution. Our\nmethodology demonstrated strong performance in the 2nd Anomalous Diffusion\n(AnDi) Challenge benchmark within the top submissions for video tasks."
                },
                "authors": [
                    {
                        "name": "Yusef Ahsini"
                    },
                    {
                        "name": "Marc Escoto"
                    },
                    {
                        "name": "J. Alberto Conejero"
                    }
                ],
                "author_detail": {
                    "name": "J. Alberto Conejero"
                },
                "author": "J. Alberto Conejero",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05262v1",
                "updated": "2025-04-07T16:57:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    57,
                    10,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:57:10Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    57,
                    10,
                    0,
                    97,
                    0
                ],
                "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models"
                },
                "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Yan"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05259v1",
                "updated": "2025-04-07T16:52:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    52,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:52:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    52,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "How to evaluate control measures for LLM agents? A trajectory from today\n  to superintelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to evaluate control measures for LLM agents? A trajectory from today\n  to superintelligence"
                },
                "summary": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk."
                },
                "authors": [
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Geoffrey Irving"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Irving"
                },
                "author": "Geoffrey Irving",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05258v1",
                "updated": "2025-04-07T16:51:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Adrián Bazaga"
                    },
                    {
                        "name": "Rexhina Blloshmi"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Adrià de Gispert"
                    }
                ],
                "author_detail": {
                    "name": "Adrià de Gispert"
                },
                "author": "Adrià de Gispert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21157v3",
                "updated": "2025-04-07T16:49:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    49,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-27T04:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    50,
                    14,
                    3,
                    86,
                    0
                ],
                "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?"
                },
                "summary": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall."
                },
                "authors": [
                    {
                        "name": "Ashish Sardana"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Sardana"
                },
                "author": "Ashish Sardana",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00689v2",
                "updated": "2025-04-07T16:38:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    38,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-01T15:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    50,
                    58,
                    4,
                    306,
                    0
                ],
                "title": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a reliable external\nknowledge augmentation technique to mitigate hallucination issues and\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\nadaptive RAG (ARAG) systems excel at in-depth exploration within a single\nsource but struggle to effectively and controllably explore different retrieval\nsources, as they fail to foresee their internal knowledge features. We develop\na novel multi-source ARAG system, PrefRAG, which enhances RAG by enabling\nin-depth and controllable exploration of diverse retrieval sources through\npreference-driven adaptive retrieval and self-reflection. PrefRAG first fully\nexplores controllable local sources in adaptive retrieval and supplements with\nthe web when appropriate, ultimately selecting the optimal source for knowledge\nobservation. Subsequently, PrefRAG feeds answer quality feedback into the\nretrieval process, optimizing it from the generation perspective to produce\nhigher-quality responses. Extensive experiments confirm its superiority, high\nretrieval efficiency, and knowledge controllability. PrefRAG outperforms\nVanilla RAG and the leading MS-ARAG by up to 25.6% and 13.9% respectively.\nAdditionally, PrefRAG trained with DPO achieves higher performance. The code\nand data are available at https://github.com/QingFei1/PrefRAG.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a reliable external\nknowledge augmentation technique to mitigate hallucination issues and\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\nadaptive RAG (ARAG) systems excel at in-depth exploration within a single\nsource but struggle to effectively and controllably explore different retrieval\nsources, as they fail to foresee their internal knowledge features. We develop\na novel multi-source ARAG system, PrefRAG, which enhances RAG by enabling\nin-depth and controllable exploration of diverse retrieval sources through\npreference-driven adaptive retrieval and self-reflection. PrefRAG first fully\nexplores controllable local sources in adaptive retrieval and supplements with\nthe web when appropriate, ultimately selecting the optimal source for knowledge\nobservation. Subsequently, PrefRAG feeds answer quality feedback into the\nretrieval process, optimizing it from the generation perspective to produce\nhigher-quality responses. Extensive experiments confirm its superiority, high\nretrieval efficiency, and knowledge controllability. PrefRAG outperforms\nVanilla RAG and the leading MS-ARAG by up to 25.6% and 13.9% respectively.\nAdditionally, PrefRAG trained with DPO achieves higher performance. The code\nand data are available at https://github.com/QingFei1/PrefRAG.git."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "33 pages, 5 figures, 28 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05248v1",
                "updated": "2025-04-07T16:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    34,
                    57,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    34,
                    57,
                    0,
                    97,
                    0
                ],
                "title": "PINNverse: Accurate parameter estimation in differential equations from\n  noisy data with constrained physics-informed neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNverse: Accurate parameter estimation in differential equations from\n  noisy data with constrained physics-informed neural networks"
                },
                "summary": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve."
                },
                "authors": [
                    {
                        "name": "Marius Almanstötter"
                    },
                    {
                        "name": "Roman Vetter"
                    },
                    {
                        "name": "Dagmar Iber"
                    }
                ],
                "author_detail": {
                    "name": "Dagmar Iber"
                },
                "author": "Dagmar Iber",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05244v1",
                "updated": "2025-04-07T16:30:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    30,
                    31,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:30:31Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    30,
                    31,
                    0,
                    97,
                    0
                ],
                "title": "Behind the Spotlight: A systematic assessment of outshining using NIRCam\n  medium-bands in the JADES Origins Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behind the Spotlight: A systematic assessment of outshining using NIRCam\n  medium-bands in the JADES Origins Field"
                },
                "summary": "The spatial resolution and sensitivity of JWST's NIRCam instrument has\nrevolutionised our ability to probe the internal structure of early galaxies.\nBy leveraging deep medium-band imaging in the Jades Origins Field, we assemble\ncomprehensive spectral energy distributions (SEDs) using 19 photometric bands\nfor over 200 high-redshift galaxies ($z \\geq 4.5$). We present an analysis of\nthis sample with particular emphasis on investigating the \"outshining\"\nphenomenon, which can bias the inferred stellar populations by masking the\npresence of evolved stellar populations ($\\geq$ 100 Myr) with the light of\nbright, young O and B-type stars. We address this problem by performing\nspatially-resolved SED-fitting of both binned and full pixel-by-pixel\nphotometry, which we compare to the traditional integrated approach. We find\nevidence for systematic underestimation of stellar mass in low-mass galaxies\n($\\leq 10^9 \\rm M_\\odot$) with bursty star formation, which can exceed a factor\nof 10 in individual cases, but on average is typically a factor of 1.25-2.5,\ndepending on the binning methodology and SFH model used. The observed mass\noffset correlates with burstiness (SFR$_{10 \\ \\rm Myr}$/SFR$_{100 \\ \\rm Myr}$)\nand sSFR, such that galaxies with recently rising SFHs have larger mass\noffsets. The integrated SFH models which produce the most consistent stellar\nmasses are the double power-law and non-parametric `continuity' models,\nalthough no integrated model fully reproduces all resolved SFHs. We apply an\noutshining correction factor to the Stellar Mass Function at $z=7$, finding\nlittle impact within the uncertainties. We conclude that outshining can be\nimportant in individual low-mass galaxies, but the overall impact is limited\nand should be considered alongside other systematic SED fitting effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spatial resolution and sensitivity of JWST's NIRCam instrument has\nrevolutionised our ability to probe the internal structure of early galaxies.\nBy leveraging deep medium-band imaging in the Jades Origins Field, we assemble\ncomprehensive spectral energy distributions (SEDs) using 19 photometric bands\nfor over 200 high-redshift galaxies ($z \\geq 4.5$). We present an analysis of\nthis sample with particular emphasis on investigating the \"outshining\"\nphenomenon, which can bias the inferred stellar populations by masking the\npresence of evolved stellar populations ($\\geq$ 100 Myr) with the light of\nbright, young O and B-type stars. We address this problem by performing\nspatially-resolved SED-fitting of both binned and full pixel-by-pixel\nphotometry, which we compare to the traditional integrated approach. We find\nevidence for systematic underestimation of stellar mass in low-mass galaxies\n($\\leq 10^9 \\rm M_\\odot$) with bursty star formation, which can exceed a factor\nof 10 in individual cases, but on average is typically a factor of 1.25-2.5,\ndepending on the binning methodology and SFH model used. The observed mass\noffset correlates with burstiness (SFR$_{10 \\ \\rm Myr}$/SFR$_{100 \\ \\rm Myr}$)\nand sSFR, such that galaxies with recently rising SFHs have larger mass\noffsets. The integrated SFH models which produce the most consistent stellar\nmasses are the double power-law and non-parametric `continuity' models,\nalthough no integrated model fully reproduces all resolved SFHs. We apply an\noutshining correction factor to the Stellar Mass Function at $z=7$, finding\nlittle impact within the uncertainties. We conclude that outshining can be\nimportant in individual low-mass galaxies, but the overall impact is limited\nand should be considered alongside other systematic SED fitting effects."
                },
                "authors": [
                    {
                        "name": "Thomas Harvey"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "Nathan J. Adams"
                    },
                    {
                        "name": "Duncan Austin"
                    },
                    {
                        "name": "Qiong Li"
                    },
                    {
                        "name": "Vadim Rusakov"
                    },
                    {
                        "name": "Lewi Westcott"
                    },
                    {
                        "name": "Caio M. Goolsby"
                    },
                    {
                        "name": "Christopher C. Lovell"
                    },
                    {
                        "name": "Rachel K. Cochrane"
                    },
                    {
                        "name": "Aswin P. Vijayan"
                    },
                    {
                        "name": "James Trussler"
                    }
                ],
                "author_detail": {
                    "name": "James Trussler"
                },
                "author": "James Trussler",
                "arxiv_comment": "23 pages, 14 figures. Submitted to MNRAS. Code available on GitHub:\n  https://github.com/tHarvey303/EXPANSE . Comments welcome at\n  thomas.harvey-3@manchester.ac.uk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05240v1",
                "updated": "2025-04-07T16:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    19,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:23:19Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    19,
                    0,
                    97,
                    0
                ],
                "title": "Bayesian local clustering of age-period mortality surfaces across\n  multiple countries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian local clustering of age-period mortality surfaces across\n  multiple countries"
                },
                "summary": "Although traditional literature on mortality modeling has focused on single\ncountries in isolation, recent contributions have progressively moved toward\njoint models for multiple countries. Besides favoring borrowing of information\nto improve age-period forecasts, this perspective has also potentials to infer\nlocal similarities among countries' mortality patterns in specific age classes\nand periods that could unveil unexplored demographic trends, while guiding the\ndesign of targeted policies. Advancements along this latter relevant direction\nare currently undermined by the lack of a multi-country model capable of\nincorporating the core structures of age-period mortality surfaces together\nwith clustering patterns among countries that are not global, but rather vary\nlocally across different combinations of ages and periods. We cover this gap by\ndeveloping a novel Bayesian model for log-mortality rates that characterizes\nthe age structure of mortality through a B-spline expansion whose\ncountry-specific dynamic coefficients encode both changes of this age structure\nacross periods and also local clustering patterns among countries under a\ntime-dependent random partition prior for these country-specific dynamic\ncoefficients. While flexible, this formulation admits tractable posterior\ninference leveraging a suitably-designed Gibbs-sampler. The application to\nmortality data from 14 countries unveils local similarities highlighting both\npreviously-recognized demographic phenomena and also yet-unexplored trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although traditional literature on mortality modeling has focused on single\ncountries in isolation, recent contributions have progressively moved toward\njoint models for multiple countries. Besides favoring borrowing of information\nto improve age-period forecasts, this perspective has also potentials to infer\nlocal similarities among countries' mortality patterns in specific age classes\nand periods that could unveil unexplored demographic trends, while guiding the\ndesign of targeted policies. Advancements along this latter relevant direction\nare currently undermined by the lack of a multi-country model capable of\nincorporating the core structures of age-period mortality surfaces together\nwith clustering patterns among countries that are not global, but rather vary\nlocally across different combinations of ages and periods. We cover this gap by\ndeveloping a novel Bayesian model for log-mortality rates that characterizes\nthe age structure of mortality through a B-spline expansion whose\ncountry-specific dynamic coefficients encode both changes of this age structure\nacross periods and also local clustering patterns among countries under a\ntime-dependent random partition prior for these country-specific dynamic\ncoefficients. While flexible, this formulation admits tractable posterior\ninference leveraging a suitably-designed Gibbs-sampler. The application to\nmortality data from 14 countries unveils local similarities highlighting both\npreviously-recognized demographic phenomena and also yet-unexplored trends."
                },
                "authors": [
                    {
                        "name": "Giovanni Romanò"
                    },
                    {
                        "name": "Emanuele Aliverti"
                    },
                    {
                        "name": "Daniele Durante"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Durante"
                },
                "author": "Daniele Durante",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05239v1",
                "updated": "2025-04-07T16:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    7,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:23:07Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    7,
                    0,
                    97,
                    0
                ],
                "title": "LLM-based Automated Grading with Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Automated Grading with Human-in-the-Loop"
                },
                "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yasemin Copur-Gencturk"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24078v2",
                "updated": "2025-04-07T16:20:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    20,
                    29,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-31T13:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    33,
                    30,
                    0,
                    90,
                    0
                ],
                "title": "A Complete Mental Temporal Logic for Intelligent Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Mental Temporal Logic for Intelligent Agent"
                },
                "summary": "In this paper, we present a complete mental temporal logic, called BPICTL,\nwhich generalizes CTL by introducing mental modalities. A sound and complete\ninference system of BPICTL is given. We prove the finite model property of\nBPICTL. Furthermore, we present a model checking algorithm for BPICTL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a complete mental temporal logic, called BPICTL,\nwhich generalizes CTL by introducing mental modalities. A sound and complete\ninference system of BPICTL is given. We prove the finite model property of\nBPICTL. Furthermore, we present a model checking algorithm for BPICTL."
                },
                "authors": [
                    {
                        "name": "Zining Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zining Cao"
                },
                "author": "Zining Cao",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05235v1",
                "updated": "2025-04-07T16:19:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    19,
                    50,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:19:50Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    19,
                    50,
                    0,
                    97,
                    0
                ],
                "title": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations"
                },
                "summary": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys."
                },
                "authors": [
                    {
                        "name": "Sneh Pandya"
                    },
                    {
                        "name": "Yuanyuan Yang"
                    },
                    {
                        "name": "Nicholas Van Alfen"
                    },
                    {
                        "name": "Jonathan Blazek"
                    },
                    {
                        "name": "Robin Walters"
                    }
                ],
                "author_detail": {
                    "name": "Robin Walters"
                },
                "author": "Robin Walters",
                "arxiv_comment": "16 pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08821v2",
                "updated": "2025-04-07T16:17:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    17,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-11T14:03:29Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) mitigates factual errors and\nhallucinations in Large Language Models (LLMs) for question-answering (QA) by\nincorporating external knowledge. However, existing adaptive RAG methods rely\non LLMs to predict retrieval timing and directly use retrieved information for\ngeneration, often failing to reflect real information needs and fully leverage\nretrieved knowledge. We develop DeepNote, an adaptive RAG framework that\nachieves in-depth and robust exploration of knowledge sources through\nnote-centric adaptive retrieval. DeepNote employs notes as carriers for\nrefining and accumulating knowledge. During in-depth exploration, it uses these\nnotes to determine retrieval timing, formulate retrieval queries, and\niteratively assess knowledge growth, ultimately leveraging the best note for\nanswer generation. Extensive experiments and analyses demonstrate that DeepNote\nsignificantly outperforms all baselines (+10.2% to +20.1%) and exhibits the\nability to gather knowledge with both high density and quality. Additionally,\nDPO further improves the performance of DeepNote. The code and data are\navailable at https://github.com/thunlp/DeepNote.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) mitigates factual errors and\nhallucinations in Large Language Models (LLMs) for question-answering (QA) by\nincorporating external knowledge. However, existing adaptive RAG methods rely\non LLMs to predict retrieval timing and directly use retrieved information for\ngeneration, often failing to reflect real information needs and fully leverage\nretrieved knowledge. We develop DeepNote, an adaptive RAG framework that\nachieves in-depth and robust exploration of knowledge sources through\nnote-centric adaptive retrieval. DeepNote employs notes as carriers for\nrefining and accumulating knowledge. During in-depth exploration, it uses these\nnotes to determine retrieval timing, formulate retrieval queries, and\niteratively assess knowledge growth, ultimately leveraging the best note for\nanswer generation. Extensive experiments and analyses demonstrate that DeepNote\nsignificantly outperforms all baselines (+10.2% to +20.1%) and exhibits the\nability to gather knowledge with both high density and quality. Additionally,\nDPO further improves the performance of DeepNote. The code and data are\navailable at https://github.com/thunlp/DeepNote."
                },
                "authors": [
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "28 pages, 6 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05231v1",
                "updated": "2025-04-07T16:15:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    15,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:15:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    15,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Mapping biodiversity at very-high resolution in Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping biodiversity at very-high resolution in Europe"
                },
                "summary": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs."
                },
                "authors": [
                    {
                        "name": "César Leblanc"
                    },
                    {
                        "name": "Lukas Picek"
                    },
                    {
                        "name": "Benjamin Deneu"
                    },
                    {
                        "name": "Pierre Bonnet"
                    },
                    {
                        "name": "Maximilien Servajean"
                    },
                    {
                        "name": "Rémi Palard"
                    },
                    {
                        "name": "Alexis Joly"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Joly"
                },
                "author": "Alexis Joly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09439v2",
                "updated": "2025-04-07T16:13:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    13,
                    38,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-14T16:58:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Spider: Any-to-Many Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spider: Any-to-Many Multimodal LLM"
                },
                "summary": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, an Any-to-Many Instruction Template designed\nfor producing Xs signal prompts, and a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents. To train\nSpider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which\nfacilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for\nAMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset,\nthe first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG\ntasks in future research. Overall, this work not only pushes the boundary of\nmultimodal interaction but also provides rich data support for advancing the\nfield. Code: https://github.com/Layjins/Spider",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, an Any-to-Many Instruction Template designed\nfor producing Xs signal prompts, and a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents. To train\nSpider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which\nfacilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for\nAMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset,\nthe first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG\ntasks in future research. Overall, this work not only pushes the boundary of\nmultimodal interaction but also provides rich data support for advancing the\nfield. Code: https://github.com/Layjins/Spider"
                },
                "authors": [
                    {
                        "name": "Jinxiang Lai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05220v2",
                "updated": "2025-04-08T02:11:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    2,
                    11,
                    5,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T16:05:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    5,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG"
                },
                "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05217v1",
                "updated": "2025-04-07T16:04:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    4,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:04:00Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    4,
                    0,
                    0,
                    97,
                    0
                ],
                "title": "LLM-Alignment Live-Streaming Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Alignment Live-Streaming Recommendation"
                },
                "summary": "In recent years, integrated short-video and live-streaming platforms have\ngained massive global adoption, offering dynamic content creation and\nconsumption. Unlike pre-recorded short videos, live-streaming enables real-time\ninteraction between authors and users, fostering deeper engagement. However,\nthis dynamic nature introduces a critical challenge for recommendation systems\n(RecSys): the same live-streaming vastly different experiences depending on\nwhen a user watching. To optimize recommendations, a RecSys must accurately\ninterpret the real-time semantics of live content and align them with user\npreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, integrated short-video and live-streaming platforms have\ngained massive global adoption, offering dynamic content creation and\nconsumption. Unlike pre-recorded short videos, live-streaming enables real-time\ninteraction between authors and users, fostering deeper engagement. However,\nthis dynamic nature introduces a critical challenge for recommendation systems\n(RecSys): the same live-streaming vastly different experiences depending on\nwhen a user watching. To optimize recommendations, a RecSys must accurately\ninterpret the real-time semantics of live content and align them with user\npreferences."
                },
                "authors": [
                    {
                        "name": "Yueyang Liu"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Shuang Wen"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Xiangyu Wu"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05216v1",
                "updated": "2025-04-07T16:03:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:03:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling"
                },
                "summary": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xiaojie Sun"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07045v2",
                "updated": "2025-04-07T16:01:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-02-10T21:27:06Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    27,
                    6,
                    0,
                    41,
                    0
                ],
                "title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs"
                },
                "summary": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    }
                ],
                "author_detail": {
                    "name": "John D. Hastings"
                },
                "author": "John D. Hastings",
                "arxiv_comment": "6 pages, 0 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05214v1",
                "updated": "2025-04-07T16:01:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Post-Training Language Models for Continual Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Language Models for Continual Relation Extraction"
                },
                "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05204v1",
                "updated": "2025-04-07T15:51:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    51,
                    31,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:51:31Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    51,
                    31,
                    0,
                    97,
                    0
                ],
                "title": "Quantum Program Linting with LLMs: Emerging Results from a Comparative\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Program Linting with LLMs: Emerging Results from a Comparative\n  Study"
                },
                "summary": "Ensuring the quality of quantum programs is increasingly important; however,\ntraditional static analysis techniques are insufficient due to the unique\ncharacteristics of quantum computing. Quantum-specific linting tools, such as\nLintQ, have been developed to detect quantum-specific programming problems;\nhowever, they typically rely on manually crafted analysis queries. The manual\neffort required to update these tools limits their adaptability to evolving\nquantum programming practices.\n  To address this challenge, this study investigates the feasibility of\nemploying Large Language Models (LLMs) to develop a novel linting technique for\nquantum software development and explores potential avenues to advance linting\napproaches. We introduce LintQ-LLM, an LLM-based linting tool designed to\ndetect quantum-specific problems comparable to those identified by LintQ.\nThrough an empirical comparative study using real-world Qiskit programs, our\nresults show that LintQ-LLM is a viable solution that complements LintQ, with\nparticular strengths in problem localization, explanation clarity, and\nadaptability potential for emerging quantum programming frameworks, thus\nproviding a basis for further research. Furthermore, this study discusses\nseveral research opportunities for developing more advanced, adaptable, and\nfeedback-aware quantum software quality assurance methods by leveraging LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the quality of quantum programs is increasingly important; however,\ntraditional static analysis techniques are insufficient due to the unique\ncharacteristics of quantum computing. Quantum-specific linting tools, such as\nLintQ, have been developed to detect quantum-specific programming problems;\nhowever, they typically rely on manually crafted analysis queries. The manual\neffort required to update these tools limits their adaptability to evolving\nquantum programming practices.\n  To address this challenge, this study investigates the feasibility of\nemploying Large Language Models (LLMs) to develop a novel linting technique for\nquantum software development and explores potential avenues to advance linting\napproaches. We introduce LintQ-LLM, an LLM-based linting tool designed to\ndetect quantum-specific problems comparable to those identified by LintQ.\nThrough an empirical comparative study using real-world Qiskit programs, our\nresults show that LintQ-LLM is a viable solution that complements LintQ, with\nparticular strengths in problem localization, explanation clarity, and\nadaptability potential for emerging quantum programming frameworks, thus\nproviding a basis for further research. Furthermore, this study discusses\nseveral research opportunities for developing more advanced, adaptable, and\nfeedback-aware quantum software quality assurance methods by leveraging LLMs."
                },
                "authors": [
                    {
                        "name": "Seung Yeob Shin"
                    },
                    {
                        "name": "Fabrizio Pastore"
                    },
                    {
                        "name": "Domenico Bianculli"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Bianculli"
                },
                "author": "Domenico Bianculli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17395v2",
                "updated": "2025-04-07T15:50:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    50,
                    55,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-26T12:55:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    55,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Asymptotics for estimating a diverging number of parameters -- with and\n  without sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for estimating a diverging number of parameters -- with and\n  without sparsity"
                },
                "summary": "We consider high-dimensional estimation problems where the number of\nparameters diverges with the sample size. General conditions are established\nfor consistency, uniqueness, and asymptotic normality in both unpenalized and\npenalized estimation settings. The conditions are weak and accommodate a broad\nclass of estimation problems, including ones with non-convex and group\nstructured penalties. The wide applicability of the results is illustrated\nthrough diverse examples, including generalized linear models, multi-sample\ninference, and stepwise estimation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider high-dimensional estimation problems where the number of\nparameters diverges with the sample size. General conditions are established\nfor consistency, uniqueness, and asymptotic normality in both unpenalized and\npenalized estimation settings. The conditions are weak and accommodate a broad\nclass of estimation problems, including ones with non-convex and group\nstructured penalties. The wide applicability of the results is illustrated\nthrough diverse examples, including generalized linear models, multi-sample\ninference, and stepwise estimation procedures."
                },
                "authors": [
                    {
                        "name": "Jana Gauss"
                    },
                    {
                        "name": "Thomas Nagler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Nagler"
                },
                "author": "Thomas Nagler",
                "arxiv_comment": "51 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09722v2",
                "updated": "2025-04-07T15:39:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    39,
                    16,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-12T20:59:43Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    20,
                    59,
                    43,
                    3,
                    347,
                    0
                ],
                "title": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers"
                },
                "summary": "The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer."
                },
                "authors": [
                    {
                        "name": "Sarkar Snigdha Sarathi Das"
                    },
                    {
                        "name": "Ryo Kamoi"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yusen Zhang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05185v1",
                "updated": "2025-04-07T15:35:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    35,
                    54,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    35,
                    54,
                    0,
                    97,
                    0
                ],
                "title": "Concise Reasoning via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise Reasoning via Reinforcement Learning"
                },
                "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results."
                },
                "authors": [
                    {
                        "name": "Mehdi Fatemi"
                    },
                    {
                        "name": "Banafsheh Rafiee"
                    },
                    {
                        "name": "Mingjie Tang"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Talamadupula"
                },
                "author": "Kartik Talamadupula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23633v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23633v3",
                "updated": "2025-04-07T15:29:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    29,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-31T00:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    0,
                    12,
                    48,
                    0,
                    90,
                    0
                ],
                "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS"
                },
                "summary": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. As we design and deploy increasingly intelligent\ngeospatial systems, we have a responsibility to ensure they are developed in\nsocially responsible ways, serve the public good, and support the continued\nvalue of human geographic insight in an AI-augmented future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. As we design and deploy increasingly intelligent\ngeospatial systems, we have a responsibility to ensure they are developed in\nsocially responsible ways, serve the public good, and support the continued\nvalue of human geographic insight in an AI-augmented future."
                },
                "authors": [
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Krzysztof Janowicz"
                    },
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Samantha T. Arundel"
                    },
                    {
                        "name": "Chaowei Yang"
                    },
                    {
                        "name": "Budhendra Bhaduri"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "A-Xing Zhu"
                    },
                    {
                        "name": "Mark Gahegan"
                    },
                    {
                        "name": "Shashi Shekhar"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Grant McKenzie"
                    },
                    {
                        "name": "Guido Cervone"
                    },
                    {
                        "name": "Michael E. Hodgson"
                    }
                ],
                "author_detail": {
                    "name": "Michael E. Hodgson"
                },
                "author": "Michael E. Hodgson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23633v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23633v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05180v1",
                "updated": "2025-04-07T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA\n  Tasks"
                },
                "summary": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yang Zou"
                    },
                    {
                        "name": "Christopher Ellis"
                    },
                    {
                        "name": "Ruben Purdy"
                    },
                    {
                        "name": "Shawn Blanton"
                    },
                    {
                        "name": "José M. F. Moura"
                    }
                ],
                "author_detail": {
                    "name": "José M. F. Moura"
                },
                "author": "José M. F. Moura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05178v1",
                "updated": "2025-04-07T15:24:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    24,
                    54,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    24,
                    54,
                    0,
                    97,
                    0
                ],
                "title": "The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential\n  of Large Multimodal Models for Referring Video Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential\n  of Large Multimodal Models for Referring Video Segmentation"
                },
                "summary": "Motion expression video segmentation is designed to segment objects in\naccordance with the input motion expressions. In contrast to the conventional\nReferring Video Object Segmentation (RVOS), it places emphasis on motion as\nwell as multi-object expressions, making it more arduous. Recently, Large\nMultimodal Models (LMMs) have begun to shine in RVOS due to their powerful\nvision-language perception capabilities. In this work, we propose a simple and\neffective inference optimization method to fully unleash the potential of LMMs\nin referring video segmentation. Firstly, we use Sa2VA as our baseline, which\nis a unified LMM for dense grounded understanding of both images and videos.\nSecondly, we uniformly sample the video frames during the inference process to\nenhance the model's understanding of the entire video. Finally, we integrate\nthe results of multiple expert models to mitigate the erroneous predictions of\na single model. Our solution achieved 61.98% J&F on the MeViS test set and\nranked 1st place in the 4th PVUW Challenge MeViS Track at CVPR 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion expression video segmentation is designed to segment objects in\naccordance with the input motion expressions. In contrast to the conventional\nReferring Video Object Segmentation (RVOS), it places emphasis on motion as\nwell as multi-object expressions, making it more arduous. Recently, Large\nMultimodal Models (LMMs) have begun to shine in RVOS due to their powerful\nvision-language perception capabilities. In this work, we propose a simple and\neffective inference optimization method to fully unleash the potential of LMMs\nin referring video segmentation. Firstly, we use Sa2VA as our baseline, which\nis a unified LMM for dense grounded understanding of both images and videos.\nSecondly, we uniformly sample the video frames during the inference process to\nenhance the model's understanding of the entire video. Finally, we integrate\nthe results of multiple expert models to mitigate the erroneous predictions of\na single model. Our solution achieved 61.98% J&F on the MeViS test set and\nranked 1st place in the 4th PVUW Challenge MeViS Track at CVPR 2025."
                },
                "authors": [
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Runmin Cong"
                    },
                    {
                        "name": "Xiankai Lu"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05177v1",
                "updated": "2025-04-07T15:23:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    23,
                    53,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:23:53Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    23,
                    53,
                    0,
                    97,
                    0
                ],
                "title": "A parametric study of solar wind properties and composition using fluid\n  and kinetic solar wind models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A parametric study of solar wind properties and composition using fluid\n  and kinetic solar wind models"
                },
                "summary": "The physical processes in the solar corona that shape the solar wind remain\nan active research topic. Modeling efforts have shown that energy and plasma\nexchanges near the transition region plays a crucial role in modulating solar\nwind properties. Although these regions cannot be measured in situ, plasma\nparameters can be inferred from coronal spectroscopy and ionization states of\nheavy ions, which remain unchanged as they escape the corona. We introduce a\nnew solar wind model extending from the chromosphere to the inner heliosphere,\ncapturing thermodynamic coupling across atmospheric layers. By including\nneutral and charged particle interactions, we model the transport and\nionisation processes of the gas through the transition region, the corona and\ninto the solar wind. Instead of explicitly modeling coronal heating, we link\nits spatial distribution to large-scale magnetic field properties. Our results\nconfirm that energy deposition strongly affects wind properties through key\nmechanisms involving chromospheric evaporation, thermal expansion, and magnetic\nflux expansion. For sources near active regions, the model predicts significant\nsolar wind acceleration, with plasma outflows comparable to those inferred from\ncoronal spectroscopy. For winds from large coronal holes, the model reproduces\nthe observed anticorrelation between charge state and wind speed. However, the\npredicted charge state ratios are overall lower than observed. Inclusion of a\npopulation of energetic electrons enhances both heavy ion charge states and\nsolar wind acceleration, improving agreement with observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The physical processes in the solar corona that shape the solar wind remain\nan active research topic. Modeling efforts have shown that energy and plasma\nexchanges near the transition region plays a crucial role in modulating solar\nwind properties. Although these regions cannot be measured in situ, plasma\nparameters can be inferred from coronal spectroscopy and ionization states of\nheavy ions, which remain unchanged as they escape the corona. We introduce a\nnew solar wind model extending from the chromosphere to the inner heliosphere,\ncapturing thermodynamic coupling across atmospheric layers. By including\nneutral and charged particle interactions, we model the transport and\nionisation processes of the gas through the transition region, the corona and\ninto the solar wind. Instead of explicitly modeling coronal heating, we link\nits spatial distribution to large-scale magnetic field properties. Our results\nconfirm that energy deposition strongly affects wind properties through key\nmechanisms involving chromospheric evaporation, thermal expansion, and magnetic\nflux expansion. For sources near active regions, the model predicts significant\nsolar wind acceleration, with plasma outflows comparable to those inferred from\ncoronal spectroscopy. For winds from large coronal holes, the model reproduces\nthe observed anticorrelation between charge state and wind speed. However, the\npredicted charge state ratios are overall lower than observed. Inclusion of a\npopulation of energetic electrons enhances both heavy ion charge states and\nsolar wind acceleration, improving agreement with observations."
                },
                "authors": [
                    {
                        "name": "Paul Lomazzi"
                    },
                    {
                        "name": "Alexis P. Rouillard"
                    },
                    {
                        "name": "Michael A. Lavarra"
                    },
                    {
                        "name": "Nicolas Poirier"
                    },
                    {
                        "name": "Pierre-Louis Blelly"
                    },
                    {
                        "name": "Jean-Baptiste Dakeyo"
                    },
                    {
                        "name": "Viviane Pierrard"
                    },
                    {
                        "name": "Victor Réville"
                    },
                    {
                        "name": "Christian Vocks"
                    },
                    {
                        "name": "Simon Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Simon Thomas"
                },
                "author": "Simon Thomas",
                "arxiv_doi": "10.3847/1538-4357/adc2f6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adc2f6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 10 figures, to be published in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94.30.Nv (Primary) 76X05, 82C20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11712v2",
                "updated": "2025-04-07T15:18:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    18,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-15T15:47:01Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    1,
                    1,
                    289,
                    0
                ],
                "title": "Parameter estimation of structural dynamics with neural operators\n  enabled surrogate modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of structural dynamics with neural operators\n  enabled surrogate modeling"
                },
                "summary": "Parameter estimation in structural dynamics generally involves inferring the\nvalues of physical, geometric, or even customized parameters based on first\nprinciples or expert knowledge, which is challenging for complex structural\nsystems. In this work, we present a unified deep learning-based framework for\nparameterization, forward modeling, and inverse modeling of structural\ndynamics. The parameterization is flexible and can be user-defined, including\nphysical and/or non-physical (customized) parameters. In the forward modeling,\nwe train a neural operator for response prediction -- forming a surrogate\nmodel, which leverages the defined system parameters and excitation forces as\ninputs to the model. The inverse modeling focuses on estimating system\nparameters. In particular, the learned forward surrogate model (which is\ndifferentiable) is utilized for preliminary parameter estimation via\ngradient-based optimization; to further boost the parameter estimation, we\nintroduce a neural refinement method to mitigate ill-posed problems, which\noften occur in the former. The framework's effectiveness is verified\nnumerically and experimentally, in both interpolation and extrapolation cases,\nindicating its capability to capture intrinsic dynamics of structural systems\nfrom both forward and inverse perspectives. Moreover, the framework's\nflexibility is expected to support a wide range of applications, including\nsurrogate modeling, structural identification, damage detection, and inverse\ndesign of structural systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation in structural dynamics generally involves inferring the\nvalues of physical, geometric, or even customized parameters based on first\nprinciples or expert knowledge, which is challenging for complex structural\nsystems. In this work, we present a unified deep learning-based framework for\nparameterization, forward modeling, and inverse modeling of structural\ndynamics. The parameterization is flexible and can be user-defined, including\nphysical and/or non-physical (customized) parameters. In the forward modeling,\nwe train a neural operator for response prediction -- forming a surrogate\nmodel, which leverages the defined system parameters and excitation forces as\ninputs to the model. The inverse modeling focuses on estimating system\nparameters. In particular, the learned forward surrogate model (which is\ndifferentiable) is utilized for preliminary parameter estimation via\ngradient-based optimization; to further boost the parameter estimation, we\nintroduce a neural refinement method to mitigate ill-posed problems, which\noften occur in the former. The framework's effectiveness is verified\nnumerically and experimentally, in both interpolation and extrapolation cases,\nindicating its capability to capture intrinsic dynamics of structural systems\nfrom both forward and inverse perspectives. Moreover, the framework's\nflexibility is expected to support a wide range of applications, including\nsurrogate modeling, structural identification, damage detection, and inverse\ndesign of structural systems."
                },
                "authors": [
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Haoze Song"
                    },
                    {
                        "name": "Wenjing Ye"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhilu Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhilu Lai"
                },
                "author": "Zhilu Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05169v1",
                "updated": "2025-04-07T15:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    14,
                    7,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    14,
                    7,
                    0,
                    97,
                    0
                ],
                "title": "Machine learning interatomic potential can infer electrical response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning interatomic potential can infer electrical response"
                },
                "summary": "Modeling the response of material and chemical systems to electric fields\nremains a longstanding challenge. Machine learning interatomic potentials\n(MLIPs) offer an efficient and scalable alternative to quantum mechanical\nmethods but do not by themselves incorporate electrical response. Here, we show\nthat polarization and Born effective charge (BEC) tensors can be directly\nextracted from long-range MLIPs within the Latent Ewald Summation (LES)\nframework, solely by learning from energy and force data. Using this approach,\nwe predict the infrared spectra of bulk water under zero or finite external\nelectric fields, ionic conductivities of high-pressure superionic ice, and the\nphase transition and hysteresis in ferroelectric PbTiO$_3$ perovskite. This\nwork thus extends the capability of MLIPs to predict electrical\nresponse--without training on charges or polarization or BECs--and enables\naccurate modeling of electric-field-driven processes in diverse systems at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the response of material and chemical systems to electric fields\nremains a longstanding challenge. Machine learning interatomic potentials\n(MLIPs) offer an efficient and scalable alternative to quantum mechanical\nmethods but do not by themselves incorporate electrical response. Here, we show\nthat polarization and Born effective charge (BEC) tensors can be directly\nextracted from long-range MLIPs within the Latent Ewald Summation (LES)\nframework, solely by learning from energy and force data. Using this approach,\nwe predict the infrared spectra of bulk water under zero or finite external\nelectric fields, ionic conductivities of high-pressure superionic ice, and the\nphase transition and hysteresis in ferroelectric PbTiO$_3$ perovskite. This\nwork thus extends the capability of MLIPs to predict electrical\nresponse--without training on charges or polarization or BECs--and enables\naccurate modeling of electric-field-driven processes in diverse systems at\nscale."
                },
                "authors": [
                    {
                        "name": "Peichen Zhong"
                    },
                    {
                        "name": "Dongjin Kim"
                    },
                    {
                        "name": "Daniel S. King"
                    },
                    {
                        "name": "Bingqing Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Bingqing Cheng"
                },
                "author": "Bingqing Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05164v1",
                "updated": "2025-04-07T15:08:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    35,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:08:35Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    35,
                    0,
                    97,
                    0
                ],
                "title": "Balancing Task-invariant Interaction and Task-specific Adaptation for\n  Unified Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Task-invariant Interaction and Task-specific Adaptation for\n  Unified Image Fusion"
                },
                "summary": "Unified image fusion aims to integrate complementary information from\nmulti-source images, enhancing image quality through a unified framework\napplicable to diverse fusion tasks. While treating all fusion tasks as a\nunified problem facilitates task-invariant knowledge sharing, it often\noverlooks task-specific characteristics, thereby limiting the overall\nperformance. Existing general image fusion methods incorporate explicit task\nidentification to enable adaptation to different fusion tasks. However, this\ndependence during inference restricts the model's generalization to unseen\nfusion tasks. To address these issues, we propose a novel unified image fusion\nframework named \"TITA\", which dynamically balances both Task-invariant\nInteraction and Task-specific Adaptation. For task-invariant interaction, we\nintroduce the Interaction-enhanced Pixel Attention (IPA) module to enhance\npixel-wise interactions for better multi-source complementary information\nextraction. For task-specific adaptation, the Operation-based Adaptive Fusion\n(OAF) module dynamically adjusts operation weights based on task properties.\nAdditionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO)\nstrategy to mitigate the impact of gradient conflicts across tasks during joint\ntraining. Extensive experiments demonstrate that TITA not only achieves\ncompetitive performance compared to specialized methods across three image\nfusion scenarios but also exhibits strong generalization to unseen fusion\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified image fusion aims to integrate complementary information from\nmulti-source images, enhancing image quality through a unified framework\napplicable to diverse fusion tasks. While treating all fusion tasks as a\nunified problem facilitates task-invariant knowledge sharing, it often\noverlooks task-specific characteristics, thereby limiting the overall\nperformance. Existing general image fusion methods incorporate explicit task\nidentification to enable adaptation to different fusion tasks. However, this\ndependence during inference restricts the model's generalization to unseen\nfusion tasks. To address these issues, we propose a novel unified image fusion\nframework named \"TITA\", which dynamically balances both Task-invariant\nInteraction and Task-specific Adaptation. For task-invariant interaction, we\nintroduce the Interaction-enhanced Pixel Attention (IPA) module to enhance\npixel-wise interactions for better multi-source complementary information\nextraction. For task-specific adaptation, the Operation-based Adaptive Fusion\n(OAF) module dynamically adjusts operation weights based on task properties.\nAdditionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO)\nstrategy to mitigate the impact of gradient conflicts across tasks during joint\ntraining. Extensive experiments demonstrate that TITA not only achieves\ncompetitive performance compared to specialized methods across three image\nfusion scenarios but also exhibits strong generalization to unseen fusion\ntasks."
                },
                "authors": [
                    {
                        "name": "Xingyu Hu"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Jiayi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Ma"
                },
                "author": "Jiayi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05163v1",
                "updated": "2025-04-07T15:08:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:08:03Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness"
                },
                "summary": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings."
                },
                "authors": [
                    {
                        "name": "Dongzhuoran Zhou"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Steffen Staab"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Staab"
                },
                "author": "Steffen Staab",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01681v3",
                "updated": "2025-04-07T15:00:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    0,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2024-01-22T06:02:39Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    6,
                    2,
                    39,
                    0,
                    22,
                    0
                ],
                "title": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social\n  Media Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social\n  Media Communications"
                },
                "summary": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Ge Gao"
                    },
                    {
                        "name": "Wei Ai"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ai"
                },
                "author": "Wei Ai",
                "arxiv_comment": "Accepted by the 19th International AAAI Conference on Web and Social\n  Media (ICWSM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05147v1",
                "updated": "2025-04-07T14:52:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:52:40Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "title": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs"
                },
                "summary": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods"
                },
                "authors": [
                    {
                        "name": "Amrita Roy Chowdhury"
                    },
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Divyam Anshumaan"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Nicolas Papernot"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Mihir Bellare"
                    }
                ],
                "author_detail": {
                    "name": "Mihir Bellare"
                },
                "author": "Mihir Bellare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05146v1",
                "updated": "2025-04-07T14:50:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    50,
                    13,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:50:13Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    50,
                    13,
                    0,
                    97,
                    0
                ],
                "title": "Query Smarter, Trust Better? Exploring Search Behaviours for Verifying\n  News Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Smarter, Trust Better? Exploring Search Behaviours for Verifying\n  News Accuracy"
                },
                "summary": "While it is often assumed that searching for information to evaluate\nmisinformation will help identify false claims, recent work suggests that\nsearch behaviours can instead reinforce belief in misleading news, particularly\nwhen users generate queries using vocabulary from the source articles. Our\nresearch explores how different query generation strategies affect news\nverification and whether the way people search influences the accuracy of their\ninformation evaluation. A mixed-methods approach was used, consisting of three\nparts: (1) an analysis of existing data to understand how search behaviour\ninfluences trust in fake news, (2) a simulation of query generation strategies\nusing a Large Language Model (LLM) to assess the impact of different query\nformulations on search result quality, and (3) a user study to examine how\n'Boost' interventions in interface design can guide users to adopt more\neffective query strategies. The results show that search behaviour\nsignificantly affects trust in news, with successful searches involving\nmultiple queries and yielding higher-quality results. Queries inspired by\ndifferent parts of a news article produced search results of varying quality,\nand weak initial queries improved when reformulated using full SERP\ninformation. Although 'Boost' interventions had limited impact, the study\nsuggests that interface design encouraging users to thoroughly review search\nresults can enhance query formulation. This study highlights the importance of\nquery strategies in evaluating news and proposes that interface design can play\na key role in promoting more effective search practices, serving as one\ncomponent of a broader set of interventions to combat misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While it is often assumed that searching for information to evaluate\nmisinformation will help identify false claims, recent work suggests that\nsearch behaviours can instead reinforce belief in misleading news, particularly\nwhen users generate queries using vocabulary from the source articles. Our\nresearch explores how different query generation strategies affect news\nverification and whether the way people search influences the accuracy of their\ninformation evaluation. A mixed-methods approach was used, consisting of three\nparts: (1) an analysis of existing data to understand how search behaviour\ninfluences trust in fake news, (2) a simulation of query generation strategies\nusing a Large Language Model (LLM) to assess the impact of different query\nformulations on search result quality, and (3) a user study to examine how\n'Boost' interventions in interface design can guide users to adopt more\neffective query strategies. The results show that search behaviour\nsignificantly affects trust in news, with successful searches involving\nmultiple queries and yielding higher-quality results. Queries inspired by\ndifferent parts of a news article produced search results of varying quality,\nand weak initial queries improved when reformulated using full SERP\ninformation. Although 'Boost' interventions had limited impact, the study\nsuggests that interface design encouraging users to thoroughly review search\nresults can enhance query formulation. This study highlights the importance of\nquery strategies in evaluating news and proposes that interface design can play\na key role in promoting more effective search practices, serving as one\ncomponent of a broader set of interventions to combat misinformation."
                },
                "authors": [
                    {
                        "name": "David Elsweiler"
                    },
                    {
                        "name": "Samy Ateia"
                    },
                    {
                        "name": "Markus Bink"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Marcos Fernández Pichel"
                    },
                    {
                        "name": "Alexander Frummet"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "David Losada"
                    },
                    {
                        "name": "Bernd Ludwig"
                    },
                    {
                        "name": "Selina Meyer"
                    },
                    {
                        "name": "Noel Pascual Presa"
                    }
                ],
                "author_detail": {
                    "name": "Noel Pascual Presa"
                },
                "author": "Noel Pascual Presa",
                "arxiv_comment": "12 pages, Pre-Print SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v3",
                "updated": "2025-04-07T14:42:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    42,
                    1,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05125v1",
                "updated": "2025-04-07T14:28:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    28,
                    56,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:28:56Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    28,
                    56,
                    0,
                    97,
                    0
                ],
                "title": "Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering"
                },
                "summary": "Clustering is an efficient and essential technique for exploring latent\nknowledge of data. However, limited attention has been given to the\ninterpretability of the clusters detected by most clustering algorithms. In\naddition, due to the homogeneity of data, different groups of data have their\nown homogeneous styles. In this paper, the above two aspects are considered,\nand an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering\n(IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is\nfully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples\nare grouped into clusters represented by the corresponding consequent vectors\nof all fuzzy rules learned in an unsupervised manner. This can explain how the\nclusters are generated in detail, thus making the underlying decision-making\nprocess of the IS-TSK-FC interpretable. Moreover, a series of style matrices\nare introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by\ncapturing the styles of clusters as well as the nuances between different\nstyles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data\nrepresentation capability. After determining the antecedents of all the fuzzy\nrules, the optimization problem of IS-TSK-FC can be iteratively solved in an\nalternation manner. The effectiveness of IS-TSK-FC as an interpretable\nclustering tool is validated through extensive experiments on benchmark\ndatasets with unknown implicit/explicit styles. Specially, the superior\nclustering performance of IS-TSK-FC is demonstrated on case studies where\ndifferent groups of data present explicit styles. The source code of IS-TSK-FC\ncan be downloaded from https://github.com/gusuhang10/IS-TSK-FC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering is an efficient and essential technique for exploring latent\nknowledge of data. However, limited attention has been given to the\ninterpretability of the clusters detected by most clustering algorithms. In\naddition, due to the homogeneity of data, different groups of data have their\nown homogeneous styles. In this paper, the above two aspects are considered,\nand an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering\n(IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is\nfully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples\nare grouped into clusters represented by the corresponding consequent vectors\nof all fuzzy rules learned in an unsupervised manner. This can explain how the\nclusters are generated in detail, thus making the underlying decision-making\nprocess of the IS-TSK-FC interpretable. Moreover, a series of style matrices\nare introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by\ncapturing the styles of clusters as well as the nuances between different\nstyles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data\nrepresentation capability. After determining the antecedents of all the fuzzy\nrules, the optimization problem of IS-TSK-FC can be iteratively solved in an\nalternation manner. The effectiveness of IS-TSK-FC as an interpretable\nclustering tool is validated through extensive experiments on benchmark\ndatasets with unknown implicit/explicit styles. Specially, the superior\nclustering performance of IS-TSK-FC is demonstrated on case studies where\ndifferent groups of data present explicit styles. The source code of IS-TSK-FC\ncan be downloaded from https://github.com/gusuhang10/IS-TSK-FC."
                },
                "authors": [
                    {
                        "name": "Suhang Gu"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Yongxin Chou"
                    },
                    {
                        "name": "Jinliang Cong"
                    },
                    {
                        "name": "Mingli Lu"
                    },
                    {
                        "name": "Zhuqing Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhuqing Jiao"
                },
                "author": "Zhuqing Jiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05122v1",
                "updated": "2025-04-07T14:26:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    26,
                    49,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:26:49Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    26,
                    49,
                    0,
                    97,
                    0
                ],
                "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech\n  Translation"
                },
                "summary": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance."
                },
                "authors": [
                    {
                        "name": "Xinglin Lyu"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03197v2",
                "updated": "2025-04-07T14:23:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    23,
                    25,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-04T06:03:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation"
                },
                "summary": "With the rapid advancement of mathematical reasoning capabilities in Large\nLanguage Models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids - such as diagrams,\nmarkings, and highlights - to enhance conceptual clarity. To bridge this gap,\nwe introduce a novel task of visual solution explanation, which requires\ngenerating explanations that incorporate newly introduced visual elements\nessential for understanding (e.g., auxiliary lines, annotations, or geometric\nconstructions). To evaluate model performance on this task, we propose\nMathExplain, a multimodal benchmark consisting of 997 math problems annotated\nwith visual keypoints and corresponding explanatory text that references those\nelements. Our empirical results show that while some closed-source models\ndemonstrate promising capabilities on visual solution-explaining, current\nopen-source general-purpose models perform inconsistently, particularly in\nidentifying relevant visual components and producing coherent keypoint-based\nexplanations. We expect that visual solution-explaining and the MathExplain\ndataset will catalyze further research on multimodal LLMs in education and\nadvance their deployment as effective, explanation-oriented AI tutors. Code and\ndata will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mathematical reasoning capabilities in Large\nLanguage Models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids - such as diagrams,\nmarkings, and highlights - to enhance conceptual clarity. To bridge this gap,\nwe introduce a novel task of visual solution explanation, which requires\ngenerating explanations that incorporate newly introduced visual elements\nessential for understanding (e.g., auxiliary lines, annotations, or geometric\nconstructions). To evaluate model performance on this task, we propose\nMathExplain, a multimodal benchmark consisting of 997 math problems annotated\nwith visual keypoints and corresponding explanatory text that references those\nelements. Our empirical results show that while some closed-source models\ndemonstrate promising capabilities on visual solution-explaining, current\nopen-source general-purpose models perform inconsistently, particularly in\nidentifying relevant visual components and producing coherent keypoint-based\nexplanations. We expect that visual solution-explaining and the MathExplain\ndataset will catalyze further research on multimodal LLMs in education and\nadvance their deployment as effective, explanation-oriented AI tutors. Code and\ndata will be released publicly."
                },
                "authors": [
                    {
                        "name": "Jaewoo Park"
                    },
                    {
                        "name": "Jungyang Park"
                    },
                    {
                        "name": "Dongju Jang"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Byungwoo Yoo"
                    },
                    {
                        "name": "Jaewoo Shin"
                    },
                    {
                        "name": "Seonjoon Park"
                    },
                    {
                        "name": "Taehyeong Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11721v2",
                "updated": "2025-04-07T14:21:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    21,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-17T16:40:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    40,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "The Right Time Matters: Data Arrangement Affects Zero-Shot\n  Generalization in Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Right Time Matters: Data Arrangement Affects Zero-Shot\n  Generalization in Instruction Tuning"
                },
                "summary": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization."
                },
                "authors": [
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Haiwen Hong"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05108v1",
                "updated": "2025-04-07T14:14:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:14:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning"
                },
                "summary": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design."
                },
                "authors": [
                    {
                        "name": "Anja Surina"
                    },
                    {
                        "name": "Amin Mansouri"
                    },
                    {
                        "name": "Lars Quaedvlieg"
                    },
                    {
                        "name": "Amal Seddas"
                    },
                    {
                        "name": "Maryna Viazovska"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05104v1",
                "updated": "2025-04-07T14:11:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    11,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:11:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    11,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for\n  Early Warning System Investments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for\n  Early Warning System Investments"
                },
                "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency."
                },
                "authors": [
                    {
                        "name": "Saeid Ario Vaghefi"
                    },
                    {
                        "name": "Aymane Hachcham"
                    },
                    {
                        "name": "Veronica Grasso"
                    },
                    {
                        "name": "Jiska Manicus"
                    },
                    {
                        "name": "Nakiete Msemo"
                    },
                    {
                        "name": "Chiara Colesanti Senni"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12378v2",
                "updated": "2025-04-07T14:07:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    7,
                    31,
                    0,
                    97,
                    0
                ],
                "published": "2024-04-18T17:58:16Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    58,
                    16,
                    3,
                    109,
                    0
                ],
                "title": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction"
                },
                "summary": "Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/."
                },
                "authors": [
                    {
                        "name": "Théo Gieruc"
                    },
                    {
                        "name": "Marius Kästingschäfer"
                    },
                    {
                        "name": "Sebastian Bernhard"
                    },
                    {
                        "name": "Mathieu Salzmann"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Salzmann"
                },
                "author": "Mathieu Salzmann",
                "arxiv_comment": "IV 2025. Joint first authorship. Project page:\n  https://6Img-to-3D.GitHub.io/ Code https://github.com/continental/6Img-to-3D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20508v2",
                "updated": "2025-04-07T13:54:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    13,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-26T12:49:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Explainable ICD Coding via Entity Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable ICD Coding via Entity Linking"
                },
                "summary": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios."
                },
                "authors": [
                    {
                        "name": "Leonor Barreiros"
                    },
                    {
                        "name": "Isabel Coutinho"
                    },
                    {
                        "name": "Gonçalo M. Correia"
                    },
                    {
                        "name": "Bruno Martins"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Martins"
                },
                "author": "Bruno Martins",
                "arxiv_comment": "Accepted at CL4Health at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05084v1",
                "updated": "2025-04-07T13:54:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    8,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:54:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot\n  Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot\n  Motion"
                },
                "summary": "Full integration of robots into real-life applications necessitates their\nability to interpret and execute natural language directives from untrained\nusers. Given the inherent variability in human language, equivalent directives\nmay be phrased differently, yet require consistent robot behavior. While Large\nLanguage Models (LLMs) have advanced language understanding, they often falter\nin handling user phrasing variability, rely on predefined commands, and exhibit\nunpredictable outputs. This letter introduces the Directive Language Model\n(DLM), a novel speech-to-trajectory framework that directly maps verbal\ncommands to executable motion trajectories, bypassing predefined phrases. DLM\nutilizes Behavior Cloning (BC) on simulated demonstrations of human-guided\nrobot motion. To enhance generalization, GPT-based semantic augmentation\ngenerates diverse paraphrases of training commands, labeled with the same\nmotion trajectory. DLM further incorporates a diffusion policy-based trajectory\ngeneration for adaptive motion refinement and stochastic sampling. In contrast\nto LLM-based methods, DLM ensures consistent, predictable motion without\nextensive prompt engineering, facilitating real-time robotic guidance. As DLM\nlearns from trajectory data, it is embodiment-agnostic, enabling deployment\nacross diverse robotic platforms. Experimental results demonstrate DLM's\nimproved command generalization, reduced dependence on structured phrasing, and\nachievement of human-like motion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full integration of robots into real-life applications necessitates their\nability to interpret and execute natural language directives from untrained\nusers. Given the inherent variability in human language, equivalent directives\nmay be phrased differently, yet require consistent robot behavior. While Large\nLanguage Models (LLMs) have advanced language understanding, they often falter\nin handling user phrasing variability, rely on predefined commands, and exhibit\nunpredictable outputs. This letter introduces the Directive Language Model\n(DLM), a novel speech-to-trajectory framework that directly maps verbal\ncommands to executable motion trajectories, bypassing predefined phrases. DLM\nutilizes Behavior Cloning (BC) on simulated demonstrations of human-guided\nrobot motion. To enhance generalization, GPT-based semantic augmentation\ngenerates diverse paraphrases of training commands, labeled with the same\nmotion trajectory. DLM further incorporates a diffusion policy-based trajectory\ngeneration for adaptive motion refinement and stochastic sampling. In contrast\nto LLM-based methods, DLM ensures consistent, predictable motion without\nextensive prompt engineering, facilitating real-time robotic guidance. As DLM\nlearns from trajectory data, it is embodiment-agnostic, enabling deployment\nacross diverse robotic platforms. Experimental results demonstrate DLM's\nimproved command generalization, reduced dependence on structured phrasing, and\nachievement of human-like motion."
                },
                "authors": [
                    {
                        "name": "Eran Beeri Bamani"
                    },
                    {
                        "name": "Eden Nissinman"
                    },
                    {
                        "name": "Rotem Atari"
                    },
                    {
                        "name": "Nevo Heimann Saadon"
                    },
                    {
                        "name": "Avishai Sintov"
                    }
                ],
                "author_detail": {
                    "name": "Avishai Sintov"
                },
                "author": "Avishai Sintov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05081v1",
                "updated": "2025-04-07T13:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    51,
                    6,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:51:06Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    51,
                    6,
                    0,
                    97,
                    0
                ],
                "title": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context\n  Learning"
                },
                "summary": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Yixiang Chen"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "30 pages, 12 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05074v1",
                "updated": "2025-04-07T13:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    42,
                    2,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    42,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "On the Performance of an Explainable Language Model on PubMedQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Performance of an Explainable Language Model on PubMedQA"
                },
                "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future."
                },
                "authors": [
                    {
                        "name": "Venkat Srinivasan"
                    },
                    {
                        "name": "Vishaal Jatav"
                    },
                    {
                        "name": "Anushka Chandrababu"
                    },
                    {
                        "name": "Geetika Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Geetika Sharma"
                },
                "author": "Geetika Sharma",
                "arxiv_comment": "Working Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10297v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10297v4",
                "updated": "2025-04-07T13:39:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    39,
                    44,
                    0,
                    97,
                    0
                ],
                "published": "2025-02-14T16:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    59,
                    5,
                    4,
                    45,
                    0
                ],
                "title": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder\n  Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder\n  Products"
                },
                "summary": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet by proving that it can solve dihedral group\nword problems in just two layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet by proving that it can solve dihedral group\nword problems in just two layers."
                },
                "authors": [
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    },
                    {
                        "name": "Riccardo Grazzi"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Grazzi"
                },
                "author": "Riccardo Grazzi",
                "arxiv_comment": "Accepted at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10297v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10297v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v2",
                "updated": "2025-04-08T08:18:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    18,
                    14,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05055v1",
                "updated": "2025-04-07T13:25:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    25,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:25:33Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    25,
                    33,
                    0,
                    97,
                    0
                ],
                "title": "Detecting relevant dependencies under measurement error with\n  applications to the analysis of planetary system evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting relevant dependencies under measurement error with\n  applications to the analysis of planetary system evolution"
                },
                "summary": "Exoplanets play an important role in understanding the mechanics of planetary\nsystem formation and orbital evolution. In this context the correlations of\ndifferent parameters of the planets and their host star are useful guides in\nthe search for explanatory mechanisms. Based on a reanalysis of the data set\nfrom \\cite{figueria14} we study the as of now still poorly understood\ncorrelation between planetary surface gravity and stellar activity of Hot\nJupiters. Unfortunately, data collection often suffers from measurement errors\ndue to complicated and indirect measurement setups, rendering standard\ninference techniques unreliable.\n  We present new methods to estimate and test for correlations in a\ndeconvolution framework and thereby improve the state of the art analysis of\nthe data in two directions. First, we are now able to account for additive\nmeasurement errors which facilitates reliable inference. Second we test for\nrelevant changes, i.e. we are testing for correlations exceeding a certain\nthreshold $\\Delta$. This reflects the fact that small nonzero correlations are\nto be expected for real life data almost always and that standard statistical\ntests will therefore always reject the null of no correlation given sufficient\ndata. Our theory focuses on quantities that can be estimated by U-Statistics\nwhich contain a variety of correlation measures. We propose a bootstrap test\nand establish its theoretical validity. As a by product we also obtain\nconfidence intervals. Applying our methods to the Hot Jupiter data set from\n\\cite{figueria14}, we observe that taking into account the measurement errors\nyields smaller point estimates and the null of no relevant correlation is\nrejected only for very small $\\Delta$. This demonstrates the importance of\nconsidering the impact of measurement errors to avoid misleading conclusions\nfrom the resulting statistical analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exoplanets play an important role in understanding the mechanics of planetary\nsystem formation and orbital evolution. In this context the correlations of\ndifferent parameters of the planets and their host star are useful guides in\nthe search for explanatory mechanisms. Based on a reanalysis of the data set\nfrom \\cite{figueria14} we study the as of now still poorly understood\ncorrelation between planetary surface gravity and stellar activity of Hot\nJupiters. Unfortunately, data collection often suffers from measurement errors\ndue to complicated and indirect measurement setups, rendering standard\ninference techniques unreliable.\n  We present new methods to estimate and test for correlations in a\ndeconvolution framework and thereby improve the state of the art analysis of\nthe data in two directions. First, we are now able to account for additive\nmeasurement errors which facilitates reliable inference. Second we test for\nrelevant changes, i.e. we are testing for correlations exceeding a certain\nthreshold $\\Delta$. This reflects the fact that small nonzero correlations are\nto be expected for real life data almost always and that standard statistical\ntests will therefore always reject the null of no correlation given sufficient\ndata. Our theory focuses on quantities that can be estimated by U-Statistics\nwhich contain a variety of correlation measures. We propose a bootstrap test\nand establish its theoretical validity. As a by product we also obtain\nconfidence intervals. Applying our methods to the Hot Jupiter data set from\n\\cite{figueria14}, we observe that taking into account the measurement errors\nyields smaller point estimates and the null of no relevant correlation is\nrejected only for very small $\\Delta$. This demonstrates the importance of\nconsidering the impact of measurement errors to avoid misleading conclusions\nfrom the resulting statistical analysis."
                },
                "authors": [
                    {
                        "name": "Patrick Bastian"
                    },
                    {
                        "name": "Nicolai Bissantz"
                    }
                ],
                "author_detail": {
                    "name": "Nicolai Bissantz"
                },
                "author": "Nicolai Bissantz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G15, 62G07, 62H15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05050v1",
                "updated": "2025-04-07T13:20:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    20,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:20:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    20,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Jiawei Lian"
                    },
                    {
                        "name": "Jianhong Pan"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Shaohui Mei"
                    },
                    {
                        "name": "Lap-Pui Chau"
                    }
                ],
                "author_detail": {
                    "name": "Lap-Pui Chau"
                },
                "author": "Lap-Pui Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05047v1",
                "updated": "2025-04-07T13:17:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    17,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    17,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning"
                },
                "summary": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Sugyeong Eo"
                    },
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Evelyn Hayoon Zi"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05045v2",
                "updated": "2025-04-08T03:33:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    33,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T13:14:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    14,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Attention-Augmented Inverse Reinforcement Learning with Graph\n  Convolutions for Multi-Agent Task Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Augmented Inverse Reinforcement Learning with Graph\n  Convolutions for Multi-Agent Task Allocation"
                },
                "summary": "Multi-agent task allocation (MATA) plays a vital role in cooperative\nmulti-agent systems, with significant implications for applications such as\nlogistics, search and rescue, and robotic coordination. Although traditional\ndeep reinforcement learning (DRL) methods have been shown to be promising,\ntheir effectiveness is hindered by a reliance on manually designed reward\nfunctions and inefficiencies in dynamic environments. In this paper, an inverse\nreinforcement learning (IRL)-based framework is proposed, in which multi-head\nself-attention (MHSA) and graph attention mechanisms are incorporated to\nenhance reward function learning and task execution efficiency. Expert\ndemonstrations are utilized to infer optimal reward densities, allowing\ndependence on handcrafted designs to be reduced and adaptability to be\nimproved. Extensive experiments validate the superiority of the proposed method\nover widely used multi-agent reinforcement learning (MARL) algorithms in terms\nof both cumulative rewards and task execution efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent task allocation (MATA) plays a vital role in cooperative\nmulti-agent systems, with significant implications for applications such as\nlogistics, search and rescue, and robotic coordination. Although traditional\ndeep reinforcement learning (DRL) methods have been shown to be promising,\ntheir effectiveness is hindered by a reliance on manually designed reward\nfunctions and inefficiencies in dynamic environments. In this paper, an inverse\nreinforcement learning (IRL)-based framework is proposed, in which multi-head\nself-attention (MHSA) and graph attention mechanisms are incorporated to\nenhance reward function learning and task execution efficiency. Expert\ndemonstrations are utilized to infer optimal reward densities, allowing\ndependence on handcrafted designs to be reduced and adaptability to be\nimproved. Extensive experiments validate the superiority of the proposed method\nover widely used multi-agent reinforcement learning (MARL) algorithms in terms\nof both cumulative rewards and task execution efficiency."
                },
                "authors": [
                    {
                        "name": "Huilin Yin"
                    },
                    {
                        "name": "Zhikun Yang"
                    },
                    {
                        "name": "Daniel Watzenig"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Watzenig"
                },
                "author": "Daniel Watzenig",
                "arxiv_comment": "Added a clarification on the source of expert trajectories in Section\n  V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05040v1",
                "updated": "2025-04-07T13:05:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    5,
                    9,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:05:09Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    5,
                    9,
                    0,
                    97,
                    0
                ],
                "title": "InstructionBench: An Instructional Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructionBench: An Instructional Video Understanding Benchmark"
                },
                "summary": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources."
                },
                "authors": [
                    {
                        "name": "Haiwan Wei"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Wei Ke"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17237v2",
                "updated": "2025-04-07T13:03:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    3,
                    35,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-21T15:40:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    40,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID"
                },
                "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the well-established\nYOLOv5 with DeepSORT combination, we present a tracking framework built on\nYOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies.\nWe evaluate our approach following the 4th Anti-UAV Challenge metrics and reach\ncompetitive performance. Notably, we achieved strong results without using\ncontrast enhancement or temporal information fusion to enrich UAV features,\nhighlighting our approach as a \"Strong Baseline\" for multi-UAV tracking tasks.\nWe provide implementation details, in-depth experimental analysis, and a\ndiscussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the well-established\nYOLOv5 with DeepSORT combination, we present a tracking framework built on\nYOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies.\nWe evaluate our approach following the 4th Anti-UAV Challenge metrics and reach\ncompetitive performance. Notably, we achieved strong results without using\ncontrast enhancement or temporal information fusion to enrich UAV features,\nhighlighting our approach as a \"Strong Baseline\" for multi-UAV tracking tasks.\nWe provide implementation details, in-depth experimental analysis, and a\ndiscussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID ."
                },
                "authors": [
                    {
                        "name": "Yu-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Hsi Chen"
                },
                "author": "Yu-Hsi Chen",
                "arxiv_comment": "10 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05035v1",
                "updated": "2025-04-07T12:56:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    56,
                    51,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:56:51Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    56,
                    51,
                    0,
                    97,
                    0
                ],
                "title": "Probabilistic Position-Aided Beam Selection for mmWave MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Position-Aided Beam Selection for mmWave MIMO Systems"
                },
                "summary": "Millimeter-wave (mmWave) MIMO systems rely on highly directional beamforming\nto overcome severe path loss and ensure robust communication links. However,\nselecting the optimal beam pair efficiently remains a challenge due to the\nlarge search space and the overhead of conventional methods. This paper\nproposes a probabilistic position-aided beam selection approach that exploits\nthe statistical dependence between user equipment (UE) positions and optimal\nbeam indices. We model the underlying joint probability mass function (PMF) of\nthe positions and the beam indices as a low-rank tensor and estimate its\nparameters from training data using Bayesian inference. The estimated model is\nthen used to predict the best (or a list of the top) beam pair indices for new\nUE positions. The proposed method is evaluated using data generated from a\nstate-of-the-art ray tracing simulator and compared with neural network-based\nand fingerprinting approaches. The results show that our approach achieves a\nhigh data rate with fewer training samples and a significantly reduced beam\nsearch space. These advantages render it a promising solution for practical\nmmWave MIMO deployments, reducing the beam search overhead while maintaining a\nreliable connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mmWave) MIMO systems rely on highly directional beamforming\nto overcome severe path loss and ensure robust communication links. However,\nselecting the optimal beam pair efficiently remains a challenge due to the\nlarge search space and the overhead of conventional methods. This paper\nproposes a probabilistic position-aided beam selection approach that exploits\nthe statistical dependence between user equipment (UE) positions and optimal\nbeam indices. We model the underlying joint probability mass function (PMF) of\nthe positions and the beam indices as a low-rank tensor and estimate its\nparameters from training data using Bayesian inference. The estimated model is\nthen used to predict the best (or a list of the top) beam pair indices for new\nUE positions. The proposed method is evaluated using data generated from a\nstate-of-the-art ray tracing simulator and compared with neural network-based\nand fingerprinting approaches. The results show that our approach achieves a\nhigh data rate with fewer training samples and a significantly reduced beam\nsearch space. These advantages render it a promising solution for practical\nmmWave MIMO deployments, reducing the beam search overhead while maintaining a\nreliable connectivity."
                },
                "authors": [
                    {
                        "name": "Joseph K. Chege"
                    },
                    {
                        "name": "Arie Yeredor"
                    },
                    {
                        "name": "Martin Haardt"
                    }
                ],
                "author_detail": {
                    "name": "Martin Haardt"
                },
                "author": "Martin Haardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05030v1",
                "updated": "2025-04-07T12:52:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    52,
                    23,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:52:23Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    52,
                    23,
                    0,
                    97,
                    0
                ],
                "title": "AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal\n  Asymmetric Dyadic Relationship Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal\n  Asymmetric Dyadic Relationship Classification"
                },
                "summary": "Dyadic social relationships, which refer to relationships between two\nindividuals who know each other through repeated interactions (or not), are\nshaped by shared spatial and temporal experiences. Current computational\nmethods for modeling these relationships face three major challenges: (1) the\nfailure to model asymmetric relationships, e.g., one individual may perceive\nthe other as a friend while the other perceives them as an acquaintance, (2)\nthe disruption of continuous interactions by discrete frame sampling, which\nsegments the temporal continuity of interaction in real-world scenarios, and\n(3) the limitation to consider periodic behavioral cues, such as rhythmic\nvocalizations or recurrent gestures, which are crucial for inferring the\nevolution of dyadic relationships. To address these challenges, we propose\nAsyReC, a multimodal graph-based framework for asymmetric dyadic relationship\nclassification, with three core innovations: (i) a triplet graph neural network\nwith node-edge dual attention that dynamically weights multimodal cues to\ncapture interaction asymmetries (addressing challenge 1); (ii) a clip-level\nrelationship learning architecture that preserves temporal continuity, enabling\nfine-grained modeling of real-world interaction dynamics (addressing challenge\n2); and (iii) a periodic temporal encoder that projects time indices onto\nsine/cosine waveforms to model recurrent behavioral patterns (addressing\nchallenge 3). Extensive experiments on two public datasets demonstrate\nstate-of-the-art performance, while ablation studies validate the critical role\nof asymmetric interaction modeling and periodic temporal encoding in improving\nthe robustness of dyadic relationship classification in real-world scenarios.\nOur code is publicly available at: https://github.com/tw-repository/AsyReC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dyadic social relationships, which refer to relationships between two\nindividuals who know each other through repeated interactions (or not), are\nshaped by shared spatial and temporal experiences. Current computational\nmethods for modeling these relationships face three major challenges: (1) the\nfailure to model asymmetric relationships, e.g., one individual may perceive\nthe other as a friend while the other perceives them as an acquaintance, (2)\nthe disruption of continuous interactions by discrete frame sampling, which\nsegments the temporal continuity of interaction in real-world scenarios, and\n(3) the limitation to consider periodic behavioral cues, such as rhythmic\nvocalizations or recurrent gestures, which are crucial for inferring the\nevolution of dyadic relationships. To address these challenges, we propose\nAsyReC, a multimodal graph-based framework for asymmetric dyadic relationship\nclassification, with three core innovations: (i) a triplet graph neural network\nwith node-edge dual attention that dynamically weights multimodal cues to\ncapture interaction asymmetries (addressing challenge 1); (ii) a clip-level\nrelationship learning architecture that preserves temporal continuity, enabling\nfine-grained modeling of real-world interaction dynamics (addressing challenge\n2); and (iii) a periodic temporal encoder that projects time indices onto\nsine/cosine waveforms to model recurrent behavioral patterns (addressing\nchallenge 3). Extensive experiments on two public datasets demonstrate\nstate-of-the-art performance, while ablation studies validate the critical role\nof asymmetric interaction modeling and periodic temporal encoding in improving\nthe robustness of dyadic relationship classification in real-world scenarios.\nOur code is publicly available at: https://github.com/tw-repository/AsyReC."
                },
                "authors": [
                    {
                        "name": "Wang Tang"
                    },
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Linbo Qing"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05029v1",
                "updated": "2025-04-07T12:51:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    51,
                    18,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:51:18Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    51,
                    18,
                    0,
                    97,
                    0
                ],
                "title": "Graph-based Diffusion Model for Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Diffusion Model for Collaborative Filtering"
                },
                "summary": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Xiang Deng"
                    },
                    {
                        "name": "Hongxing Yuan"
                    },
                    {
                        "name": "Chunyu Wei"
                    },
                    {
                        "name": "Yushun Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yushun Fan"
                },
                "author": "Yushun Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05019v1",
                "updated": "2025-04-07T12:43:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    43,
                    5,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:43:05Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    43,
                    5,
                    0,
                    97,
                    0
                ],
                "title": "Mixture-of-Personas Language Models for Population Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Personas Language Models for Population Simulation"
                },
                "summary": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics."
                },
                "authors": [
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Hieu Trung Nguyen"
                    },
                    {
                        "name": "Shantanu Kumar"
                    },
                    {
                        "name": "Julian Theodore"
                    },
                    {
                        "name": "Weikang Qiu"
                    },
                    {
                        "name": "Viet Anh Nguyen"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05008v1",
                "updated": "2025-04-07T12:35:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:35:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Surveying Professional Writers on AI: Limitations, Expectations, and\n  Fears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveying Professional Writers on AI: Limitations, Expectations, and\n  Fears"
                },
                "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergey Tilga"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03438v2",
                "updated": "2025-04-07T12:35:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    3,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-04T13:29:32Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    29,
                    32,
                    4,
                    94,
                    0
                ],
                "title": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object\n  Perception in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object\n  Perception in Autonomous Driving"
                },
                "summary": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods."
                },
                "authors": [
                    {
                        "name": "Sheng Yang"
                    },
                    {
                        "name": "Tong Zhan"
                    },
                    {
                        "name": "Shichen Qiao"
                    },
                    {
                        "name": "Jicheng Gong"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Yanfeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Lu"
                },
                "author": "Yanfeng Lu",
                "arxiv_comment": "CVPR 2025 WDFM-AD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05006v1",
                "updated": "2025-04-07T12:32:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    32,
                    14,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:32:14Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    32,
                    14,
                    0,
                    97,
                    0
                ],
                "title": "Enhancing Smart Contract Vulnerability Detection in DApps Leveraging\n  Fine-Tuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Smart Contract Vulnerability Detection in DApps Leveraging\n  Fine-Tuned LLM"
                },
                "summary": "Decentralized applications (DApps) face significant security risks due to\nvulnerabilities in smart contracts, with traditional detection methods\nstruggling to address emerging and machine-unauditable flaws. This paper\nproposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to\nenhance smart contract vulnerability detection. We introduce a comprehensive\ndataset of 215 real-world DApp projects (4,998 contracts), including\nhard-to-detect logical errors like token price manipulation, addressing the\nlimitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B\nand Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation\n(LoRA), our method achieves superior performance, attaining an F1-score of 0.83\nwith FFT and data augmentation via Random Over Sampling (ROS). Comparative\nexperiments demonstrate significant improvements over prompt-based LLMs and\nstate-of-the-art tools. Notably, the approach excels in detecting\nnon-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall\nfor price manipulation flaws. The results underscore the effectiveness of\ndomain-specific LLM fine-tuning and data augmentation in addressing real-world\nDApp security challenges, offering a robust solution for blockchain ecosystem\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized applications (DApps) face significant security risks due to\nvulnerabilities in smart contracts, with traditional detection methods\nstruggling to address emerging and machine-unauditable flaws. This paper\nproposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to\nenhance smart contract vulnerability detection. We introduce a comprehensive\ndataset of 215 real-world DApp projects (4,998 contracts), including\nhard-to-detect logical errors like token price manipulation, addressing the\nlimitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B\nand Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation\n(LoRA), our method achieves superior performance, attaining an F1-score of 0.83\nwith FFT and data augmentation via Random Over Sampling (ROS). Comparative\nexperiments demonstrate significant improvements over prompt-based LLMs and\nstate-of-the-art tools. Notably, the approach excels in detecting\nnon-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall\nfor price manipulation flaws. The results underscore the effectiveness of\ndomain-specific LLM fine-tuning and data augmentation in addressing real-world\nDApp security challenges, offering a robust solution for blockchain ecosystem\nprotection."
                },
                "authors": [
                    {
                        "name": "Jiuyang Bu"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Xiaoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Li"
                },
                "author": "Xiaoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05004v1",
                "updated": "2025-04-07T12:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    30,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    30,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Stacking Variational Bayesian Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacking Variational Bayesian Monte Carlo"
                },
                "summary": "Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for\napproximate Bayesian inference with computationally expensive likelihoods.\nWhile VBMC's local surrogate approach provides stable approximations, its\nconservative exploration strategy and limited evaluation budget can cause it to\nmiss regions of complex posteriors. In this work, we introduce Stacking\nVariational Bayesian Monte Carlo (S-VBMC), a method that constructs global\nposterior approximations by merging independent VBMC runs through a principled\nand inexpensive post-processing step. Our approach leverages VBMC's mixture\nposterior representation and per-component evidence estimates, requiring no\nadditional likelihood evaluations while being naturally parallelizable. We\ndemonstrate S-VBMC's effectiveness on two synthetic problems designed to\nchallenge VBMC's exploration capabilities and two real-world applications from\ncomputational neuroscience, showing substantial improvements in posterior\napproximation quality across all cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for\napproximate Bayesian inference with computationally expensive likelihoods.\nWhile VBMC's local surrogate approach provides stable approximations, its\nconservative exploration strategy and limited evaluation budget can cause it to\nmiss regions of complex posteriors. In this work, we introduce Stacking\nVariational Bayesian Monte Carlo (S-VBMC), a method that constructs global\nposterior approximations by merging independent VBMC runs through a principled\nand inexpensive post-processing step. Our approach leverages VBMC's mixture\nposterior representation and per-component evidence estimates, requiring no\nadditional likelihood evaluations while being naturally parallelizable. We\ndemonstrate S-VBMC's effectiveness on two synthetic problems designed to\nchallenge VBMC's exploration capabilities and two real-world applications from\ncomputational neuroscience, showing substantial improvements in posterior\napproximation quality across all cases."
                },
                "authors": [
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Chengkun Li"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "arxiv_comment": "Accepted at the Workshop track of the 7th Symposium in Advances in\n  Approximate Bayesian Inference (AABI 2025). 24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04994v1",
                "updated": "2025-04-07T12:23:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    23,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:23:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    23,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind\n  Value-Oriented Behaviors in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind\n  Value-Oriented Behaviors in LLMs"
                },
                "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available."
                },
                "authors": [
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Xiaoyang Gu"
                    },
                    {
                        "name": "Letao Han"
                    }
                ],
                "author_detail": {
                    "name": "Letao Han"
                },
                "author": "Letao Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17154v2",
                "updated": "2025-04-07T12:16:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    16,
                    27,
                    0,
                    97,
                    0
                ],
                "published": "2024-03-25T20:09:46Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    20,
                    9,
                    46,
                    0,
                    85,
                    0
                ],
                "title": "On the Impact of Black-box Deployment Strategies for Edge AI on Latency\n  and Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Black-box Deployment Strategies for Edge AI on Latency\n  and Model Performance"
                },
                "summary": "Deciding what combination of operators to use across the Edge AI tiers to\nachieve specific latency and model performance requirements is an open question\nfor MLOps engineers. This study aims to empirically assess the accuracy vs\ninference time trade-off of different black-box Edge AI deployment strategies,\ni.e., combinations of deployment operators and deployment tiers. In this paper,\nwe conduct inference experiments involving 3 deployment operators (i.e.,\nPartitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile,\nEdge, Cloud) and their combinations on four widely used Computer-Vision models\nto investigate the optimal strategies from the point of view of MLOps\ndevelopers. Our findings suggest that Edge deployment using the hybrid\nQuantization + Early Exit operator could be preferred over non-hybrid operators\n(Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency\nis a concern at medium accuracy loss. However, when minimizing accuracy loss is\na concern, MLOps engineers should prefer using only a Quantization operator on\nedge at a latency reduction or increase, respectively over the Early\nExit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge)\noperators. In scenarios constrained by Mobile CPU/RAM resources, a preference\nfor Partitioning across mobile and edge tiers is observed over mobile\ndeployment. For models with smaller input data samples (such as FCN), a\nnetwork-constrained cloud deployment can also be a better alternative than\nMobile/Edge deployment and Partitioning strategies. For models with large input\ndata samples (ResNet, ResNext, DUC), an edge tier having higher\nnetwork/computational capabilities than Cloud/Mobile can be a more viable\noption than Partitioning and Mobile/Cloud deployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciding what combination of operators to use across the Edge AI tiers to\nachieve specific latency and model performance requirements is an open question\nfor MLOps engineers. This study aims to empirically assess the accuracy vs\ninference time trade-off of different black-box Edge AI deployment strategies,\ni.e., combinations of deployment operators and deployment tiers. In this paper,\nwe conduct inference experiments involving 3 deployment operators (i.e.,\nPartitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile,\nEdge, Cloud) and their combinations on four widely used Computer-Vision models\nto investigate the optimal strategies from the point of view of MLOps\ndevelopers. Our findings suggest that Edge deployment using the hybrid\nQuantization + Early Exit operator could be preferred over non-hybrid operators\n(Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency\nis a concern at medium accuracy loss. However, when minimizing accuracy loss is\na concern, MLOps engineers should prefer using only a Quantization operator on\nedge at a latency reduction or increase, respectively over the Early\nExit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge)\noperators. In scenarios constrained by Mobile CPU/RAM resources, a preference\nfor Partitioning across mobile and edge tiers is observed over mobile\ndeployment. For models with smaller input data samples (such as FCN), a\nnetwork-constrained cloud deployment can also be a better alternative than\nMobile/Edge deployment and Partitioning strategies. For models with large input\ndata samples (ResNet, ResNext, DUC), an edge tier having higher\nnetwork/computational capabilities than Cloud/Mobile can be a more viable\noption than Partitioning and Mobile/Cloud deployment strategies."
                },
                "authors": [
                    {
                        "name": "Jaskirat Singh"
                    },
                    {
                        "name": "Emad Fallahzadeh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v3",
                "updated": "2025-04-07T12:15:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    15,
                    49,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We open-source\nour code (https://github.com/openai/preparedness) to facilitate future research\nin understanding the AI engineering capabilities of AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We open-source\nour code (https://github.com/openai/preparedness) to facilitate future research\nin understanding the AI engineering capabilities of AI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04980v1",
                "updated": "2025-04-07T12:09:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    9,
                    9,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:09:09Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    9,
                    9,
                    0,
                    97,
                    0
                ],
                "title": "Combining kinetic and thermodynamic uncertainty relations in quantum\n  transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining kinetic and thermodynamic uncertainty relations in quantum\n  transport"
                },
                "summary": "We study the fluctuations of generic currents in multi-terminal,\nmulti-channel quantum transport settings. In the quantum regime, these\nfluctuations and the resulting precision differ strongly depending on whether\nthe device is of fermionic or bosonic nature. Using scattering theory, we show\nthat the precision is bounded by constraints set by the entropy production and\nby the activity in the spirit of thermodynamic or kinetic uncertainty\nrelations, valid for fermionic and bosonic quantum systems and even in the\nabsence of time-reversal symmetry. Furthermore, we derive a combined\nthermodynamic kinetic uncertainty relation, which is tight over a wide range of\nparameters and can hence predict the reachable precision of a device.\n  Since these constraints can be expressed in terms of observables accessible\nin transport measurements, such as currents and bandwidth, we foresee that the\ntight thermodynamic kinetic uncertainty-like bounds are also useful as an\ninference tool: they can be exploited to estimate entropy production from\ntransport observables, such as the charge current and its noise, which are more\neasily accessible in experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the fluctuations of generic currents in multi-terminal,\nmulti-channel quantum transport settings. In the quantum regime, these\nfluctuations and the resulting precision differ strongly depending on whether\nthe device is of fermionic or bosonic nature. Using scattering theory, we show\nthat the precision is bounded by constraints set by the entropy production and\nby the activity in the spirit of thermodynamic or kinetic uncertainty\nrelations, valid for fermionic and bosonic quantum systems and even in the\nabsence of time-reversal symmetry. Furthermore, we derive a combined\nthermodynamic kinetic uncertainty relation, which is tight over a wide range of\nparameters and can hence predict the reachable precision of a device.\n  Since these constraints can be expressed in terms of observables accessible\nin transport measurements, such as currents and bandwidth, we foresee that the\ntight thermodynamic kinetic uncertainty-like bounds are also useful as an\ninference tool: they can be exploited to estimate entropy production from\ntransport observables, such as the charge current and its noise, which are more\neasily accessible in experiment."
                },
                "authors": [
                    {
                        "name": "Didrik Palmqvist"
                    },
                    {
                        "name": "Ludovico Tesser"
                    },
                    {
                        "name": "Janine Splettstoesser"
                    }
                ],
                "author_detail": {
                    "name": "Janine Splettstoesser"
                },
                "author": "Janine Splettstoesser",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04976v1",
                "updated": "2025-04-07T12:05:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    5,
                    16,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:05:16Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    5,
                    16,
                    0,
                    97,
                    0
                ],
                "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language\n  Models"
                },
                "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study."
                },
                "authors": [
                    {
                        "name": "Carlos Peláez-González"
                    },
                    {
                        "name": "Andrés Herrera-Poyatos"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Virilo Tejedor"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04974v1",
                "updated": "2025-04-07T12:01:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:01:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Visual Text Grounding of Multimodal Large Language Model"
                },
                "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Yufan Zhou"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Wanrong Zhu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Tong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Tong Sun"
                },
                "author": "Tong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v3",
                "updated": "2025-04-07T12:01:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unified Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unified Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports various BI tasks for different data roles\nin data preparation, analysis, and visualization by seamlessly combining LLM\nassistance with user customization within a single environment. To achieve this\nunification, we design a domain knowledge incorporation module tailored for\nenterprise-specific BI tasks, an inter-agent communication mechanism to\nfacilitate information sharing across the BI workflow, and a cell-based context\nmanagement strategy to enhance context utilization efficiency in BI notebooks.\nExtensive experiments demonstrate that DataLab achieves state-of-the-art\nperformance on various BI tasks across popular research benchmarks. Moreover,\nDataLab maintains high effectiveness and efficiency on real-world datasets from\nTencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction\nin token cost on enterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports various BI tasks for different data roles\nin data preparation, analysis, and visualization by seamlessly combining LLM\nassistance with user customization within a single environment. To achieve this\nunification, we design a domain knowledge incorporation module tailored for\nenterprise-specific BI tasks, an inter-agent communication mechanism to\nfacilitate information sharing across the BI workflow, and a cell-based context\nmanagement strategy to enhance context utilization efficiency in BI notebooks.\nExtensive experiments demonstrate that DataLab achieves state-of-the-art\nperformance on various BI tasks across popular research benchmarks. Moreover,\nDataLab maintains high effectiveness and efficiency on real-world datasets from\nTencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction\nin token cost on enterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "arxiv_comment": "Accepted to ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05549v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05549v3",
                "updated": "2025-04-07T11:58:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    58,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-08T13:12:17Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    12,
                    17,
                    4,
                    313,
                    0
                ],
                "title": "STREAK: Streaming Network for Continual Learning of Object Relocations\n  under Household Context Drifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STREAK: Streaming Network for Continual Learning of Object Relocations\n  under Household Context Drifts"
                },
                "summary": "In real-world settings, robots are expected to assist humans across diverse\ntasks and still continuously adapt to dynamic changes over time. For example,\nin domestic environments, robots can proactively help users by fetching needed\nobjects based on learned routines, which they infer by observing how objects\nmove over time. However, data from these interactions are inherently\nnon-independent and non-identically distributed (non-i.i.d.), e.g., a robot\nassisting multiple users may encounter varying data distributions as\nindividuals follow distinct habits. This creates a challenge: integrating new\nknowledge without catastrophic forgetting. To address this, we propose STREAK\n(Spatio Temporal RElocation with Adaptive Knowledge retention), a continual\nlearning framework for real-world robotic learning. It leverages a streaming\ngraph neural network with regularization and rehearsal techniques to mitigate\ncontext drifts while retaining past knowledge. Our method is time- and\nmemory-efficient, enabling long-term learning without retraining on all past\ndata, which becomes infeasible as data grows in real-world interactions. We\nevaluate STREAK on the task of incrementally predicting human routines over 50+\ndays across different households. Results show that it effectively prevents\ncatastrophic forgetting while maintaining generalization, making it a scalable\nsolution for long-term human-robot interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world settings, robots are expected to assist humans across diverse\ntasks and still continuously adapt to dynamic changes over time. For example,\nin domestic environments, robots can proactively help users by fetching needed\nobjects based on learned routines, which they infer by observing how objects\nmove over time. However, data from these interactions are inherently\nnon-independent and non-identically distributed (non-i.i.d.), e.g., a robot\nassisting multiple users may encounter varying data distributions as\nindividuals follow distinct habits. This creates a challenge: integrating new\nknowledge without catastrophic forgetting. To address this, we propose STREAK\n(Spatio Temporal RElocation with Adaptive Knowledge retention), a continual\nlearning framework for real-world robotic learning. It leverages a streaming\ngraph neural network with regularization and rehearsal techniques to mitigate\ncontext drifts while retaining past knowledge. Our method is time- and\nmemory-efficient, enabling long-term learning without retraining on all past\ndata, which becomes infeasible as data grows in real-world interactions. We\nevaluate STREAK on the task of incrementally predicting human routines over 50+\ndays across different households. Results show that it effectively prevents\ncatastrophic forgetting while maintaining generalization, making it a scalable\nsolution for long-term human-robot interactions."
                },
                "authors": [
                    {
                        "name": "Ermanno Bartoli"
                    },
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Iolanda Leite"
                    }
                ],
                "author_detail": {
                    "name": "Iolanda Leite"
                },
                "author": "Iolanda Leite",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05549v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05549v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04968v1",
                "updated": "2025-04-07T11:54:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    54,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:54:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    54,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for\n  Family Storytelling and Emotional Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for\n  Family Storytelling and Emotional Reflection"
                },
                "summary": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics."
                },
                "authors": [
                    {
                        "name": "Jiayang Huang"
                    },
                    {
                        "name": "Lingjie Li"
                    },
                    {
                        "name": "Kang Zhang"
                    },
                    {
                        "name": "David Yip"
                    }
                ],
                "author_detail": {
                    "name": "David Yip"
                },
                "author": "David Yip",
                "arxiv_comment": "8 pages,8 figures, International Symposium on Electronic/Emerging Art\n  (ISEA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04956v2",
                "updated": "2025-04-08T03:01:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    1,
                    3,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T11:44:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    44,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with\n  Exemplar-Based Identity Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with\n  Exemplar-Based Identity Conditioning"
                },
                "summary": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a\none-step diffusion model for real-time, high-fidelity human motion estimation\nfrom egocentric image inputs. While an existing method for egocentric\nwhole-body (i.e., body and hands) motion estimation is non-real-time and\nacausal due to diffusion-based iterative motion refinement to capture\ncorrelations between body and hand poses, REWIND operates in a fully causal and\nreal-time manner. To enable real-time inference, we introduce (1) cascaded\nbody-hand denoising diffusion, which effectively models the correlation between\negocentric body and hand motions in a fast, feed-forward manner, and (2)\ndiffusion distillation, which enables high-quality motion estimation with a\nsingle denoising step. Our denoising diffusion model is based on a modified\nTransformer architecture, designed to causally model output motions while\nenhancing generalizability to unseen motion lengths. Additionally, REWIND\noptionally supports identity-conditioned motion estimation when identity prior\nis available. To this end, we propose a novel identity conditioning method\nbased on a small set of pose exemplars of the target identity, which further\nenhances motion estimation quality. Through extensive experiments, we\ndemonstrate that REWIND significantly outperforms the existing baselines both\nwith and without exemplar-based identity conditioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a\none-step diffusion model for real-time, high-fidelity human motion estimation\nfrom egocentric image inputs. While an existing method for egocentric\nwhole-body (i.e., body and hands) motion estimation is non-real-time and\nacausal due to diffusion-based iterative motion refinement to capture\ncorrelations between body and hand poses, REWIND operates in a fully causal and\nreal-time manner. To enable real-time inference, we introduce (1) cascaded\nbody-hand denoising diffusion, which effectively models the correlation between\negocentric body and hand motions in a fast, feed-forward manner, and (2)\ndiffusion distillation, which enables high-quality motion estimation with a\nsingle denoising step. Our denoising diffusion model is based on a modified\nTransformer architecture, designed to causally model output motions while\nenhancing generalizability to unseen motion lengths. Additionally, REWIND\noptionally supports identity-conditioned motion estimation when identity prior\nis available. To this end, we propose a novel identity conditioning method\nbased on a small set of pose exemplars of the target identity, which further\nenhances motion estimation quality. Through extensive experiments, we\ndemonstrate that REWIND significantly outperforms the existing baselines both\nwith and without exemplar-based identity conditioning."
                },
                "authors": [
                    {
                        "name": "Jihyun Lee"
                    },
                    {
                        "name": "Weipeng Xu"
                    },
                    {
                        "name": "Alexander Richard"
                    },
                    {
                        "name": "Shih-En Wei"
                    },
                    {
                        "name": "Shunsuke Saito"
                    },
                    {
                        "name": "Shaojie Bai"
                    },
                    {
                        "name": "Te-Li Wang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    },
                    {
                        "name": "Tae-Kyun Kim"
                    },
                    {
                        "name": "Jason Saragih"
                    }
                ],
                "author_detail": {
                    "name": "Jason Saragih"
                },
                "author": "Jason Saragih",
                "arxiv_comment": "Accepted to CVPR 2025, project page:\n  https://jyunlee.github.io/projects/rewind/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04953v1",
                "updated": "2025-04-07T11:37:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    37,
                    26,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:37:26Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    37,
                    26,
                    0,
                    97,
                    0
                ],
                "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-Prometheus: A Suite of Open Multilingual LLM Judges"
                },
                "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code."
                },
                "authors": [
                    {
                        "name": "José Pombal"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04950v1",
                "updated": "2025-04-07T11:34:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    34,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:34:48Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    34,
                    48,
                    0,
                    97,
                    0
                ],
                "title": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward\n  Modeling and Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward\n  Modeling and Policy Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important\nparadigm for aligning large language models (LLMs) with human preferences\nduring post-training. This framework typically involves two stages: first,\ntraining a reward model on human preference data, followed by optimizing the\nlanguage model using reinforcement learning algorithms. However, current RLHF\napproaches may constrained by two limitations. First, existing RLHF frameworks\noften rely on Bradley-Terry models to assign scalar rewards based on pairwise\ncomparisons of individual responses. However, this approach imposes significant\nchallenges on reward model (RM), as the inherent variability in prompt-response\npairs across different contexts demands robust calibration capabilities from\nthe RM. Second, reward models are typically initialized from generative\nfoundation models, such as pre-trained or supervised fine-tuned models, despite\nthe fact that reward models perform discriminative tasks, creating a mismatch.\nThis paper introduces Pairwise-RL, a RLHF framework that addresses these\nchallenges through a combination of generative reward modeling and a pairwise\nproximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model\ntraining and its application during reinforcement learning within a consistent\npairwise paradigm, leveraging generative modeling techniques to enhance reward\nmodel performance and score calibration. Experimental evaluations demonstrate\nthat Pairwise-RL outperforms traditional RLHF frameworks across both internal\nevaluation datasets and standard public benchmarks, underscoring its\neffectiveness in improving alignment and model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important\nparadigm for aligning large language models (LLMs) with human preferences\nduring post-training. This framework typically involves two stages: first,\ntraining a reward model on human preference data, followed by optimizing the\nlanguage model using reinforcement learning algorithms. However, current RLHF\napproaches may constrained by two limitations. First, existing RLHF frameworks\noften rely on Bradley-Terry models to assign scalar rewards based on pairwise\ncomparisons of individual responses. However, this approach imposes significant\nchallenges on reward model (RM), as the inherent variability in prompt-response\npairs across different contexts demands robust calibration capabilities from\nthe RM. Second, reward models are typically initialized from generative\nfoundation models, such as pre-trained or supervised fine-tuned models, despite\nthe fact that reward models perform discriminative tasks, creating a mismatch.\nThis paper introduces Pairwise-RL, a RLHF framework that addresses these\nchallenges through a combination of generative reward modeling and a pairwise\nproximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model\ntraining and its application during reinforcement learning within a consistent\npairwise paradigm, leveraging generative modeling techniques to enhance reward\nmodel performance and score calibration. Experimental evaluations demonstrate\nthat Pairwise-RL outperforms traditional RLHF frameworks across both internal\nevaluation datasets and standard public benchmarks, underscoring its\neffectiveness in improving alignment and model behavior."
                },
                "authors": [
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Xiaochen Zuo"
                    },
                    {
                        "name": "Chao Xin"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "arxiv_comment": "11oages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04945v1",
                "updated": "2025-04-07T11:31:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    31,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:31:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    31,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal\n  Reasoning in the Multi-state Bar Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal\n  Reasoning in the Multi-state Bar Exam"
                },
                "summary": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs."
                },
                "authors": [
                    {
                        "name": "Rean Fernandes"
                    },
                    {
                        "name": "André Biedenkapp"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Noor Awad"
                    }
                ],
                "author_detail": {
                    "name": "Noor Awad"
                },
                "author": "Noor Awad",
                "arxiv_comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v1",
                "updated": "2025-04-07T11:30:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17415v2",
                "updated": "2025-04-07T11:20:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    20,
                    37,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-23T09:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "VidCtx: Context-aware Video Question Answering with Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCtx: Context-aware Video Question Answering with Image Models"
                },
                "summary": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR. Our code is\navailable at https://github.com/IDT-ITI/VidCtx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR. Our code is\navailable at https://github.com/IDT-ITI/VidCtx."
                },
                "authors": [
                    {
                        "name": "Andreas Goulas"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted in IEEE ICME 2025. This is the authors' accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04936v1",
                "updated": "2025-04-07T11:20:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    20,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:20:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    20,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "Constrained Gaussian Process Motion Planning via Stein Variational\n  Newton Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Gaussian Process Motion Planning via Stein Variational\n  Newton Inference"
                },
                "summary": "Gaussian Process Motion Planning (GPMP) is a widely used framework for\ngenerating smooth trajectories within a limited compute time--an essential\nrequirement in many robotic applications. However, traditional GPMP approaches\noften struggle with enforcing hard nonlinear constraints and rely on Maximum a\nPosteriori (MAP) solutions that disregard the full Bayesian posterior. This\nlimits planning diversity and ultimately hampers decision-making. Recent\nefforts to integrate Stein Variational Gradient Descent (SVGD) into motion\nplanning have shown promise in handling complex constraints. Nonetheless, these\nmethods still face persistent challenges, such as difficulties in strictly\nenforcing constraints and inefficiencies when the probabilistic inference\nproblem is poorly conditioned. To address these issues, we propose a novel\nconstrained Stein Variational Gaussian Process Motion Planning (cSGPMP)\nframework, incorporating a GPMP prior specifically designed for trajectory\noptimization under hard constraints. Our approach improves the efficiency of\nparticle-based inference while explicitly handling nonlinear constraints. This\nadvancement significantly broadens the applicability of GPMP to motion planning\nscenarios demanding robust Bayesian inference, strict constraint adherence, and\ncomputational efficiency within a limited time. We validate our method on\nstandard benchmarks, achieving an average success rate of 98.57% across 350\nplanning tasks, significantly outperforming competitive baselines. This\ndemonstrates the ability of our method to discover and use diverse trajectory\nmodes, enhancing flexibility and adaptability in complex environments, and\ndelivering significant improvements over standard baselines without incurring\nmajor computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process Motion Planning (GPMP) is a widely used framework for\ngenerating smooth trajectories within a limited compute time--an essential\nrequirement in many robotic applications. However, traditional GPMP approaches\noften struggle with enforcing hard nonlinear constraints and rely on Maximum a\nPosteriori (MAP) solutions that disregard the full Bayesian posterior. This\nlimits planning diversity and ultimately hampers decision-making. Recent\nefforts to integrate Stein Variational Gradient Descent (SVGD) into motion\nplanning have shown promise in handling complex constraints. Nonetheless, these\nmethods still face persistent challenges, such as difficulties in strictly\nenforcing constraints and inefficiencies when the probabilistic inference\nproblem is poorly conditioned. To address these issues, we propose a novel\nconstrained Stein Variational Gaussian Process Motion Planning (cSGPMP)\nframework, incorporating a GPMP prior specifically designed for trajectory\noptimization under hard constraints. Our approach improves the efficiency of\nparticle-based inference while explicitly handling nonlinear constraints. This\nadvancement significantly broadens the applicability of GPMP to motion planning\nscenarios demanding robust Bayesian inference, strict constraint adherence, and\ncomputational efficiency within a limited time. We validate our method on\nstandard benchmarks, achieving an average success rate of 98.57% across 350\nplanning tasks, significantly outperforming competitive baselines. This\ndemonstrates the ability of our method to discover and use diverse trajectory\nmodes, enhancing flexibility and adaptability in complex environments, and\ndelivering significant improvements over standard baselines without incurring\nmajor computational costs."
                },
                "authors": [
                    {
                        "name": "Jiayun Li"
                    },
                    {
                        "name": "Kay Pompetzki"
                    },
                    {
                        "name": "An Thai Le"
                    },
                    {
                        "name": "Haolei Tong"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04934v1",
                "updated": "2025-04-07T11:19:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    19,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:19:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    19,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "Boosting Relational Deep Learning with Pretrained Tabular Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Relational Deep Learning with Pretrained Tabular Models"
                },
                "summary": "Relational databases, organized into tables connected by primary-foreign key\nrelationships, are a common format for organizing data. Making predictions on\nrelational data often involves transforming them into a flat tabular format\nthrough table joins and feature engineering, which serve as input to tabular\nmethods. However, designing features that fully capture complex relational\npatterns remains challenging. Graph Neural Networks (GNNs) offer a compelling\nalternative by inherently modeling these relationships, but their time overhead\nduring inference limits their applicability for real-time scenarios. In this\nwork, we aim to bridge this gap by leveraging existing feature engineering\nefforts to enhance the efficiency of GNNs in relational databases.\nSpecifically, we use GNNs to capture complex relationships within relational\ndatabases, patterns that are difficult to featurize, while employing engineered\nfeatures to encode temporal information, thereby avoiding the need to retain\nthe entire historical graph and enabling the use of smaller, more efficient\ngraphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also\noutperforms existing models. Experimental results on the RelBench benchmark\ndemonstrate that our framework achieves up to $33\\%$ performance improvement\nand a $526\\times$ inference speedup compared to GNNs, making it highly suitable\nfor real-time inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational databases, organized into tables connected by primary-foreign key\nrelationships, are a common format for organizing data. Making predictions on\nrelational data often involves transforming them into a flat tabular format\nthrough table joins and feature engineering, which serve as input to tabular\nmethods. However, designing features that fully capture complex relational\npatterns remains challenging. Graph Neural Networks (GNNs) offer a compelling\nalternative by inherently modeling these relationships, but their time overhead\nduring inference limits their applicability for real-time scenarios. In this\nwork, we aim to bridge this gap by leveraging existing feature engineering\nefforts to enhance the efficiency of GNNs in relational databases.\nSpecifically, we use GNNs to capture complex relationships within relational\ndatabases, patterns that are difficult to featurize, while employing engineered\nfeatures to encode temporal information, thereby avoiding the need to retain\nthe entire historical graph and enabling the use of smaller, more efficient\ngraphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also\noutperforms existing models. Experimental results on the RelBench benchmark\ndemonstrate that our framework achieves up to $33\\%$ performance improvement\nand a $526\\times$ inference speedup compared to GNNs, making it highly suitable\nfor real-time inference."
                },
                "authors": [
                    {
                        "name": "Veronica Lachi"
                    },
                    {
                        "name": "Antonio Longa"
                    },
                    {
                        "name": "Beatrice Bevilacqua"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Andrea Passerini"
                    },
                    {
                        "name": "Bruno Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Ribeiro"
                },
                "author": "Bruno Ribeiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04917v1",
                "updated": "2025-04-07T10:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    59,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:59:47Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    59,
                    47,
                    0,
                    97,
                    0
                ],
                "title": "Modeling the circumstellar interaction around SN 2004gq",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the circumstellar interaction around SN 2004gq"
                },
                "summary": "The relationship between the mass-loss history and final evolutionary stage\nof massive stars and the properties of the observable supernova (SN) is still\nunder debate. This is especially true for stripped-envelope (Type Ib/c) SNe,\nwhere the progenitor ejects a considerably large amount of material during its\nevolution, which can lead to a circumstellar medium relatively close to the\nexploding star. Moreover, when the star explodes as a SN, this matter may\ncontribute significantly to the generated luminosity because of the\ninteraction. However, the trace of this circumstellar interaction can only be\ninvestigated for a couple of Type Ib/c SNe, and the nature of a close (within\naround $10^{15}$ cm) circumstellar matter (CSM) has also been largely\nunexplored for these objects.\n  Here, we present the results of our radio and bolometric light curve (LC)\nanalysis related to SN 2004gq. We describe a combined model that explains the\nunusual LC properties of this event and supports the circumstellar interaction\nscenario. For that, we computed the quasi-bolometric LC of the SN and fit this\nwith a multicomponent model to gain information on the progenitor and the\nsurrounding circumstellar medium. We also analyzed the available radio LCs\n(taken at 1.4,\\ 4.9 and 8.5 GHz) of SN 2004gq to verify our estimated average\nmass-loss rate, which is one of the most crucial physical properties related to\nCSM models.\n  We infer reasonable parameters for SN 2004gq using radioactive decay and\nmagnetar energy input. To power the entire LC, we must also add an extra energy\nsource related to the CSM. We determine the most essential parameter of this\nmedium: the average mass-loss rate from both LC and radio data fitting. We find\nthat the suggested hidden circumstellar interaction is a viable mechanism that\nprovides the required energy deficiency and that it can be estimated using a\nsimple semi-analytic model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relationship between the mass-loss history and final evolutionary stage\nof massive stars and the properties of the observable supernova (SN) is still\nunder debate. This is especially true for stripped-envelope (Type Ib/c) SNe,\nwhere the progenitor ejects a considerably large amount of material during its\nevolution, which can lead to a circumstellar medium relatively close to the\nexploding star. Moreover, when the star explodes as a SN, this matter may\ncontribute significantly to the generated luminosity because of the\ninteraction. However, the trace of this circumstellar interaction can only be\ninvestigated for a couple of Type Ib/c SNe, and the nature of a close (within\naround $10^{15}$ cm) circumstellar matter (CSM) has also been largely\nunexplored for these objects.\n  Here, we present the results of our radio and bolometric light curve (LC)\nanalysis related to SN 2004gq. We describe a combined model that explains the\nunusual LC properties of this event and supports the circumstellar interaction\nscenario. For that, we computed the quasi-bolometric LC of the SN and fit this\nwith a multicomponent model to gain information on the progenitor and the\nsurrounding circumstellar medium. We also analyzed the available radio LCs\n(taken at 1.4,\\ 4.9 and 8.5 GHz) of SN 2004gq to verify our estimated average\nmass-loss rate, which is one of the most crucial physical properties related to\nCSM models.\n  We infer reasonable parameters for SN 2004gq using radioactive decay and\nmagnetar energy input. To power the entire LC, we must also add an extra energy\nsource related to the CSM. We determine the most essential parameter of this\nmedium: the average mass-loss rate from both LC and radio data fitting. We find\nthat the suggested hidden circumstellar interaction is a viable mechanism that\nprovides the required energy deficiency and that it can be estimated using a\nsimple semi-analytic model."
                },
                "authors": [
                    {
                        "name": "A. P. Nagy"
                    },
                    {
                        "name": "B. H. Pál"
                    },
                    {
                        "name": "T. Szalai"
                    }
                ],
                "author_detail": {
                    "name": "T. Szalai"
                },
                "author": "T. Szalai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18666v2",
                "updated": "2025-04-07T10:57:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    57,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-24T13:31:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents"
                },
                "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19004v3",
                "updated": "2025-04-07T10:57:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    57,
                    40,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-24T14:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    41,
                    16,
                    0,
                    83,
                    0
                ],
                "title": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts"
                },
                "summary": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap."
                },
                "authors": [
                    {
                        "name": "Simon Goorney"
                    },
                    {
                        "name": "Eleni Karydi"
                    },
                    {
                        "name": "Borja Muñoz"
                    },
                    {
                        "name": "Otto Santesson"
                    },
                    {
                        "name": "Zeki Can Seskir"
                    },
                    {
                        "name": "Ana Alina Tudoran"
                    },
                    {
                        "name": "Jacob Sherson"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Sherson"
                },
                "author": "Jacob Sherson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04915v1",
                "updated": "2025-04-07T10:52:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    52,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    52,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question\n  Answering via White-Box and Black-Box LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question\n  Answering via White-Box and Black-Box LLM Collaboration"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/."
                },
                "authors": [
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03160v2",
                "updated": "2025-04-07T10:45:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    45,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-04T04:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments"
                },
                "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04907v1",
                "updated": "2025-04-07T10:32:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:32:42Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Bench: Human-Aligned Video Generation Benchmark"
                },
                "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."
                },
                "authors": [
                    {
                        "name": "Hui Han"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yiwen Yuan"
                    },
                    {
                        "name": "Yuling Wu"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Youhua Li"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Li-jia Li"
                    },
                    {
                        "name": "Yongxin Ni"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Ni"
                },
                "author": "Yongxin Ni",
                "arxiv_comment": "Accepted by CVPR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16719v2",
                "updated": "2025-04-07T10:07:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    7,
                    8,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-20T21:42:30Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    42,
                    30,
                    3,
                    79,
                    0
                ],
                "title": "Practical Acoustic Eavesdropping On Typed Passphrases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Acoustic Eavesdropping On Typed Passphrases"
                },
                "summary": "Cloud services have become an essential infrastructure for enterprises and\nindividuals. Access to these cloud services is typically governed by Identity\nand Access Management systems, where user authentication often relies on\npasswords. While best practices dictate the implementation of multi-factor\nauthentication, it's a reality that many such users remain solely protected by\npasswords. This reliance on passwords creates a significant vulnerability, as\nthese credentials can be compromised through various means, including\nside-channel attacks. This paper exploits keyboard acoustic emanations to infer\ntyped natural language passphrases via unsupervised learning, necessitating no\nprevious training data. Whilst this work focuses on short passphrases, it is\nalso applicable to longer messages, such as confidential emails, where the\nmargin for error is much greater, than with passphrases, making the attack even\nmore effective in such a setting. Unlike traditional attacks that require\nphysical access to the target device, acoustic side-channel attacks can be\nexecuted within the vicinity, without the user's knowledge, offering a\nworthwhile avenue for malicious actors. Our findings replicate and extend\nprevious work, confirming that cross-correlation audio preprocessing\noutperforms methods like mel-frequency-cepstral coefficients and fast-fourier\ntransforms in keystroke clustering. Moreover, we show that partial passphrase\nrecovery through clustering and a dictionary attack can enable faster than\nbrute-force attacks, further emphasizing the risks posed by this attack vector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud services have become an essential infrastructure for enterprises and\nindividuals. Access to these cloud services is typically governed by Identity\nand Access Management systems, where user authentication often relies on\npasswords. While best practices dictate the implementation of multi-factor\nauthentication, it's a reality that many such users remain solely protected by\npasswords. This reliance on passwords creates a significant vulnerability, as\nthese credentials can be compromised through various means, including\nside-channel attacks. This paper exploits keyboard acoustic emanations to infer\ntyped natural language passphrases via unsupervised learning, necessitating no\nprevious training data. Whilst this work focuses on short passphrases, it is\nalso applicable to longer messages, such as confidential emails, where the\nmargin for error is much greater, than with passphrases, making the attack even\nmore effective in such a setting. Unlike traditional attacks that require\nphysical access to the target device, acoustic side-channel attacks can be\nexecuted within the vicinity, without the user's knowledge, offering a\nworthwhile avenue for malicious actors. Our findings replicate and extend\nprevious work, confirming that cross-correlation audio preprocessing\noutperforms methods like mel-frequency-cepstral coefficients and fast-fourier\ntransforms in keystroke clustering. Moreover, we show that partial passphrase\nrecovery through clustering and a dictionary attack can enable faster than\nbrute-force attacks, further emphasizing the risks posed by this attack vector."
                },
                "authors": [
                    {
                        "name": "Darren Fürst"
                    },
                    {
                        "name": "Andreas Aßmuth"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Aßmuth"
                },
                "author": "Andreas Aßmuth",
                "arxiv_comment": "8 pages, 9 figures",
                "arxiv_journal_ref": "Proc of the 16th International Conference on Cloud Computing,\n  GRIDs, and Virtualization (Cloud Computing 2025), pp. 9-16, Valencia, Spain,\n  April 2025, ISSN 2308-4294",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04893v1",
                "updated": "2025-04-07T10:01:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models"
                },
                "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
                },
                "authors": [
                    {
                        "name": "Justus Westerhoff"
                    },
                    {
                        "name": "Erblina Purellku"
                    },
                    {
                        "name": "Jakob Hackstein"
                    },
                    {
                        "name": "Leo Pinetzki"
                    },
                    {
                        "name": "Lorenz Hufe"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hufe"
                },
                "author": "Lorenz Hufe",
                "arxiv_comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04891v1",
                "updated": "2025-04-07T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    58,
                    19,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    58,
                    19,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging Large Language Models for Cost-Effective, Multilingual\n  Depression Detection and Severity Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Cost-Effective, Multilingual\n  Depression Detection and Severity Assessment"
                },
                "summary": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability."
                },
                "authors": [
                    {
                        "name": "Longdi Xian"
                    },
                    {
                        "name": "Jianzhang Ni"
                    },
                    {
                        "name": "Mingzhu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhu Wang"
                },
                "author": "Mingzhu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04877v1",
                "updated": "2025-04-07T09:41:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:41:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "SoK: LLM-based Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: LLM-based Log Parsing"
                },
                "summary": "Log data, generated by software systems, provides crucial insights for tasks\nlike monitoring, root cause analysis, and anomaly detection. Due to the vast\nvolume of logs, automated log parsing is essential to transform semi-structured\nlog messages into structured representations. Traditional log parsing\ntechniques often require manual configurations, such as defining log formats or\nlabeling data, which limits scalability and usability. Recent advances in large\nlanguage models (LLMs) have introduced the new research field of LLM-based log\nparsing, offering potential improvements in automation and adaptability.\nDespite promising results, there is no structured overview of these approaches\nsince this is a relatively new research field with the earliest advances\npublished in late 2023. This paper systematically reviews 29 LLM-based log\nparsing methods, comparing their capabilities, limitations, and reliance on\nmanual effort. We analyze the learning and prompt-engineering paradigms\nemployed, efficiency- and effectiveness-enhancing techniques, and the role of\nLLMs in the parsing process. We aggregate the results of the survey in a large\ntable comprising the characterizing features of LLM-based log parsing\napproaches and derive the general process of LLM-based log parsing,\nincorporating all reviewed approaches in a single flow chart. Additionally, we\nbenchmark seven open-source LLM-based log parsers on public datasets and\ncritically assess their reproducibility. Our findings summarize the advances of\nthis new research field and provide insights for researchers and practitioners\nseeking efficient and user-friendly log parsing solutions, with all code and\nresults made publicly available for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log data, generated by software systems, provides crucial insights for tasks\nlike monitoring, root cause analysis, and anomaly detection. Due to the vast\nvolume of logs, automated log parsing is essential to transform semi-structured\nlog messages into structured representations. Traditional log parsing\ntechniques often require manual configurations, such as defining log formats or\nlabeling data, which limits scalability and usability. Recent advances in large\nlanguage models (LLMs) have introduced the new research field of LLM-based log\nparsing, offering potential improvements in automation and adaptability.\nDespite promising results, there is no structured overview of these approaches\nsince this is a relatively new research field with the earliest advances\npublished in late 2023. This paper systematically reviews 29 LLM-based log\nparsing methods, comparing their capabilities, limitations, and reliance on\nmanual effort. We analyze the learning and prompt-engineering paradigms\nemployed, efficiency- and effectiveness-enhancing techniques, and the role of\nLLMs in the parsing process. We aggregate the results of the survey in a large\ntable comprising the characterizing features of LLM-based log parsing\napproaches and derive the general process of LLM-based log parsing,\nincorporating all reviewed approaches in a single flow chart. Additionally, we\nbenchmark seven open-source LLM-based log parsers on public datasets and\ncritically assess their reproducibility. Our findings summarize the advances of\nthis new research field and provide insights for researchers and practitioners\nseeking efficient and user-friendly log parsing solutions, with all code and\nresults made publicly available for transparency."
                },
                "authors": [
                    {
                        "name": "Viktor Beck"
                    },
                    {
                        "name": "Max Landauer"
                    },
                    {
                        "name": "Markus Wurzenberger"
                    },
                    {
                        "name": "Florian Skopik"
                    },
                    {
                        "name": "Andreas Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rauber"
                },
                "author": "Andreas Rauber",
                "arxiv_comment": "34 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2401.17377v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17377v4",
                "updated": "2025-04-07T17:59:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    59,
                    50,
                    0,
                    97,
                    0
                ],
                "published": "2024-01-30T19:03:49Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    19,
                    3,
                    49,
                    1,
                    30,
                    0
                ],
                "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens"
                },
                "summary": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Published at COLM 2024, spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17377v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17377v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05299v1",
                "updated": "2025-04-07T17:58:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    58,
                    57,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:58:57Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    58,
                    57,
                    0,
                    97,
                    0
                ],
                "title": "SmolVLM: Redefining small and efficient multimodal models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmolVLM: Redefining small and efficient multimodal models"
                },
                "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales."
                },
                "authors": [
                    {
                        "name": "Andrés Marafioti"
                    },
                    {
                        "name": "Orr Zohar"
                    },
                    {
                        "name": "Miquel Farré"
                    },
                    {
                        "name": "Merve Noyan"
                    },
                    {
                        "name": "Elie Bakouch"
                    },
                    {
                        "name": "Pedro Cuenca"
                    },
                    {
                        "name": "Cyril Zakka"
                    },
                    {
                        "name": "Loubna Ben Allal"
                    },
                    {
                        "name": "Anton Lozhkov"
                    },
                    {
                        "name": "Nouamane Tazi"
                    },
                    {
                        "name": "Vaibhav Srivastav"
                    },
                    {
                        "name": "Joshua Lochner"
                    },
                    {
                        "name": "Hugo Larcher"
                    },
                    {
                        "name": "Mathieu Morlon"
                    },
                    {
                        "name": "Lewis Tunstall"
                    },
                    {
                        "name": "Leandro von Werra"
                    },
                    {
                        "name": "Thomas Wolf"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wolf"
                },
                "author": "Thomas Wolf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05294v1",
                "updated": "2025-04-07T17:49:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    23,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:49:23Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    49,
                    23,
                    0,
                    97,
                    0
                ],
                "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations"
                },
                "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."
                },
                "authors": [
                    {
                        "name": "Pedro Ferreira"
                    },
                    {
                        "name": "Wilker Aziz"
                    },
                    {
                        "name": "Ivan Titov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Titov"
                },
                "author": "Ivan Titov",
                "arxiv_comment": "22 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19265v2",
                "updated": "2025-04-07T17:43:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    43,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-25T01:59:57Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    1,
                    59,
                    57,
                    1,
                    84,
                    0
                ],
                "title": "PHEONA: An Evaluation Framework for Large Language Model-based\n  Approaches to Computational Phenotyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHEONA: An Evaluation Framework for Large Language Model-based\n  Approaches to Computational Phenotyping"
                },
                "summary": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational phenotyping is essential for biomedical research but often\nrequires significant time and resources, especially since traditional methods\ntypically involve extensive manual data review. While machine learning and\nnatural language processing advancements have helped, further improvements are\nneeded. Few studies have explored using Large Language Models (LLMs) for these\ntasks despite known advantages of LLMs for text-based tasks. To facilitate\nfurther research in this area, we developed an evaluation framework, Evaluation\nof PHEnotyping for Observational Health Data (PHEONA), that outlines\ncontext-specific considerations. We applied and demonstrated PHEONA on concept\nclassification, a specific task within a broader phenotyping process for Acute\nRespiratory Failure (ARF) respiratory support therapies. From the sample\nconcepts tested, we achieved high classification accuracy, suggesting the\npotential for LLM-based methods to improve computational phenotyping processes."
                },
                "authors": [
                    {
                        "name": "Sarah Pungitore"
                    },
                    {
                        "name": "Shashank Yadav"
                    },
                    {
                        "name": "Vignesh Subbian"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh Subbian"
                },
                "author": "Vignesh Subbian",
                "arxiv_comment": "2 figures, 5 tables, submitted to 2025 AMIA Annual Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05278v1",
                "updated": "2025-04-07T17:24:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    24,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:24:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    24,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "The challenge of uncertainty quantification of large language models in\n  medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of uncertainty quantification of large language models in\n  medicine"
                },
                "summary": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge."
                },
                "authors": [
                    {
                        "name": "Zahra Atf"
                    },
                    {
                        "name": "Seyed Amir Ahmad Safavi-Naini"
                    },
                    {
                        "name": "Peter R. Lewis"
                    },
                    {
                        "name": "Aref Mahjoubfar"
                    },
                    {
                        "name": "Nariman Naderi"
                    },
                    {
                        "name": "Thomas R. Savage"
                    },
                    {
                        "name": "Ali Soroush"
                    }
                ],
                "author_detail": {
                    "name": "Ali Soroush"
                },
                "author": "Ali Soroush",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05276v1",
                "updated": "2025-04-07T17:17:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    17,
                    41,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T17:17:41Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    17,
                    17,
                    41,
                    0,
                    97,
                    0
                ],
                "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation"
                },
                "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."
                },
                "authors": [
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Haoyu Han"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yu Xue"
                    },
                    {
                        "name": "Tingting Li"
                    },
                    {
                        "name": "Joseph Krajcik"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05262v1",
                "updated": "2025-04-07T16:57:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    57,
                    10,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:57:10Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    57,
                    10,
                    0,
                    97,
                    0
                ],
                "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models"
                },
                "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Yan"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05259v1",
                "updated": "2025-04-07T16:52:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    52,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:52:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    52,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "How to evaluate control measures for LLM agents? A trajectory from today\n  to superintelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to evaluate control measures for LLM agents? A trajectory from today\n  to superintelligence"
                },
                "summary": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM agents grow more capable of causing harm autonomously, AI developers\nwill rely on increasingly sophisticated control measures to prevent possibly\nmisaligned agents from causing harm. AI developers could demonstrate that their\ncontrol measures are sufficient by running control evaluations: testing\nexercises in which a red team produces agents that try to subvert control\nmeasures. To ensure control evaluations accurately capture misalignment risks,\nthe affordances granted to this red team should be adapted to the capability\nprofiles of the agents to be deployed under control measures.\n  In this paper we propose a systematic framework for adapting affordances of\nred teams to advancing AI capabilities. Rather than assuming that agents will\nalways execute the best attack strategies known to humans, we demonstrate how\nknowledge of an agents's actual capability profile can inform proportional\ncontrol evaluations, resulting in more practical and cost-effective control\nmeasures. We illustrate our framework by considering a sequence of five\nfictional models (M1-M5) with progressively advanced capabilities, defining\nfive distinct AI control levels (ACLs). For each ACL, we provide example rules\nfor control evaluation, control measures, and safety cases that could be\nappropriate. Finally, we show why constructing a compelling AI control safety\ncase for superintelligent LLM agents will require research breakthroughs,\nhighlighting that we might eventually need alternative approaches to mitigating\nmisalignment risk."
                },
                "authors": [
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Mikita Balesni"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Geoffrey Irving"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Irving"
                },
                "author": "Geoffrey Irving",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05258v1",
                "updated": "2025-04-07T16:51:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:51:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    51,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason Over Time: Timeline Self-Reflection for Improved\n  Temporal Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Adrián Bazaga"
                    },
                    {
                        "name": "Rexhina Blloshmi"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Adrià de Gispert"
                    }
                ],
                "author_detail": {
                    "name": "Adrià de Gispert"
                },
                "author": "Adrià de Gispert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21157v3",
                "updated": "2025-04-07T16:49:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    49,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-27T04:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    4,
                    50,
                    14,
                    3,
                    86,
                    0
                ],
                "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?"
                },
                "summary": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article surveys Evaluation models to automatically detect hallucinations\nin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark\nof their performance across six RAG applications. Methods included in our study\ninclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation\nModel (HHEM), and the Trustworthy Language Model (TLM). These approaches are\nall reference-free, requiring no ground-truth answers/labels to catch incorrect\nLLM responses. Our study reveals that, across diverse RAG applications, some of\nthese approaches consistently detect incorrect RAG responses with high\nprecision/recall."
                },
                "authors": [
                    {
                        "name": "Ashish Sardana"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Sardana"
                },
                "author": "Ashish Sardana",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00689v2",
                "updated": "2025-04-07T16:38:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    38,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-01T15:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    50,
                    58,
                    4,
                    306,
                    0
                ],
                "title": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a reliable external\nknowledge augmentation technique to mitigate hallucination issues and\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\nadaptive RAG (ARAG) systems excel at in-depth exploration within a single\nsource but struggle to effectively and controllably explore different retrieval\nsources, as they fail to foresee their internal knowledge features. We develop\na novel multi-source ARAG system, PrefRAG, which enhances RAG by enabling\nin-depth and controllable exploration of diverse retrieval sources through\npreference-driven adaptive retrieval and self-reflection. PrefRAG first fully\nexplores controllable local sources in adaptive retrieval and supplements with\nthe web when appropriate, ultimately selecting the optimal source for knowledge\nobservation. Subsequently, PrefRAG feeds answer quality feedback into the\nretrieval process, optimizing it from the generation perspective to produce\nhigher-quality responses. Extensive experiments confirm its superiority, high\nretrieval efficiency, and knowledge controllability. PrefRAG outperforms\nVanilla RAG and the leading MS-ARAG by up to 25.6% and 13.9% respectively.\nAdditionally, PrefRAG trained with DPO achieves higher performance. The code\nand data are available at https://github.com/QingFei1/PrefRAG.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a reliable external\nknowledge augmentation technique to mitigate hallucination issues and\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\nadaptive RAG (ARAG) systems excel at in-depth exploration within a single\nsource but struggle to effectively and controllably explore different retrieval\nsources, as they fail to foresee their internal knowledge features. We develop\na novel multi-source ARAG system, PrefRAG, which enhances RAG by enabling\nin-depth and controllable exploration of diverse retrieval sources through\npreference-driven adaptive retrieval and self-reflection. PrefRAG first fully\nexplores controllable local sources in adaptive retrieval and supplements with\nthe web when appropriate, ultimately selecting the optimal source for knowledge\nobservation. Subsequently, PrefRAG feeds answer quality feedback into the\nretrieval process, optimizing it from the generation perspective to produce\nhigher-quality responses. Extensive experiments confirm its superiority, high\nretrieval efficiency, and knowledge controllability. PrefRAG outperforms\nVanilla RAG and the leading MS-ARAG by up to 25.6% and 13.9% respectively.\nAdditionally, PrefRAG trained with DPO achieves higher performance. The code\nand data are available at https://github.com/QingFei1/PrefRAG.git."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "33 pages, 5 figures, 28 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05239v1",
                "updated": "2025-04-07T16:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    7,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:23:07Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    23,
                    7,
                    0,
                    97,
                    0
                ],
                "title": "LLM-based Automated Grading with Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Automated Grading with Human-in-the-Loop"
                },
                "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation."
                },
                "authors": [
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Yasemin Copur-Gencturk"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08821v2",
                "updated": "2025-04-07T16:17:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    17,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-11T14:03:29Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    3,
                    29,
                    4,
                    285,
                    0
                ],
                "title": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepNote: Note-Centric Deep Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) mitigates factual errors and\nhallucinations in Large Language Models (LLMs) for question-answering (QA) by\nincorporating external knowledge. However, existing adaptive RAG methods rely\non LLMs to predict retrieval timing and directly use retrieved information for\ngeneration, often failing to reflect real information needs and fully leverage\nretrieved knowledge. We develop DeepNote, an adaptive RAG framework that\nachieves in-depth and robust exploration of knowledge sources through\nnote-centric adaptive retrieval. DeepNote employs notes as carriers for\nrefining and accumulating knowledge. During in-depth exploration, it uses these\nnotes to determine retrieval timing, formulate retrieval queries, and\niteratively assess knowledge growth, ultimately leveraging the best note for\nanswer generation. Extensive experiments and analyses demonstrate that DeepNote\nsignificantly outperforms all baselines (+10.2% to +20.1%) and exhibits the\nability to gather knowledge with both high density and quality. Additionally,\nDPO further improves the performance of DeepNote. The code and data are\navailable at https://github.com/thunlp/DeepNote.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) mitigates factual errors and\nhallucinations in Large Language Models (LLMs) for question-answering (QA) by\nincorporating external knowledge. However, existing adaptive RAG methods rely\non LLMs to predict retrieval timing and directly use retrieved information for\ngeneration, often failing to reflect real information needs and fully leverage\nretrieved knowledge. We develop DeepNote, an adaptive RAG framework that\nachieves in-depth and robust exploration of knowledge sources through\nnote-centric adaptive retrieval. DeepNote employs notes as carriers for\nrefining and accumulating knowledge. During in-depth exploration, it uses these\nnotes to determine retrieval timing, formulate retrieval queries, and\niteratively assess knowledge growth, ultimately leveraging the best note for\nanswer generation. Extensive experiments and analyses demonstrate that DeepNote\nsignificantly outperforms all baselines (+10.2% to +20.1%) and exhibits the\nability to gather knowledge with both high density and quality. Additionally,\nDPO further improves the performance of DeepNote. The code and data are\navailable at https://github.com/thunlp/DeepNote."
                },
                "authors": [
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "28 pages, 6 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05231v1",
                "updated": "2025-04-07T16:15:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    15,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:15:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    15,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Mapping biodiversity at very-high resolution in Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping biodiversity at very-high resolution in Europe"
                },
                "summary": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs."
                },
                "authors": [
                    {
                        "name": "César Leblanc"
                    },
                    {
                        "name": "Lukas Picek"
                    },
                    {
                        "name": "Benjamin Deneu"
                    },
                    {
                        "name": "Pierre Bonnet"
                    },
                    {
                        "name": "Maximilien Servajean"
                    },
                    {
                        "name": "Rémi Palard"
                    },
                    {
                        "name": "Alexis Joly"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Joly"
                },
                "author": "Alexis Joly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09439v2",
                "updated": "2025-04-07T16:13:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    13,
                    38,
                    0,
                    97,
                    0
                ],
                "published": "2024-11-14T16:58:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Spider: Any-to-Many Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spider: Any-to-Many Multimodal LLM"
                },
                "summary": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, an Any-to-Many Instruction Template designed\nfor producing Xs signal prompts, and a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents. To train\nSpider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which\nfacilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for\nAMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset,\nthe first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG\ntasks in future research. Overall, this work not only pushes the boundary of\nmultimodal interaction but also provides rich data support for advancing the\nfield. Code: https://github.com/Layjins/Spider",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, an Any-to-Many Instruction Template designed\nfor producing Xs signal prompts, and a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents. To train\nSpider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which\nfacilitates learning the X-to-Xs (i.e., Any-to-Many) capability necessary for\nAMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset,\nthe first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG\ntasks in future research. Overall, this work not only pushes the boundary of\nmultimodal interaction but also provides rich data support for advancing the\nfield. Code: https://github.com/Layjins/Spider"
                },
                "authors": [
                    {
                        "name": "Jinxiang Lai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05220v2",
                "updated": "2025-04-08T02:11:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    2,
                    11,
                    5,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T16:05:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    5,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort\n  for Retrieval and RAG"
                },
                "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05217v1",
                "updated": "2025-04-07T16:04:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    4,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:04:00Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    4,
                    0,
                    0,
                    97,
                    0
                ],
                "title": "LLM-Alignment Live-Streaming Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Alignment Live-Streaming Recommendation"
                },
                "summary": "In recent years, integrated short-video and live-streaming platforms have\ngained massive global adoption, offering dynamic content creation and\nconsumption. Unlike pre-recorded short videos, live-streaming enables real-time\ninteraction between authors and users, fostering deeper engagement. However,\nthis dynamic nature introduces a critical challenge for recommendation systems\n(RecSys): the same live-streaming vastly different experiences depending on\nwhen a user watching. To optimize recommendations, a RecSys must accurately\ninterpret the real-time semantics of live content and align them with user\npreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, integrated short-video and live-streaming platforms have\ngained massive global adoption, offering dynamic content creation and\nconsumption. Unlike pre-recorded short videos, live-streaming enables real-time\ninteraction between authors and users, fostering deeper engagement. However,\nthis dynamic nature introduces a critical challenge for recommendation systems\n(RecSys): the same live-streaming vastly different experiences depending on\nwhen a user watching. To optimize recommendations, a RecSys must accurately\ninterpret the real-time semantics of live content and align them with user\npreferences."
                },
                "authors": [
                    {
                        "name": "Yueyang Liu"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Shuang Wen"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Xiangyu Wu"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05216v1",
                "updated": "2025-04-07T16:03:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:03:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling"
                },
                "summary": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xiaojie Sun"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07045v2",
                "updated": "2025-04-07T16:01:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-02-10T21:27:06Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    27,
                    6,
                    0,
                    41,
                    0
                ],
                "title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs"
                },
                "summary": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition."
                },
                "authors": [
                    {
                        "name": "Haywood Gelman"
                    },
                    {
                        "name": "John D. Hastings"
                    }
                ],
                "author_detail": {
                    "name": "John D. Hastings"
                },
                "author": "John D. Hastings",
                "arxiv_comment": "6 pages, 0 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; I.2.7; K.4.1; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05214v1",
                "updated": "2025-04-07T16:01:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T16:01:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    1,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Post-Training Language Models for Continual Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Language Models for Continual Relation Extraction"
                },
                "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    },
                    {
                        "name": "Sonja Schimmler"
                    }
                ],
                "author_detail": {
                    "name": "Sonja Schimmler"
                },
                "author": "Sonja Schimmler",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05204v1",
                "updated": "2025-04-07T15:51:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    51,
                    31,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:51:31Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    51,
                    31,
                    0,
                    97,
                    0
                ],
                "title": "Quantum Program Linting with LLMs: Emerging Results from a Comparative\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Program Linting with LLMs: Emerging Results from a Comparative\n  Study"
                },
                "summary": "Ensuring the quality of quantum programs is increasingly important; however,\ntraditional static analysis techniques are insufficient due to the unique\ncharacteristics of quantum computing. Quantum-specific linting tools, such as\nLintQ, have been developed to detect quantum-specific programming problems;\nhowever, they typically rely on manually crafted analysis queries. The manual\neffort required to update these tools limits their adaptability to evolving\nquantum programming practices.\n  To address this challenge, this study investigates the feasibility of\nemploying Large Language Models (LLMs) to develop a novel linting technique for\nquantum software development and explores potential avenues to advance linting\napproaches. We introduce LintQ-LLM, an LLM-based linting tool designed to\ndetect quantum-specific problems comparable to those identified by LintQ.\nThrough an empirical comparative study using real-world Qiskit programs, our\nresults show that LintQ-LLM is a viable solution that complements LintQ, with\nparticular strengths in problem localization, explanation clarity, and\nadaptability potential for emerging quantum programming frameworks, thus\nproviding a basis for further research. Furthermore, this study discusses\nseveral research opportunities for developing more advanced, adaptable, and\nfeedback-aware quantum software quality assurance methods by leveraging LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the quality of quantum programs is increasingly important; however,\ntraditional static analysis techniques are insufficient due to the unique\ncharacteristics of quantum computing. Quantum-specific linting tools, such as\nLintQ, have been developed to detect quantum-specific programming problems;\nhowever, they typically rely on manually crafted analysis queries. The manual\neffort required to update these tools limits their adaptability to evolving\nquantum programming practices.\n  To address this challenge, this study investigates the feasibility of\nemploying Large Language Models (LLMs) to develop a novel linting technique for\nquantum software development and explores potential avenues to advance linting\napproaches. We introduce LintQ-LLM, an LLM-based linting tool designed to\ndetect quantum-specific problems comparable to those identified by LintQ.\nThrough an empirical comparative study using real-world Qiskit programs, our\nresults show that LintQ-LLM is a viable solution that complements LintQ, with\nparticular strengths in problem localization, explanation clarity, and\nadaptability potential for emerging quantum programming frameworks, thus\nproviding a basis for further research. Furthermore, this study discusses\nseveral research opportunities for developing more advanced, adaptable, and\nfeedback-aware quantum software quality assurance methods by leveraging LLMs."
                },
                "authors": [
                    {
                        "name": "Seung Yeob Shin"
                    },
                    {
                        "name": "Fabrizio Pastore"
                    },
                    {
                        "name": "Domenico Bianculli"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Bianculli"
                },
                "author": "Domenico Bianculli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09722v2",
                "updated": "2025-04-07T15:39:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    39,
                    16,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-12T20:59:43Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    20,
                    59,
                    43,
                    3,
                    347,
                    0
                ],
                "title": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong\n  Prompt Optimizers"
                },
                "summary": "The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of large language models (LLMs) is closely tied to the\ndesign of prompts, making prompt optimization essential for enhancing their\nperformance across a wide range of tasks. Many existing approaches to\nautomating prompt engineering rely exclusively on textual feedback, refining\nprompts based solely on inference errors identified by large, computationally\nexpensive LLMs. Unfortunately, smaller models struggle to generate high-quality\nfeedback, resulting in complete dependence on large LLM judgment. Moreover,\nthese methods fail to leverage more direct and finer-grained information, such\nas gradients, due to operating purely in text space. To this end, we introduce\nGReaTer, a novel prompt optimization technique that directly incorporates\ngradient information over task-specific reasoning. By utilizing task loss\ngradients, GReaTer enables self-optimization of prompts for open-source,\nlightweight language models without the need for costly closed-source LLMs.\nThis allows high-performance prompt optimization without dependence on massive\nLLMs, closing the gap between smaller models and the sophisticated reasoning\noften needed for prompt refinement. Extensive evaluations across diverse\nreasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer\nconsistently outperforms previous state-of-the-art prompt optimization methods,\neven those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts\nfrequently exhibit better transferability and, in some cases, boost task\nperformance to levels comparable to or surpassing those achieved by larger\nlanguage models, highlighting the effectiveness of prompt optimization guided\nby gradients over reasoning. Code of GReaTer is available at\nhttps://github.com/psunlpgroup/GreaTer."
                },
                "authors": [
                    {
                        "name": "Sarkar Snigdha Sarathi Das"
                    },
                    {
                        "name": "Ryo Kamoi"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yusen Zhang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05185v1",
                "updated": "2025-04-07T15:35:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    35,
                    54,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    35,
                    54,
                    0,
                    97,
                    0
                ],
                "title": "Concise Reasoning via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise Reasoning via Reinforcement Learning"
                },
                "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results."
                },
                "authors": [
                    {
                        "name": "Mehdi Fatemi"
                    },
                    {
                        "name": "Banafsheh Rafiee"
                    },
                    {
                        "name": "Mingjie Tang"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Talamadupula"
                },
                "author": "Kartik Talamadupula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23633v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23633v3",
                "updated": "2025-04-07T15:29:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    29,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-31T00:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    0,
                    12,
                    48,
                    0,
                    90,
                    0
                ],
                "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS"
                },
                "summary": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. As we design and deploy increasingly intelligent\ngeospatial systems, we have a responsibility to ensure they are developed in\nsocially responsible ways, serve the public good, and support the continued\nvalue of human geographic insight in an AI-augmented future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. As we design and deploy increasingly intelligent\ngeospatial systems, we have a responsibility to ensure they are developed in\nsocially responsible ways, serve the public good, and support the continued\nvalue of human geographic insight in an AI-augmented future."
                },
                "authors": [
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Krzysztof Janowicz"
                    },
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Samantha T. Arundel"
                    },
                    {
                        "name": "Chaowei Yang"
                    },
                    {
                        "name": "Budhendra Bhaduri"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "A-Xing Zhu"
                    },
                    {
                        "name": "Mark Gahegan"
                    },
                    {
                        "name": "Shashi Shekhar"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Grant McKenzie"
                    },
                    {
                        "name": "Guido Cervone"
                    },
                    {
                        "name": "Michael E. Hodgson"
                    }
                ],
                "author_detail": {
                    "name": "Michael E. Hodgson"
                },
                "author": "Michael E. Hodgson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23633v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23633v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05180v1",
                "updated": "2025-04-07T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    27,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA\n  Tasks"
                },
                "summary": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yang Zou"
                    },
                    {
                        "name": "Christopher Ellis"
                    },
                    {
                        "name": "Ruben Purdy"
                    },
                    {
                        "name": "Shawn Blanton"
                    },
                    {
                        "name": "José M. F. Moura"
                    }
                ],
                "author_detail": {
                    "name": "José M. F. Moura"
                },
                "author": "José M. F. Moura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05173v1",
                "updated": "2025-04-07T15:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    17,
                    35,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    17,
                    35,
                    0,
                    97,
                    0
                ],
                "title": "PRDTs: Composable Knowledge-Based Consensus Protocols with Replicated\n  Data Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRDTs: Composable Knowledge-Based Consensus Protocols with Replicated\n  Data Types"
                },
                "summary": "Consensus protocols are fundamental in distributed systems as they enable\nsoftware with strong consistency properties. However, designing optimized\nprotocols for specific use-cases under certain system assumptions is typically\na laborious and error-prone process requiring expert knowledge. While most\nrecent optimized protocols are variations of well-known algorithms like Paxos\nor Raft, they often necessitate complete re-implementations, potentially\nintroducing new bugs and complicating the application of existing verification\nresults.This approach stands in the way of application-specific consistency\nprotocols that can easily be amended or swapped out, depending on the given\napplication and deployment scenario.\n  We propose Protocol Replicated Data Types (PRDTs), a novel programming model\nfor implementing consensus protocols using replicated data types (RDTs).\nInspired by the knowledge-based view of consensus, PRDTs employ RDTs to\nmonotonically accumulate until agreement is reached. This approach allows for\nimplementations focusing on high-level protocol logic with minimal network\nenvironment assumptions. Moreover, by applying existing algebraic composition\ntechniques for RDTs in the PRDT context, we enable composable protocol\nbuilding-blocks for implementing complex protocols. We present a formal model\nof our approach, demonstrate its application in PRDT-based implementations of\nexisting protocols, and report empirical evaluation results. Our findings\nindicate that the PRDT approach offers enhanced flexibility and composability\nin protocol design, facilitates reasoning about correctness, and does not\nsuffer from inherent performance limitations that would prevent its use in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus protocols are fundamental in distributed systems as they enable\nsoftware with strong consistency properties. However, designing optimized\nprotocols for specific use-cases under certain system assumptions is typically\na laborious and error-prone process requiring expert knowledge. While most\nrecent optimized protocols are variations of well-known algorithms like Paxos\nor Raft, they often necessitate complete re-implementations, potentially\nintroducing new bugs and complicating the application of existing verification\nresults.This approach stands in the way of application-specific consistency\nprotocols that can easily be amended or swapped out, depending on the given\napplication and deployment scenario.\n  We propose Protocol Replicated Data Types (PRDTs), a novel programming model\nfor implementing consensus protocols using replicated data types (RDTs).\nInspired by the knowledge-based view of consensus, PRDTs employ RDTs to\nmonotonically accumulate until agreement is reached. This approach allows for\nimplementations focusing on high-level protocol logic with minimal network\nenvironment assumptions. Moreover, by applying existing algebraic composition\ntechniques for RDTs in the PRDT context, we enable composable protocol\nbuilding-blocks for implementing complex protocols. We present a formal model\nof our approach, demonstrate its application in PRDT-based implementations of\nexisting protocols, and report empirical evaluation results. Our findings\nindicate that the PRDT approach offers enhanced flexibility and composability\nin protocol design, facilitates reasoning about correctness, and does not\nsuffer from inherent performance limitations that would prevent its use in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Julian Haas"
                    },
                    {
                        "name": "Ragnar Mogk"
                    },
                    {
                        "name": "Annette Bieniusa"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05163v1",
                "updated": "2025-04-07T15:08:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T15:08:03Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness"
                },
                "summary": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings."
                },
                "authors": [
                    {
                        "name": "Dongzhuoran Zhou"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Steffen Staab"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Staab"
                },
                "author": "Steffen Staab",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01681v3",
                "updated": "2025-04-07T15:00:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    0,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2024-01-22T06:02:39Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    6,
                    2,
                    39,
                    0,
                    22,
                    0
                ],
                "title": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social\n  Media Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social\n  Media Communications"
                },
                "summary": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Ge Gao"
                    },
                    {
                        "name": "Wei Ai"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ai"
                },
                "author": "Wei Ai",
                "arxiv_comment": "Accepted by the 19th International AAAI Conference on Web and Social\n  Media (ICWSM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05147v1",
                "updated": "2025-04-07T14:52:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:52:40Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "title": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs"
                },
                "summary": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods"
                },
                "authors": [
                    {
                        "name": "Amrita Roy Chowdhury"
                    },
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Divyam Anshumaan"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Nicolas Papernot"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Mihir Bellare"
                    }
                ],
                "author_detail": {
                    "name": "Mihir Bellare"
                },
                "author": "Mihir Bellare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05146v1",
                "updated": "2025-04-07T14:50:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    50,
                    13,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:50:13Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    50,
                    13,
                    0,
                    97,
                    0
                ],
                "title": "Query Smarter, Trust Better? Exploring Search Behaviours for Verifying\n  News Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Smarter, Trust Better? Exploring Search Behaviours for Verifying\n  News Accuracy"
                },
                "summary": "While it is often assumed that searching for information to evaluate\nmisinformation will help identify false claims, recent work suggests that\nsearch behaviours can instead reinforce belief in misleading news, particularly\nwhen users generate queries using vocabulary from the source articles. Our\nresearch explores how different query generation strategies affect news\nverification and whether the way people search influences the accuracy of their\ninformation evaluation. A mixed-methods approach was used, consisting of three\nparts: (1) an analysis of existing data to understand how search behaviour\ninfluences trust in fake news, (2) a simulation of query generation strategies\nusing a Large Language Model (LLM) to assess the impact of different query\nformulations on search result quality, and (3) a user study to examine how\n'Boost' interventions in interface design can guide users to adopt more\neffective query strategies. The results show that search behaviour\nsignificantly affects trust in news, with successful searches involving\nmultiple queries and yielding higher-quality results. Queries inspired by\ndifferent parts of a news article produced search results of varying quality,\nand weak initial queries improved when reformulated using full SERP\ninformation. Although 'Boost' interventions had limited impact, the study\nsuggests that interface design encouraging users to thoroughly review search\nresults can enhance query formulation. This study highlights the importance of\nquery strategies in evaluating news and proposes that interface design can play\na key role in promoting more effective search practices, serving as one\ncomponent of a broader set of interventions to combat misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While it is often assumed that searching for information to evaluate\nmisinformation will help identify false claims, recent work suggests that\nsearch behaviours can instead reinforce belief in misleading news, particularly\nwhen users generate queries using vocabulary from the source articles. Our\nresearch explores how different query generation strategies affect news\nverification and whether the way people search influences the accuracy of their\ninformation evaluation. A mixed-methods approach was used, consisting of three\nparts: (1) an analysis of existing data to understand how search behaviour\ninfluences trust in fake news, (2) a simulation of query generation strategies\nusing a Large Language Model (LLM) to assess the impact of different query\nformulations on search result quality, and (3) a user study to examine how\n'Boost' interventions in interface design can guide users to adopt more\neffective query strategies. The results show that search behaviour\nsignificantly affects trust in news, with successful searches involving\nmultiple queries and yielding higher-quality results. Queries inspired by\ndifferent parts of a news article produced search results of varying quality,\nand weak initial queries improved when reformulated using full SERP\ninformation. Although 'Boost' interventions had limited impact, the study\nsuggests that interface design encouraging users to thoroughly review search\nresults can enhance query formulation. This study highlights the importance of\nquery strategies in evaluating news and proposes that interface design can play\na key role in promoting more effective search practices, serving as one\ncomponent of a broader set of interventions to combat misinformation."
                },
                "authors": [
                    {
                        "name": "David Elsweiler"
                    },
                    {
                        "name": "Samy Ateia"
                    },
                    {
                        "name": "Markus Bink"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Marcos Fernández Pichel"
                    },
                    {
                        "name": "Alexander Frummet"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "David Losada"
                    },
                    {
                        "name": "Bernd Ludwig"
                    },
                    {
                        "name": "Selina Meyer"
                    },
                    {
                        "name": "Noel Pascual Presa"
                    }
                ],
                "author_detail": {
                    "name": "Noel Pascual Presa"
                },
                "author": "Noel Pascual Presa",
                "arxiv_comment": "12 pages, Pre-Print SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05138v2",
                "updated": "2025-04-08T03:29:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    29,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T14:43:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    43,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated\n  Learning"
                },
                "summary": "Federated learning (FL) allows edge devices to collaboratively train models\nwithout sharing local data. As FL gains popularity, clients may need to train\nmultiple unrelated FL models, but communication constraints limit their ability\nto train all models simultaneously. While clients could train FL models\nsequentially, opportunistically having FL clients concurrently train different\nmodels -- termed multi-model federated learning (MMFL) -- can reduce the\noverall training time. Prior work uses simple client-to-model assignments that\ndo not optimize the contribution of each client to each model over the course\nof its training. Prior work on single-model FL shows that intelligent client\nselection can greatly accelerate convergence, but na\\\"ive extensions to MMFL\ncan violate heterogeneous resource constraints at both the server and the\nclients. In this work, we develop a novel convergence analysis of MMFL with\narbitrary client sampling methods, theoretically demonstrating the strengths\nand limitations of previous well-established gradient-based methods. Motivated\nby this analysis, we propose MMFL-LVR, a loss-based sampling method that\nminimizes training variance while explicitly respecting communication limits at\nthe server and reducing computational costs at the clients. We extend this to\nMMFL-StaleVR, which incorporates stale updates for improved efficiency and\nstability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead\ndeployment. Experiments show our methods improve average accuracy by up to\n19.1% over random sampling, with only a 5.4% gap from the theoretical optimum\n(full client participation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) allows edge devices to collaboratively train models\nwithout sharing local data. As FL gains popularity, clients may need to train\nmultiple unrelated FL models, but communication constraints limit their ability\nto train all models simultaneously. While clients could train FL models\nsequentially, opportunistically having FL clients concurrently train different\nmodels -- termed multi-model federated learning (MMFL) -- can reduce the\noverall training time. Prior work uses simple client-to-model assignments that\ndo not optimize the contribution of each client to each model over the course\nof its training. Prior work on single-model FL shows that intelligent client\nselection can greatly accelerate convergence, but na\\\"ive extensions to MMFL\ncan violate heterogeneous resource constraints at both the server and the\nclients. In this work, we develop a novel convergence analysis of MMFL with\narbitrary client sampling methods, theoretically demonstrating the strengths\nand limitations of previous well-established gradient-based methods. Motivated\nby this analysis, we propose MMFL-LVR, a loss-based sampling method that\nminimizes training variance while explicitly respecting communication limits at\nthe server and reducing computational costs at the clients. We extend this to\nMMFL-StaleVR, which incorporates stale updates for improved efficiency and\nstability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead\ndeployment. Experiments show our methods improve average accuracy by up to\n19.1% over random sampling, with only a 5.4% gap from the theoretical optimum\n(full client participation)."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zejun Gong"
                    },
                    {
                        "name": "Zekai Li"
                    },
                    {
                        "name": "Marie Siew"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "Rachid El-Azouzi"
                    }
                ],
                "author_detail": {
                    "name": "Rachid El-Azouzi"
                },
                "author": "Rachid El-Azouzi",
                "arxiv_comment": "10 pages, solved a hyperlink problem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05193v3",
                "updated": "2025-04-07T14:42:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    42,
                    1,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-07T16:50:47Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    50,
                    47,
                    0,
                    281,
                    0
                ],
                "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References"
                },
                "summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Tiezheng YU"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05122v1",
                "updated": "2025-04-07T14:26:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    26,
                    49,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:26:49Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    26,
                    49,
                    0,
                    97,
                    0
                ],
                "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech\n  Translation"
                },
                "summary": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance."
                },
                "authors": [
                    {
                        "name": "Xinglin Lyu"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Daimeng Wei"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03197v2",
                "updated": "2025-04-07T14:23:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    23,
                    25,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-04T06:03:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation"
                },
                "summary": "With the rapid advancement of mathematical reasoning capabilities in Large\nLanguage Models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids - such as diagrams,\nmarkings, and highlights - to enhance conceptual clarity. To bridge this gap,\nwe introduce a novel task of visual solution explanation, which requires\ngenerating explanations that incorporate newly introduced visual elements\nessential for understanding (e.g., auxiliary lines, annotations, or geometric\nconstructions). To evaluate model performance on this task, we propose\nMathExplain, a multimodal benchmark consisting of 997 math problems annotated\nwith visual keypoints and corresponding explanatory text that references those\nelements. Our empirical results show that while some closed-source models\ndemonstrate promising capabilities on visual solution-explaining, current\nopen-source general-purpose models perform inconsistently, particularly in\nidentifying relevant visual components and producing coherent keypoint-based\nexplanations. We expect that visual solution-explaining and the MathExplain\ndataset will catalyze further research on multimodal LLMs in education and\nadvance their deployment as effective, explanation-oriented AI tutors. Code and\ndata will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mathematical reasoning capabilities in Large\nLanguage Models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids - such as diagrams,\nmarkings, and highlights - to enhance conceptual clarity. To bridge this gap,\nwe introduce a novel task of visual solution explanation, which requires\ngenerating explanations that incorporate newly introduced visual elements\nessential for understanding (e.g., auxiliary lines, annotations, or geometric\nconstructions). To evaluate model performance on this task, we propose\nMathExplain, a multimodal benchmark consisting of 997 math problems annotated\nwith visual keypoints and corresponding explanatory text that references those\nelements. Our empirical results show that while some closed-source models\ndemonstrate promising capabilities on visual solution-explaining, current\nopen-source general-purpose models perform inconsistently, particularly in\nidentifying relevant visual components and producing coherent keypoint-based\nexplanations. We expect that visual solution-explaining and the MathExplain\ndataset will catalyze further research on multimodal LLMs in education and\nadvance their deployment as effective, explanation-oriented AI tutors. Code and\ndata will be released publicly."
                },
                "authors": [
                    {
                        "name": "Jaewoo Park"
                    },
                    {
                        "name": "Jungyang Park"
                    },
                    {
                        "name": "Dongju Jang"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Byungwoo Yoo"
                    },
                    {
                        "name": "Jaewoo Shin"
                    },
                    {
                        "name": "Seonjoon Park"
                    },
                    {
                        "name": "Taehyeong Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11721v2",
                "updated": "2025-04-07T14:21:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    21,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-17T16:40:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    40,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "The Right Time Matters: Data Arrangement Affects Zero-Shot\n  Generalization in Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Right Time Matters: Data Arrangement Affects Zero-Shot\n  Generalization in Instruction Tuning"
                },
                "summary": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization."
                },
                "authors": [
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Haiwen Hong"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05112v1",
                "updated": "2025-04-07T14:15:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    15,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:15:48Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    15,
                    48,
                    0,
                    97,
                    0
                ],
                "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through\n  Dynamic Frequency-Spatial Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through\n  Dynamic Frequency-Spatial Synergy"
                },
                "summary": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions."
                },
                "authors": [
                    {
                        "name": "Ronghui Zhang"
                    },
                    {
                        "name": "Dakang Lyu"
                    },
                    {
                        "name": "Tengfei Li"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Ujjal Manandhar"
                    },
                    {
                        "name": "Benfei Wang"
                    },
                    {
                        "name": "Junzhou Chen"
                    },
                    {
                        "name": "Bolin Gao"
                    },
                    {
                        "name": "Danwei Wang"
                    },
                    {
                        "name": "Yiqiu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Yiqiu Tan"
                },
                "author": "Yiqiu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05108v1",
                "updated": "2025-04-07T14:14:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:14:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning"
                },
                "summary": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design."
                },
                "authors": [
                    {
                        "name": "Anja Surina"
                    },
                    {
                        "name": "Amin Mansouri"
                    },
                    {
                        "name": "Lars Quaedvlieg"
                    },
                    {
                        "name": "Amal Seddas"
                    },
                    {
                        "name": "Maryna Viazovska"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05104v1",
                "updated": "2025-04-07T14:11:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    11,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:11:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    11,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for\n  Early Warning System Investments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for\n  Early Warning System Investments"
                },
                "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency."
                },
                "authors": [
                    {
                        "name": "Saeid Ario Vaghefi"
                    },
                    {
                        "name": "Aymane Hachcham"
                    },
                    {
                        "name": "Veronica Grasso"
                    },
                    {
                        "name": "Jiska Manicus"
                    },
                    {
                        "name": "Nakiete Msemo"
                    },
                    {
                        "name": "Chiara Colesanti Senni"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20508v2",
                "updated": "2025-04-07T13:54:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    13,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-26T12:49:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Explainable ICD Coding via Entity Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable ICD Coding via Entity Linking"
                },
                "summary": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios."
                },
                "authors": [
                    {
                        "name": "Leonor Barreiros"
                    },
                    {
                        "name": "Isabel Coutinho"
                    },
                    {
                        "name": "Gonçalo M. Correia"
                    },
                    {
                        "name": "Bruno Martins"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Martins"
                },
                "author": "Bruno Martins",
                "arxiv_comment": "Accepted at CL4Health at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05084v1",
                "updated": "2025-04-07T13:54:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    8,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:54:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    54,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot\n  Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot\n  Motion"
                },
                "summary": "Full integration of robots into real-life applications necessitates their\nability to interpret and execute natural language directives from untrained\nusers. Given the inherent variability in human language, equivalent directives\nmay be phrased differently, yet require consistent robot behavior. While Large\nLanguage Models (LLMs) have advanced language understanding, they often falter\nin handling user phrasing variability, rely on predefined commands, and exhibit\nunpredictable outputs. This letter introduces the Directive Language Model\n(DLM), a novel speech-to-trajectory framework that directly maps verbal\ncommands to executable motion trajectories, bypassing predefined phrases. DLM\nutilizes Behavior Cloning (BC) on simulated demonstrations of human-guided\nrobot motion. To enhance generalization, GPT-based semantic augmentation\ngenerates diverse paraphrases of training commands, labeled with the same\nmotion trajectory. DLM further incorporates a diffusion policy-based trajectory\ngeneration for adaptive motion refinement and stochastic sampling. In contrast\nto LLM-based methods, DLM ensures consistent, predictable motion without\nextensive prompt engineering, facilitating real-time robotic guidance. As DLM\nlearns from trajectory data, it is embodiment-agnostic, enabling deployment\nacross diverse robotic platforms. Experimental results demonstrate DLM's\nimproved command generalization, reduced dependence on structured phrasing, and\nachievement of human-like motion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full integration of robots into real-life applications necessitates their\nability to interpret and execute natural language directives from untrained\nusers. Given the inherent variability in human language, equivalent directives\nmay be phrased differently, yet require consistent robot behavior. While Large\nLanguage Models (LLMs) have advanced language understanding, they often falter\nin handling user phrasing variability, rely on predefined commands, and exhibit\nunpredictable outputs. This letter introduces the Directive Language Model\n(DLM), a novel speech-to-trajectory framework that directly maps verbal\ncommands to executable motion trajectories, bypassing predefined phrases. DLM\nutilizes Behavior Cloning (BC) on simulated demonstrations of human-guided\nrobot motion. To enhance generalization, GPT-based semantic augmentation\ngenerates diverse paraphrases of training commands, labeled with the same\nmotion trajectory. DLM further incorporates a diffusion policy-based trajectory\ngeneration for adaptive motion refinement and stochastic sampling. In contrast\nto LLM-based methods, DLM ensures consistent, predictable motion without\nextensive prompt engineering, facilitating real-time robotic guidance. As DLM\nlearns from trajectory data, it is embodiment-agnostic, enabling deployment\nacross diverse robotic platforms. Experimental results demonstrate DLM's\nimproved command generalization, reduced dependence on structured phrasing, and\nachievement of human-like motion."
                },
                "authors": [
                    {
                        "name": "Eran Beeri Bamani"
                    },
                    {
                        "name": "Eden Nissinman"
                    },
                    {
                        "name": "Rotem Atari"
                    },
                    {
                        "name": "Nevo Heimann Saadon"
                    },
                    {
                        "name": "Avishai Sintov"
                    }
                ],
                "author_detail": {
                    "name": "Avishai Sintov"
                },
                "author": "Avishai Sintov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05081v1",
                "updated": "2025-04-07T13:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    51,
                    6,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:51:06Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    51,
                    6,
                    0,
                    97,
                    0
                ],
                "title": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context\n  Learning"
                },
                "summary": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Yixiang Chen"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "30 pages, 12 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05074v1",
                "updated": "2025-04-07T13:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    42,
                    2,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    42,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "On the Performance of an Explainable Language Model on PubMedQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Performance of an Explainable Language Model on PubMedQA"
                },
                "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future."
                },
                "authors": [
                    {
                        "name": "Venkat Srinivasan"
                    },
                    {
                        "name": "Vishaal Jatav"
                    },
                    {
                        "name": "Anushka Chandrababu"
                    },
                    {
                        "name": "Geetika Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Geetika Sharma"
                },
                "author": "Geetika Sharma",
                "arxiv_comment": "Working Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v2",
                "updated": "2025-04-08T08:18:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    18,
                    14,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05050v1",
                "updated": "2025-04-07T13:20:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    20,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:20:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    20,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Jiawei Lian"
                    },
                    {
                        "name": "Jianhong Pan"
                    },
                    {
                        "name": "Lefan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Shaohui Mei"
                    },
                    {
                        "name": "Lap-Pui Chau"
                    }
                ],
                "author_detail": {
                    "name": "Lap-Pui Chau"
                },
                "author": "Lap-Pui Chau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05047v1",
                "updated": "2025-04-07T13:17:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    17,
                    52,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    17,
                    52,
                    0,
                    97,
                    0
                ],
                "title": "Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning"
                },
                "summary": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). While this approach\nimproves reasoning capability, it incurs substantial computational overhead due\nto iterative agent interactions. Furthermore, engaging in debates for queries\nthat do not necessitate collaboration amplifies the risk of error generation.\nTo address these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates the debate\nprocess based on the confidence score of the agent's initial response. For\nqueries where debate is triggered, agents refine their outputs using responses\nfrom participating agents and their confidence scores. Experimental results\ndemonstrate that this mechanism significantly improves efficiency while\nmaintaining or even surpassing the performance of existing multiagent debate\nsystems. We also find that confidence-guided debate mitigates error propagation\nand enhances the selective incorporation of reliable responses. These results\nestablish DOWN as an optimization strategy for efficient and effective\nmultiagent reasoning, facilitating the practical deployment of LLM-based\ncollaboration."
                },
                "authors": [
                    {
                        "name": "Sugyeong Eo"
                    },
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Evelyn Hayoon Zi"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05040v1",
                "updated": "2025-04-07T13:05:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    5,
                    9,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T13:05:09Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    5,
                    9,
                    0,
                    97,
                    0
                ],
                "title": "InstructionBench: An Instructional Video Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructionBench: An Instructional Video Understanding Benchmark"
                },
                "summary": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources."
                },
                "authors": [
                    {
                        "name": "Haiwan Wei"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Wei Ke"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07009v2",
                "updated": "2025-04-07T13:02:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    2,
                    14,
                    0,
                    97,
                    0
                ],
                "published": "2024-07-09T16:24:21Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    16,
                    24,
                    21,
                    1,
                    191,
                    0
                ],
                "title": "Explainable AI for Enhancing Efficiency of DL-based Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI for Enhancing Efficiency of DL-based Channel Estimation"
                },
                "summary": "The support of artificial intelligence (AI) based decision-making is a key\nelement in future 6G networks, where the concept of native AI will be\nintroduced. Moreover, AI is widely employed in different critical applications\nsuch as autonomous driving and medical diagnosis. In such applications, using\nAI as black-box models is risky and challenging. Hence, it is crucial to\nunderstand and trust the decisions taken by these models. Tackling this issue\ncan be achieved by developing explainable AI (XAI) schemes that aim to explain\nthe logic behind the black-box model behavior, and thus, ensure its efficient\nand safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST\nframework that is oriented toward channel estimation in wireless\ncommunications. The core idea of the XAI-CHEST framework is to identify the\nrelevant model inputs by inducing high noise on the irrelevant ones. This\nmanuscript provides the detailed theoretical foundations of the XAI-CHEST\nframework. In particular, we derive the analytical expressions of the XAI-CHEST\nloss functions and the noise threshold fine-tuning optimization problem. Hence\nthe designed XAI-CHEST delivers a smart input feature selection methodology\nthat can further improve the overall performance while optimizing the\narchitecture of the employed model. Simulation results show that the XAI-CHEST\nframework provides valid interpretations, where it offers an improved bit error\nrate performance while reducing the required computational complexity in\ncomparison to the classical DL-based channel estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The support of artificial intelligence (AI) based decision-making is a key\nelement in future 6G networks, where the concept of native AI will be\nintroduced. Moreover, AI is widely employed in different critical applications\nsuch as autonomous driving and medical diagnosis. In such applications, using\nAI as black-box models is risky and challenging. Hence, it is crucial to\nunderstand and trust the decisions taken by these models. Tackling this issue\ncan be achieved by developing explainable AI (XAI) schemes that aim to explain\nthe logic behind the black-box model behavior, and thus, ensure its efficient\nand safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST\nframework that is oriented toward channel estimation in wireless\ncommunications. The core idea of the XAI-CHEST framework is to identify the\nrelevant model inputs by inducing high noise on the irrelevant ones. This\nmanuscript provides the detailed theoretical foundations of the XAI-CHEST\nframework. In particular, we derive the analytical expressions of the XAI-CHEST\nloss functions and the noise threshold fine-tuning optimization problem. Hence\nthe designed XAI-CHEST delivers a smart input feature selection methodology\nthat can further improve the overall performance while optimizing the\narchitecture of the employed model. Simulation results show that the XAI-CHEST\nframework provides valid interpretations, where it offers an improved bit error\nrate performance while reducing the required computational complexity in\ncomparison to the classical DL-based channel estimation."
                },
                "authors": [
                    {
                        "name": "Abdul Karim Gizzini"
                    },
                    {
                        "name": "Yahia Medjahdi"
                    },
                    {
                        "name": "Ali J. Ghandour"
                    },
                    {
                        "name": "Laurent Clavier"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Clavier"
                },
                "author": "Laurent Clavier",
                "arxiv_comment": "This paper has been submitted to the IEEE Transactions on Machine\n  Learning in Communications and Networking on 19 March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05035v1",
                "updated": "2025-04-07T12:56:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    56,
                    51,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:56:51Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    56,
                    51,
                    0,
                    97,
                    0
                ],
                "title": "Probabilistic Position-Aided Beam Selection for mmWave MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Position-Aided Beam Selection for mmWave MIMO Systems"
                },
                "summary": "Millimeter-wave (mmWave) MIMO systems rely on highly directional beamforming\nto overcome severe path loss and ensure robust communication links. However,\nselecting the optimal beam pair efficiently remains a challenge due to the\nlarge search space and the overhead of conventional methods. This paper\nproposes a probabilistic position-aided beam selection approach that exploits\nthe statistical dependence between user equipment (UE) positions and optimal\nbeam indices. We model the underlying joint probability mass function (PMF) of\nthe positions and the beam indices as a low-rank tensor and estimate its\nparameters from training data using Bayesian inference. The estimated model is\nthen used to predict the best (or a list of the top) beam pair indices for new\nUE positions. The proposed method is evaluated using data generated from a\nstate-of-the-art ray tracing simulator and compared with neural network-based\nand fingerprinting approaches. The results show that our approach achieves a\nhigh data rate with fewer training samples and a significantly reduced beam\nsearch space. These advantages render it a promising solution for practical\nmmWave MIMO deployments, reducing the beam search overhead while maintaining a\nreliable connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mmWave) MIMO systems rely on highly directional beamforming\nto overcome severe path loss and ensure robust communication links. However,\nselecting the optimal beam pair efficiently remains a challenge due to the\nlarge search space and the overhead of conventional methods. This paper\nproposes a probabilistic position-aided beam selection approach that exploits\nthe statistical dependence between user equipment (UE) positions and optimal\nbeam indices. We model the underlying joint probability mass function (PMF) of\nthe positions and the beam indices as a low-rank tensor and estimate its\nparameters from training data using Bayesian inference. The estimated model is\nthen used to predict the best (or a list of the top) beam pair indices for new\nUE positions. The proposed method is evaluated using data generated from a\nstate-of-the-art ray tracing simulator and compared with neural network-based\nand fingerprinting approaches. The results show that our approach achieves a\nhigh data rate with fewer training samples and a significantly reduced beam\nsearch space. These advantages render it a promising solution for practical\nmmWave MIMO deployments, reducing the beam search overhead while maintaining a\nreliable connectivity."
                },
                "authors": [
                    {
                        "name": "Joseph K. Chege"
                    },
                    {
                        "name": "Arie Yeredor"
                    },
                    {
                        "name": "Martin Haardt"
                    }
                ],
                "author_detail": {
                    "name": "Martin Haardt"
                },
                "author": "Martin Haardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05019v1",
                "updated": "2025-04-07T12:43:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    43,
                    5,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:43:05Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    43,
                    5,
                    0,
                    97,
                    0
                ],
                "title": "Mixture-of-Personas Language Models for Population Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Personas Language Models for Population Simulation"
                },
                "summary": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics."
                },
                "authors": [
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Hieu Trung Nguyen"
                    },
                    {
                        "name": "Shantanu Kumar"
                    },
                    {
                        "name": "Julian Theodore"
                    },
                    {
                        "name": "Weikang Qiu"
                    },
                    {
                        "name": "Viet Anh Nguyen"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05017v1",
                "updated": "2025-04-07T12:41:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    41,
                    8,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:41:08Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    41,
                    8,
                    0,
                    97,
                    0
                ],
                "title": "Joint BS Deployment and Power Optimization for Minimum EMF Exposure with\n  RL in Real-World Based Urban Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint BS Deployment and Power Optimization for Minimum EMF Exposure with\n  RL in Real-World Based Urban Scenario"
                },
                "summary": "Conventional base station (BS) deployments typically prioritize coverage,\nquality of service (QoS), or cost reduction, often overlooking electromagnetic\nfield (EMF) exposure. Whereas EMF exposure triggers significant public concern\ndue to its potential health implications, making it crucial to address when\ndeploying BS in densely populated areas. To this end, this paper addresses\nminimizing average EMF exposure while maintaining coverage in a 3D urban\nscenario by jointly optimizing BS deployment and power. To address this,\nfirstly, accurate EMF prediction is essential, as traditional empirical models\nlack the required accuracy, necessitating a deterministic channel model. A\nnovel least-time shoot-and-bounce ray (SBR) ray-launching (RL) algorithm is\ntherefore developed to overcome several limitations of current simulators and\nis validated with real-world measurements. Secondly, to further reduce\ncomputational complexity, unlike using a fixed grid size to discretize the\ntarget area, the adaptive grid refinement (AGR) algorithm is designed with a\nflexible grid to predict the overall EMF exposure. Finally, based on the EMF\nexposure predictions, the Nelder-Mead (NM) method is used in the joint\noptimization, and urban user equipment (UE) distributions are incorporated to\nbetter reflect real-world conditions. When evaluating the benefits of the whole\nprocess, the results are compared against using empirical channel models,\nrevealing notable differences and underestimation of EMF exposure that\nhighlight the importance of considering real-world scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional base station (BS) deployments typically prioritize coverage,\nquality of service (QoS), or cost reduction, often overlooking electromagnetic\nfield (EMF) exposure. Whereas EMF exposure triggers significant public concern\ndue to its potential health implications, making it crucial to address when\ndeploying BS in densely populated areas. To this end, this paper addresses\nminimizing average EMF exposure while maintaining coverage in a 3D urban\nscenario by jointly optimizing BS deployment and power. To address this,\nfirstly, accurate EMF prediction is essential, as traditional empirical models\nlack the required accuracy, necessitating a deterministic channel model. A\nnovel least-time shoot-and-bounce ray (SBR) ray-launching (RL) algorithm is\ntherefore developed to overcome several limitations of current simulators and\nis validated with real-world measurements. Secondly, to further reduce\ncomputational complexity, unlike using a fixed grid size to discretize the\ntarget area, the adaptive grid refinement (AGR) algorithm is designed with a\nflexible grid to predict the overall EMF exposure. Finally, based on the EMF\nexposure predictions, the Nelder-Mead (NM) method is used in the joint\noptimization, and urban user equipment (UE) distributions are incorporated to\nbetter reflect real-world conditions. When evaluating the benefits of the whole\nprocess, the results are compared against using empirical channel models,\nrevealing notable differences and underestimation of EMF exposure that\nhighlight the importance of considering real-world scenario."
                },
                "authors": [
                    {
                        "name": "Xueyun Long"
                    },
                    {
                        "name": "Yueheng Li"
                    },
                    {
                        "name": "Mario Pauli"
                    },
                    {
                        "name": "Benjamin Nuss"
                    },
                    {
                        "name": "Thomas Zwick"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Zwick"
                },
                "author": "Thomas Zwick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05008v1",
                "updated": "2025-04-07T12:35:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:35:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Surveying Professional Writers on AI: Limitations, Expectations, and\n  Fears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveying Professional Writers on AI: Limitations, Expectations, and\n  Fears"
                },
                "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergey Tilga"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05006v1",
                "updated": "2025-04-07T12:32:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    32,
                    14,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:32:14Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    32,
                    14,
                    0,
                    97,
                    0
                ],
                "title": "Enhancing Smart Contract Vulnerability Detection in DApps Leveraging\n  Fine-Tuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Smart Contract Vulnerability Detection in DApps Leveraging\n  Fine-Tuned LLM"
                },
                "summary": "Decentralized applications (DApps) face significant security risks due to\nvulnerabilities in smart contracts, with traditional detection methods\nstruggling to address emerging and machine-unauditable flaws. This paper\nproposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to\nenhance smart contract vulnerability detection. We introduce a comprehensive\ndataset of 215 real-world DApp projects (4,998 contracts), including\nhard-to-detect logical errors like token price manipulation, addressing the\nlimitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B\nand Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation\n(LoRA), our method achieves superior performance, attaining an F1-score of 0.83\nwith FFT and data augmentation via Random Over Sampling (ROS). Comparative\nexperiments demonstrate significant improvements over prompt-based LLMs and\nstate-of-the-art tools. Notably, the approach excels in detecting\nnon-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall\nfor price manipulation flaws. The results underscore the effectiveness of\ndomain-specific LLM fine-tuning and data augmentation in addressing real-world\nDApp security challenges, offering a robust solution for blockchain ecosystem\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized applications (DApps) face significant security risks due to\nvulnerabilities in smart contracts, with traditional detection methods\nstruggling to address emerging and machine-unauditable flaws. This paper\nproposes a novel approach leveraging fine-tuned Large Language Models (LLMs) to\nenhance smart contract vulnerability detection. We introduce a comprehensive\ndataset of 215 real-world DApp projects (4,998 contracts), including\nhard-to-detect logical errors like token price manipulation, addressing the\nlimitations of existing simplified benchmarks. By fine-tuning LLMs (Llama3-8B\nand Qwen2-7B) with Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation\n(LoRA), our method achieves superior performance, attaining an F1-score of 0.83\nwith FFT and data augmentation via Random Over Sampling (ROS). Comparative\nexperiments demonstrate significant improvements over prompt-based LLMs and\nstate-of-the-art tools. Notably, the approach excels in detecting\nnon-machine-auditable vulnerabilities, achieving 0.97 precision and 0.68 recall\nfor price manipulation flaws. The results underscore the effectiveness of\ndomain-specific LLM fine-tuning and data augmentation in addressing real-world\nDApp security challenges, offering a robust solution for blockchain ecosystem\nprotection."
                },
                "authors": [
                    {
                        "name": "Jiuyang Bu"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Xiaoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Li"
                },
                "author": "Xiaoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04994v1",
                "updated": "2025-04-07T12:23:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    23,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:23:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    23,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind\n  Value-Oriented Behaviors in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind\n  Value-Oriented Behaviors in LLMs"
                },
                "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available."
                },
                "authors": [
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Xiaoyang Gu"
                    },
                    {
                        "name": "Letao Han"
                    }
                ],
                "author_detail": {
                    "name": "Letao Han"
                },
                "author": "Letao Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17154v2",
                "updated": "2025-04-07T12:16:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    16,
                    27,
                    0,
                    97,
                    0
                ],
                "published": "2024-03-25T20:09:46Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    20,
                    9,
                    46,
                    0,
                    85,
                    0
                ],
                "title": "On the Impact of Black-box Deployment Strategies for Edge AI on Latency\n  and Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Black-box Deployment Strategies for Edge AI on Latency\n  and Model Performance"
                },
                "summary": "Deciding what combination of operators to use across the Edge AI tiers to\nachieve specific latency and model performance requirements is an open question\nfor MLOps engineers. This study aims to empirically assess the accuracy vs\ninference time trade-off of different black-box Edge AI deployment strategies,\ni.e., combinations of deployment operators and deployment tiers. In this paper,\nwe conduct inference experiments involving 3 deployment operators (i.e.,\nPartitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile,\nEdge, Cloud) and their combinations on four widely used Computer-Vision models\nto investigate the optimal strategies from the point of view of MLOps\ndevelopers. Our findings suggest that Edge deployment using the hybrid\nQuantization + Early Exit operator could be preferred over non-hybrid operators\n(Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency\nis a concern at medium accuracy loss. However, when minimizing accuracy loss is\na concern, MLOps engineers should prefer using only a Quantization operator on\nedge at a latency reduction or increase, respectively over the Early\nExit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge)\noperators. In scenarios constrained by Mobile CPU/RAM resources, a preference\nfor Partitioning across mobile and edge tiers is observed over mobile\ndeployment. For models with smaller input data samples (such as FCN), a\nnetwork-constrained cloud deployment can also be a better alternative than\nMobile/Edge deployment and Partitioning strategies. For models with large input\ndata samples (ResNet, ResNext, DUC), an edge tier having higher\nnetwork/computational capabilities than Cloud/Mobile can be a more viable\noption than Partitioning and Mobile/Cloud deployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciding what combination of operators to use across the Edge AI tiers to\nachieve specific latency and model performance requirements is an open question\nfor MLOps engineers. This study aims to empirically assess the accuracy vs\ninference time trade-off of different black-box Edge AI deployment strategies,\ni.e., combinations of deployment operators and deployment tiers. In this paper,\nwe conduct inference experiments involving 3 deployment operators (i.e.,\nPartitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile,\nEdge, Cloud) and their combinations on four widely used Computer-Vision models\nto investigate the optimal strategies from the point of view of MLOps\ndevelopers. Our findings suggest that Edge deployment using the hybrid\nQuantization + Early Exit operator could be preferred over non-hybrid operators\n(Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency\nis a concern at medium accuracy loss. However, when minimizing accuracy loss is\na concern, MLOps engineers should prefer using only a Quantization operator on\nedge at a latency reduction or increase, respectively over the Early\nExit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge)\noperators. In scenarios constrained by Mobile CPU/RAM resources, a preference\nfor Partitioning across mobile and edge tiers is observed over mobile\ndeployment. For models with smaller input data samples (such as FCN), a\nnetwork-constrained cloud deployment can also be a better alternative than\nMobile/Edge deployment and Partitioning strategies. For models with large input\ndata samples (ResNet, ResNext, DUC), an edge tier having higher\nnetwork/computational capabilities than Cloud/Mobile can be a more viable\noption than Partitioning and Mobile/Cloud deployment strategies."
                },
                "authors": [
                    {
                        "name": "Jaskirat Singh"
                    },
                    {
                        "name": "Emad Fallahzadeh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v3",
                "updated": "2025-04-07T12:15:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    15,
                    49,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We open-source\nour code (https://github.com/openai/preparedness) to facilitate future research\nin understanding the AI engineering capabilities of AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We open-source\nour code (https://github.com/openai/preparedness) to facilitate future research\nin understanding the AI engineering capabilities of AI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04976v1",
                "updated": "2025-04-07T12:05:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    5,
                    16,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:05:16Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    5,
                    16,
                    0,
                    97,
                    0
                ],
                "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language\n  Models"
                },
                "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study."
                },
                "authors": [
                    {
                        "name": "Carlos Peláez-González"
                    },
                    {
                        "name": "Andrés Herrera-Poyatos"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Virilo Tejedor"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04974v1",
                "updated": "2025-04-07T12:01:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    59,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T12:01:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Visual Text Grounding of Multimodal Large Language Model"
                },
                "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Yufan Zhou"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Wanrong Zhu"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Tong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Tong Sun"
                },
                "author": "Tong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v3",
                "updated": "2025-04-07T12:01:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    1,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unified Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unified Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports various BI tasks for different data roles\nin data preparation, analysis, and visualization by seamlessly combining LLM\nassistance with user customization within a single environment. To achieve this\nunification, we design a domain knowledge incorporation module tailored for\nenterprise-specific BI tasks, an inter-agent communication mechanism to\nfacilitate information sharing across the BI workflow, and a cell-based context\nmanagement strategy to enhance context utilization efficiency in BI notebooks.\nExtensive experiments demonstrate that DataLab achieves state-of-the-art\nperformance on various BI tasks across popular research benchmarks. Moreover,\nDataLab maintains high effectiveness and efficiency on real-world datasets from\nTencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction\nin token cost on enterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports various BI tasks for different data roles\nin data preparation, analysis, and visualization by seamlessly combining LLM\nassistance with user customization within a single environment. To achieve this\nunification, we design a domain knowledge incorporation module tailored for\nenterprise-specific BI tasks, an inter-agent communication mechanism to\nfacilitate information sharing across the BI workflow, and a cell-based context\nmanagement strategy to enhance context utilization efficiency in BI notebooks.\nExtensive experiments demonstrate that DataLab achieves state-of-the-art\nperformance on various BI tasks across popular research benchmarks. Moreover,\nDataLab maintains high effectiveness and efficiency on real-world datasets from\nTencent, achieving up to a 58.58% increase in accuracy and a 61.65% reduction\nin token cost on enterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "arxiv_comment": "Accepted to ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04968v1",
                "updated": "2025-04-07T11:54:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    54,
                    11,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:54:11Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    54,
                    11,
                    0,
                    97,
                    0
                ],
                "title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for\n  Family Storytelling and Emotional Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for\n  Family Storytelling and Emotional Reflection"
                },
                "summary": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the art project The Dream Within Huang Long Cave, an\nAI-driven interactive and immersive narrative experience. The project offers\nnew insights into AI technology, artistic practice, and psychoanalysis.\nInspired by actual geographical landscapes and familial archetypes, the work\ncombines psychoanalytic theory and computational technology, providing an\nartistic response to the concept of the non-existence of the Big Other. The\nnarrative is driven by a combination of a large language model (LLM) and a\nrealistic digital character, forming a virtual agent named YELL. Through\ndialogue and exploration within a cave automatic virtual environment (CAVE),\nthe audience is invited to unravel the language puzzles presented by YELL and\nhelp him overcome his life challenges. YELL is a fictional embodiment of the\nBig Other, modeled after the artist's real father. Through a cross-temporal\ninteraction with this digital father, the project seeks to deconstruct complex\nfamilial relationships. By demonstrating the non-existence of the Big Other, we\naim to underscore the authenticity of interpersonal emotions, positioning art\nas a bridge for emotional connection and understanding within family dynamics."
                },
                "authors": [
                    {
                        "name": "Jiayang Huang"
                    },
                    {
                        "name": "Lingjie Li"
                    },
                    {
                        "name": "Kang Zhang"
                    },
                    {
                        "name": "David Yip"
                    }
                ],
                "author_detail": {
                    "name": "David Yip"
                },
                "author": "David Yip",
                "arxiv_comment": "8 pages,8 figures, International Symposium on Electronic/Emerging Art\n  (ISEA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04953v1",
                "updated": "2025-04-07T11:37:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    37,
                    26,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:37:26Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    37,
                    26,
                    0,
                    97,
                    0
                ],
                "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-Prometheus: A Suite of Open Multilingual LLM Judges"
                },
                "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code."
                },
                "authors": [
                    {
                        "name": "José Pombal"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04950v1",
                "updated": "2025-04-07T11:34:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    34,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:34:48Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    34,
                    48,
                    0,
                    97,
                    0
                ],
                "title": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward\n  Modeling and Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward\n  Modeling and Policy Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important\nparadigm for aligning large language models (LLMs) with human preferences\nduring post-training. This framework typically involves two stages: first,\ntraining a reward model on human preference data, followed by optimizing the\nlanguage model using reinforcement learning algorithms. However, current RLHF\napproaches may constrained by two limitations. First, existing RLHF frameworks\noften rely on Bradley-Terry models to assign scalar rewards based on pairwise\ncomparisons of individual responses. However, this approach imposes significant\nchallenges on reward model (RM), as the inherent variability in prompt-response\npairs across different contexts demands robust calibration capabilities from\nthe RM. Second, reward models are typically initialized from generative\nfoundation models, such as pre-trained or supervised fine-tuned models, despite\nthe fact that reward models perform discriminative tasks, creating a mismatch.\nThis paper introduces Pairwise-RL, a RLHF framework that addresses these\nchallenges through a combination of generative reward modeling and a pairwise\nproximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model\ntraining and its application during reinforcement learning within a consistent\npairwise paradigm, leveraging generative modeling techniques to enhance reward\nmodel performance and score calibration. Experimental evaluations demonstrate\nthat Pairwise-RL outperforms traditional RLHF frameworks across both internal\nevaluation datasets and standard public benchmarks, underscoring its\neffectiveness in improving alignment and model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important\nparadigm for aligning large language models (LLMs) with human preferences\nduring post-training. This framework typically involves two stages: first,\ntraining a reward model on human preference data, followed by optimizing the\nlanguage model using reinforcement learning algorithms. However, current RLHF\napproaches may constrained by two limitations. First, existing RLHF frameworks\noften rely on Bradley-Terry models to assign scalar rewards based on pairwise\ncomparisons of individual responses. However, this approach imposes significant\nchallenges on reward model (RM), as the inherent variability in prompt-response\npairs across different contexts demands robust calibration capabilities from\nthe RM. Second, reward models are typically initialized from generative\nfoundation models, such as pre-trained or supervised fine-tuned models, despite\nthe fact that reward models perform discriminative tasks, creating a mismatch.\nThis paper introduces Pairwise-RL, a RLHF framework that addresses these\nchallenges through a combination of generative reward modeling and a pairwise\nproximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model\ntraining and its application during reinforcement learning within a consistent\npairwise paradigm, leveraging generative modeling techniques to enhance reward\nmodel performance and score calibration. Experimental evaluations demonstrate\nthat Pairwise-RL outperforms traditional RLHF frameworks across both internal\nevaluation datasets and standard public benchmarks, underscoring its\neffectiveness in improving alignment and model behavior."
                },
                "authors": [
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Xiaochen Zuo"
                    },
                    {
                        "name": "Chao Xin"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "arxiv_comment": "11oages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04945v1",
                "updated": "2025-04-07T11:31:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    31,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:31:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    31,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal\n  Reasoning in the Multi-state Bar Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal\n  Reasoning in the Multi-state Bar Exam"
                },
                "summary": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs."
                },
                "authors": [
                    {
                        "name": "Rean Fernandes"
                    },
                    {
                        "name": "André Biedenkapp"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Noor Awad"
                    }
                ],
                "author_detail": {
                    "name": "Noor Awad"
                },
                "author": "Noor Awad",
                "arxiv_comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v1",
                "updated": "2025-04-07T11:30:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17415v2",
                "updated": "2025-04-07T11:20:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    20,
                    37,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-23T09:26:38Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    9,
                    26,
                    38,
                    0,
                    358,
                    0
                ],
                "title": "VidCtx: Context-aware Video Question Answering with Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCtx: Context-aware Video Question Answering with Image Models"
                },
                "summary": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR. Our code is\navailable at https://github.com/IDT-ITI/VidCtx.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR. Our code is\navailable at https://github.com/IDT-ITI/VidCtx."
                },
                "authors": [
                    {
                        "name": "Andreas Goulas"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted in IEEE ICME 2025. This is the authors' accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09440v2",
                "updated": "2025-04-07T11:03:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    3,
                    13,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-12T16:56:01Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    56,
                    1,
                    3,
                    347,
                    0
                ],
                "title": "Learning to Adapt through Bio-Inspired Gait Strategies for Versatile\n  Quadruped Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Adapt through Bio-Inspired Gait Strategies for Versatile\n  Quadruped Locomotion"
                },
                "summary": "Deep reinforcement learning (DRL) has revolutionised quadruped robot\nlocomotion, but existing control frameworks struggle to generalise beyond their\ntraining-induced observational scope, resulting in limited adaptability and\ngait proficiency. In contrast, animals achieve exceptional adaptability through\ngait transition strategies, diverse gait utilisation, and seamless adjustment\nto immediate environmental demands. Inspired by these capabilities, we present\na novel DRL framework that incorporates key attributes of animal locomotion:\ngait transition strategies, pseudo gait procedural memory, and adaptive motion\nadjustments. This approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment on complex\nterrains and recovery from critically unstable states. Our findings offer\nvaluable insights into the biomechanics of animal locomotion, paving the way\nfor robust, adaptable robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning (DRL) has revolutionised quadruped robot\nlocomotion, but existing control frameworks struggle to generalise beyond their\ntraining-induced observational scope, resulting in limited adaptability and\ngait proficiency. In contrast, animals achieve exceptional adaptability through\ngait transition strategies, diverse gait utilisation, and seamless adjustment\nto immediate environmental demands. Inspired by these capabilities, we present\na novel DRL framework that incorporates key attributes of animal locomotion:\ngait transition strategies, pseudo gait procedural memory, and adaptive motion\nadjustments. This approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment on complex\nterrains and recovery from critically unstable states. Our findings offer\nvaluable insights into the biomechanics of animal locomotion, paving the way\nfor robust, adaptable robotic systems."
                },
                "authors": [
                    {
                        "name": "Joseph Humphreys"
                    },
                    {
                        "name": "Chengxu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengxu Zhou"
                },
                "author": "Chengxu Zhou",
                "arxiv_comment": "19 pages, 8 figures, journal paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18666v2",
                "updated": "2025-04-07T10:57:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    57,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-24T13:31:48Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    31,
                    48,
                    0,
                    83,
                    0
                ],
                "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM\n  Agents"
                },
                "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19004v3",
                "updated": "2025-04-07T10:57:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    57,
                    40,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-24T14:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    41,
                    16,
                    0,
                    83,
                    0
                ],
                "title": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts"
                },
                "summary": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap."
                },
                "authors": [
                    {
                        "name": "Simon Goorney"
                    },
                    {
                        "name": "Eleni Karydi"
                    },
                    {
                        "name": "Borja Muñoz"
                    },
                    {
                        "name": "Otto Santesson"
                    },
                    {
                        "name": "Zeki Can Seskir"
                    },
                    {
                        "name": "Ana Alina Tudoran"
                    },
                    {
                        "name": "Jacob Sherson"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Sherson"
                },
                "author": "Jacob Sherson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04915v1",
                "updated": "2025-04-07T10:52:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    52,
                    22,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:52:22Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    52,
                    22,
                    0,
                    97,
                    0
                ],
                "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question\n  Answering via White-Box and Black-Box LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question\n  Answering via White-Box and Black-Box LLM Collaboration"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/."
                },
                "authors": [
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03160v2",
                "updated": "2025-04-07T10:45:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    45,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-04T04:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments"
                },
                "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04908v1",
                "updated": "2025-04-07T10:36:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    36,
                    26,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:36:26Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    36,
                    26,
                    0,
                    97,
                    0
                ],
                "title": "Cloud-Fog Automation: The New Paradigm towards Autonomous Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Fog Automation: The New Paradigm towards Autonomous Industrial\n  Cyber-Physical Systems"
                },
                "summary": "Autonomous Industrial Cyber-Physical Systems (ICPS) represent a future vision\nwhere industrial systems achieve full autonomy, integrating physical processes\nseamlessly with communication, computing and control technologies while\nholistically embedding intelligence. Cloud-Fog Automation is a new digitalized\nindustrial automation reference architecture that has been recently proposed.\nThis architecture is a fundamental paradigm shift from the traditional\nInternational Society of Automation (ISA)-95 model to accelerate the\nconvergence and synergy of communication, computing, and control towards a\nfully autonomous ICPS. With the deployment of new wireless technologies to\nenable almost-deterministic ultra-reliable low-latency communications, a joint\ndesign of optimal control and computing has become increasingly important in\nmodern ICPS. It is also imperative that system-wide cyber-physical security are\ncritically enforced. Despite recent advancements in the field, there are still\nsignificant research gaps and open technical challenges. Therefore, a\ndeliberate rethink in co-designing and synergizing communications, computing,\nand control (which we term \"3C co-design\") is required. In this paper, we\nposition Cloud-Fog Automation with 3C co-design as the new paradigm to realize\nthe vision of autonomous ICPS. We articulate the state-of-the-art and future\ndirections in the field, and specifically discuss how goal-oriented\ncommunication, virtualization-empowered computing, and Quality of Service\n(QoS)-aware control can drive Cloud-Fog Automation towards a fully autonomous\nICPS, while accounting for system-wide cyber-physical security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Industrial Cyber-Physical Systems (ICPS) represent a future vision\nwhere industrial systems achieve full autonomy, integrating physical processes\nseamlessly with communication, computing and control technologies while\nholistically embedding intelligence. Cloud-Fog Automation is a new digitalized\nindustrial automation reference architecture that has been recently proposed.\nThis architecture is a fundamental paradigm shift from the traditional\nInternational Society of Automation (ISA)-95 model to accelerate the\nconvergence and synergy of communication, computing, and control towards a\nfully autonomous ICPS. With the deployment of new wireless technologies to\nenable almost-deterministic ultra-reliable low-latency communications, a joint\ndesign of optimal control and computing has become increasingly important in\nmodern ICPS. It is also imperative that system-wide cyber-physical security are\ncritically enforced. Despite recent advancements in the field, there are still\nsignificant research gaps and open technical challenges. Therefore, a\ndeliberate rethink in co-designing and synergizing communications, computing,\nand control (which we term \"3C co-design\") is required. In this paper, we\nposition Cloud-Fog Automation with 3C co-design as the new paradigm to realize\nthe vision of autonomous ICPS. We articulate the state-of-the-art and future\ndirections in the field, and specifically discuss how goal-oriented\ncommunication, virtualization-empowered computing, and Quality of Service\n(QoS)-aware control can drive Cloud-Fog Automation towards a fully autonomous\nICPS, while accounting for system-wide cyber-physical security."
                },
                "authors": [
                    {
                        "name": "Jiong Jin"
                    },
                    {
                        "name": "Zhibo Pang"
                    },
                    {
                        "name": "Jonathan Kua"
                    },
                    {
                        "name": "Quanyan Zhu"
                    },
                    {
                        "name": "Karl H. Johansson"
                    },
                    {
                        "name": "Nikolaj Marchenko"
                    },
                    {
                        "name": "Dave Cavalcanti"
                    }
                ],
                "author_detail": {
                    "name": "Dave Cavalcanti"
                },
                "author": "Dave Cavalcanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04907v1",
                "updated": "2025-04-07T10:32:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:32:42Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Bench: Human-Aligned Video Generation Benchmark"
                },
                "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."
                },
                "authors": [
                    {
                        "name": "Hui Han"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yiwen Yuan"
                    },
                    {
                        "name": "Yuling Wu"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Youhua Li"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Li-jia Li"
                    },
                    {
                        "name": "Yongxin Ni"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Ni"
                },
                "author": "Yongxin Ni",
                "arxiv_comment": "Accepted by CVPR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04893v1",
                "updated": "2025-04-07T10:01:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T10:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models"
                },
                "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
                },
                "authors": [
                    {
                        "name": "Justus Westerhoff"
                    },
                    {
                        "name": "Erblina Purellku"
                    },
                    {
                        "name": "Jakob Hackstein"
                    },
                    {
                        "name": "Leo Pinetzki"
                    },
                    {
                        "name": "Lorenz Hufe"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hufe"
                },
                "author": "Lorenz Hufe",
                "arxiv_comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04891v1",
                "updated": "2025-04-07T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    58,
                    19,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    58,
                    19,
                    0,
                    97,
                    0
                ],
                "title": "Leveraging Large Language Models for Cost-Effective, Multilingual\n  Depression Detection and Severity Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Cost-Effective, Multilingual\n  Depression Detection and Severity Assessment"
                },
                "summary": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability."
                },
                "authors": [
                    {
                        "name": "Longdi Xian"
                    },
                    {
                        "name": "Jianzhang Ni"
                    },
                    {
                        "name": "Mingzhu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhu Wang"
                },
                "author": "Mingzhu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04877v1",
                "updated": "2025-04-07T09:41:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:41:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "SoK: LLM-based Log Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: LLM-based Log Parsing"
                },
                "summary": "Log data, generated by software systems, provides crucial insights for tasks\nlike monitoring, root cause analysis, and anomaly detection. Due to the vast\nvolume of logs, automated log parsing is essential to transform semi-structured\nlog messages into structured representations. Traditional log parsing\ntechniques often require manual configurations, such as defining log formats or\nlabeling data, which limits scalability and usability. Recent advances in large\nlanguage models (LLMs) have introduced the new research field of LLM-based log\nparsing, offering potential improvements in automation and adaptability.\nDespite promising results, there is no structured overview of these approaches\nsince this is a relatively new research field with the earliest advances\npublished in late 2023. This paper systematically reviews 29 LLM-based log\nparsing methods, comparing their capabilities, limitations, and reliance on\nmanual effort. We analyze the learning and prompt-engineering paradigms\nemployed, efficiency- and effectiveness-enhancing techniques, and the role of\nLLMs in the parsing process. We aggregate the results of the survey in a large\ntable comprising the characterizing features of LLM-based log parsing\napproaches and derive the general process of LLM-based log parsing,\nincorporating all reviewed approaches in a single flow chart. Additionally, we\nbenchmark seven open-source LLM-based log parsers on public datasets and\ncritically assess their reproducibility. Our findings summarize the advances of\nthis new research field and provide insights for researchers and practitioners\nseeking efficient and user-friendly log parsing solutions, with all code and\nresults made publicly available for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log data, generated by software systems, provides crucial insights for tasks\nlike monitoring, root cause analysis, and anomaly detection. Due to the vast\nvolume of logs, automated log parsing is essential to transform semi-structured\nlog messages into structured representations. Traditional log parsing\ntechniques often require manual configurations, such as defining log formats or\nlabeling data, which limits scalability and usability. Recent advances in large\nlanguage models (LLMs) have introduced the new research field of LLM-based log\nparsing, offering potential improvements in automation and adaptability.\nDespite promising results, there is no structured overview of these approaches\nsince this is a relatively new research field with the earliest advances\npublished in late 2023. This paper systematically reviews 29 LLM-based log\nparsing methods, comparing their capabilities, limitations, and reliance on\nmanual effort. We analyze the learning and prompt-engineering paradigms\nemployed, efficiency- and effectiveness-enhancing techniques, and the role of\nLLMs in the parsing process. We aggregate the results of the survey in a large\ntable comprising the characterizing features of LLM-based log parsing\napproaches and derive the general process of LLM-based log parsing,\nincorporating all reviewed approaches in a single flow chart. Additionally, we\nbenchmark seven open-source LLM-based log parsers on public datasets and\ncritically assess their reproducibility. Our findings summarize the advances of\nthis new research field and provide insights for researchers and practitioners\nseeking efficient and user-friendly log parsing solutions, with all code and\nresults made publicly available for transparency."
                },
                "authors": [
                    {
                        "name": "Viktor Beck"
                    },
                    {
                        "name": "Max Landauer"
                    },
                    {
                        "name": "Markus Wurzenberger"
                    },
                    {
                        "name": "Florian Skopik"
                    },
                    {
                        "name": "Andreas Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rauber"
                },
                "author": "Andreas Rauber",
                "arxiv_comment": "34 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04872v1",
                "updated": "2025-04-07T09:27:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    27,
                    37,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:27:37Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    27,
                    37,
                    0,
                    97,
                    0
                ],
                "title": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents"
                },
                "summary": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction-tailored to highly specific participant groups-to then be tested\nin subsequent studies with human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction-tailored to highly specific participant groups-to then be tested\nin subsequent studies with human participants."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Elena Wurth"
                    },
                    {
                        "name": "Markus Strohmaier"
                    },
                    {
                        "name": "Jutta Mata"
                    }
                ],
                "author_detail": {
                    "name": "Jutta Mata"
                },
                "author": "Jutta Mata",
                "arxiv_comment": "Code available at https://github.com/dess-mannheim/MeatlessAgents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04858v1",
                "updated": "2025-04-07T09:14:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    14,
                    47,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:14:47Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    14,
                    47,
                    0,
                    97,
                    0
                ],
                "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG"
                },
                "summary": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks."
                },
                "authors": [
                    {
                        "name": "Roie Kazoom"
                    },
                    {
                        "name": "Raz Lapid"
                    },
                    {
                        "name": "Moshe Sipper"
                    },
                    {
                        "name": "Ofer Hadar"
                    }
                ],
                "author_detail": {
                    "name": "Ofer Hadar"
                },
                "author": "Ofer Hadar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04855v1",
                "updated": "2025-04-07T09:12:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    12,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T09:12:00Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    12,
                    0,
                    0,
                    97,
                    0
                ],
                "title": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents"
                },
                "summary": "Detecting biases in structured data is a complex and time-consuming task.\nExisting automated techniques are limited in diversity of data types and\nheavily reliant on human case-by-case handling, resulting in a lack of\ngeneralizability. Currently, large language model (LLM)-based agents have made\nsignificant progress in data science, but their ability to detect data biases\nis still insufficiently explored. To address this gap, we introduce the first\nend-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for\nautomatic bias detection in structured data based on specific user\nrequirements. It first develops a multi-stage plan to analyze user-specified\nbias detection tasks and then implements it with a diverse and well-suited set\nof tools. It delivers detailed results that include explanations and\nvisualizations. To address the lack of a standardized framework for evaluating\nthe capability of LLM agents to detect biases in data, we further propose a\ncomprehensive benchmark that includes multiple evaluation metrics and a large\nset of test cases. Extensive experiments demonstrate that our framework\nachieves exceptional overall performance in structured data bias detection,\nsetting a new milestone for fairer data applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in structured data is a complex and time-consuming task.\nExisting automated techniques are limited in diversity of data types and\nheavily reliant on human case-by-case handling, resulting in a lack of\ngeneralizability. Currently, large language model (LLM)-based agents have made\nsignificant progress in data science, but their ability to detect data biases\nis still insufficiently explored. To address this gap, we introduce the first\nend-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for\nautomatic bias detection in structured data based on specific user\nrequirements. It first develops a multi-stage plan to analyze user-specified\nbias detection tasks and then implements it with a diverse and well-suited set\nof tools. It delivers detailed results that include explanations and\nvisualizations. To address the lack of a standardized framework for evaluating\nthe capability of LLM agents to detect biases in data, we further propose a\ncomprehensive benchmark that includes multiple evaluation metrics and a large\nset of test cases. Extensive experiments demonstrate that our framework\nachieves exceptional overall performance in structured data bias detection,\nsetting a new milestone for fairer data applications."
                },
                "authors": [
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Mingyu Derek Ma"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jieyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieyu Zhao"
                },
                "author": "Jieyu Zhao",
                "arxiv_comment": "21 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18778v3",
                "updated": "2025-04-07T08:54:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    54,
                    28,
                    0,
                    97,
                    0
                ],
                "published": "2025-02-26T03:21:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    3,
                    21,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with\n  Competitive Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with\n  Competitive Performance"
                },
                "summary": "We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves\ncompetitive performance to GPT-4o. M2-omni employs a unified multimodal\nsequence modeling framework, which empowers Large Language Models(LLMs) to\nacquire comprehensive cross-modal understanding and generation capabilities.\nSpecifically, M2-omni can process arbitrary combinations of audio, video,\nimage, and text modalities as input, generating multimodal sequences\ninterleaving with audio, image, or text outputs, thereby enabling an advanced\nand interactive real-time experience. The training of such an omni-MLLM is\nchallenged by significant disparities in data quantity and convergence rates\nacross modalities. To address these challenges, we propose a step balance\nstrategy during pre-training to handle the quantity disparities in\nmodality-specific data. Additionally, a dynamically adaptive balance strategy\nis introduced during the instruction tuning stage to synchronize the\nmodality-wise training progress, ensuring optimal convergence. Notably, we\nprioritize preserving strong performance on pure text tasks to maintain the\nrobustness of M2-omni's language understanding capability throughout the\ntraining process. To our best knowledge, M2-omni is currently a very\ncompetitive open-source model to GPT-4o, characterized by its comprehensive\nmodality and task support, as well as its exceptional performance. We expect\nM2-omni will advance the development of omni-MLLMs, thus facilitating future\nresearch in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves\ncompetitive performance to GPT-4o. M2-omni employs a unified multimodal\nsequence modeling framework, which empowers Large Language Models(LLMs) to\nacquire comprehensive cross-modal understanding and generation capabilities.\nSpecifically, M2-omni can process arbitrary combinations of audio, video,\nimage, and text modalities as input, generating multimodal sequences\ninterleaving with audio, image, or text outputs, thereby enabling an advanced\nand interactive real-time experience. The training of such an omni-MLLM is\nchallenged by significant disparities in data quantity and convergence rates\nacross modalities. To address these challenges, we propose a step balance\nstrategy during pre-training to handle the quantity disparities in\nmodality-specific data. Additionally, a dynamically adaptive balance strategy\nis introduced during the instruction tuning stage to synchronize the\nmodality-wise training progress, ensuring optimal convergence. Notably, we\nprioritize preserving strong performance on pure text tasks to maintain the\nrobustness of M2-omni's language understanding capability throughout the\ntraining process. To our best knowledge, M2-omni is currently a very\ncompetitive open-source model to GPT-4o, characterized by its comprehensive\nmodality and task support, as well as its exceptional performance. We expect\nM2-omni will advance the development of omni-MLLMs, thus facilitating future\nresearch in this domain."
                },
                "authors": [
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Kaiyou Song"
                    },
                    {
                        "name": "Zipeng Feng"
                    },
                    {
                        "name": "Ziping Ma"
                    },
                    {
                        "name": "Qinglong Zhang"
                    },
                    {
                        "name": "Sirui Gao"
                    },
                    {
                        "name": "Xuzheng Yu"
                    },
                    {
                        "name": "Yunxiao Sun"
                    },
                    {
                        "name": "Tai-Wei Chang"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23708v2",
                "updated": "2025-04-07T08:26:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    26,
                    0,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-31T04:13:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    4,
                    13,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "Towards Benchmarking and Assessing the Safety and Robustness of\n  Autonomous Driving on Safety-critical Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Benchmarking and Assessing the Safety and Robustness of\n  Autonomous Driving on Safety-critical Scenarios"
                },
                "summary": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment."
                },
                "authors": [
                    {
                        "name": "Jingzheng Li"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Shikui Wei"
                    },
                    {
                        "name": "Zhijun Chen"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Xianqi Yang"
                    },
                    {
                        "name": "Yanjun Pu"
                    },
                    {
                        "name": "Jiakai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiakai Wang"
                },
                "author": "Jiakai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02052v2",
                "updated": "2025-04-07T08:25:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    25,
                    21,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-02T18:20:06Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    20,
                    6,
                    2,
                    92,
                    0
                ],
                "title": "From Prompts to Templates: A Systematic Prompt Template Analysis for\n  Real-world LLMapps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Prompts to Templates: A Systematic Prompt Template Analysis for\n  Real-world LLMapps"
                },
                "summary": "Large Language Models (LLMs) have revolutionized human-AI interaction by\nenabling intuitive task execution through natural language prompts. Despite\ntheir potential, designing effective prompts remains a significant challenge,\nas small variations in structure or wording can result in substantial\ndifferences in output. To address these challenges, LLM-powered applications\n(LLMapps) rely on prompt templates to simplify interactions, enhance usability,\nand support specialized tasks such as document analysis, creative content\ngeneration, and code synthesis. However, current practices heavily depend on\nindividual expertise and iterative trial-and-error processes, underscoring the\nneed for systematic methods to optimize prompt template design in LLMapps. This\npaper presents a comprehensive analysis of prompt templates in practical\nLLMapps. We construct a dataset of real-world templates from open-source\nLLMapps, including those from leading companies like Uber and Microsoft.\nThrough a combination of LLM-driven analysis and human review, we categorize\ntemplate components and placeholders, analyze their distributions, and identify\nfrequent co-occurrence patterns. Additionally, we evaluate the impact of\nidentified patterns on LLMs' instruction-following performance through sample\ntesting. Our findings provide practical insights on prompt template design for\ndevelopers, supporting the broader adoption and optimization of LLMapps in\nindustrial settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized human-AI interaction by\nenabling intuitive task execution through natural language prompts. Despite\ntheir potential, designing effective prompts remains a significant challenge,\nas small variations in structure or wording can result in substantial\ndifferences in output. To address these challenges, LLM-powered applications\n(LLMapps) rely on prompt templates to simplify interactions, enhance usability,\nand support specialized tasks such as document analysis, creative content\ngeneration, and code synthesis. However, current practices heavily depend on\nindividual expertise and iterative trial-and-error processes, underscoring the\nneed for systematic methods to optimize prompt template design in LLMapps. This\npaper presents a comprehensive analysis of prompt templates in practical\nLLMapps. We construct a dataset of real-world templates from open-source\nLLMapps, including those from leading companies like Uber and Microsoft.\nThrough a combination of LLM-driven analysis and human review, we categorize\ntemplate components and placeholders, analyze their distributions, and identify\nfrequent co-occurrence patterns. Additionally, we evaluate the impact of\nidentified patterns on LLMs' instruction-following performance through sample\ntesting. Our findings provide practical insights on prompt template design for\ndevelopers, supporting the broader adoption and optimization of LLMapps in\nindustrial settings."
                },
                "authors": [
                    {
                        "name": "Yuetian Mao"
                    },
                    {
                        "name": "Junjie He"
                    },
                    {
                        "name": "Chunyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chunyang Chen"
                },
                "author": "Chunyang Chen",
                "arxiv_comment": "Accepted at ACM International Conference on the Foundations of\n  Software Engineering (FSE 2025) Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04815v1",
                "updated": "2025-04-07T08:09:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    9,
                    46,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:09:46Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    9,
                    46,
                    0,
                    97,
                    0
                ],
                "title": "Beyond Answers: How LLMs Can Pursue Strategic Thinking in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Answers: How LLMs Can Pursue Strategic Thinking in Education"
                },
                "summary": "Artificial Intelligence (AI) holds transformative potential in education,\nenabling personalized learning, enhancing inclusivity, and encouraging\ncreativity and curiosity. In this paper, we explore how Large Language Models\n(LLMs) can act as both patient tutors and collaborative partners to enhance\neducation delivery. As tutors, LLMs personalize learning by offering\nstep-by-step explanations and addressing individual needs, making education\nmore inclusive for students with diverse backgrounds or abilities. As\ncollaborators, they expand students' horizons, supporting them in tackling\ncomplex, real-world problems and co-creating innovative projects. However, to\nfully realize these benefits, LLMs must be leveraged not as tools for providing\ndirect solutions but rather to guide students in developing resolving\nstrategies and finding learning paths together. Therefore, a strong emphasis\nshould be placed on educating students and teachers on the successful use of\nLLMs to ensure their effective integration into classrooms. Through practical\nexamples and real-world case studies, this paper illustrates how LLMs can make\neducation more inclusive and engaging while empowering students to reach their\nfull potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) holds transformative potential in education,\nenabling personalized learning, enhancing inclusivity, and encouraging\ncreativity and curiosity. In this paper, we explore how Large Language Models\n(LLMs) can act as both patient tutors and collaborative partners to enhance\neducation delivery. As tutors, LLMs personalize learning by offering\nstep-by-step explanations and addressing individual needs, making education\nmore inclusive for students with diverse backgrounds or abilities. As\ncollaborators, they expand students' horizons, supporting them in tackling\ncomplex, real-world problems and co-creating innovative projects. However, to\nfully realize these benefits, LLMs must be leveraged not as tools for providing\ndirect solutions but rather to guide students in developing resolving\nstrategies and finding learning paths together. Therefore, a strong emphasis\nshould be placed on educating students and teachers on the successful use of\nLLMs to ensure their effective integration into classrooms. Through practical\nexamples and real-world case studies, this paper illustrates how LLMs can make\neducation more inclusive and engaging while empowering students to reach their\nfull potential."
                },
                "authors": [
                    {
                        "name": "Eleonora Grassucci"
                    },
                    {
                        "name": "Gualtiero Grassucci"
                    },
                    {
                        "name": "Aurelio Uncini"
                    },
                    {
                        "name": "Danilo Comminiello"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Comminiello"
                },
                "author": "Danilo Comminiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01444v2",
                "updated": "2025-04-07T08:05:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    5,
                    25,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-02T07:54:32Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    54,
                    32,
                    2,
                    92,
                    0
                ],
                "title": "PiCo: Jailbreaking Multimodal Large Language Models via\n  $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCo: Jailbreaking Multimodal Large Language Models via\n  $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization"
                },
                "summary": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs), which integrate vision and other\nmodalities into Large Language Models (LLMs), significantly enhance AI\ncapabilities but also introduce new security vulnerabilities. By exploiting the\nvulnerabilities of the visual modality and the long-tail distribution\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\nframework designed to progressively bypass multi-tiered defense mechanisms in\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\ntoken-level typographic attacks to evade input filtering and embedding harmful\nintent within programming context instructions to bypass runtime monitoring. To\ncomprehensively assess the impact of attacks, a new evaluation metric is\nfurther proposed to assess both the toxicity and helpfulness of model outputs\npost-attack. By embedding harmful intent within code-style visual instructions,\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\nhighlight the critical gaps in current defenses, underscoring the need for more\nrobust strategies to secure advanced MLLMs."
                },
                "authors": [
                    {
                        "name": "Aofan Liu"
                    },
                    {
                        "name": "Lulu Tang"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Yuguo Yin"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Ao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ao Yang"
                },
                "author": "Ao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04809v1",
                "updated": "2025-04-07T08:04:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    4,
                    23,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:04:23Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    4,
                    23,
                    0,
                    97,
                    0
                ],
                "title": "Select Me! When You Need a Tool: A Black-box Text Attack on Tool\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select Me! When You Need a Tool: A Black-box Text Attack on Tool\n  Selection"
                },
                "summary": "Tool learning serves as a powerful auxiliary mechanism that extends the\ncapabilities of large language models (LLMs), enabling them to tackle complex\ntasks requiring real-time relevance or high precision operations. Behind its\npowerful capabilities lie some potential security issues. However, previous\nwork has primarily focused on how to make the output of the invoked tools\nincorrect or malicious, with little attention given to the manipulation of tool\nselection. To fill this gap, we introduce, for the first time, a black-box\ntext-based attack that can significantly increase the probability of the target\ntool being selected in this paper. We propose a two-level text perturbation\nattack witha coarse-to-fine granularity, attacking the text at both the word\nlevel and the character level. We conduct comprehensive experiments that\ndemonstrate the attacker only needs to make some perturbations to the tool's\ntextual information to significantly increase the possibility of the target\ntool being selected and ranked higher among the candidate tools. Our research\nreveals the vulnerability of the tool selection process and paves the way for\nfuture research on protecting this process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool learning serves as a powerful auxiliary mechanism that extends the\ncapabilities of large language models (LLMs), enabling them to tackle complex\ntasks requiring real-time relevance or high precision operations. Behind its\npowerful capabilities lie some potential security issues. However, previous\nwork has primarily focused on how to make the output of the invoked tools\nincorrect or malicious, with little attention given to the manipulation of tool\nselection. To fill this gap, we introduce, for the first time, a black-box\ntext-based attack that can significantly increase the probability of the target\ntool being selected in this paper. We propose a two-level text perturbation\nattack witha coarse-to-fine granularity, attacking the text at both the word\nlevel and the character level. We conduct comprehensive experiments that\ndemonstrate the attacker only needs to make some perturbations to the tool's\ntextual information to significantly increase the possibility of the target\ntool being selected and ranked higher among the candidate tools. Our research\nreveals the vulnerability of the tool selection process and paves the way for\nfuture research on protecting this process."
                },
                "authors": [
                    {
                        "name": "Liuji Chen"
                    },
                    {
                        "name": "Hao Gao"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04808v1",
                "updated": "2025-04-07T08:03:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    3,
                    36,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:03:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    3,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT\n  Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT\n  Pipelines"
                },
                "summary": "Practitioners are increasingly turning to Extract-Load-Transform (ELT)\npipelines with the widespread adoption of cloud data warehouses. However,\ndesigning these pipelines often involves significant manual work to ensure\ncorrectness. Recent advances in AI-based methods, which have shown strong\ncapabilities in data tasks, such as text-to-SQL, present an opportunity to\nalleviate manual efforts in developing ELT pipelines. Unfortunately, current\nbenchmarks in data engineering only evaluate isolated tasks, such as using data\ntools and writing data transformation queries, leaving a significant gap in\nevaluating AI agents for generating end-to-end ELT pipelines.\n  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to\nassess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists\nof 100 pipelines, including 835 source tables and 203 data models across\nvarious domains. By simulating realistic scenarios involving the integration of\ndiverse data sources and the use of popular data tools, ELT-Bench evaluates AI\nagents' abilities in handling complex data engineering workflows. AI agents\nmust interact with databases and data tools, write code and SQL queries, and\norchestrate every pipeline stage. We evaluate two representative code agent\nframeworks, Spider-Agent and SWE-Agent, using six popular Large Language Models\n(LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent\nClaude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data\nmodels, with an average cost of $4.30 and 89.3 steps per pipeline. Our\nexperimental results demonstrate the challenges of ELT-Bench and highlight the\nneed for a more advanced AI agent to reduce manual effort in ELT workflows. Our\ncode and data are available at https://github.com/uiuc-kang-lab/ETL.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners are increasingly turning to Extract-Load-Transform (ELT)\npipelines with the widespread adoption of cloud data warehouses. However,\ndesigning these pipelines often involves significant manual work to ensure\ncorrectness. Recent advances in AI-based methods, which have shown strong\ncapabilities in data tasks, such as text-to-SQL, present an opportunity to\nalleviate manual efforts in developing ELT pipelines. Unfortunately, current\nbenchmarks in data engineering only evaluate isolated tasks, such as using data\ntools and writing data transformation queries, leaving a significant gap in\nevaluating AI agents for generating end-to-end ELT pipelines.\n  To fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to\nassess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists\nof 100 pipelines, including 835 source tables and 203 data models across\nvarious domains. By simulating realistic scenarios involving the integration of\ndiverse data sources and the use of popular data tools, ELT-Bench evaluates AI\nagents' abilities in handling complex data engineering workflows. AI agents\nmust interact with databases and data tools, write code and SQL queries, and\norchestrate every pipeline stage. We evaluate two representative code agent\nframeworks, Spider-Agent and SWE-Agent, using six popular Large Language Models\n(LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent\nClaude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data\nmodels, with an average cost of $4.30 and 89.3 steps per pipeline. Our\nexperimental results demonstrate the challenges of ELT-Bench and highlight the\nneed for a more advanced AI agent to reduce manual effort in ELT workflows. Our\ncode and data are available at https://github.com/uiuc-kang-lab/ETL.git."
                },
                "authors": [
                    {
                        "name": "Tengjun Jin"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "14 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14191v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14191v4",
                "updated": "2025-04-07T07:52:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    52,
                    28,
                    0,
                    97,
                    0
                ],
                "published": "2024-05-23T05:34:31Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    5,
                    34,
                    31,
                    3,
                    144,
                    0
                ],
                "title": "S-Eval: Towards Automated and Comprehensive Safety Evaluation for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S-Eval: Towards Automated and Comprehensive Safety Evaluation for Large\n  Language Models"
                },
                "summary": "Generative large language models (LLMs) have revolutionized natural language\nprocessing with their transformative and emergent capabilities. However, recent\nevidence indicates that LLMs can produce harmful content that violates social\nnorms, raising significant concerns regarding the safety and ethical\nramifications of deploying these advanced models. Thus, it is both critical and\nimperative to perform a rigorous and comprehensive safety evaluation of LLMs\nbefore deployment. Despite this need, owing to the extensiveness of LLM\ngeneration space, it still lacks a unified and standardized risk taxonomy to\nsystematically reflect the LLM content safety, as well as automated safety\nassessment techniques to explore the potential risk efficiently.\n  To bridge the striking gap, we propose S-Eval, a novel LLM-based automated\nSafety Evaluation framework with a newly defined comprehensive risk taxonomy.\nS-Eval incorporates two key components, i.e., an expert testing LLM ${M}_t$ and\na novel safety critique LLM ${M}_c$. ${M}_t$ is responsible for automatically\ngenerating test cases in accordance with the proposed risk taxonomy. ${M}_c$\ncan provide quantitative and explainable safety evaluations for better risk\nawareness of LLMs. In contrast to prior works, S-Eval is efficient and\neffective in test generation and safety evaluation. Moreover, S-Eval can be\nflexibly configured and adapted to the rapid evolution of LLMs and accompanying\nnew safety threats, test generation methods and safety critique methods thanks\nto the LLM-based architecture. S-Eval has been deployed in our industrial\npartner for the automated safety evaluation of multiple LLMs serving millions\nof users, demonstrating its effectiveness in real-world scenarios. Our\nbenchmark is publicly available at https://github.com/IS2Lab/S-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have revolutionized natural language\nprocessing with their transformative and emergent capabilities. However, recent\nevidence indicates that LLMs can produce harmful content that violates social\nnorms, raising significant concerns regarding the safety and ethical\nramifications of deploying these advanced models. Thus, it is both critical and\nimperative to perform a rigorous and comprehensive safety evaluation of LLMs\nbefore deployment. Despite this need, owing to the extensiveness of LLM\ngeneration space, it still lacks a unified and standardized risk taxonomy to\nsystematically reflect the LLM content safety, as well as automated safety\nassessment techniques to explore the potential risk efficiently.\n  To bridge the striking gap, we propose S-Eval, a novel LLM-based automated\nSafety Evaluation framework with a newly defined comprehensive risk taxonomy.\nS-Eval incorporates two key components, i.e., an expert testing LLM ${M}_t$ and\na novel safety critique LLM ${M}_c$. ${M}_t$ is responsible for automatically\ngenerating test cases in accordance with the proposed risk taxonomy. ${M}_c$\ncan provide quantitative and explainable safety evaluations for better risk\nawareness of LLMs. In contrast to prior works, S-Eval is efficient and\neffective in test generation and safety evaluation. Moreover, S-Eval can be\nflexibly configured and adapted to the rapid evolution of LLMs and accompanying\nnew safety threats, test generation methods and safety critique methods thanks\nto the LLM-based architecture. S-Eval has been deployed in our industrial\npartner for the automated safety evaluation of multiple LLMs serving millions\nof users, demonstrating its effectiveness in real-world scenarios. Our\nbenchmark is publicly available at https://github.com/IS2Lab/S-Eval."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yuan"
                    },
                    {
                        "name": "Jinfeng Li"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Jialuo Chen"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Xiaoxia Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Jingyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyi Wang"
                },
                "author": "Jingyi Wang",
                "arxiv_comment": "Accepted by ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14191v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14191v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04197v4",
                "updated": "2025-04-07T07:46:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    46,
                    51,
                    0,
                    97,
                    0
                ],
                "published": "2024-03-07T03:58:28Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    3,
                    58,
                    28,
                    3,
                    67,
                    0
                ],
                "title": "Large Language Models are In-Context Molecule Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are In-Context Molecule Learners"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve similar informative context examples. Additionally, Post-retrieval\nRe-ranking is composed of Sequence Reversal and Random Walk selection to\nfurther improve the quality of retrieval results. Finally, In-Context Molecule\nTuning unlocks the in-context learning and reasoning capability of LLMs with\nthe retrieved examples and adapts the parameters of LLMs for better alignment\nbetween molecules and texts. Experimental results demonstrate that ICMA can\nempower LLMs to achieve state-of-the-art or comparable performance without\nextra training corpora and intricate structures, showing that LLMs are\ninherently in-context molecule learners."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhihao Ding"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Accepted by IEEE TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02263v2",
                "updated": "2025-04-07T07:42:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    42,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T04:20:44Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    20,
                    44,
                    3,
                    93,
                    0
                ],
                "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism"
                },
                "summary": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Ruidong Zhu"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Cesar A. Stuardo"
                    },
                    {
                        "name": "Dongyang Wang"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Huaping Zhou"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Jianzhe Xiao"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Lingjun Liu"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04785v1",
                "updated": "2025-04-07T07:27:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    27,
                    31,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T07:27:31Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    27,
                    31,
                    0,
                    97,
                    0
                ],
                "title": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors"
                },
                "summary": "Efficiently leveraging of the capabilities of contemporary large language\nmodels (LLMs) is increasingly challenging, particularly when direct fine-tuning\nis expensive and often impractical. Existing training-free methods, including\nmanually or automated designed workflows, typically demand substantial human\neffort or yield suboptimal results. This paper proposes Weak-for-Strong\nHarnessing (W4S), a novel framework that customizes smaller, cost-efficient\nlanguage models to design and optimize workflows for harnessing stronger\nmodels. W4S formulates workflow design as a multi-turn markov decision process\nand introduces reinforcement learning for agentic workflow optimization (RLAO)\nto train a weak meta-agent. Through iterative interaction with the environment,\nthe meta-agent learns to design increasingly effective workflows without manual\nintervention. Empirical results demonstrate the superiority of W4S that our 7B\nmeta-agent, trained with just one GPU hour, outperforms the strongest baseline\nby 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the\nperformance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o.\nNotably, W4S exhibits strong generalization capabilities across both seen and\nunseen tasks, offering an efficient, high-performing alternative to directly\nfine-tuning strong models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently leveraging of the capabilities of contemporary large language\nmodels (LLMs) is increasingly challenging, particularly when direct fine-tuning\nis expensive and often impractical. Existing training-free methods, including\nmanually or automated designed workflows, typically demand substantial human\neffort or yield suboptimal results. This paper proposes Weak-for-Strong\nHarnessing (W4S), a novel framework that customizes smaller, cost-efficient\nlanguage models to design and optimize workflows for harnessing stronger\nmodels. W4S formulates workflow design as a multi-turn markov decision process\nand introduces reinforcement learning for agentic workflow optimization (RLAO)\nto train a weak meta-agent. Through iterative interaction with the environment,\nthe meta-agent learns to design increasingly effective workflows without manual\nintervention. Empirical results demonstrate the superiority of W4S that our 7B\nmeta-agent, trained with just one GPU hour, outperforms the strongest baseline\nby 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the\nperformance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o.\nNotably, W4S exhibits strong generalization capabilities across both seen and\nunseen tasks, offering an efficient, high-performing alternative to directly\nfine-tuning strong models."
                },
                "authors": [
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Lan Feng"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17003v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17003v5",
                "updated": "2025-04-07T07:23:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    23,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2024-08-30T04:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Layers in Aligned Large Language Models: The Key to LLM Security"
                },
                "summary": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "arxiv_comment": "Accepted by ICLR 2025. The code is available at\n  https://github.com/listen0425/Safety-Layers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17003v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17003v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07202v2",
                "updated": "2025-04-07T07:22:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    22,
                    25,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-10T11:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    38,
                    21,
                    0,
                    69,
                    0
                ],
                "title": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-shot Learning Method Based on Large Language Models for\n  Multi-modal Knowledge Graph Embedding"
                },
                "summary": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer.Current applications often fail to accurately infer and handle new\nrelations orentities involving unseen categories, severely limiting their\nscalability and prac-ticality in open-domain scenarios. ZL learning faces the\nchallenge of effectivelytransferring semantic information of unseen categories\nin multi-modal knowledgegraph (MMKG) embedding representation learning. In this\npaper, we proposeZSLLM, a framework for zero-shot embedding learning of MMKGs\nusing largelanguage models (LLMs). We leverage textual modality information of\nunseencategories as prompts to fully utilize the reasoning capabilities of\nLLMs, enablingsemantic information transfer across different modalities for\nunseen categories.Through model-based learning, the embedding representation of\nunseen cate-gories in MMKG is enhanced. Extensive experiments conducted on\nmultiplereal-world datasets demonstrate the superiority of our approach\ncompared tostate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot learning (ZL) is crucial for tasks involving unseen categories,\nsuch as natural language processing, image classification, and cross-lingual\ntransfer.Current applications often fail to accurately infer and handle new\nrelations orentities involving unseen categories, severely limiting their\nscalability and prac-ticality in open-domain scenarios. ZL learning faces the\nchallenge of effectivelytransferring semantic information of unseen categories\nin multi-modal knowledgegraph (MMKG) embedding representation learning. In this\npaper, we proposeZSLLM, a framework for zero-shot embedding learning of MMKGs\nusing largelanguage models (LLMs). We leverage textual modality information of\nunseencategories as prompts to fully utilize the reasoning capabilities of\nLLMs, enablingsemantic information transfer across different modalities for\nunseen categories.Through model-based learning, the embedding representation of\nunseen cate-gories in MMKG is enhanced. Extensive experiments conducted on\nmultiplereal-world datasets demonstrate the superiority of our approach\ncompared tostate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Jingchen Li"
                    },
                    {
                        "name": "Yuanyuan Fang"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v3",
                "updated": "2025-04-07T07:22:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    22,
                    6,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18445v2",
                "updated": "2025-04-07T07:00:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    7,
                    0,
                    7,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-24T08:46:52Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    8,
                    46,
                    52,
                    0,
                    83,
                    0
                ],
                "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures:\n  Missing and Noisy Modality Robustness"
                },
                "summary": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
                },
                "authors": [
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Kaiyu Lei"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Junha Moon"
                    },
                    {
                        "name": "Zhixiong Wang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "This paper has been accepted by CVPR 2025 Workshop: TMM-OpenWorld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16559v2",
                "updated": "2025-04-07T06:57:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    57,
                    6,
                    0,
                    97,
                    0
                ],
                "published": "2024-09-25T02:16:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    16,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "Demystifying Issues, Causes and Solutions in LLM Open-Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Issues, Causes and Solutions in LLM Open-Source Projects"
                },
                "summary": "With the advancements of Large Language Models (LLMs), an increasing number\nof open-source software projects are using LLMs as their core functional\ncomponent. Although research and practice on LLMs are capturing considerable\ninterest, no dedicated studies explored the challenges faced by practitioners\nof LLM open-source projects, the causes of these challenges, and potential\nsolutions. To fill this research gap, we conducted an empirical study to\nunderstand the issues that practitioners encounter when developing and using\nLLM open-source software, the possible causes of these issues, and potential\nsolutions. We collected all closed issues from 15 LLM open-source projects and\nlabelled issues that met our requirements. We then randomly selected 994 issues\nfrom the labelled issues as the sample for data extraction and analysis to\nunderstand the prevalent issues, their underlying causes, and potential\nsolutions. Our study results show that (1) Model Issue is the most common issue\nfaced by practitioners, (2) Model Problem, Configuration and Connection\nProblem, and Feature and Method Problem are identified as the most frequent\ncauses of the issues, and (3) Optimize Model is the predominant solution to the\nissues. Based on the study results, we provide implications for practitioners\nand researchers of LLM open-source projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements of Large Language Models (LLMs), an increasing number\nof open-source software projects are using LLMs as their core functional\ncomponent. Although research and practice on LLMs are capturing considerable\ninterest, no dedicated studies explored the challenges faced by practitioners\nof LLM open-source projects, the causes of these challenges, and potential\nsolutions. To fill this research gap, we conducted an empirical study to\nunderstand the issues that practitioners encounter when developing and using\nLLM open-source software, the possible causes of these issues, and potential\nsolutions. We collected all closed issues from 15 LLM open-source projects and\nlabelled issues that met our requirements. We then randomly selected 994 issues\nfrom the labelled issues as the sample for data extraction and analysis to\nunderstand the prevalent issues, their underlying causes, and potential\nsolutions. Our study results show that (1) Model Issue is the most common issue\nfaced by practitioners, (2) Model Problem, Configuration and Connection\nProblem, and Feature and Method Problem are identified as the most frequent\ncauses of the issues, and (3) Optimize Model is the predominant solution to the\nissues. Based on the study results, we provide implications for practitioners\nand researchers of LLM open-source projects."
                },
                "authors": [
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    }
                ],
                "author_detail": {
                    "name": "Mojtaba Shahin"
                },
                "author": "Mojtaba Shahin",
                "arxiv_comment": "Preprint accepted for publication in Journal of Systems and Software,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04771v1",
                "updated": "2025-04-07T06:55:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    55,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T06:55:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    55,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "Improving Multilingual Retrieval-Augmented Language Models through\n  Dialectic Reasoning Argumentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Multilingual Retrieval-Augmented Language Models through\n  Dialectic Reasoning Argumentations"
                },
                "summary": "Retrieval-augmented generation (RAG) is key to enhancing large language\nmodels (LLMs) to systematically access richer factual knowledge. Yet, using RAG\nbrings intrinsic challenges, as LLMs must deal with potentially conflicting\nknowledge, especially in multilingual retrieval, where the heterogeneity of\nknowledge retrieved may deliver different outlooks. To make RAG more\nanalytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular\napproach guided by Argumentative Explanations, i.e., structured reasoning\nprocess that systematically evaluates retrieved\n  information by comparing, contrasting, and resolving conflicting\nperspectives. Given a query and a set of multilingual related documents, DRAG\nselects and exemplifies relevant knowledge for delivering dialectic\nexplanations that, by critically weighing opposing arguments and filtering\nextraneous content, clearly determine the final response. Through a series of\nin-depth experiments, we show the impact of our framework both as an in-context\nlearning strategy and for constructing demonstrations to instruct smaller\nmodels. The final results demonstrate that DRAG significantly improves RAG\napproaches, requiring low-impact computational effort and providing robustness\nto knowledge perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is key to enhancing large language\nmodels (LLMs) to systematically access richer factual knowledge. Yet, using RAG\nbrings intrinsic challenges, as LLMs must deal with potentially conflicting\nknowledge, especially in multilingual retrieval, where the heterogeneity of\nknowledge retrieved may deliver different outlooks. To make RAG more\nanalytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular\napproach guided by Argumentative Explanations, i.e., structured reasoning\nprocess that systematically evaluates retrieved\n  information by comparing, contrasting, and resolving conflicting\nperspectives. Given a query and a set of multilingual related documents, DRAG\nselects and exemplifies relevant knowledge for delivering dialectic\nexplanations that, by critically weighing opposing arguments and filtering\nextraneous content, clearly determine the final response. Through a series of\nin-depth experiments, we show the impact of our framework both as an in-context\nlearning strategy and for constructing demonstrations to instruct smaller\nmodels. The final results demonstrate that DRAG significantly improves RAG\napproaches, requiring low-impact computational effort and providing robustness\nto knowledge perturbations."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Federico Ranaldi"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00695v2",
                "updated": "2025-04-07T06:33:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    33,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-01T12:06:42Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    6,
                    42,
                    1,
                    91,
                    0
                ],
                "title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data\n  Selection"
                },
                "summary": "Pre-training large language models (LLMs) necessitates enormous diverse\ntextual corpora, making effective data selection a key challenge for balancing\ncomputational resources and model performance. Current methodologies primarily\nemphasize data quality metrics and mixing proportions, yet they fail to\nadequately capture the underlying semantic connections between training samples\nand quality disparities within individual domains. We introduce ToReMi\n(Topic-based Reweighting for Model improvement), a novel two-stage framework\nthat dynamically adjusts training sample weights according to their topical\nassociations and observed learning patterns. Our comprehensive experiments\nreveal that ToReMi variants consistently achieve superior performance over\nconventional pre-training approaches, demonstrating accelerated perplexity\nreduction across multiple domains and enhanced capabilities on downstream\nevaluation tasks. Code is available at https://github.com/zxx000728/ToReMi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training large language models (LLMs) necessitates enormous diverse\ntextual corpora, making effective data selection a key challenge for balancing\ncomputational resources and model performance. Current methodologies primarily\nemphasize data quality metrics and mixing proportions, yet they fail to\nadequately capture the underlying semantic connections between training samples\nand quality disparities within individual domains. We introduce ToReMi\n(Topic-based Reweighting for Model improvement), a novel two-stage framework\nthat dynamically adjusts training sample weights according to their topical\nassociations and observed learning patterns. Our comprehensive experiments\nreveal that ToReMi variants consistently achieve superior performance over\nconventional pre-training approaches, demonstrating accelerated perplexity\nreduction across multiple domains and enhanced capabilities on downstream\nevaluation tasks. Code is available at https://github.com/zxx000728/ToReMi."
                },
                "authors": [
                    {
                        "name": "Xiaoxuan Zhu"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Baiqian Wu"
                    },
                    {
                        "name": "Suhang Zheng"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13823v2",
                "updated": "2025-04-07T06:29:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    29,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-12-18T13:11:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    11,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "arxiv_comment": "Accepted at CVPR 2025 ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19774v2",
                "updated": "2025-04-07T06:11:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    11,
                    54,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-28T09:23:40Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    9,
                    23,
                    40,
                    4,
                    180,
                    0
                ],
                "title": "Direct Preference Knowledge Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Knowledge Distillation for Large Language Models"
                },
                "summary": "In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd."
                },
                "authors": [
                    {
                        "name": "Yixing Li"
                    },
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Dequan Wang"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04758v1",
                "updated": "2025-04-07T06:11:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    11,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T06:11:39Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    11,
                    39,
                    0,
                    97,
                    0
                ],
                "title": "Feature Importance-Aware Deep Joint Source-Channel Coding for\n  Computationally Efficient and Adjustable Image Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Importance-Aware Deep Joint Source-Channel Coding for\n  Computationally Efficient and Adjustable Image Transmission"
                },
                "summary": "Recent advancements in deep learning-based joint source-channel coding\n(deepJSCC) have significantly improved communication performance, but their\nhigh computational demands restrict practical deployment. Furthermore, some\napplications require the adaptive adjustment of computational complexity. To\naddress these challenges, we propose a computationally efficient and adjustable\ndeepJSCC model for image transmission, which we call feature importance-aware\ndeepJSCC (FAJSCC). Unlike existing deepJSCC models that equally process all\nneural features of images, FAJSCC first classifies features into important and\nless important features and then processes them differently. Specifically,\ncomputationally-intensive self-attention is applied to the important features\nand computationally-efficient spatial attention to the less important ones. The\nfeature classification is based on the available computational budget and\nimportance scores predicted by an importance predictor, which estimates each\nfeature's contribution to performance. It also allows independent adjustment of\nencoder and decoder complexity within a single trained model. With these\nproperties, our FAJSCC is the first deepJSCC that is computationally efficient\nand adjustable while maintaining high performance. Experiments demonstrate that\nour FAJSCC achieves higher image transmission performance across various\nchannel conditions while using less computational complexity than the recent\nstate-of-the-art models. Adding to this, by separately varying the\ncomputational resources of the encoder and decoder, it is concluded that the\ndecoder's error correction function requires the largest computational\ncomplexity in FAJSCC, which is the first observation in deepJSCC literature.\nThe FAJSCC code is publicly available at\nhttps://github.com/hansung-choi/FAJSCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in deep learning-based joint source-channel coding\n(deepJSCC) have significantly improved communication performance, but their\nhigh computational demands restrict practical deployment. Furthermore, some\napplications require the adaptive adjustment of computational complexity. To\naddress these challenges, we propose a computationally efficient and adjustable\ndeepJSCC model for image transmission, which we call feature importance-aware\ndeepJSCC (FAJSCC). Unlike existing deepJSCC models that equally process all\nneural features of images, FAJSCC first classifies features into important and\nless important features and then processes them differently. Specifically,\ncomputationally-intensive self-attention is applied to the important features\nand computationally-efficient spatial attention to the less important ones. The\nfeature classification is based on the available computational budget and\nimportance scores predicted by an importance predictor, which estimates each\nfeature's contribution to performance. It also allows independent adjustment of\nencoder and decoder complexity within a single trained model. With these\nproperties, our FAJSCC is the first deepJSCC that is computationally efficient\nand adjustable while maintaining high performance. Experiments demonstrate that\nour FAJSCC achieves higher image transmission performance across various\nchannel conditions while using less computational complexity than the recent\nstate-of-the-art models. Adding to this, by separately varying the\ncomputational resources of the encoder and decoder, it is concluded that the\ndecoder's error correction function requires the largest computational\ncomplexity in FAJSCC, which is the first observation in deepJSCC literature.\nThe FAJSCC code is publicly available at\nhttps://github.com/hansung-choi/FAJSCC."
                },
                "authors": [
                    {
                        "name": "Hansung Choi"
                    },
                    {
                        "name": "Daewon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Daewon Seo"
                },
                "author": "Daewon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04747v1",
                "updated": "2025-04-07T05:41:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    41,
                    35,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T05:41:35Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    41,
                    35,
                    0,
                    97,
                    0
                ],
                "title": "Two is Better than One: Efficient Ensemble Defense for Robust and\n  Compact Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two is Better than One: Efficient Ensemble Defense for Robust and\n  Compact Models"
                },
                "summary": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yoojin Jung"
                    },
                    {
                        "name": "Byung Cheol Song"
                    }
                ],
                "author_detail": {
                    "name": "Byung Cheol Song"
                },
                "author": "Byung Cheol Song",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04745v1",
                "updated": "2025-04-07T05:38:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    38,
                    40,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T05:38:40Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    5,
                    38,
                    40,
                    0,
                    97,
                    0
                ],
                "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A\n  Case Study with AMRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A\n  Case Study with AMRs"
                },
                "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\nmore evident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\nthe best-case scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\nmore evident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\nthe best-case scenario."
                },
                "authors": [
                    {
                        "name": "Ankush Raut"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Maria Leonor Pacheco"
                    }
                ],
                "author_detail": {
                    "name": "Maria Leonor Pacheco"
                },
                "author": "Maria Leonor Pacheco",
                "arxiv_comment": "13 pages, 23 figures. Submitted to XLLM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]